---
title: "Resolvendo Captchas com o {luz}"
date: "2021-12-27"
tags: ["torch", "luz"]
categories: ["Tutoriais"]
image: "images/posts/banner/captcha-luz.png"
author: ["Julio"]
summary: "Resolvendo Captchas usando o pacote {luz}, com detalhes sobre datasets, dataloaders e m√©tricas customizadas."
draft: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE
)
```

Sabe aquelas imagens que aparecem quando voc√™ est√° preenchendo um formul√°rio ou quer acessar uma p√°gina espec√≠fica, pedindo para voc√™ decifrar o texto? Isso √© o que chamamos de CAPTCHA (**Completely Automated Public Turing test to tell Computers and Humans Apart**).


<div style="width:70%;margin:auto;">
![](/images/posts/conteudo/captcha-luz/20170412233525_2n73k7.jpeg)
</div>

<center>Figura: Exemplo de captcha do site do Tribunal Regional do Trabalho (TRT).</center>


O Captcha √© um desafio computacional criado para ser dif√≠cil de resolver por m√°quinas mas f√°cil de resolver por pessoas. No in√≠cio, a maioria dos Captchas eram formados por letras aleat√≥rias inseridas em imagens ou √°udios com algum ru√≠do. Com a populariza√ß√£o das t√©cnicas de Deep Learning, esses desafios se tornaram f√°ceis de resolver por m√°quinas, desde que exista uma base suficientemente grande de exemplos classificados.

Em posts anteriores aqui do blog, eu mostrei como resolvemos Captchas na m√£o ou usando redes neurais convolucionais. Todos esses posts usavam o `{keras}` como backend, que era a tecnologia de deep learning que eu conhecia na √©poca.

> Observa√ß√£o: alguns dos posts anteriores de Captchas n√£o est√£o vis√≠veis. Algum dia vou reviv√™-los!

Neste post, vou mostrar como resolver um Captcha de texto em imagem usando o pacote `{luz}`. O pacote surgiu para facilitar a vida de quem constr√≥i modelos de Deep Learning usando o `{torch}`.

## O que √© {torch}? O que √© {luz}?

O `{torch}` √© um pacote do R criado pelo nosso s√≥cio [Daniel Falbel](https://github.com/dfalbel), que tamb√©m trabalha na RStudio. O pacote √© uma adapta√ß√£o do `pytorch`, biblioteca de python muito popular para fazer contas de √°lgebra matricial e diferencia√ß√£o autom√°tica utilizando GPUs, que s√£o os principais ingredientes utilizados na constru√ß√£o e ajuste de modelos de Deep Learning. A empresa por tr√°s do projeto Torch √© o Facebook (Meta?).

<div style="max-width:200px;margin:auto;">

![](https://torch.mlverse.org/css/images/hex/torch.png)
</div>

A dificuldade do pacote `{torch}` para pessoas que est√£o come√ßando √© a complexidade da sintaxe. A ferramenta √© super flex√≠vel, permitindo o ajuste de modelos customizados, mas a constru√ß√£o desses modelos est√° longe de ser trivial. Por exemplo, √© comum precisar implementar os passos de atualiza√ß√£o dos par√¢metros manualmente a partir de algumas fun√ß√µes b√°sicas do pacote.

O pacote `{luz}` -- sim, o nome √© uma brincadeira com a luz que sai de uma tocha... ü•Å -- vem com a ideia de facilitar um pouco esse processo. Para conseguir rodar um modelo de deep learning usando o luz, precisamos de apenas i) um dataset/dataloader constru√≠do para gerar amostras das bases de dados de treino/valida√ß√£o e ii) um modelo, que deve ser um m√≥dulo do torch. 

<div style="max-width:200px;margin:auto;">

![](https://torch.mlverse.org/css/images/hex/luz.png)
</div>

Vamos investigar os elementos do luz a seguir. Como exemplo, utilizaremos os nossos queridos Captchas, para n√£o ficar na mesmisse do MNIST üòâ

O exemplo da vez ser√° o TRT, pois a base √© pequenininha e √© um Captcha relativamente f√°cil de resolver.

```{r include=FALSE}
knitr::include_graphics("../../static/images/posts/conteudo/captcha-luz/20170412233525_2n73k7.jpeg")
```

<div style="min-width:200px;margin:auto;">

![](/images/posts/conteudo/captcha-luz/20170412233525_2n73k7.jpeg)
</div>

## Dados, datasets e dataloaders

Uma caracter√≠stica importante de modelos de deep learning √© que as bases de dados costumam ser bem grandes. Por isso, foram desenvolvidas t√©cnicas que atualizam os par√¢metros do modelo com base apenas em uma amostra dos dados, sem a necessidade de carregar todas as observa√ß√µes na mem√≥ria do computador. Essa estrat√©gia parece um pouco estranha de in√≠cio, mas existem muitos estudos que demonstram que elas n√£o s√≥ funcionam como tamb√©m t√™m chances de trazer propriedades de regulariza√ß√£o ao modelo.

Nesse contexto surgem os `dataset()`s e `dataloader()`s do torch. O dataset √© um objeto respons√°vel por informar como uma base de dados deve ser acessada (por exemplo, lendo uma imagem, um arquivo csv ou um √°udio), e de que forma podemos obter um item dessa base. O dataset √© um objeto de classe R6, que √© uma esp√©cie de orienta√ß√£o a objetos do R desenvolvida pela RStudio.

Abaixo, temos o esqueleto de um dataset do torch:

```{r dataset, eval=FALSE}
# isso cria um dataset generator
ds_generator <- torch::dataset(
  name = "meu nome!",
  objeto_arbitrario = "voc√™ pode criar objetos arbitr√°rios aqui",
  
  # isso √© rodado quando voc√™ cria o dataset(), com par√¢metros
  initialize = function(parametro1, parametro2, repeticoes) {
    # c√≥digo arbitr√°rio
    dados <- matrix(
      c(parametro1, parametro2),
      ncol = 2, nrow = repeticoes, byrow = TRUE
    )
    ## registra os dados para ser usado em outro lugar
    self$dados <- dados
  },
  
  # fun√ß√£o que pega um item da base
  .getitem = function(index) {
    self$dados[index, ]
  },
  
  # fun√ß√£o que mede o tamanho da base
  .length = function() {
    nrow(self$dados)
  }
)

# para criar o dataset mesmo:

ds <- ds_generator("ola", "mundo", 5)
```

    <meu nome!>
      Inherits from: <dataset>
      Public:
        .getitem: function (ii) 
        .length: function () 
        clone: function (deep = FALSE) 
        dados: ola ola ola ola ola mundo mundo mundo mundo mundo
        initialize: function (parametro1, parametro2, repeticoes) 
        objeto_arbitrario: voc√™ pode criar objetos arbitr√°rios aqui

Os objetos registrados podem ser acessados dessa forma:

```{r dataset-show, eval=FALSE}
ds$dados
```
Legal! agora temos nosso dataset. Agora precisamos de um m√©todo para gerar amostras desse dataset. Esse √© o __dataloader__, que pode ser criado assim:

         [,1]  [,2]   
    [1,] "ola" "mundo"
    [2,] "ola" "mundo"
    [3,] "ola" "mundo"
    [4,] "ola" "mundo"
    [5,] "ola" "mundo"

```{r dataloader, eval=FALSE}
dl <- torch::dataloader(ds, batch_size = 2)
dl
```

    <dataloader>
      Public:
        .auto_collation: active binding
        .dataset_kind: map
        .has_getbatch: FALSE
        .index_sampler: active binding
        .iter: function () 
        .length: function () 
        batch_sampler: utils_sampler_batch, utils_sampler, R6
        batch_size: 2
        clone: function (deep = FALSE) 
        collate_fn: function (batch) 
        dataset: meu nome!, dataset, R6
        drop_last: FALSE
        generator: NULL
        initialize: function (dataset, batch_size = 1, shuffle = FALSE, sampler = NULL, 
        multiprocessing_context: NULL
        num_workers: 0
        pin_memory: FALSE
        sampler: utils_sampler_sequential, utils_sampler, R6
        timeout: -1
        worker_globals: NULL
        worker_init_fn: NULL
        worker_packages: NULL

Isso pode ser usado para gerar amostras da base de dados, o que √© importante para a etapa de ajuste de modelos realizadas pelo `{torch}` e pelo `{luz}`. 

No caso dos captchas, eu montei uma fun√ß√£o do pacote `{captcha}` que cria os dataloaders de treino e valida√ß√£o a partir de uma fun√ß√£o que faz o download dos arquivos brutos.

```{r dataloader-all, eval=FALSE}

path <- "caminho/para/arquivos/brutos"

# cria o dataset. No caso, estamos usando a base do TRT, 
# que tem apenas 400 Captchas.

captcha_ds <- captcha::captcha_dataset(
  root = "data-raw/trt",
  captcha = "trt2",
  download = TRUE
)


# cria os dataloaders de treino e valida√ß√£o

## amostra de indices
id_train <- sample(1:length(captcha_ds), .8 * length(captcha_ds))

## cria o dataloader de treino
captcha_dl_train <- torch::dataloader(
  torch::dataset_subset(captcha_ds, id_train),
  batch_size = 32,
  shuffle = TRUE
)

## cria o dataloader de valida√ß√£o
captcha_dl_valid <- torch::dataloader(
  torch::dataset_subset(captcha_ds, -id_train),
  batch_size = 32
)
```

## Modelo

Um modelo do torch tamb√©m √© um objeto de classe R6, mas que precisa ter os par√¢metros `initialize=` e `forward=` implementados. Aqui um exemplo que soma um n√∫mero ao valor de entrada:

```{r model, eval=FALSE}
model_generator <- torch::nn_module(
  "meu modelo!",
  # aqui inicializamos nosso modelo
  initialize = function(parametro) {
    self$valor <- parametro
  },
  # aqui calculamos o que o modelo deve calcular
  forward = function(x) {
    x + self$valor
  }
)

model_generator
```

    <meu modelo!> object generator
      Inherits from: <inherit>
      Public:
        .classes: meu modelo! nn_module
        initialize: function (parametro) 
        forward: function (x) 
        clone: function (deep = FALSE) 
      Parent env: <environment: 0x5594bc41f3c0>
      Locked objects: FALSE
      Locked class: FALSE
      Portable: TRUE

Por exemplo, o modelo abaixo somar√° 10 ao valor de entrada

```{r model-show, eval=FALSE}
modelo <- model_generator(10)
modelo(1)
```

    [1] 11

No nosso caso, utilizaremos um modelo j√° implementado no pacote `{captcha}`, que utiliza redes neurais convolucionais. √â importante mencionar que n√£o √© necess√°rio criar o modelo: utilizaremos apenas o model generator.

```{r model-example, eval=FALSE}
captcha::net_captcha
```

    <CAPTCHA-CNN> object generator
      Inherits from: <inherit>
      Public:
        .classes: CAPTCHA-CNN nn_module
        initialize: function (input_dim, output_ndigits, output_vocab_size) 
        forward: function (x) 
        clone: function (deep = FALSE) 
      Parent env: <environment: 0x557dc107da60>
      Locked objects: FALSE
      Locked class: FALSE
      Portable: TRUE


## Treino

E agora chegamos na parte mais legal. Com o `{luz}`, temos uma sintaxe parecida com o `{keras}`, em que fazemos o setup do modelo e depois ajustamos, com direito a uma barrinha de progresso que j√° vem automaticamente ü§©


```{r model-run, eval=FALSE}
fitted <- captcha::net_captcha |>
  # aqui colocamos a fun√ß√£o de perda e o otimizador
  luz::setup(
    loss = torch::nn_multilabel_soft_margin_loss(),
    optimizer = torch::optim_adam
  ) |>
  # aqui n√≥s colocamos os hiperpar√¢metros do modelo
  # no caso, precisamos passar informa√ß√µes sobre a dimens√£o
  # da imagem, o tamanho do vocabul√°rio e a quantidade de letras
  # em um captcha
  luz::set_hparams(
    input_dim = dim(captcha_ds$data)[-1],
    output_vocab_size = dim(captcha_ds$target)[3],
    output_ndigits = dim(captcha_ds$target)[2]
  ) |>
  # aqui n√≥s colocamos os hiperpar√¢metros de otimiza√ß√£o
  luz::set_opt_hparams(
    lr = .1
  ) |>
  # ajustar o modelo
  luz::fit(
    captcha_dl_train,
    valid_data = captcha_dl_valid,
    epochs = 30
  )
```

E pronto! Agora √© s√≥ ver o modelo rodar e mexer nos hiperpar√¢metros para que o modelo alcance a acur√°cia desejada.

## Extra: m√©trica customizada

Uma coisa que ficou faltando no c√≥digo anterior √© uma ideia de como fica a acur√°cia do modelo ao longo do ajuste. Como n√£o existe uma m√©trica j√° pronta para calcular o quanto acertamos das 6 letras ao mesmo tempo, precisamos implementar uma m√©trica customizada para o Captcha.

Isso n√£o √© um grande desafio para o `{luz}`! Assim como nos datasets e modelos, podemos criar uma m√©trica com uma classe R6, usando a fun√ß√£o `luz::luz_metric()`. Essa classe deve ter 4 elementos: `abbrev=`, o nome da m√©trica, `initialize=` que seta os par√¢metros iniciais, `update=` que atualiza a m√©trica a partir de um conjunto novo de outputs e predi√ß√µes e `compute=` que calcula a m√©trica atualizada. 

No caso dos Captchas, a m√©trica fica assim:

```{r, eval=FALSE}
captcha_accuracy <- luz::luz_metric(
  abbrev = "Captcha Acc",
  initialize = function() {
    # inicializa os par√¢metros
    self$correct <- 0
    self$total <- 0
  },
  update = function(preds, target) {
    # extra√≠mos o token com maior probabilidade associada, para cada letra
    pred <- torch::torch_argmax(preds, dim = 3)
    # fazemos o mesmo para a vari√°vel resposta
    tgt <- torch::torch_argmax(target$squeeze(), dim = 3)
    # comparamos os dois resultados e somamos a quantidade de resultados iguais
    new_correct <- (pred == tgt)$to(dtype = torch::torch_float())$sum()$item()
    # atualizamos os valores
    self$correct <- self$correct + new_correct
    self$total <- self$total + pred$numel()
  },
  compute = function() {
    # calcula a propor√ß√£o de acertos
    self$correct / self$total
  }
)
```

## Resultados

O c√≥digo completo de ajuste do modelo pode ser encontrado [aqui](). A sintaxe do pacote `{captcha}` ainda est√° inst√°vel, ent√£o pode ser que o c√≥digo quebre no futuro.

```{r include=FALSE}
knitr::include_graphics("../../static/images/posts/conteudo/captcha-luz/luz.gif")


```

![](/images/posts/conteudo/captcha-luz/luz.gif)

Com o c√≥digo, consegui chegar em uma acur√°cia de 90% com apenas 400 exemplos classificados!

## Wrap-up

Neste post, vimos que

- O pacote `{luz}` √© uma alterativa ao `{keras}` que usa o `{torch}` como backend.
- Para trabalhar com o `{luz}`, √© necess√°rio criar datasets, dataloaders e modelos na forma de classes R6.
- Com o `{luz}`, √© poss√≠vel criar m√©tricas customizadas para acompanhar o desempenho de um modelo.
- Os modelos de redes neurais convolucionais funcionam bem em tarefas de vis√£o computacional, como os Captchas.

√â isso. Happy coding ;)
