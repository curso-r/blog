---
title: "Modelos beseados em árvores são imunes à multicolinearidade?"
date: "2018-05-22"
tags: ["machine learning", "random forest", "multicolinearidade"]
categories: ["conceitos"]
image: "images/posts/banner/banner-arvore-e-multicolinearidade.webp"
author: ["Athos", "Julio"]
summary: "Modelos baseados em árvores como árvores de decisão, random forest, ligthGBM e xgboost são conhecidos, dentre outras qualidades, pela sua robustês diante do problema de multicolinearidade. Esse post mostra que isso não é totalmente verdade."
---

<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<p>Modelos baseados em árvores como árvores de decisão, random forest, ligthGBM e xgboost são conhecidos, dentre outras qualidades, pela sua robustês diante do problema de multicolinearidade. É sabido que seu poder preditivo não se abala na presença de variáveis extremamente correlacionadas.</p>
<p>Porém, quem nunca usou um Random Forest pra fazer seleção de variáveis? Pegar, por exemplo, as top 10 mais importantes e descartar o resto?</p>
<p>Ou até mesmo arriscou uma interpretação e concluiu sobre a ordem das variáveis mais importantes?</p>
<p>Abaixo mostraremos o porquê não devemos ignorar a questão da multicolinearidade completamente!</p>
<div id="um-modelo-bonitinho" class="section level2">
<h2>Um modelo bonitinho</h2>
<p>Primeiro vamos ajustar um modelo bonitinho, livre de multicolinearidade. Suponha que queiramos prever <code>Petal.Length</code> utilizando as medidas das sépalas (<code>Sepal.Width</code> e <code>Sepal.Length</code>) da nossa boa e velha base <code>iris</code>.</p>
<pre class="r"><code>library(tidyverse)
## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──
## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
## ✓ tibble  3.0.4     ✓ dplyr   1.0.2
## ✓ tidyr   1.1.2     ✓ stringr 1.4.0
## ✓ readr   1.4.0     ✓ forcats 0.5.0
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()

iris2 &lt;- iris %&gt;% select(Sepal.Length, Sepal.Width, Petal.Length)
iris2 %&gt;% cor %&gt;% corrplot::corrplot()</code></pre>
<p><img src="/posts/2018-05-22-arvore-e-multicolinearidade_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>O gráfico acima mostra que as variáveis explicativas não são fortemente correlacionadas. Ajustando uma random fores, temos a seguinte ordem de importância das variáveis:</p>
<pre class="r"><code>library(randomForest)
iris2_rf &lt;- randomForest(Petal.Length ~ ., data = iris2)
varImpPlot(iris2_rf)</code></pre>
<p><img src="/posts/2018-05-22-arvore-e-multicolinearidade_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Sem surpresas. Agora vamos para o problema!</p>
</div>
<div id="um-modelo-com-feinho" class="section level2">
<h2>Um modelo com feinho</h2>
<p>Vamos forjar uma situação extrema em que muitas variáveis sejam multicolineares. Vou fazer isso repetindo a coluna <code>Sepal.Length</code> várias vezes.</p>
<pre class="r"><code>iris3 &lt;- accumulate(1:20, ~{
  .x[[paste0(&quot;Sepal.Length&quot;, .y)]] &lt;-  iris2$Sepal.Length
  .x
}, .init = iris2)

iris3[[20]] %&gt;% cor %&gt;% corrplot::corrplot(order = &quot;alphabet&quot;)</code></pre>
<p><img src="/posts/2018-05-22-arvore-e-multicolinearidade_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Agora a coisa tá feia! Temos 20 variáveis perfeitamente colineares. Mesmo assim um random forest nessa nova base não perderia poder preditivo.</p>
<p>Mas como ficou a importância das variáveis?</p>
<pre class="r"><code>iris3_rf &lt;- randomForest(Petal.Length ~ ., data = iris3[[20]])
varImpPlot(iris3_rf)</code></pre>
<p><img src="/posts/2018-05-22-arvore-e-multicolinearidade_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Aqui o jogo já se inverteu: concluiríamos que <code>Sepal.Width</code> é mais importante de todas as variáveis!</p>
</div>
<div id="seleção-de-variáveis-furado" class="section level2">
<h2>Seleção de variáveis furado</h2>
<p>O gráfico abaixo mostra que quanto mais variáveis correlacionadas tivermos, menor a importância de TODAS ELAS SIMULTANEAMENTE! É como se as variáveis colineares repartissem a importância entre elas.</p>
<pre class="r"><code># ajusta random forest para bases com 1 a 20 repetições de `Sepal.Length`
rfs &lt;- map(iris3, ~ randomForest(Petal.Length ~ ., data = .x) %&gt;% importance)

# extrai as importâncias das repetições de `Sepal.Length`
importancia &lt;- map_dfr(rfs, ~{
  .x %&gt;% 
    as.data.frame() %&gt;% 
    tibble::rownames_to_column() %&gt;% 
    dplyr::filter(stringr::str_detect(rowname, &quot;^Sepal.Length&quot;))
}, .id = &quot;n_repeticoes&quot;) %&gt;%
  mutate(n_repeticoes = as.numeric(n_repeticoes))

# Gráfico do número de variáveis multicolineares vs importância
importancia %&gt;%
  ggplot(aes(x = n_repeticoes, y = IncNodePurity)) +
  geom_point() +
  geom_hline(yintercept = 40, size = 1, linetype = &quot;dashed&quot;, colour = &quot;red&quot;) +
  labs(x = &quot;Qtd de repetições da coluna `Sepal.Length`&quot;, y = &quot;Importância&quot;, title = &quot;Gráfico da relação entre o número de variáveis multicolineares vs importância&quot;)</code></pre>
<p><img src="/posts/2018-05-22-arvore-e-multicolinearidade_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Na prática, se estabelecessemos um corte no valor de importância pra descartar variáveis (como ilustrado pela linha vermelha), teríamos um problema em potencial: poderíamos estar jogando fora informação muito importante.</p>
</div>
<div id="como-tratar-multicolinearidade-então" class="section level2">
<h2>Como tratar multicolinearidade, então?</h2>
<p>Algumas maneiras de lidar com multicolinearidade são:</p>
<ul>
<li>Observar a matriz de correlação</li>
<li><a href="http://www.sthda.com/english/articles/39-regression-model-diagnostics/160-multicollinearity-essentials-and-vif-in-r/">VIF</a></li>
<li><a href="https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/">Recursive feature elimination</a></li>
</ul>
</div>
<div id="conclusão" class="section level2">
<h2>Conclusão</h2>
<p>Cuidado ao jogar tudo no caldeirão! Devemos sempre nos preocupar com multicolinearidade, mesmo ajustando modelos baseados em árvores.</p>
<p>Abs!</p>
</div>
