---
title: "Quebrando Captchas - Parte VI: Redes Generativas Adversariais com Classificador Auxiliar"
date: "2019-03-11"
categories: ["análises"]
tags: ["captcha"]
image: "images/posts/banner/captcha_06.webp"
author: ["Julio"]
summary: "Nesse post vou trabalhar com Captchas e GANs. Se você não sabe o que é um Captcha e como estamos trabalhando para resolvê-los até agora, recomendo que você veja a série de posts da Curso-R sobre o tema. Se você não sabe nada de GAN, acredito que seja possível acompanhar esse post."
editor_options: 
  chunk_output_type: inline
---

<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<p>Nesse post vou trabalhar com Captchas e GANs. Se você não sabe o que é um Captcha e como estamos trabalhando para resolvê-los até agora, recomendo que você veja a <a href="https://www.curso-r.com/tags/captcha/">série de posts da Curso-R</a> sobre o tema. Se você não sabe nada de GAN, acredito que seja possível acompanhar esse post. No entanto, não acho que esse é o exemplo mais simples possível de GAN: se você quiser um “hello world” no tema, recomendo ver <a href="https://tensorflow.rstudio.com/keras/articles/examples/mnist_acgan.html">esse exemplo</a> e <a href="https://arxiv.org/pdf/1406.2661.pdf">esse paper</a>.</p>
<p><strong>Objetivo</strong>: criar um modelo capaz de, ao mesmo tempo, gerar novos Captchas e também resolver Captchas existentes.</p>
<p><strong>Motivo</strong>: o motivo principal de usar modelos generativos é aproveitar o fato de existirem muito mais Captchas não classificados do que classificados. De certa forma, saber criar novos Captchas pode auxiliar no trabalho de resolvê-los.</p>
<p><strong>Base de dados</strong>: ao invés de utilizar um Captcha real, vamos trabalhar com um Captcha sintético criado partir do MNIST. Chamarei esses carinhas de MNIST-Captcha. A ideia foi simplificar o problema para facilitar o desenvolvimento da solução. Um problema que enfrentamos no Deep Learning é que existem tantos hiperparâmetros a serem escolhidos que é muito difícil achar configurações que funcionam bem. Em problemas mais simples, é mais fácil achar esses parâmetros.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="/images/posts/conteudo/gan/mnist.webp" alt="Exemplo de imagem do MNIST." width="23%" />
<p class="caption">
Figura 1: Exemplo de imagem do MNIST.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="/images/posts/conteudo/gan/mnist-captcha.webp" alt="Exemplo de MNIST-Captcha." width="50%" />
<p class="caption">
Figura 2: Exemplo de MNIST-Captcha.
</p>
</div>
<div id="o-que-é-gan" class="section level3">
<h3>O que é GAN?</h3>
<p>A sigla GAN significa <em>Generative Adversarial Networks</em>. Não confunda com GAM (<em>Generalized Additive Models</em>)! Trata-se de um modelo recente, introduzido em 2014 pelo Ian Goodfellow, autor do famoso livro <a href="">DeepLearningBook</a>, que está ganhando cada vez mais espaço na comunidade científica, por conta de seus impressionantes resultados.</p>
<p>GAN, na verdade, são dois modelos co-dependentes, o <strong>gerador</strong> e o <strong>discriminador</strong>. O gerador tenta gerar novos Captchas a partir de nada, indistinguíveis dos originais, e o discriminador tenta discriminar se os Captchas criados pelo gerador são reais ou não. Ou seja, um fica brigando com o outro, e no final temos um excelente gerador de novos Captchas e um excelente discriminador de Captchas.</p>
</div>
<div id="o-que-é-ac-gan" class="section level3">
<h3>O que é AC-GAN?</h3>
<p>Quando nosso interesse está na predição, simples GANs não ajudam, pois o discriminadores só sabem verificar se uma imagem é original ou fake. No entanto, podemos adicionar respostas no discriminador, tornando-o um modelo duplo, capaz de discriminar imagens reais e falsas e, ao mesmo tempo, capaz de predizer o valor do Captcha.</p>
<div id="ok-mas-não-está-claro" class="section level4">
<h4>OK, mas não está claro</h4>
<p>Não mesmo. Esses carinhas não são simples. O ideal é sentar na cadeira e tentar entender o que o modelo faz. Eu fiz minha lição de casa, e consegui extrair esses passos:</p>
<ol style="list-style-type: decimal">
<li>Coletar uma amostra das imagens de treino</li>
<li>Gerar Captchas fake.</li>
<li>Juntar com Captchas reais.</li>
<li>Andar 1 iteração no ajuste do discriminador.</li>
<li>Gerar mais Captchas fake.</li>
<li>Criar uma variável resposta sintética para avaliar a qualidade do gerador.</li>
<li>Andar 1 iteração no ajuste do gerador, usando o discriminador para avaliar se as imagens foram bem geradas ou não.</li>
<li>Voltar ao passo 1 até coletar a totalidade das imagens de treino</li>
<li>Aplicar os passos 1 até 7 com as imagens de teste, para avaliar a performance dos modelos.</li>
</ol>
<p>Os passos são até tranquilos de implementar, mas são difíceis de entender. Particularmente, as partes que tive mais dificuldade foram 6 e 7.</p>
<p>Vamos então repassar esses pontos, só que agora assumindo que temos as funções <code>generator()</code> e <code>discriminator()</code> em mãos.</p>
</div>
</div>
<div id="estrutura-do-ac-gan" class="section level3">
<h3>Estrutura do AC-GAN</h3>
<div id="o-generator" class="section level4">
<h4>O <code>generator()</code></h4>
<p>O <code>generator()</code> é uma função que recebe como input i) um vetor de valores aleatórios e ii) uma label de um Captcha, e retorna a imagem de um Captcha. Os valores aleatórios são variáveis latentes que ajudam a criar os ruídos do Captcha: se não fossem eles, todos os Captchas gerados para uma dada label seriam iguais.</p>
<p>Matematicamente, o gerador <span class="math inline">\(g\)</span> de uma imagem <span class="math inline">\(i\)</span> se comporta da seguinte forma:</p>
<p><span class="math display">\[
\mathbf X_i = g(\mathbf Y_i, \boldsymbol \varepsilon_i),
\]</span></p>
<p>onde</p>
<ul>
<li><span class="math inline">\(\mathbf X_i\)</span> é uma imagem. No caso da RFB, uma imagem 50x180.</li>
<li><span class="math inline">\(\mathbf Y_i\)</span> é a label do Captcha, encodada na forma 0-1, como mostrada por <a href="https://www.curso-r.com/blog/2017-06-29-captcha-dados/">esse post</a></li>
<li><span class="math inline">\(\boldsymbol \varepsilon_i\)</span> é um erro aleatório, usado para que os resultados gerados sejam diferentes toda vez.</li>
<li><span class="math inline">\(g\)</span> é uma função altamente não linear, capaz de receber os inputs mencionados e retornar uma imagem.</li>
</ul>
<p>Uma forma interessante de representar <code>g()</code> é através de uma <em>rede convolucional</em> ao contrário. Veja os comentários da função para entender o que ela está fazendo.</p>
<pre class="r"><code>build_generator &lt;- function() {
  
  # ruído
  latent &lt;- layer_input(c(2, 10), name = &quot;noise&quot;)
  
  # resposta
  label &lt;- layer_input(c(2, 10), name = &quot;sampled_labels&quot;)
  
  # obtém o input a partir da soma da resposta e do ruído
  input &lt;- layer_add(list(label, latent))
  
  output &lt;- input %&gt;%
    
    # destrói as dimensões existentes e coloca num vetor
    layer_flatten() %&gt;% 
    
    # remodela as dimensões iniciais da imagem
    # aqui, ela fica com 4 &quot;cores&quot; (canais),
    # 7 linhas e 14 colunas
    layer_dense(4 * 7 * 14, activation = &quot;tanh&quot;) %&gt;%
    layer_reshape(c(4, 7, 14)) %&gt;%
    
    # A rede convolucional transpose com strides aumenta 
    # o tamanho da imagem. Agora está em 14 x 28
    layer_conv_2d_transpose(64, 3, padding = &quot;same&quot;, strides = c(2, 2)) %&gt;%
    layer_activation_leaky_relu() %&gt;% 
    
    # Agora está em 28 x 56
    layer_conv_2d_transpose(64, 3, padding = &quot;same&quot;, strides = c(2, 2)) %&gt;%
    layer_activation_leaky_relu() %&gt;% 
    
    # Agora está em 28 x 112 (note que strides = c(1, 2))
    layer_conv_2d_transpose(32, 3, padding = &quot;same&quot;, strides = c(1, 2)) %&gt;%
    layer_activation_leaky_relu() %&gt;% 
    
    # mais algumas convolucionais para gerar não-linearidade
    layer_conv_2d(16, 3, padding = &quot;same&quot;) %&gt;%
    layer_activation_leaky_relu() %&gt;%
    
    layer_conv_2d(16, 3, padding = &quot;same&quot;) %&gt;%
    layer_activation_leaky_relu() %&gt;%
    
    layer_conv_2d(8, 3, padding = &quot;same&quot;) %&gt;%
    layer_activation_leaky_relu() %&gt;%
    
    # Reduzir o número de cores para 1 (imagem preto e branco)
    layer_conv_2d( 1, 3, padding = &quot;same&quot;, activation = &quot;tanh&quot;)
  
  # coloca os dois inputs e o output no modelo
  keras_model(list(latent, image_class), output)
}

build_generator()</code></pre>
<pre><code>Model
___________________________________________________________________________________________
Layer (type)                  Output Shape        Param #   Connected to                   
===========================================================================================
noise (InputLayer)            (None, 4, 10)       0                                        
___________________________________________________________________________________________
sampled_labels (InputLayer)   (None, 4, 10)       0                                        
___________________________________________________________________________________________
add_2 (Add)                   (None, 4, 10)       0         noise[0][0]                    
                                                            sampled_labels[0][0]           
___________________________________________________________________________________________
flatten_4 (Flatten)           (None, 40)          0         add_2[0][0]                    
___________________________________________________________________________________________
dense_10 (Dense)              (None, 392)         16072     flatten_4[0][0]                
___________________________________________________________________________________________
reshape_5 (Reshape)           (None, 4, 7, 14)    0         dense_10[0][0]                 
___________________________________________________________________________________________
conv2d_transpose_4 (Conv2DTra (None, 64, 14, 28)  2368      reshape_5[0][0]                
___________________________________________________________________________________________
leaky_re_lu_10 (LeakyReLU)    (None, 64, 14, 28)  0         conv2d_transpose_4[0][0]       
___________________________________________________________________________________________
conv2d_transpose_5 (Conv2DTra (None, 64, 28, 56)  36928     leaky_re_lu_10[0][0]           
___________________________________________________________________________________________
leaky_re_lu_11 (LeakyReLU)    (None, 64, 28, 56)  0         conv2d_transpose_5[0][0]       
___________________________________________________________________________________________
conv2d_transpose_6 (Conv2DTra (None, 32, 28, 112) 18464     leaky_re_lu_11[0][0]           
___________________________________________________________________________________________
leaky_re_lu_12 (LeakyReLU)    (None, 32, 28, 112) 0         conv2d_transpose_6[0][0]       
___________________________________________________________________________________________
conv2d_8 (Conv2D)             (None, 1, 28, 112)  289       leaky_re_lu_12[0][0]           
===========================================================================================
Total params: 74,121
Trainable params: 74,121
Non-trainable params: 0
___________________________________________________________________________________________</code></pre>
<p>No caso, nossa resposta é 4747:</p>
<pre><code>     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    0    0    0    1    0    0    0    0    0     0
[2,]    0    0    0    0    0    0    1    0    0     0
[3,]    0    0    0    1    0    0    0    0    0     0
[4,]    0    0    0    0    0    0    1    0    0     0</code></pre>
<p>E temos uma matriz de números aleatórios do mesmo tamanho:</p>
<pre><code>      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]
[1,] -0.06  0.02 -0.08  0.16  0.03 -0.08  0.05  0.07  0.06 -0.03
[2,]  0.15  0.04 -0.06 -0.22  0.11  0.00  0.00  0.09  0.08  0.06
[3,]  0.09  0.08  0.01 -0.20  0.06 -0.01 -0.02 -0.15 -0.05  0.04
[4,]  0.14 -0.01  0.04 -0.01 -0.14 -0.04 -0.04 -0.01  0.11  0.08</code></pre>
<p>Queremos gerar uma resposta com ruído a partir da resposta e de um vetor de números aleatórios. Fazemos isso simplesmente somando as duas quantidades:</p>
<pre class="r"><code>round(y + z , 2)</code></pre>
<pre><code>      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]
[1,] -0.06  0.02 -0.08  1.16  0.03 -0.08  0.05  0.07  0.06 -0.03
[2,]  0.15  0.04 -0.06 -0.22  0.11  0.00  1.00  0.09  0.08  0.06
[3,]  0.09  0.08  0.01  0.80  0.06 -0.01 -0.02 -0.15 -0.05  0.04
[4,]  0.14 -0.01  0.04 -0.01 -0.14 -0.04  0.96 -0.01  0.11  0.08</code></pre>
<p>Observe que os valores da resposta continuam destacados. O resultado da aplicação do <code>generator()</code> é uma imagem com as mesmas dimensões do MNIST-Captcha e os números dados no input. Queremos que essa imagem seja o mais parecida possível com uma imagem real.</p>
</div>
<div id="o-discriminator" class="section level4">
<h4>O <code>discriminator()</code></h4>
<p>O discriminador é um modelo preditivo, mas ele prevê duas coisas: i) se a imagem recebida é real ou fake e ii) qual é a label de uma imagem recebida. Se o gerador for bom, (i) terá dificuldades para funcionar. Se o gerador for muito ruim (i) conseguirá prever resultados com facilidade.</p>
<p>Vamos a um exemplo de discriminador, também usando redes convolucionais:</p>
<pre class="r"><code>build_discriminator &lt;- function() {
  
  image &lt;- layer_input(shape = c(1, 28, 28*4))
  
  # rede convolucional LENET-5 bem comum
  output &lt;- image %&gt;%
    layer_conv_2d(32, 5, padding = &quot;same&quot;) %&gt;%
    layer_activation_leaky_relu() %&gt;% 
    layer_max_pooling_2d() %&gt;% 
    layer_conv_2d(64, 5, padding = &quot;same&quot;) %&gt;%
    layer_activation_leaky_relu() %&gt;% 
    layer_max_pooling_2d() %&gt;% 
    layer_conv_2d(64, 3, padding = &quot;same&quot;) %&gt;%
    layer_activation_leaky_relu() %&gt;% 
    layer_flatten() %&gt;% 
    layer_dense(64, activation = &quot;relu&quot;) %&gt;%
    layer_dropout(0.2) %&gt;%  
    layer_dense(128, activation = &quot;relu&quot;)
  
  cnn &lt;- keras_model(image, output)
  features &lt;- cnn(image)
  
  # chega a primeira resposta (fake, não fake)
  fake &lt;- features %&gt;% 
    layer_dense(32, activation = &quot;tanh&quot;) %&gt;% 
    layer_dense(1, activation = &quot;sigmoid&quot;, name = &quot;generation&quot;)
  
  # chega na segunda resposta (Y)
  aux &lt;- features %&gt;%
    layer_dense(4 * 10, activation = &quot;relu&quot;) %&gt;% 
    layer_reshape(c(4, 10)) %&gt;% 
    layer_activation(&quot;softmax&quot;, name = &quot;auxiliary&quot;)
  
  # junta os dois no resultado
  keras_model(image, list(fake, aux))
}

build_discriminator()</code></pre>
<pre><code>Model
___________________________________________________________________________________________
Layer (type)                  Output Shape        Param #   Connected to                   
===========================================================================================
input_4 (InputLayer)          (None, 1, 28, 112)  0                                        
___________________________________________________________________________________________
model_5 (Model)               (None, 128)         900224    input_4[0][0]                  
___________________________________________________________________________________________
dense_9 (Dense)               (None, 40)          5160      model_5[1][0]                  
___________________________________________________________________________________________
dense_8 (Dense)               (None, 32)          4128      model_5[1][0]                  
___________________________________________________________________________________________
reshape_4 (Reshape)           (None, 4, 10)       0         dense_9[0][0]                  
___________________________________________________________________________________________
generation (Dense)            (None, 1)           33        dense_8[0][0]                  
___________________________________________________________________________________________
auxiliary (Activation)        (None, 4, 10)       0         reshape_4[0][0]                
===========================================================================================
Total params: 1,819,090
Trainable params: 909,545
Non-trainable params: 909,545
___________________________________________________________________________________________</code></pre>
<p>Note que o resultado é uma lista de dois outputs.</p>
</div>
<div id="o-gan" class="section level4">
<h4>O <code>gan()</code></h4>
<p><code>gan()</code> é a função que junta o discriminador e o gerador. Em teoria, ela não serve para nada: é só um meio prático de ajustar o modelo do gerador. Fazemos isso gerando algumas imagens falsas e avaliando a qualidade do gerador por quanto ele é capaz de enganar o discriminador. Se o discriminador é muito ruim, qualquer gerador vai mandar bem. Se o discriminador é muito bom, o gerador terá de criar Captchas excepcionais para ganhar pontos.</p>
<p>O <code>gan()</code> é definido de forma muito simples, com:</p>
<pre class="r"><code># placeholder para ruído
latent &lt;- layer_input(shape = list(4L, 10L))
# placeholder para resposta
image_class &lt;- layer_input(shape = list(4L, 10L))
# imagem falsa
fake &lt;- generator(list(latent, image_class))

# Only want to be able to train generation for the combined model
# Só queremos treinar o generator nessa parte;
# utilizaremos o discriminator apenas para 
# avaliação da qualidade do gerador.
# Por isso, congelamos o discriminator aqui.
freeze_weights(discriminator)
results &lt;- discriminator(fake)

gan &lt;- keras_model(list(latent, image_class), results)
gan</code></pre>
<pre><code>Model
___________________________________________________________________________________________
Layer (type)                  Output Shape        Param #   Connected to                   
===========================================================================================
input_5 (InputLayer)          (None, 4, 10)       0                                        
___________________________________________________________________________________________
input_6 (InputLayer)          (None, 4, 10)       0                                        
___________________________________________________________________________________________
model_7 (Model)               (None, 1, 28, 112)  74121     input_5[0][0]                  
                                                            input_6[0][0]                  
___________________________________________________________________________________________
model_6 (Model)               [(None, 1), (None,  909545    model_7[1][0]                  
===========================================================================================
Total params: 983,666
Trainable params: 74,121
Non-trainable params: 909,545
___________________________________________________________________________________________</code></pre>
</div>
</div>
<div id="mas-funciona" class="section level2">
<h2>Mas funciona?</h2>
<p>É uma pergunta natural. Esse modelo é meio maluco, pois usa uma parte do modelo para avaliar a qualidade do outro. Mas sim, temos alguns resultados matemáticos que garantem que estamos em um bom território.</p>
<p>Meu desejo era colocá-los aqui, mas ainda não sei explicar todos os detalhes. Pretendo estudar mais e adicionar em novos posts.</p>
</div>
<div id="exemplo-de-aplicação-nos-mnist-captcha" class="section level2">
<h2>Exemplo de aplicação nos MNIST-Captcha</h2>
<p>O exemplo completo que rodei com os MNIST-Captchas está <a href="https://gist.github.com/jtrecenti/149fc2ae4fe66652624f2c37f11d9286/">neste link</a>. Decidi não colocar o código completo aqui pois <del>a minha implementação está muito feia</del> o post ficaria muito longo.</p>
<p>Muito bem, se o modelo serve para prever e para gerar, vamos avaliar a qualidade nesses quesitos.</p>
<div id="prevê-bem" class="section level3">
<h3>Prevê bem?</h3>
<p>A taxa de acerto de cada dígito do MNIST-Captcha foi de 95%. Como temos 4 dígitos nesse caso, o acerto de todo o Captcha seria de 81%. Comparado com os modelos funcionando em produção do pacote <code>decryptr</code>, esse resultado é ruim. Lá, as taxas de acerto do Captcha são de no mínimo 93% e tem casos que acertamos tudo. Mas eu acredito que o motivo disso é que não ajustamos bem os hiperparâmetros. Se fizermos isso, provavelmente ficará melhor.</p>
</div>
<div id="gera-bem" class="section level3">
<h3>Gera bem?</h3>
<p>Vamos ver! Para testar o gerador, montamos uma função que gera e plota uma imagem a partir de um vetor de respostas:</p>
<pre class="r"><code>generate_image &lt;- function(num = sample(0:9, 4)) {
  
  # Gerando barulho
  noise &lt;- rnorm(4 * 10, 0, .01) %&gt;%
    array(dim = c(4, 10)) %&gt;% 
    array_reshape(c(1, dim(.)))
  
  # Resposta (aleatorizada ou input)
  sampled_labels &lt;- num %&gt;%
    matrix(ncol = 4) %&gt;% 
    # gera o one-hot dessa resposta
    transform_to_matrix() %&gt;% 
    array_reshape(c(1, dim(.)))
  
  # Aplicar generator
  img &lt;- predict(generator, list(noise, sampled_labels))[1,1,,]
  
  # a imagem foi deixada em (-1,1). Arrumando
  par(mar = rep(0, 4))
  plot(as.raster((img + 1) / 2))
}</code></pre>
<p>Rodando para alguns exemplos:</p>
<pre class="r"><code>gerar_imagem(c(1, 2, 4, 4))</code></pre>
<p><img src="/images/posts/conteudo/gan/1244.webp" width="50%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>gerar_imagem(c(4, 0, 4, 7))</code></pre>
<p><img src="/images/posts/conteudo/gan/4047.webp" width="50%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>gerar_imagem(c(9, 9, 9, 9))</code></pre>
<p><img src="/images/posts/conteudo/gan/9999.webp" width="50%" style="display: block; margin: auto;" /></p>
<p>O que achou? Note que a última está errada.</p>
</div>
</div>
<div id="próximos-passos" class="section level2">
<h2>Próximos passos</h2>
<ul>
<li>Fazer o GAN funcionar para mais Captchas.</li>
<li>Testar <a href="https://github.com/soumith/ganhacks">algumas dicas que o Athos me passou</a>.</li>
<li>Buscar uma forma de aproveitar a informação parcial advinda de <em>oráculos</em>. Fica o mistério para os próximos posts.</li>
</ul>
</div>
<div id="wrap-up" class="section level2">
<h2>Wrap-up</h2>
<ul>
<li>GAN é um modelo interessante que pode nos ajudar a montar modelos não supervisionados (diretamente) e supervisionados (AC-GAN).</li>
<li>Para montar um GAN, você precisa definir um gerador e um discriminador.</li>
<li>O AC-GAN parece ser uma boa abordagem para Captchas, considerando que temos muitas imagens disponíveis e poucas classificadas.</li>
<li>Ainda temos muito a descobrir sobre esse modelo.</li>
</ul>
</div>
<div id="agradecimentos" class="section level2">
<h2>Agradecimentos</h2>
<p>Sempre ao <a href="https://www.curso-r.com/author/daniel/">Daniel Falbel</a>, que é meu guru do Deep Learning e autor do exemplo inicial que foi adaptado para esse post. Também agradeço ao <a href="https://www.curso-r.com/author/athos/">Athos</a> pelos insights e links que me passou!</p>
</div>
