[{"author":["William"],"categories":["tutoriais"],"contents":" Introdução As funções lag() e lead() são recursos poderosos do pacote dplyr no R que nos permitem analisar diferenças e tendências temporais em conjuntos de dados. Com essas funções, podemos facilmente comparar valores anteriores e posteriores em uma sequência, o que é útil para identificar padrões, mudanças e comportamentos ao longo do tempo. Neste post, exploraremos a funcionalidade dessas funções usando um exemplo prático.\nAntes de prosseguirmos, certifique-se de ter instalado o pacote dplyr.\ninstall.packages(\u0026quot;dplyr\u0026quot;) Sintaxe A sintaxe básica da função lag() é a seguinte:\ndplyr::lag(x, n = 1L, default = NULL, order_by = NULL, ...) x: um vetor.\nn: o número de posições para trás que queremos retroceder. Por padrão, é 1, o que significa que a função retornará o valor anterior à posição atual.\ndefault: valor padrão a ser retornado caso não haja valor anterior disponível. Por padrão, é definido como NULL, o que significa que, se não houver valor anterior, a função retornará NA.\norder_by: uma coluna usada para ordenar os dados. Se não for especificado, a função lag() assumirá que os dados estão na ordem em que aparecem no conjunto de dados.\nExemplo simples Considere o seguinte conjunto de dados:\ntab \u0026lt;- tibble::tibble( medida = c(1, 2, 1, 3, 4, 1, 2, 0) ) Neste exemplo, temos uma coluna chamada “medida” que contém valores numéricos. Vamos começar utilizando a função lag() para criar uma nova coluna chamada “lag_medida”, que armazenará o valor anterior de “medida” em cada linha:\ntab |\u0026gt; dplyr::mutate( lag_medida = dplyr::lag(medida) ) ## # A tibble: 8 × 2 ## medida lag_medida ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 NA ## 2 2 1 ## 3 1 2 ## 4 3 1 ## 5 4 3 ## 6 1 4 ## 7 2 1 ## 8 0 2 Observe que a primeira linha da coluna “lag_medida” é NA porque não há um valor anterior para a primeira observação.\nAgora, vamos criar uma nova coluna chamada “flag_aumentou”, que indicará se o valor atual de “medida” é maior que o valor anterior:\ntab |\u0026gt; dplyr::mutate( lag_medida = dplyr::lag(medida), flag_aumentou = medida \u0026gt; lag_medida ) ## # A tibble: 8 × 3 ## medida lag_medida flag_aumentou ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; ## 1 1 NA NA ## 2 2 1 TRUE ## 3 1 2 FALSE ## 4 3 1 TRUE ## 5 4 3 TRUE ## 6 1 4 FALSE ## 7 2 1 TRUE ## 8 0 2 FALSE A coluna flag_aumentou nos informa se a medida atual é maior que a medida anterior (TRUE) ou não (FALSE). Esse tipo de informação pode ser útil para identificar momentos de crescimento ou decrescimento em séries temporais.\nExemplo com dados agrupados Vamos agora explorar como utilizá-la em dados agrupados por indivíduo. Essa situação é comum quando temos uma série temporal para cada indivíduo em nosso conjunto de dados e desejamos analisar as variações dentro de cada grupo ao longo do tempo. Considere a tabela:\ntab \u0026lt;- tibble::tibble( individuo = rep(1:2, each = 4), # Dois indivíduos com 4 medidas cada medida = c(1, 2 , 1, 3, 4, 1, 2, 0) ) tab ## # A tibble: 8 × 2 ## individuo medida ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 ## 2 1 2 ## 3 1 1 ## 4 1 3 ## 5 2 4 ## 6 2 1 ## 7 2 2 ## 8 2 0 A seguir, utilizamos a função lag() para criar uma nova coluna com a medida anterior, agrupado por indivíduo.\ntab |\u0026gt; dplyr::group_by(individuo) |\u0026gt; dplyr::mutate( lag_medida = dplyr::lag(medida) ) ## # A tibble: 8 × 3 ## # Groups: individuo [2] ## individuo medida lag_medida ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 NA ## 2 1 2 1 ## 3 1 1 2 ## 4 1 3 1 ## 5 2 4 NA ## 6 2 1 4 ## 7 2 2 1 ## 8 2 0 2 Observe que a coluna lag_medida agora contém a medida anterior para cada indivíduo, respeitando os grupos criados pela coluna individuo.\nVamos dar um passo adiante e criar uma nova coluna chamada flag_aumentou, assim como fizemos no exemplo anterior:\ntab |\u0026gt; dplyr::group_by(individuo) |\u0026gt; dplyr::mutate( lag_medida = dplyr::lag(medida), flag_aumentou = medida \u0026gt; lag_medida ) ## # A tibble: 8 × 4 ## # Groups: individuo [2] ## individuo medida lag_medida flag_aumentou ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; ## 1 1 1 NA NA ## 2 1 2 1 TRUE ## 3 1 1 2 FALSE ## 4 1 3 1 TRUE ## 5 2 4 NA NA ## 6 2 1 4 FALSE ## 7 2 2 1 TRUE ## 8 2 0 2 FALSE Agora temos a coluna flag_aumentou indicando se a medida atual é maior que a medida anterior, dentro de cada grupo de indivíduo.\nA função lead(). Enquanto lag() retorna valores anteriores de uma coluna, a função lead() retorna valores seguintes de uma coluna em relação à posição atual.\nComo exercício, usando a função lead(), tente reproduzir os dois exemplos acima comparando o valor atual da coluna medida agora com o seu valor posterior (e não mais com o anterior).\nÉ isso! Dúvidas, sugestões e críticas, mande aqui nos comentários.\n","permalink":"https://blog.curso-r.com/posts/2023-07-24-dplyr-lead-lag/","tags":["programação","dplyr"],"title":"Explorando o pacote dplyr: lag/lead"},{"author":["Caio"],"categories":["tutoriais"],"contents":" Ano passado, o time da Curso-R se reuniu para conversar sobre a nova estrutura dos nossos cursos. Um dos pontos da pauta era a organização dos nossos repositórios que, ao longo dos anos, foi ficando cada vez mais difícil de manter.\nCada oferecimento de cada curso tem um repositório no GitHub, o que nos permite personalizar o conteúdo de uma turma sem afetar as outras. Mas essa granularidade cria um problema: o que fazer com o material que é comum a todas as turmas de um mesmo curso?\nO problema Imagine que existe uma turma A, que participou do curso R para Ciência de Dados III este ano, e uma turma B, que vai fazer o curso no ano que vem.\nNo passado nós copiávamos todo o conteúdo do repo A para o repo B, mas hoje em dia nós criamos os repositórios dos cursos com antecedência. Isso significa que qualquer alteração no material durante o oferecimento A precisaria ser propagada cuidadosamente para para o oferecimento B; infelizmente isso pode dar muito trabalho.\nA solução que encontramos foi criar um repositório main para cada curso, ou seja, um repo central que contém apenas os slides. Assim, os repos das turmas só precisam hospedar o conteúdo que muda de um oferecimento para o outro (exercícios, anexos, comentários, etc.) e qualquer alteração nos slides imediatamente se aplica a todos os repos satélites.\nTudo funcionou perfeitamente bem até que decidimos fazer a primeira mudança na ementa de um curso. O problema desta estratégia é qualquer alteração no main é propagada para o passado; logo, se durante o oferecimento B resolvermos fazer uma reestruturação grande no material, a turma A vai perder a versão dela e ficará sem as suas referências.\nO ideal seria ter uma maneira de atualizar os slides seletivamente, ou seja, manter a turma A na versão anterior do main e passar a B para a versão nova. Isso tudo sem quebrar os repos feitos com antecedência.\nSubmódulos É aí que entram os submódulos. De acordo com a documentação, eles são essencialmente repos dentro de outros repos, ou seja, clonamos um repo (o submódulo) dentro de um repo hospedeiro. Inception.\nComo eu posso usar submódulos para resolver meu problema com as turmas? Eu posso continuar usando o repo main para armazenar os slides, mas, ao invés de deixá-lo isolado, ele seria clonado como submódulo dentro do repo de cada turma. Neste arranjo, eu posso apontar o submódulo da turma A para a versão antiga do main e manter o da turma B apontado para a versão mais nova.\nPara começar a usar submódulos, eu recomendo executar os seguintes comandos Git, pois eles garantem que o seu ambiente estará adequadamente preparado:\ngit config --global submodule.recurse true git config --global push.recurseSubmodules check Agora, dentro do repositório desejado, eu posso rodar o comando abaixo para trazer o repo main como um submódulo:\ngit submodule add https://github.com/curso-r/main-r4ds-3.git materiais/ Isso é muito similar a fazer um clone normal! No caso, o repositório main-r4ds-3 será clonado na pasta materiais/ no repo hospedeiro.\nA partir de agora, eu posso usar sempre o repo hospedeiro, sem me preocupar com o main. Se eu fizer uma alteração na pasta materiais/, basta fazer um commit como qualquer outro! A atualização não vai para o repo da turma, mas sim para o main.\ncd materiais/ git add -A git commit -m \u0026quot;Alteração no main\u0026quot; git push Se voltarmos para a pasta um nível acima e executarmos git status, vamos ver que houve uma alteração em um arquivo chamado .gitmodules. Isso quer dizer que o submódulo foi atualizado no GitHub, não que ele foi atualizado no repo da turma.\nEste é o pulo do gato: podemos atualizar o main quantas vezes quisermos, mas as atualizações só serão propagadas para o repo da turma se aceitarmos a alteração.\ncd ../ git status git add -A git commit -m \u0026quot;Aceitando alterações do main\u0026quot; git push Isso permite que o repo de uma turma antiga como a A mantenha a sua referência a uma versão anterior do main ao mesmo passo que o repo das turmas novas podem ter suas referências atualizadas com facilidade. Se você quiser um exemplo de como fica uma referência no GitHub, dê uma olhada na demo de submódulos que fizemos na Curso-R: https://github.com/curso-r/202211-demo-submod.\nE isso é tudo! Se você quiser uma demonstração em vídeo, a referência que eu usei foi essa aqui do Redhwan Nacef. Se você tiver qualquer dúvida, faça um comentário aqui no post ou no nosso Discourse que nós vamos tentar ajudar o máximo possível. Até a próxima :)\n","permalink":"https://blog.curso-r.com/posts/2023-04-15-submodulos/","tags":["programação","git"],"title":"Submódulos no Git"},{"author":["Caio"],"categories":["tutoriais"],"contents":" Uma peculiaridade do R que assusta muita gente é a indexação de listas. Indexar vetores é simples, basta usar a sintaxe vetor[i], onde i é o número do elemento que você quer, mas as listas têm o problema do colchete duplo: lista[[i]]. Qual é a diferença entre os dois? Vamos tentar entender de uma vez por todas.\nIntrodução Se você não sabe o que é uma lista, o conceito é na verdade bastante simples: elas funcionam como vetores, mas aceitam objetos de vários tipos (incluindo sub-listas). Abaixo estou criando uma lista l e exibindo a sua estrutura com a função str().\nl \u0026lt;- list( objeto = \u0026quot;abc\u0026quot;, vetor = c(1, 2, 3), lista = list(TRUE, FALSE) ) str(l) #\u0026gt; List of 3 #\u0026gt; $ objeto: chr \u0026quot;abc\u0026quot; #\u0026gt; $ vetor : num [1:3] 1 2 3 #\u0026gt; $ lista :List of 2 #\u0026gt; ..$ : logi TRUE #\u0026gt; ..$ : logi FALSE Repare em algumas propriedades das listas:\nPodemos nomear os seus elementos, mas também podemos deixá-los sem nome nenhum. Isso também é possível com vetores, mas por algum motivo é mais comum ver listas nomeadas.\nDiferentemente de vetores, podemos colocar elementos de qualquer comprimento dentro de uma lista.\nUma lista pode ter sub-listas (e sub-sub-listas, sub-sub-sub-listas, etc.) Isso não afeta em nada o seu comportamento, mas vamos precisar aprender a fazer indexações profundas.\nindexação Acessar elementos de listas é um pouco mais complicado do que vetores. A base é a mesma: [i] retorna a i-ésima posição. O problema é que, nas listas, existe uma diferença entre a posição de um elemento e o elemento em si.\nA i-ésima posição, em uma lista, sempre é uma lista. Para pegar o i-ésimo elemento, precisamos usar [[i]]! Alternativamente, em listas nomeadas, podemos usar [\"nome\"] e [[\"nome\"]] (equivalente a $nome).\nl[1] #\u0026gt; $objeto #\u0026gt; [1] \u0026quot;abc\u0026quot; l[[1]] #\u0026gt; [1] \u0026quot;abc\u0026quot; l[\u0026quot;objeto\u0026quot;] #\u0026gt; $objeto #\u0026gt; [1] \u0026quot;abc\u0026quot; l[[\u0026quot;objeto\u0026quot;]] #\u0026gt; [1] \u0026quot;abc\u0026quot; l$objeto #\u0026gt; [1] \u0026quot;abc\u0026quot; É aqui que começa o nosso problema. É conceitualmente difícil de entender a diferença entre uma posição e um elemento, ou seja, quando usar [] e quando usar [[]]. Para tentar ilustrar melhor, vamos usar a metáfora da rua.\nMetáfora da rua Vamos pensar em listas como ruas. Quando usarmos [] obteremos um trecho da rua e quando usarmos [[]] obteremos a família da casa correspondente. Seguindo a lógica da metáfora, um elemento-vetor é uma casa com vários moradores e um elemento-lista é uma vila que pode ter várias casas dentro.\nSe quisermos pegar o trecho da rua que contém a primeira casa, podemos usar [i] ou [\"nome\"]. Ambos funcionam igual:\nl[1] l[\u0026quot;objeto\u0026quot;] Quando estamos falando de trechos da rua (segmentos da lista), podemos fazer seleções maiores. Abaixo, por exemplo, estamos selecionando as 2 últimas casas ou, alternativamente, todas as casas menos a primeira.\nl[2:3] l[c(\u0026quot;vetor\u0026quot;, \u0026quot;lista\u0026quot;)] l[-1] Se quisermos selecionar os integrantes de uma casa (um elemento da lista), aí precisamos usar [[i]], [[\"nome\"]] ou $nome. Note que a última opção é igual à seleção de colunas em um data frame.\nl[[1]] l[[\u0026quot;objeto\u0026quot;]] l$objeto O processo para selecionar a família da casa 2 é idêntico, não importa que a casa 2 contém um vetor e a casa 1 contém um objeto simples.\nl[[2]] l[[\u0026quot;vetor\u0026quot;]] l$vetor Idem para a casa 3, ou seja, a vila da nossa rua. O endereço é um só, mas dentro deste endereço temos uma nova casa 1 e uma nova casa 2.\nl[[3]] l[[\u0026quot;lista\u0026quot;]] l$lista Se quisermos acessar a casa 1 da vila, podemos fazer indexação profunda. Para isso, basta colocar no final da nossa expressão mais um [1]! Funcionaria exatamente igual se nós quiséssemos pegar o primeiro elemento do vetor l$vetor.\nl[[3]][1] l[[\u0026quot;lista\u0026quot;]][1] l$lista[1] Por fim, se quisermos pegar os integrantes da casa 1 da vila, basta adicionar um [[1]] na nossa indexação da vila analogamente ao que fizemos acima. No limite, não importa quantas sub-listas você tem, basta adicionar mais colchetes duplos conforme a necessidade.\nl[[3]][[1]] l[[\u0026quot;lista\u0026quot;]][[1]] l$lista[[1]] Deu para entender agora? Diga para gente nos comentários o que você achou desse exemplo e se você ficou com alguma dúvida. Até a próxima!\n","permalink":"https://blog.curso-r.com/posts/2023-04-14-indexacao/","tags":["r","programação"],"title":"Indexando listas no R"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Post atualizado! A versão original do post está aqui!\nJá precisou extrair dados de arquivos pdf? Bom, eu já. Eu trabalho com jurimetria e preciso extrair dados de diários de justiça, petições, sentenças, então já viu né…\nA primeira pergunta que você precisa fazer antes de ler um pdf é: o arquivo é digital ou digitalizado?\nSe for digital, significa que ele pode ser transcrito diretamente para vários formatos: texto, html, xml e até mesmo data.frames diretamente. Vamos usar esse exemplo de PDF digital\nSe estiver no desktop, é possível ver o documento abaixo:\nSe for digitalizado, você precisará passar um algoritmo de OCR (Optical Character Recognition) para extrair os dados. Provavelmente seu output nesse caso será sempre texto. Vamos usar esse exemplo de PDF digitalizado\nSe estiver no desktop, é possível ver o documento abaixo:\nObs: é possível que seu arquivo seja digitalizado, mas já com uma OCR passada no próprio arquivo. Nesse caso, você pode tratar o documento como digital.\nOs créditos dos pacotes abaixo vão todos para o Jeroen Ooms, um dos maiores autores de pacotes da comunidade R nos últimos dez anos. Sou fã desse cara!\nPacote {pdftools} para PDFs digitais Para instalar o {pdftools} no Windows e no Mac, basta rodar\ninstall.packages(\u0026quot;pdftools\u0026quot;) Para instalar no Linux, siga as instruções desse link.\nPDF para texto library(tidyverse) library(pdftools) pdf \u0026lt;- \u0026#39;caminho/para/pdf_digital.pdf\u0026#39; txt \u0026lt;- pdf_text(pdf) # imprimindo só os 500 primeiros caracteres da primeira página cat(str_trunc(txt[1], 500)) ## TJ/SP - Comarca de São Paulo ## Movimento Judiciário ## ## Referência: Janeiro de 2011 ## Foro: ADAMANTINA ## Unidade: 02 CUMULATIVA ## Planilha: CIVEL ## ## ## Dados da Unidade ## 1. Total de feitos em andamento 2756 ## 2. Precatórias 6 ## 3. Processos ## 3.1 Processos cíveis 2078 ## 3.1.1 De Conhecimento 1111 ## 3.1.2 De... PDF para HTML ou XML Muitas vezes queremos pegar estruturas no texto que dependem da posição dos elementos. Por exemplo, o texto em um PDF pode estar dividido em várias colunas. Para isso, o ideal seria transformar o arquivo em dados semi-estruturados como HTML ou XML, que separam os elementos do conteúdo do PDF em tags.\nInfelizmente, o pdftools não transforma em HTML nem em XML. Para soltar um HTML, vamos montar uma função que chama pdftohtml do poppler por command line.\npdf_html \u0026lt;- function(pdf) { infos \u0026lt;- pdf_info(pdf) # pega infos do pdf html \u0026lt;- tempfile(fileext = \u0026#39;.html\u0026#39;) # cria arquivo temporário # monta comando a ser executado. # no windows, você pode instalar o Poppler por aqui: # https://blog.alivate.com.au/poppler-windows/ command \u0026lt;- sprintf( \u0026#39;pdftohtml -f 1 -l %s -q -i -s -noframes %s %s\u0026#39;, infos$pages, normalizePath(pdf), html ) system(command) # roda comando e salva txt \u0026lt;- readr::read_file(html) # lê arquivo salvo file.remove(html) # remove arquivo temporário txt } Você pode brincar com o HTML usando o pacote xml2:\nlibrary(xml2) html \u0026lt;- pdf_html(pdf) html |\u0026gt; read_html() |\u0026gt; xml_find_all(\u0026#39;//div\u0026#39;) |\u0026gt; head() ## {xml_nodeset (6)} ## [1] \u0026lt;div id=\u0026quot;page1-div\u0026quot; style=\u0026quot;position:relative;width:1263px;height:892px;\u0026quot;\u0026gt; ... ## [2] \u0026lt;div id=\u0026quot;page2-div\u0026quot; style=\u0026quot;position:relative;width:1263px;height:892px;\u0026quot;\u0026gt; ... ## [3] \u0026lt;div id=\u0026quot;page3-div\u0026quot; style=\u0026quot;position:relative;width:1263px;height:892px;\u0026quot;\u0026gt; ... ## [4] \u0026lt;div id=\u0026quot;page4-div\u0026quot; style=\u0026quot;position:relative;width:1263px;height:892px;\u0026quot;\u0026gt; ... ## [5] \u0026lt;div id=\u0026quot;page5-div\u0026quot; style=\u0026quot;position:relative;width:1263px;height:892px;\u0026quot;\u0026gt; ... ## [6] \u0026lt;div id=\u0026quot;page6-div\u0026quot; style=\u0026quot;position:relative;width:1263px;height:892px;\u0026quot;\u0026gt; ... PDF para tabelas Use o {tabulizer}! Apesar de depender do polêmico {rJava} (que é um pacote chato de instalar e configurar) o {tabulizer} é capaz de extrair os dados diretamente para tabelas, de forma simples e intuitiva.\nPara instalar o {tabulizer}, siga as instruções dessa página. Já adianto que pode não ser uma tarefa fácil, principalmente por conta do {rJava}.\nExemplo: Uma vez montei esse código para estruturar um pdf contendo gastos em obras públicas. Além de usar o {tabulizer}, usei os pacotes usuais do {tidyverse} e o {janitor} para arrumar os nomes das colunas.\nlibrary(tabulizer) Vamos usar esse pdf de exemplo.\nSe estiver no desktop, é possível ver o documento abaixo:\n# No meu pc demorou 40 segundos. tab \u0026lt;- extract_tables(\u0026#39;caminho/para/pdf_compras.pdf\u0026#39;) Agora veja a magia do {tidyverse} e do pacote {janitor} posta em prática:\ntab_tidy \u0026lt;- tab |\u0026gt; # transforma matrizes em tibbles map(as_tibble) |\u0026gt; # empilha bind_rows() |\u0026gt; # arruma nomes a partir da primeira linha janitor::row_to_names(1) |\u0026gt; janitor::clean_names() |\u0026gt; # tira espaços extras mutate(across(everything(), str_squish)) A tabela abaixo mostra as primeiras cinco linhas do resultado.\nPacote {tesseract} para PDFs digitalizados O {tesseract} é uma biblioteca escrita em C e é uma das mais famosas ferramentas abertas para extração de textos a partir de imagens. O pacote em R de mesmo nome serve para usar essa biblioteca pelo R sem causar dores de cabeça.\nPara nossa felicidade, hoje em dia os pacotes {pdftools} e {tesseract} estão integrados. Dessa forma, podemos utilizar a função pdftools::pdf_text_ocr() para extrair o texto de um PDF usando OCR.\nVamos usar esse pdf de exemplo.\nSe estiver no computador, é possível ver o documento abaixo:\npdf \u0026lt;- \u0026#39;caminho/para/pdf_digitalizado.pdf\u0026#39; txt \u0026lt;- pdf_ocr_text( pdf = pdf, # caminho do arquivo page = 1 # índice da página ) ## Converting page 1 to pdf_digitalizado_1.png... done! Se o PDF tiver mais páginas, basta colocar os índices no texto.\n# imprimindo só os 300 primeiros caracteres do resultado cat(str_trunc(txt, 300)) ## This is a sample document to test the PDF Image+Text OCR Engine. ## This is a sample document to test the PDF Image+Text OCR Engine. ## This is a sample document to test the PDF Image+Text OCR Engine. ## This is a sample document to test the PDF Image+Text OCR Engine. ## This is a sample document to test the... Wrap-up Se seu pdf for digital, use pdftools::pdf_text(). Se seu pdf for digitalizado, pdftools::pdf_text_ocr(), que usa o pacote {tesseract} por trás. Existem alternativas úteis, como o Poppler em linha de comando e o pacote {tabulizer}. É isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2023-04-12-ocr/","tags":["pdf","pacotes","ocr"],"title":"PDF e OCR (atualizado!)"},{"author":["Caio"],"categories":["tutoriais"],"contents":" Recentemente a Beatriz Milz trouxe para o Slack da Curso-R uma dúvida intrigante:\n[…] Quando ordeno com arrange uma coluna, tem um valor que começa com A que aparece no final da lista!\nO exemplo dela envolvia ordenar uma coluna de textos de uma tabela. A ordenação funcionava normalmente com sort(), mas não com arrange(). Veja a demonstração a seguir:\n# Tabela exemplo df \u0026lt;- tibble::tibble(bebida = c( \u0026quot;Cerveja\u0026quot;, \u0026quot;Cachaça\u0026quot;, \u0026quot;Água\u0026quot;, \u0026quot;Vinho\u0026quot;, \u0026quot;Gim\u0026quot; )) # Tudo certo por aqui sort(df$bebida) #\u0026gt; [1] \u0026quot;Água\u0026quot; \u0026quot;Cachaça\u0026quot; \u0026quot;Cerveja\u0026quot; \u0026quot;Gim\u0026quot; \u0026quot;Vinho\u0026quot; # Água fica por último! dplyr::arrange(df, bebida) #\u0026gt; # A tibble: 5 × 1 #\u0026gt; bebida #\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 Cachaça #\u0026gt; 2 Cerveja #\u0026gt; 3 Gim #\u0026gt; 4 Vinho #\u0026gt; 5 Água O nosso principal suspeito era o mesmo de qualquer problema com strings: encoding ou, em bom português, codificação. Contudo, desta vez ele não é o culpado; os caracteres do texto estão sendo interpretados e exibidos corretamente na saída de ambos os comandos, indicando que a causa da ordenação incorreta era outra.\nEm busca de uma resposta, resolvi ler a documentação da função arrange(). Para a minha surpresa, descobri que ela tem um argumento .locale cujo valor por padrão é “o locale \"C\"”… Mas, o que isso significa? Normalmente vemos problemas de locale quando lidamos com tempo, porque essa é a opção que determina o formato de exibição das datas (DD/MM/AAAA no Brasil, MM/DD/AAAA nos EUA, etc.). Será que ela poderia ter alguma coisa a ver com a ordenação de textos?\nSeguindo as pistas na documentação da arrange(), eventualmente cheguei na função stringi::stri_opts_collator(), que ajusta as opções do ICU Collator. E foi aí que tudo fez sentido.\nCollation Collation é um termo em inglês que descreve a compilação e ordenação de qualquer tipo de informação. A (surpreendente!) realidade é que cada país e idioma tem regras diferentes de ordenação alfabética, então é necessário escolher qual método de collation o R vai usar através do locale.\nPara ilustrar a função do locale, imagine um problema mais simples: queremos que o R leia adequadamente uma data em alemão. Obviamente, ele não vai conseguir:\nlubridate::dmy(\u0026quot;6. März 2023\u0026quot;) #\u0026gt; Warning: All formats failed to parse. No formats found. #\u0026gt; [1] NA Para corrigir isso, precisamos especificar o argumento locale, indicando para o R que o conteúdo está escrito em alemão (\"de_DE\" para alemão da Alemanha):\nlubridate::dmy(\u0026quot;6. März 2023\u0026quot;, locale = \u0026quot;de_DE\u0026quot;) #\u0026gt; [1] \u0026quot;2023-03-06\u0026quot; O chocante é que o mesmo vale para a ordem alfabética. Nós geralmente não percebemos que tem algo errado com o locale da collation porque as regras de ordenação são muito parecidas em todos os países, mas existem diferenças sutis que causam problemas como o da Bea.\nO programa que está tomando essas decisões de locale por trás dos panos se chama ICU; esta biblioteca do C é a base do stringi, o motor por trás do stringr e da arrange(). Como você pode imaginar, o locale padrão da ICU é o da linguagem C de programação, que coloca letras acentuadas no fim do alfabeto.\nSendo assim, podemos resolver a questão do arrange() especificando o nosso locale (\"pt_BR\" para português do Brasil):\ndplyr::arrange(df, bebida, .locale = \u0026quot;pt_BR\u0026quot;) #\u0026gt; # A tibble: 5 × 1 #\u0026gt; bebida #\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 Água #\u0026gt; 2 Cachaça #\u0026gt; 3 Cerveja #\u0026gt; 4 Gim #\u0026gt; 5 Vinho É muito interessante ler a documentação do ICU sobre collation, pois ela deixa muito claro que é absolutamente impossível criar um locale que atenda às necessidades de todos os países:\nEm lituano, o “y” fica entre o “i” e o “k”. No espanhol tradicional, “ch” é tratado como uma única letra entre o “c” e o “d”. Em dinamarquês, “Å” é considerada uma letra separada que fica depois do “Z”. Na Suécia, “v” e “w” são consideradas variações de uma mesma letra. Em dicionários alemães, “öf” vem antes de “of”, mas em listas telefônicas a ordem preferida é a contrária. O bom é que isso também esclarece o porquê da sort() funcionar, mas a arrange() não. Enquanto a segunda usa o locale \"C\" por padrão, a primeira usa o locale americano (\"en_US\" para inglês dos EUA)! Apesar de o locale americano nos causar problemas com datas, ele é muito parecido com o brasileiro na ordem alfabética e essencialmente ignora os acentos durante a collation.\nResumo A função arrange() pode causar problemas na hora de ordenar textos em português. Isso ocorre porque o locale de collation padrão coloca todas as letras acentuadas no final do alfabeto, algo muito pouco usual no nosso idioma. A solução é especificar o argumento .locale = \"pt_BR\" para que ela use o locale apropriado ao nosso alfabeto.\n","permalink":"https://blog.curso-r.com/posts/2023-03-06-ordenando-strings/","tags":["r","programação"],"title":"Ordenando strings no R"},{"author":["Caio"],"categories":["tutoriais"],"contents":" Neste post eu vou desviar um pouco do tópico central do blog da Curso-R. Hoje não vamos falar de R ou Python ou qualquer outra linguagem de ciência de dados; hoje vamos falar sobre CSS e talvez aprender uma lição ou outra no caminho…\nO hexágono abaixo é um gerador de números aleatórios. Clique nele uma vez para obter um número de 1 a 20 e clique novamente para apagar o último resultado.\n01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 Como funciona Quem tem costume de jogar RPG sabe que o D20 é um dos dados mais importantes de todos. Se você procurar o termo “D20” no Google, ele vai inclusive mostrar pra você uma animação bonitinha que rola um um dado de 20 lados.\nMas qual é a graça de usar o Google para rolar um D20? Eu queria fazer o meu próprio rolador e, como um desafio extra, eu resolvi não usar nenhum JavaScript para isso.\nEu não gosto de JavaScript Se você não sabe o que é JavaScript ou por que isso importa, me permita fazer um breve resumo: JS é uma das linguagens de programação mais populares do mundo (ela é usada em 98% de todos os websites!) e ele permite que o seu navegador faça coisas incríveis como logar no seu Gmail ou assistir os vídeos da Curso-R no YouTube.\nO problema é que o JS é tão sensacional que as pessoas começaram a usar ele em todo lugar. Apesar de o JS ser de fato necessário para criar páginas dinâmicas, ele não é necessário na maioria dos lugares; o termo “crise de obesidade dos websites” começou a ser usado para descrever o fato de que, hoje em dia, a maior parte dos sites focam muito mais no visual e em dinamismos desnecessários do que no conteúdo e na acessibilidade.\nEu poderia ficar aqui falando sobre como o JS deixa as páginas mais inseguras, ele normalmente nem é necessário e é um custo absurdo para quem tem uma conexão lenta de internet, mas acho que você já entendeu o meu ponto. Eu queria fazer um rolador de dados (ou seja, um gerador de números aleatórios) leve e sem JavaScript.\nCSS salvou o dia Além do JS, um dos maiores componentes do desenvolvimento web é o CSS, que descreve a aparência das coisas na página. Mas o que isso tem a ver com o gerador de números? Bem, primeiramente, o CSS é super leve e 100% seguro, mas ele também tem uma funcionalidade ignorada por muitos chamada Animations Animations.\nEu quase não acreditei quando eu li pela primeira vez o post Creating randomness with pure CSS. O resumo do artigo é que você pode usar CSS para mover uma imagem de um lado para o outro por trás de uma fenda virtual e parar ela só quando o usuário clica em um botão. Se a sua imagem for um monte de cartas de baralho lado-a-lado, apenas uma delas vai aparecer quando a animação parar.\nA grande sacada é que, se a animação for rápida o suficiente, as pequenas variações nos cliques do usuário vão efetivamente fazer com a que a fenda mostre uma carta aleatória cada vez que ela for clicada!\nEsse era o substituto do JS que eu precisava. Agora tudo que eu precisava fazer era aplicar esse método a uma lista de números ao invés de uma imagem. Demorou um tempo, mas eu vasculhei a internet em busca de uma solução e eventualmente eu encontrei ela no caça-níqueis sem JS do iokaravas.\nO código dele recebe três listas de números e anima elas para cima e para baixo até que elas caiam na combinação configurada por quem fez o programa. Apesar de ser determinístico, isso poderia ser integrado com a técnica do Creating randomness para criar exatamente o que você vê agora no topo desta página.\nConclusão Resumindo: no gerador de números aleatórios acima há, na verdade, uma lista invisível de números subindo e descendo que para quando você clica nela. A aleatoriedade vem do fato de que ela está se movendo rápido demais para qualquer pessoa ser capaz de cronometrá-la.\nAlguns fatos interessantes sobre o D20:\nEle não usa JavaScript nenhum. Você pode verificar isso por conta própria inspecionando o código desta página com a aba Network do seu navegador.\nEle é leve. Enquanto o rolador de dados do Google pesa 2,6MB e carrega em 1,6s, a versão original deste aqui pesa 7,1kB (~360x mais leve) e carrega em 50ms (~30x mais rápida).\nEle é justo. Eu fiz um teste Qui-quadrado com 1000 amostras em intervalos de tempo aleatórios e o dado de fato é justo (p = 0,7575).\nÀs vezes não precisamos das ferramentas mais chiques para fazer algo legal! Até a próxima :)\n","permalink":"https://blog.curso-r.com/posts/2023-01-10-css-aleatorio/","tags":["web","probabilidade","programação"],"title":"Como gerar números aleatórios com CSS"},{"author":["Caio"],"categories":["tutoriais"],"contents":" Quem acompanha o nosso blog talvez se lembre que ano passado eu fiz uma série de posts resolvendo todos os problemas do Advent of Code de 2021 em R. Este ano eu voltei para a festa, mas com um presente para todo mundo 🎁\nO evento Se você não está por dentro, o Advent of Code é um Calendário do Advento para quem programa. Entre 1º de dezembro e o Natal de cada ano, os organizadores vão disponibilizando 1 problema de programação por dia e o objetivo é chegar em 25 de dezembro com todos os 25 exercícios resolvidos.\nAno passado, no que eu chamei de Advent of R, eu me desafiei a de fato entrar no ritmo e resolver todos os problemas no dia em que eles fossem publicados, em R e documentando o processo no blog. Eu até consegui, mas foi extremamente difícil! Este ano eu vou pegar mais leve e fazer as coisas com mais calma.\nMas eu não queria deixar o evento passar em branco… Em 2022, minha contribuição para o Advent of R é um pacote para ajudar a comunidade toda 🎉\nO pacote {aor} O objetivo do {aor} é ajudar todo mundo que programa R a resolver o Advent of Code usando essa linguagem maravilhosa. Ele tem algumas funções que ajudam a baixar os desafios e as suas entradas o mais rápido possível.\nInstalação Você pode instalar a versão em desenvolvimento do {aor} do GitHub usando o comando abaixo:\n# install.packages(\u0026quot;devtools\u0026quot;) devtools::install_github(\u0026quot;clente/aor\u0026quot;) Exemplo O uso básico do {aor} gira em torno das funções day_start() e day_continue(). Por padrão, ambas as funções baixam os enunciados do dia atual, mas vou usar um dia fixo (01/12/2022) para ficar mais fácil de entender:\n# Começar o problema do dia 01/12/2022 na pasta aoc2022/ aor::day_start(\u0026quot;2022-12-01\u0026quot;, \u0026quot;aoc2022/\u0026quot;) #\u0026gt; ✔ Fetched title. #\u0026gt; ✔ Fetched puzzle. #\u0026gt; ✔ Fetched input. # Arquivos criados fs::dir_tree(\u0026quot;aoc2022/\u0026quot;) #\u0026gt; aoc2022/ #\u0026gt; └── 01_calorie_counting #\u0026gt; ├── input.txt #\u0026gt; └── puzzle.R Essa é a cara do arquivo aoc2022/01_calorie_counting/puzzle.R (note que eu estou omitindo a maior parte das linhas do enunciado para que a saída não fique muito longa):\n# --- Day 1: Calorie Counting --- # # Santa\u0026#39;s reindeer typically eat regular reindeer food, but they need a # lot of [magical energy](/2018/day/25) to deliver presents on Christmas. # For that, their favorite snack is a special type of *star* fruit that # only grows deep in the jungle. The Elves have brought you on their # annual expedition to the grove where the fruit grows. # # ... # # Find the Elf carrying the most Calories. *How many total Calories is # that Elf carrying?* # Your input can be found on the file below: input \u0026lt;- \u0026quot;aoc2022/01_calorie_counting/input.txt\u0026quot; # Once you\u0026#39;re done with part 1, run the following line to fetch part 2: aor::day_continue(\u0026quot;2022-12-01\u0026quot;, \u0026quot;aoc2022/01_calorie_counting/puzzle.R\u0026quot;) Quando você resolver a parte 1 do exercício, você pode rodar a última linha do arquivo para automaticamente baixar a parte 2 ali mesmo!\naor::day_continue(\u0026quot;2022-12-01\u0026quot;, \u0026quot;aoc2022/01_calorie_counting/puzzle.R\u0026quot;) #\u0026gt; ✔ Fetched puzzle. E essa é a cara do arquivo logo depois de rodar day_continue() (de novo omitindo a maior parte das linhas):\n# --- Day 1: Calorie Counting --- # # Santa\u0026#39;s reindeer typically eat regular reindeer food, but they need a # lot of [magical energy](/2018/day/25) to deliver presents on Christmas. # For that, their favorite snack is a special type of *star* fruit that # only grows deep in the jungle. The Elves have brought you on their # annual expedition to the grove where the fruit grows. # # ... # # Find the Elf carrying the most Calories. *How many total Calories is # that Elf carrying?* # Your input can be found on the file below: input \u0026lt;- \u0026quot;aoc2022/01_calorie_counting/input.txt\u0026quot; # Once you\u0026#39;re done with part 1, run the following line to fetch part 2: aor::day_continue(\u0026quot;2022-12-01\u0026quot;, \u0026quot;aoc2022/01_calorie_counting/puzzle.R\u0026quot;) # --- Part Two --- # # By the time you calculate the answer to the Elves\u0026#39; question, they\u0026#39;ve # already realized that the Elf carrying the most Calories of food might # eventually *run out of snacks*. # # ... # # Find the top three Elves carrying the most Calories. *How many Calories # are those Elves carrying in total?* Espero que vocês aproveitem o Advent of R com o {aor}! Boa sorte e boas festas 🎄\n","permalink":"https://blog.curso-r.com/posts/2022-12-01-advent-of-r-2022/","tags":["advent-of-r","pacote"],"title":"{aor}: pacote do Advent of R!"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Mais de 5 anos atrás, escrevi um blog post criando um pacote do zero em minutos. Desde então, o pacote {usethis} emergiu do {devtools}, se tornando uma das ferramentas mais queridas da comunidade R. Por isso, fiz uma versão atualizada do mesmo post, utilizando as melhores práticas dos dias de hoje.\nAqui, veremos como fazer um pacote em R muito, muito rápido. Tirei várias coisas que costumo fazer nos pacotes, com dor no coração, tudo pela velocidade, mantendo só o essencial.\nDuas restrições são\nO pacote precisa ficar disponível no GitHub. O pacote precisa ter pelo menos uma função. Essa é a solução que eu acho mais segura e rápida. Você também pode usar o próprio RStudio para criar pacotes ou clonar coisas do github, mas isso pode dar mais trabalho.\nPasso 1: Crie um projeto de pacote no R Use a função usethis::create_package(\"nomeDoPacote\") Escreva o nome do seu pacote. O nome do pacote não pode ter espaços, underline (_) nem hífen (-) nem começar com números. Ao fazer isso, o {usethis} abrirá uma nova tela do RStudio, já na estrutura de pacote!\nNa tela em que você escreveu usethis::create_package(), o console ficará assim:\nObs: cuidado para não criar um pacote dentro de um repositório Git (por exemplo, dentro de um projeto que você já está trabalhando). Isso pode dar problemas de conflito.\nPasso 2: Configurar Git Rode usethis::use_git() e confirme nas duas perguntas que serão feitas a você\nSua sessão será reiniciada, e o Git está configurado 🎉\nPasso 3: configure o GitHub Rode usethis::use_github() e confirme a pergunta que será feita. Aqui estou assumindo que você tem o Git/GitHub configurado na sua máquina. Se não estiver, siga o fluxo do livro Happy Git With R.\nSe tudo der certo, seu pacote já estará disponível no seu GitHub!\nPasso 4: Crie sua função Exemplo:\n#\u0026#39; Soma 2 #\u0026#39; #\u0026#39; Recebe um vetor de números e retorna um vetor de números somando dois #\u0026#39; #\u0026#39; @param x vetor de números. #\u0026#39; #\u0026#39; @export soma_2 \u0026lt;- function(x) { x + 2 } Crie a função dentro de um arquivo com extensão .R na pasta R . Se quiser, você pode usar a função usethis::use_r() . As informações que começam com #' acima da função servem para documentar. Nesse caso, a primeira linha é o título a segunda linha é a descrição a parte que começa com @param descreve o que é o parâmetro de entrada a parte que começa com @export diz para o pacote que essa função deve estar disponível para o usuário quando ele rodar library(nomeDoPacote). Passo 5: document, commit e push! Rode devtools::document(). Commite suas alterações. Dê um push! Se não saba o que é commitar e pushar, veja o artigo do Athos sobre o uso do git e do GitHub.\nPasso 6: Instalar o pacote em outra máquina Mande o nome do seu usuário do GitHub e o nome do seu pacote para sua migue. Peça para ela rodar: remotes::install_github(\u0026#39;usuario/nomeDoPacote\u0026#39;) Agora ela poderá usar sua função! library(nomeDoPacote) soma_2(1:10) # [1] 3 4 5 6 7 8 9 10 11 12 Você também pode ver o help da função com ?soma_2:\nFIM!\nConclusões Agora você não tem desculpa para não empacotar suas soluções em R. Esse tutorial é incompleto! Para acessar mais detalhes, veja http://r-pkgs.had.co.nz, elaborado por você sabe quem. Outras pequenas dicas práticas Use sempre devtools::check() para checar se seu pacote está 100% bem construído. Use usethis::use_package() para usar funções de outros pacotes. Sempre use os :: para chamar as funções e nunca rode library() ou require() dentro de um pacote. Use usethis::use_mit_license() para adicionar um arquivo LICENSE ao seu pacote. Use usethis::use_data() para adicionar dados ao seu pacote. Use usethis::use_vignettes() para escrever um tutorial sobre seu pacote, igual a esse do dplyr, por exemplo. É isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2022-10-13-pacote-miojo-2022/","tags":["miojo"],"title":"Pacotes miojo - como fazer um pacote no R em 3 minutos [atualizado]"},{"author":["Beatriz Milz"],"categories":["Tutorial"],"contents":" Introdução As vezes precisamos usar alguma senha/Token/Key para rodar um código, como por exemplo ao usar uma API que solicita autenticação. Mas isso significa que devemos escrever a senha no próprio script .R, por exemplo?\nNão! Inclusive isso é um pouco perigoso. Imagina armazenar a senha para um serviço pago, esquecer e disponibilizar o script no GitHub? É receita para receber uma notificação do cartão de crédito cobrando algo que nem usamos… É, já ouvi casos assim!\nEntão neste post trago algumas dicas para evitar salvar as senhas nos scripts quando estiver programando.\nVariáveis de ambiente Uma forma de fazer isso é com as variáveis de ambiente (environment variables). Não vou detalhar o conceito de ambientes mas deixei um link nas referências, ao final do post.\nSimplificando, as variáveis de ambientes são objetos que são carregados ao iniciar a sessão do R. Então eles ficarão disponíveis para uso, apesar de não aparecer no painel Environment do RStudio.\nUma forma de saber quais são as nossas variáveis de ambiente é usando a função Sys.getenv() sem argumentos. Atenção, várias variáveis de configuração irão aparecer no output e não recomendo alterá-las.\nA função Sys.setenv() permite que você salve a senha/token nas variáveis de ambiente. Cuidado, rode essa etapa no console e não salve no script (pois justamente queremos não armazenar as senhas no script, certo?).\nSys.setenv(MINHA_SENHA = 1234) Para buscar o valor salvo na variável, pode usar a função Sys.getenv(). Essa função podemos usar no nosso script sem problemas!\nSys.getenv(\u0026quot;MINHA_SENHA\u0026quot;) ## [1] \u0026quot;1234\u0026quot; Indo além com a ajuda do usethis As variáveis de ambiente podem ser vinculadas ao .Rproj ativo, ou ao usuário do computador. Elas ficam salvas no arquivo .Renviron. Para abrir o arquivo e ver as variáveis de ambiente salvas, você pode usar a função usethis::edit_r_environ():\n# Abre as variáveis de ambiente para o usuário do computador usethis::edit_r_environ(scope = \u0026quot;user\u0026quot;) #\u0026gt; • Modify \u0026#39;/Users/beatrizmilz/.Renviron\u0026#39; #\u0026gt; • Restart R for changes to take effect # Abre as variáveis de ambiente para o projeto ativo usethis::edit_r_environ(scope = \u0026quot;project\u0026quot;) #\u0026gt; ✔ Setting active project to \u0026#39;/Users/beatrizmilz/Documents/GitHub/blog-en\u0026#39; #\u0026gt; • Modify \u0026#39;.Renviron\u0026#39; #\u0026gt; • Restart R for changes to take effect O padrão para armazenar variáveis neste arquivo é:\nNOME_VAR=\u0026#39;SENHAAQUI12345\u0026#39; Atenção! Depois de alterar o arquivo .Renviron, é necessário reiniciar o R para que as mudanças sejam consideradas. Isso porque esse arquivo é carregado ao iniciar o R.\nAtenção 2: Caso você use um arquivo .Renviron vinculado ao seu projeto .Rproj, cuidado ao subir esse arquivo para o GitHub, é perigoso pois pessoas poderão ver suas senhas. Se o seu repositório for público, é essencial que você adicione ele no .gitignore, assim ele será ignorado pelo Git. A função usethis::use_git_ignore() pode nos ajudar nisso:\nusethis::use_git_ignore(\u0026quot;.Renviron\u0026quot;) #\u0026gt; ✔ Adding \u0026#39;.Renviron\u0026#39; to \u0026#39;.gitignore\u0026#39; Solicitar a senha E quando queremos que a pessoa usuária possa sempre escrever a senha? Existe uma função para isso: rstudioapi::askForPassword(). Podemos inclusive escrever a frase que será mostrada ao perguntar a senha. A senha digitada será retornada como um texto, e podemos salvar em um objeto para utilizar posteriormente no que for necessário.\nsenha_informada \u0026lt;- rstudioapi::askForPassword( \u0026quot;Escreva sua senha por favor!\u0026quot; ) Pop up feito pela função askForPassword.\nGitHub Actions E quando precisamos usar senhas em códigos rodados com GitHub Actions (GHA)? Se você não conhece nada sobre GHA, eu tenho alguns posts sobre isso!\nQuando estamos usando GHA, podemos adicionar senhas na sessão SECRETS do GitHub. Assim ninguém (exceto você, quando cria um SECRETS), verá a senha/credenciais.\nIsso fica nas configurações do repositório, e o link usa esse padrão:\nhttps://github.com/SEU-USUARIO/SEU-REPOSITORIO/settings/secrets/actions Exemplo de um repositório privado meu:\nPrint screen da página de secrets.\nTambém é preciso adaptar o código do action para buscar essa variável no Secrets. Exemplo de código:\n- name: Execute Script env: GITHUB_PAT: ${{ secrets.GITHUB_PAT }} NOME_VAR: ${{ secrets.NOME_VAR }} run: | Rscript \u0026quot;seu_script.R\u0026quot; Com isso é possível acessar essas variáveis com senha, no script usando a função Sys.getenv('NOME_VAR') que mostrei acima.\nMais camadas de segurança Para mais camadas de segurança, recomendo a leitura da documentação do pacote httr2, no trecho sobre Secret management. Nesse trecho, além de apresentar a opção utilizando a função usethis::edit_r_environ(), também é apresentado as funções httr2::secret_encrypt() e htt2::secret_decrypt().\nEu ainda não testei essas funções pois o pacote httr2 é bem recente. Segundo o autor do pacote (o Hadley Wickham), com essas funções é possível usar criptografia para lidar com as senhas/tokens.\nConclusão Esperamos que este post seja interessante para você, e que as suas senhas estejam sempre protegidas! Bons estudos e até a próxima!\nReferências Environments/Ambientes - Hands-On Programming with R\nSecret management - Documentação do pacote httr2\nAgradecimentos Julio Trecenti, pela leitura prévia e sugestões feitas no texto.\nTive a ideia de escrever esse post ao responder uma pergunta feita pela aluna Mari Ribeiro, em um curso da Curso-R. Obrigada Mari!\n","permalink":"https://blog.curso-r.com/posts/2022-09-13-senha-no-script/","tags":["Boas práticas"],"title":"Como usar senhas sem escreve-las nos scripts"},{"author":["Julio","Flavia E. Rius","Beatriz"],"categories":["Divulgação"],"contents":" A rstudio::conf(2022) acabou, mas não saiu de nós. Ficaram as memórias, as coisas novas para estudar, e as novas amizades. Nesse post, falamos um pouco de nossas experiências pessoais na conferência. O texto conta com a participação especial da Flávia E. Rius, que particiou do evento comigo (Julio) e com a Beatriz. Confira!\nFlávia E. Rius Twitter: @flaviaerius\nBlog: https://flaviaerius.netlify.app\nParticipei da rstudio::conf() pela primeira vez neste ano de 2022, e última, já que ano que vem será posit::conf(), e vou contar sobre a minha experiência neste post. Pra deixar todos no contexto, vou me apresentar. Eu sou a Flávia E. Rius, cientista de dados na Mendelics/meuDNA, empresas de análise genômica (sim, do DNA!) Uso o R para a maior parte do meu trabalho, principalmente em limpeza, processamento e estruturação de dados, modelos, e criação de reports e relatórios em Rmarkdown (era pré-Quarto.) Além disso, dou mentorias online para pessoas do mundo todo em R, bioinformática e estatística. Como a Bea, apliquei para a bolsa de Diversity Scholar para participar da conferência, e fui escolhida para recebê-la (yay!)\nEssa foi a minha primeira conferência de R, e também a primeira relacionada a Data Science fora do mundo da bioinformática. Foi uma experiência completamente nova.\nEscolhi o workshop de Machine Learning With Tidymodels, que aconteceu nos primeiros dois dias do congresso, das 9 às 17h. Os outros dois dias consistiram em palestras acontecendo paralelamente em diferentes salas, e a tarefa de escolher a quais participar não foi fácil.\nA primeira impressão que tive ao chegar foi do hotel, gigantesco, e com uma estrutura pra congressos que eu nunca tinha visto antes. O hotel fica localizado numa espécie de cidade turística artificial, chamada National Harbor, no estado de Maryland.\nWorkshop: Machine Learning With Tidymodels Confesso que fui descobrir uma migalha do que eram os tidymodels uma semana antes do workshop. Escolhi ele porque queria aprender mais sobre machine learning (ML), e, apesar de saber que a ferramenta preferida para a aplicação de ML é scikit-learn no python, achei interessantíssimo aprender no R, linguagem que já é confortável pra mim.\nTidymodels são ferramentas dentro de diversos pacotes que, juntas, formam uma maneira nova de criar modelos estatísticos e de ML no R. Dentre os pacotes estão: recipes, rsample, parsnip, workflows, vetiver, etc..\nResumi em tópicos os pontos altos e não tão altos do workshop abaixo.\nPontos altos do workshop: Modularidade: com os tidymodels cada etapa é um módulo que fica guardado em uma variável e pode ser aplicada para novos dados. Por exemplo a receita do pacote recipes e o workflow do pacote workflows. Isso é feito no intuito de guardar todo o processo aplicado nos dados de treino, para, no final, repetir para os dados de teste, ou mesmo reaplicar um pré-processamento e feature engineering para um novo set de dados, por exemplo.\nProfessores: Max Kuhn, Julia Silge e Davis Vaughan foram professores excepcionais, com destaque para a Julia, que é fenomenal na didática. Ela explica de uma forma muito fácil e completa, para que todos possam entender. Todos trouxeram sua expertise, um complementando o outro sobre os as dúvidas que iam surgindo (aliás, o grupo de alunos também era muito bom, com dúvidas bastante pertinentes e complementares ao workshop.) Os Teaching Assistants (TAs) foram muito bons em ajudar com as dúvidas, e mesmo os professores, quando não estavam dando a aula, atuaram como TAs, ajudando com as dúvidas dos alunos.\nvetiver: em particular, essa ferramenta fez meus olhos brilharem, apesar de ter sido mostrada bem brevemente. É uma maneira de construir uma API a partir do R para deploy de modelos de machine learning, que já resulta na página da API, e pode até gerar a Dockerfile pra deploy da API em plataformas como o Google App Engine ou AWS.\nPontos não tão altos do workshop: Conceitos de ML: foram usados exemplos de árvores de decisão (decision trees) e random forests, mas faltou explicar com mais detalhe o que são, e em quais situações podem ser usados. Eu relevei absolutamente essa parte porque sei que não é fácil passar todo o conteúdo necessário, ainda mais em um workshop de apenas dois dias com tanto conteúdo sobre ML e aplicações na ferramenta de foco, os tidymodels. Se você tiver interesse em saber mais sobre o workshop, todo o conteúdo está disponível online:\nSlides e pré-requisitos: todos os slides foram feitos com Quarto, o que significa que são interativos, ou seja, é possível copiar código e clicar nos links dos slides.\nMaterial complementar: são informações extras sobre alguns slides, que também podem ser acessadas pelo ícone i vermelho no canto superior direito.\nLots of stickers (my first ones!) and learning in the first day of #rstudioconf2022 .\nLoving the ML with Tidymodels workshop, and the amazing people around here! pic.twitter.com/JAckV0atWw — Flávia E. Rius (@flaviaerius) July 26, 2022 Palestras Os dias 3 e 4 da conferência consistiram em palestras, sendo elas quatro keynotes, que são as palestras principais, grandes, sem nenhuma outra acontecendo em paralelo.\nA primeira keynote foi sobre tidymodels, com a Julia Silge e o Max Kuhn, e também contou com o super anúncio da mudança de nome da RStudio para Posit pelo Hadley Wickham e o JJ Allaire, fundador e CEO da RStudio (agora Posit.)\nA segunda foi com o Joe Cheng, CTO da RStudio. Ele contou a história sobre como o Shiny surgiu, e trouxe novidades como o Shiny para python e a versão no code do Shiny para R, usando apenas ferramentas de apontar e clicar.\nA terceira, com a Mine Çetinkaya e a Julia Stewart, falou sobre as maravilhas do Quarto. Uma ferramenta que me chamou muito a atenção é o multiplex, em que o link dos seus slides construídos em Quarto muda os slides conforme você muda na sua apresentação! Muito mágico! Claro que tem muita coisa interessante no Quarto além disso, recomendo dar uma olhada.\nA última, com o Jeff Leek, contou sobre o seu projeto social de trazer jovens da periferia para o aprendizado e aplicação de ciência de dados, o Data Trail. Um projeto muito bonito, em que os alunos são pagos para fazerem o curso, e, ao fim, têm acesso a um estágio na área.\nEntre as keynotes várias palestras interessantes aconteceram. As que mais me chamaram a atenção foram:\nDemistifying MLOps, da Isabel Zimmerman, onde ela fez um paralelo entre fazer cookies e os processos de construção, deploy e manutenção de modelos de machine learning usando o vetiver;\nMaking Data Pipelines With R, da Meghan S Harris, com dicas valiosas de organização e documentação para pipelines de análise de dados;\nGarbage Data, do Jim Kloet, que falou sobre o que fazer com a garbage data (dados lixo) e da importância de definir a coleta de dados corretamente;\nExploring Query Optimization, da Rebecca Hadi, sobre como lidar com queries de tabelas no SQL com mais de um bilhão de colunas;\ndm: Analyze, build and deploy relational data models, do Kirill Muller, que mostrou um pacote muito interessante para construir esquemas de bancos de dados relacionais a partir de tibbles no R;\nThe Future of missing data, do Nicholas Tierney, sobre como lidar com dados faltantes, inclusive trazendo maneiras de incluir tais dados em sua análise para entender o que pode ter acontecido com eles.\nSmall Team Large Organization: Building Impactful Shiny Dashboards at NIH, do Jon Nye, que falou sobre o uso de um modelo de machine learning para classificar solicitações de grants (verbas para pesquisa) de acordo com o tema e a relevância;\nBuilding a ggplot2 rollercoaster, do Tyler Morgan-Wall, em que ele empregou seus pacotes de animação 3D no R rayshader e rayrender para obter um cenário 3D com gráficos do ggplot, e criou o pacote rayrollercoaster especificamente para a apresentação! A montanha-russa percorre os gráficos usando o percurso que você criar. A meu ponto de vista, a montanha-russa em si é mais uma diversão, mas os outros pacotes do Tyler são bastante úteis para visualização 3D.\nLembrando que essas foram minhas palestras favoritas de acordo com o que posso aplicar e meus interesses, e dentro das que assisti.\nEm resumo, a conferência foi bastante enriquecedora. Aprendi muito sobre tidymodels, machine learning em geral, Quarto, e sobre diversos novos pacotes muito úteis no R (e alguns com versão em python também.)\nSe você estiver considerando ir à conferência no ano que vem, saiba que será um tempo muito bem aproveitado!\nNão deixe de aplicar para a bolsa de diversidade!\ntl;dr rstudio:conf(2022) foi fenomenal, com uma pegada tech, muitas novidades aplicáveis, e muita gente interessante; workshop de ML com tidymodels trouxe uma análise de dados reais aplicando conceitos de ML como split de amostra, feature engineering, reamostragem e tunagem. Valeu muito a pena (confira o conteúdo aberto em workshops.tidymodels.org); Julio, Bea e Dani aqui do curso-R são pessoas excepcionais, foi o maior prazer conhecê-los! Beatriz Milz Twitter: @BeaMilz\nBlog: https://beamilz.com\nMuito do que eu achei interessante já foi falado pela Flávia (neste post) e pelo Júlio (em posts anteriores) sobre a conferência, então vou abordar aqui o que for diferente! Além disso, adicionei alguns tweets postados durante o evento, pois é uma forma legal de ver fotos e conversas que aconteceram por lá!\nAntes da conferência Como a Flávia, essa também foi a primeira vez que participo da rstudio::conf(). Uma curiosidade é que, por pouco, eu não participei! Eu havia lido um anúncio anteriormente dizendo que o evento não aconteceria, então não fiquei acompanhando o site. Porém, no último dia disponível para submeter para a bolsa de diversidade e palestras, eu vi um tweet sobre a conferência e descobri que ela iria acontecer! Compartilhei com as minhas redes e logo corri para submeter uma palestra e também para a bolsa de diversidade.\nUm tempo depois, recebi um email com o convite para ser Teaching Assistant (TA) no workshop Building Tidy Tools, ministrado pela Emma Rand e pelo Ian Lyttle. Eu aceitei e aí começou o processo de organizar tudo para participar do evento! Também recebi depois o email dizendo que a bolsa de diversidade foi aprovada, e que a palestra que eu submeti foi aprovada como uma lightning talk (uma palestra de 5 minutos)! Enfim, fiquei extremamente grata por todas essas oportunidades.\nTomorrow starts the @rstudio Conference! I'm very excited to be part of it :)\nThanks @rstudio for the diversity scholarship!🥳\nAlso, glad to be part of the group of R Brazilian users at the conference @flaviaerius @jtrecenti @dfalbel 🌎#rstudioconf #rstats #rstudioconf2022 pic.twitter.com/WvLpFZIaw5 — Beatriz Milz (@BeaMilz) July 24, 2022 Participando do workshop como TA Participar como Teaching Assistant do workshop Building Tidy Tools foi uma experiência INCRÍVEL! As pessoas ministrantes do workshop (Emma Rand e Ian Lyttle) são extremamente gentis, e mesmo antes do workshop eles incluiam os TAs nas conversas via email para que a gente já tivesse familiaridade com a estrutura do workshop e os materiais que seriam utilizados.\nAliás, todos os materiais para este workshop estão disponíveis neste site!\nNo primeiro dia do workshop, o foco foi abordar como podemos criar um pacote: no final do primeiro dia, as pessoas participantes já tinham um pacote criado, com algumas funções e seus testes, disponível online, e com uma página web feita com pkgdown! Nesse dia, consegui auxiliar bastante pessoas com dúvidas que surgiam sobre configuração do ambiente para desenvolvimento de pacotes, e os erros que apareciam no caminho durante o workshop. Eu já tinha bastante experiência com vários dos erros que aconteciam, pois eu utilizo pacotes diariamente e também sou uma das professoras do curso de pacotes na Curso-R.\nNo segundo dia do workshop, o conteúdo foi aprofundado e o foco foi a construção de funções de forma que elas funcionem bem de acordo com a filosofia do tidyverse, sendo o destaque (na minha opinião) o conteúdo sobre design de funções e tidy eval.\nA couple of ussies at the end of “Building Tidy Tools” workshop at #rstudioconf2022@er13_r is a wonderful collaborator! Thanks to TAs @tladeras @BeaMilz @heyhayhay__ Kailey Mulligan and Elliot Murphy, who kept things moving!\nThanks also to all our students, great energy! pic.twitter.com/oclaAfO9d8 — Ian Lyttle (@ijlyttle) July 26, 2022 Em resumo: foi uma oportunidade maravilhosa, e agradeço muito à Mine Çetinkaya-Rundel (que estava na organização dos workshops neste evento), aos instrutores e outros TAs, e também aos estudantes que participaram do workshop!\nBolsa de diversidade Como comentei anteriormente, eu e a Flávia fomos selecionadas para receber uma bolsa de diversidade. Algo que gostei muito foi terem criado um canal do Slack, antes mesmo da conferência, para que pudessemos conversar e conhecer outros bolsistas! Assim, quando chegamos no evento já conheciamos várias pessoas e foi mais fácil de se conectar :) Essa foto tem o registro das pessoas que participaram como bolsistas:\nFonte: Imagem disponibilizada por Katherine Vu (RStudio).\nPalestrando na rstudio::conf() A minha palestra relâmpago foi sobre um tema que tenho aprendido nos últimos 2 anos e acho muito útil e interessante: automatizações com GitHub Actions.\nA gravação da palestra está disponível no site da conferência!\nUm destaque foi o treinamento recebido pela Articulation. Todas as pessoas que palestraram no evento receberam convites para participar de um treinamento para oferecer palestras mais interessantes, e eu participei em duas atividades. O treinamento recebido me fez reformular completamente a minha proposta, diminuindo o escopo da palestra e focando em um público-alvo mais restrito: pessoas que ainda não usam GitHub Actions!\nPalestrar na rstudio::conf() foi uma experiência muito legal! A energia da platéia foi muito boa, e várias pessoas falaram comigo após a palestra dizendo que acharam interessante!\nTodo material (slides e material complementar) está disponível no meu blog.\nO Julio me ajudou bastante durante a criação dessa palestra, dando vários feedbacks, e eu o agradeço muito! Além disso, ele comentou um pouco da minha palestra no post sobre o segundo dia do evento.\nThe material for my talk at #rstudioconf2022 about GitHub Actions is available on my blog:https://t.co/07rEJwFCPn\nThanks everyone that watched and supported!!#rstudioconf @rstudio @github @GitHubBrasil pic.twitter.com/SHF3bK09Qr — Beatriz Milz (@BeaMilz) July 28, 2022 Quarto O Quarto, para quem não conhece, é a nova geração do RMarkdown. Eu já venho usando o Quarto desde outubro de 2021, enquanto ele estava sendo desenvolvido e aprimorado. Então poucas coisas foram novidades para mim nas palestras sobre Quarto, mas eu gostei de ver como foi dado ênfase! A keynote ‘Hello Quarto’ da Mine Çetinkaya-Rundel e da Julia Stewart Lowndes foi maravilhosa, com lindas artes da Allison Horst.\nArtwork from “Hello, Quarto” keynote by Julia Lowndes and Mine Çetinkaya-Rundel, presented at RStudio Conference 2022. Illustrated by Allison Horst.\nTenho usado bastante e sei que ainda tem muita coisa legal para ser lançada relacionado ao Quarto. Eu e o Julio ficamos tão empolgados, que reestruturamos o curso de relatórios da Curso-R (próxima turma começa em 16/08/2022) para abordar bastante conteúdo sobre essa ferramenta nova!\nEncontros Uma das coisas mais divertidas foi conhecer tanta gente legal da comunidade, e conversar nos períodos das refeições e nos eventos no período da noite, que dão espaço para conversas entre participates. O tweet abaixo foi tirado na primeira noite do evento: eu e o Julio levamos alguns jogos e jogamos com algumas pessoas que conhecemos por lá!\nPart of the fun in R events are playing games with people that we meet from the community! #rstudioconf #rstudioconf2022 #hanabi @rstudio @flaviaerius @Devin_lj @jtrecenti and David Christensen 🎉 pic.twitter.com/thndW1Le9C — Beatriz Milz (@BeaMilz) July 26, 2022 Stickers Uma febre durante o evento é aumentar a coleção de stickers. Esse tweet feito pelo Luis resume vários momentos durante a conferência em que várias pessoas buscavam algum sticker de pacotes queridos em uma mesa cheia deles!\n#rstudioconf2022 pic.twitter.com/cCn5me1rjc — Luis D. Verde Arregoitia (@LuisDVerde) July 26, 2022 Eu trouxe de volta vários stickers, e já colei no meu computador e estou distribuindo entre os amigos! Esse é o meu computador após a conferência:\nNotebook com vários stickers!\nConclusão da Bea Enfim, essa conferência foi INCRÍVEL! Espero ter oportunidades de participar nas conferências da posit no futuro :)\nJulio Trecenti Como eu já comentei sobre tudo o que passei nos posts anteriores – dia 1: aqui e dia 2: aqui – serei bem breve.\nPara mim, a parte mais legal foram os encotros e reencontros presenciais. Além da companhia sempre maravilhosa da Bea e da companhia da Flávia que nos acompanhou em toda a viagem, também encontramos novas pessoas do Brasil, reencontramos amigos e conhecemos novas pessoas de fora.\nNa parte técnica, para mim o mais legal foi ver como a RStudio, agora Posit, está se aproximando de forma amigável de outras linguagens de programação. Ao invés de simplesmente começar a fazer ferramentas para o Python, a empresa decidiu criar ferramentas multi-linguagem, focando na produção científica (como o Quarto), que vai muito além das linguagens de programação.\nE claro, meu lado tiete adorou ter refeições com o Hadley e conversar com outras pessoas famosas, como o JJ Allaire, o Christophe Dervieux e o Tyler Morgan-Wall.\nThankfully Julio asked for a picture, otherwise no one would believe that we had lunch with @hadleywickham and @chrisderv 😄 @jtrecenti @flaviaerius @rprimi e @anakiki1505 ! #rstudioconf2022 #RStudioConf #posit @rstudio #RStats pic.twitter.com/R8jAOyfUxd — Beatriz Milz (@BeaMilz) July 27, 2022 Abraços e até a próxima!\n","permalink":"https://blog.curso-r.com/posts/2022-08-12-rstudio-conf-impressoes/","tags":["evento","rstudio::conf"],"title":"Impressões pessoais da rstudio::conf(2022)"},{"author":["Julio"],"categories":["Divulgação"],"contents":" Este é o segundo post descrevendo o que aconteceu de mais legal na principal conferência da RStudio. Se você quiser acompanhar o que aconteceu no dia anterior, basta entrar aqui.\nKeynote 1: Quarto all the way across the sky Julia Lowndes e Mine Çentikaya-Rundel fizeram uma apresentação linda, cheia de ilustrações da Allison Horst sobre o Quarto. Mostraram como a ferramenta simplifica a construção de relatórios, apresentações e sites usando a ferramenta.\nA coisa mais legal que aprendi é que podemos ter documentos .qmd, .Rmd e .ipynb no mesmo site. Dessa forma, é possível que pessoas colaborem usando a ferramenta que mais se sentem confortáveis, sem precisar aprender nada novo, e o Quarto dá conta das transformações.\nPara quem não conhece, outra coisa muito legal é que o Quarto não tem nada a ver com o R nem a RStudio IDE. Por isso, ele pode (e já está sendo) usado em diversas plataformas diferentes como VS Code e Jupyter. Se quiser ver um exemplo legal de uso do Quarto, dê uma olhada no livro sobre Pandas, do Wes Mckinney.\nObs: sobre o título da seção, veja esse vídeo maravilhoso que acusa minha idade.\nLightning talks As conferências de R costumam ter uma ou mais sessões das chamadas lightning talks, que são palestras de apenas 5 minutos para que as pessoas apresentem algum pacote novo ou ideia.\nComo são muitas palestras e muito rápidas, vou tentar listar as minhas favoritas abaixo.\nPacote {clock}, com Davis Vaughan. O pacote clock já existe a algum tempo e serve para complementar algumas funcionalidades do pacote lubridate. Na sua fala, Davis mostrou um exemplo de como o clock pode ser útil, como fazer a operação “2022-01-30” + 1 mês (ou seja, o que significa 1 mês depois do dia 30 de janeiro de 2022). WebR, com George Stagg. Com o WebR, podemos utilizar o R diretamente no navegador, com o web assembly. Apesar das limitações e apesar de ser mais lento, parece um excelente passo para criar sistemas simples com shiny o shiny. Mas tudo isso será no futuro. GitHub Codespaces, com David Smith. David (conhecido como revodavid, por conta da Revolution Analytics, que foi adquirida pela Microsoft) apresentou sobre uma ferramenta muito legal, que permite que seja aberto um Docker diretamente de um repositório do GitHub. É muito útil para fazer o setup de ambientes na cloud, usando por exemplo o RStudio server e instalando as dependências de pacotes. GitHub Actions, com Beatriz Milz. Ela falou sobre como essa ferramenta tem ficado popular ultimamente, sendo que o uso de GitHub actions mais que quadruplicou em 2 anos de uso apenas nas organizações da RStudio. Também mostrou como podemos utilizar actions para rodar scripts arbitrários de R, não apenas processos relacionados a CI/CD. Mais informações no blog dela. Pacote ggdist, de Mathew Kay. Esse pacote é útil para plotar distribuições de probabilidade em R, de vários jeitos diferentes. Eu sinto que o pacote tem até funções demais, o que dificulta para aprender, mas certamente é muito legal. Pacote naniar, de Nicholas Tierney. Foi uma apresentação muito engraçada, cheia de ironias. Gostei de conhecer um pouco mais sobre o pacote naniar, pois só havia usado uma dou duas funções do pacote. Palestras sobre shiny Na parte da tarde, tivemos mais duas baterias de palestras, antes da última Keynote. Muita informação!\nAcabei escolhendo a parte de shiny, por apresentar muitas novidades interessantes.\nshinyuibuilder, de Nick Strayer. Agora temos uma ferramenta para fazer o design de shiny apps! Para isso, a equipe da Posit criou uma ferramenta para codar e desenhar a parte da UI de um shinyapp. A ferramenta permite que a pessoa use tanto a interface gráfica quanto o código ao mesmo tempo. Design de shiny apps, com Greg Swinehart. Trata-se do designer da Posit, que fez uma apresentação muito animada – e longa, pois estrapolou o tempo da fala – sobre como fazer bons shiny apps. O grande anúncio é que finalmente, FINALMENTE o bslib suportará cards, então poderemos fazer pacotes com Bootstrap 5 sem sofrer para criar as boxes e cards que gostamos tanto no {bs4Dash}. Outro shiny builder, com Peter Gandenberger. Trata-se de uma mistura de shinyuibuilder com esquisse que permite a criação de apps shiny completos (com ui e server) apenas com drag e drop, com pouquíssimo esforço. Obviamente a ferramenta apresenta limitações, mas pode ser uma boa forma de começar no universo shiny para iniciantes, e pode ser uma ferramenta de produtividade para pessoas experientes. Palestras sobre Quarto Na segunda bateria de palestras, decidi participar das palestras sobre Quarto, apesar de ainda estar muito interessado na trilha de shiny. perdi as palestras do Hadley Wickham, sobre o pacote {R7} e também a palestra sobre o pacote {shinytest2}.\nA primeira palestra foi sobre a utilização do Quarto da perspectiva de uma pessoa que não programa no dia-a-dia. Apesar de não ter nada de novo na parte técnica, foi interessante para encorajar as pessoas a usar essa nova ferramenta. nbdev, de Hamel Husain, da Fast.ai. Ele falou de uma ferramenta muito doida que, a partir de um Jupyter Notebook, gera um biblioteca, uma documentação (com Quarto) e um monte de coisas que não entendi. O que eu acho é que tem algumas coisas que são meio impressionantes no Python (como documentação de qualidade) que no R são bem comuns (por conta do {roxygen2}), mas não sei. Mais uma demonstração do Quarto, por Devin Pastoor. Dessa vez foi bem live coding mesmo, e o autor mostrou vários atalhos interessantes do Quarto, como o autocomplete dos arquivos YAML e a possibilidade de criar sites com muita facilidade. Eu fiquei impressionado com a parte de instalação de extensões, que permitem adicionar novas capacidades a sites do quarto. Magic. Última Keynote E o evento estava chegando ao fim. E foi em grande estilo. Convidaram o pesquisador Jeff Leek, um dos autores do curso de Data Science do Coursera, criado pela John’s Hopkins University. Esse curso é um dos primeiros MOOCs no tema e ficou extremamente popular, com mais de um milhão de pessoas inscritas.\nNa palestra, Jeff falou sobre a iniciativa Data Trail, um trabalho maravilhoso que ele iniciou para realizar treinamentos de ciência de dados em comunidades com menos acesso. Ele mostrou como o trabalho foi pesado, mas que teve bons resultados, com várias pessoas do programa acabando empregadas. Um dos insights mais importantes que foram apresentados é que o maior desafio para as pessoas não é aprender ciência de dados, e sim ter a estrutura e o ambiente adequados para isso. Por isso que é importante ajudar com transporte e ferramentas para essas pessoas trabalharem.\nNo final, Jeff mostrou como expandir essa iniciativa. Ele mostrou como já estão sendo criados alguns projetos satélite do Data Trail em outras cidades dos Estados Unidos. Quem sabe no futuro seja possível expandir para outros países.\nOrlando 2023 No final, foi anunciado que a próxima posit::conf (ou seria from posit import conf?) será em Orlando, Flórida, em Maio de 2023. Quem tiver interesse de ir, melhor já começar a guardar uma graninha ou se preparar para pedir bolsa!\nQuer saber mais? Bom, essas foram minhas anotações do dia 01 da rstudio::conf(2022). Todas as palestras ficarão online neste link. No futuro, faremos mais posts sobre o tema!\n","permalink":"https://blog.curso-r.com/posts/2022-07-29-rstudio-conf-2022-dia2/","tags":["evento","rstudio::conf"],"title":"rstudio::conf 2022, segundo dia de palestras"},{"author":["Julio"],"categories":["Divulgação"],"contents":" A conferência A rstudio::conf é um evento anual organizado pela RStudio. O evento reúne as celebridades do R como Hadley Wickham, Jenny Bryan e toda a turma da RStudio, além de pessoas interessadas em conhecer melhor a comunidade e acessar as ferramentas mais quentes no universo da ciência de dados.\nEste ano, o evento tem foco principal no Quarto. Trata-se de um software desenvolvido para produzir documentos científicos (relatórios, artigos, apresentações), funcionando como um complemento ao Pandoc para processar códigos (em R, python, julia, javascript) inseridos nos documentos. Se você se interessou pela ferramenta, vale a pena dar uma olhada nesse post da Beatriz Milz.\nMas claro que a conferência não será só sobre Quarto. Teremos conteúdos maravilhosos nos próximos dias, como avanços no {shiny}, {tidymodels} e outras ferramentas que amamos.\nO hotel Ficamos no Gaylord National Harbor, que fica perto de Washington D.C., a cidade que abriga Biden, Kamala \u0026amp; amigos.\nPara mim (Julio) a melhor descrição foi a que a Emma (professora do workshop Building Tidy Tools) nos deu quando conversamos com ela: “it’s MAD”. É um hotel muito doido, por vários motivos. É muito muito grande, com mais de mil quartos enormes. No meio do hotel tem uma espécie de vila, onde é possível tomar café, jantar, assistir cinema e até mesmo jogar boliche! Nada faz muito sentido e é muito extravagante. Uma experiência genuinamente americana.\nNa parte destinada a conferências, o espaço é realmente magnífico. Com salas enormes e equipadas, é um lugar bastante aconchegante para assistir palestra, conhecer pessoas e fazer networking.\nOs BRs Parece que existe um ímã que junta os brasileiros na conferência. No início, achávamos que seríamos só Julio Trecenti, Beatriz Milz, Flávia Rius e Daniel Falbel no rolê. Depois, conhecemos Cristiana, Gabriela, que trabalham no Nic.br. Depois encontramos Ricardo Primi que é professor mas foi aluno da Curso-R e sua esposa Tatiana. Depois, encontramos Adriana e Carlos, que trabalha na RStudio e a gente nem sabia. Por último, conhecemos a Ana Carla, que trabalha nos EUA.\nRefeições com Hadley No segundo dia dos workshops (26/07) sentamos com nossas novas amigas BR, Cristiana e Gabriela, para trocar ideia. Em seguida, sentou o Immanuel, um pesquisador muito legal que mora na California e trabalha com educação em ciência de dados.\nE de repente, senta na nossa mesa o próprio Hadley, nosso deuso do R. Foi muito interessante, porque acho que ele queria fugir dos círculos das pessoas mais famosas, e também um pouco de assuntos relacionados a R.\nNão vou detalhar muito sobre as conversas para manter a privacidade de todos, mas foi uma experiência muito legal e muito leve. Falamos de muitos assuntos, sendo que poucos eram de fato relacionados ao R.\nPor algum motivo, o Hadley decidiu sentar com a gente novamente! No dia seguinte, almoçamos com ele, logo depois que ele fez o anúncio da mudança de nome da empresa. Acho que ele gosta dos BR. Almoçamos com ele e com o Christoph Dervieux, um dos autores do blogdown e outros pacotes relacionados a RMarkdown.\nDepois de duas refeições, nos sentimos no direito de tirar uma foto… E está aqui!!\nThankfully Julio asked for a picture, otherwise no one would believe that we had lunch with @hadleywickham and @chrisderv 😄 @jtrecenti @flaviaerius @rprimi e @anakiki1505 ! #rstudioconf2022 #RStudioConf #posit @rstudio #RStats pic.twitter.com/R8jAOyfUxd — Beatriz Milz (@BeaMilz) July 27, 2022 Keynote 0: mudança de nome Depois da musiquinha de entrada (que, por sinal, estava com volume muito alto) entra o queridíssimo Hadley Wickham para abrir o evento. Depois de suas falas iniciais, JJ Allaire, CEO da RStudio começa sua apresentação.\nA apresentação dos dois gira em torno da evolução da RStudio de uma empresa que produzia uma IDE para R para uma empresa que tinha como missão facilitar a vida de pessoas que trabalhavam com ciência de dados. Com o passar do tempo, a empresa foi criando ferramentas mais interoperáveis, possibilidade de lidar com várias linguagens e um esquema unificado para criação de documentos científicos, o Quarto.\nCom isso, pelo que disseram, a RStudio passou a sentir cada vez mais que o nome da empresa já não representava suas ações, já que todos os produtos (Connect, Package Manager etc) levam consigo a ideia de que são feitos para o R, o que não é verdade. Por isso, a empresa precisou mudar de nome.\nAgora, RStudio é Posit. O site ainda não está no ar, mas eles fizeram uma página para falar da mudança. A empresa mudará o nome totalmente em outubro/2022. Ou seja, essa foi a última rstudio::conf, e a próxima será… from posit import conf?\nKeynote 1: tidymodels 1.0.0 A apresentação de Max Kuhn e Julia Silge foi útil para mostrar como que o pacote tidymodels resolve três problemas importantes na modelagem preditiva (supervisionada): ergonomia, efetividade e segurança. Foram apresentados os detalhes de design de pacotes do tidymodels como recipes, tune, workflows, parsnip, dials, yardstick etc. A parte mais importante foi que o tidymodels está oficialmente na versão 1.0.0, o que significa que pode ser considerado estável e, portanto, utilizável para ambientes em produção. O pacote vetiver também foi anunciado, mas sem muitos detalhes de como funciona.\nBloco tidymodels censored: Pacote para integrar análise de sobrevivência e tidymodels. Eu gostei, mas fiquei com um gostinho de “quero mais”, já que o pacote ainda não permite utilização em workflows, ou seja, ainda não dá para tunar hiperparâmetros. tidyclust: Pacote para imitar a análise de cluster no framework do tidymodels. O autor teve de basicamente reescrever toda a estrutura do tidymodels – focada em modelos supervisionados – para o problema do cluster. Por enquanto está bem no início, e só funciona com o kmeans. Eu achei conceitualmente interessante, mas na prática não sei se a ideia vai pegar ou não (tomara que sim!). vetiver: trata-se de uma ferramenta com bastante hype criada pela RStudio para fazer deploy de modelos ajustados com tidymodels e scikit-learn. A ferramenta possui soluções para três problemas: versionamento de modelos (usando o pacote pins), criação de APIs (usando o pacote plumber) e monitoramento (usando os dois pacotes anteriores). O legal é que a ferramenta pretende ser agnóstica ao ambiente utilizado para ajuste do modelo. Vamos ver como funciona na prática! Bloco fluxos de análise Data pipelines: em uma palestra muito bem humorada, Meghan Harris comentou como foi sua experiência trabalhando como única cientista de dados em uma empresa relacionada à área médica. As quatro mensagens principais foram: estudo do problema, documentação, testes e sustentabilidade. Nenhuma ferramenta nova aqui. Garbage in, garbage out: em uma palestra ainda mais bem humorada, Jim Kloet comentou sobre dados que não são úteis para determidados problemas. Apresentou práticas para identificar essas bases e lidar com elas. Por trabalhar com faxina de dados, achei a palestra bastante familiar e divertida. Também, nenhuma ferramenta nova aqui. Project Immortality: para mim, foi a palestra mais útil do dia. Aprendi que em pacotes de dados que são atualizados periodicamente, pode ser interessante armazenar dados em Releases ao invés de dados na pasta /data do pacote. Isso torna o pacote bem mais leve e fácil de manter. Um pacote novo usado para escrever releases que pretendo estudar é o piggyback. Logging: Essa palestra foi útil para conhecer o pacote loggit para guardar os logs de execução de tarefas. Bloco Banco de dados Query optimization: A palestrante apresentou alguns exemplos de como algumas mudanças básicas nas queries poderiam ajudar a reduzir dramaticamente o tempo de execução. Dentre outros detalhes, ela apresentou uma ferramenta bastante útil do SQL, chamada EXPLAIN, utilizada para estimar o tempo de execução da query que foi construída. Pacote dm: A palestra do Kirill Miller foi bastante difícil de entender (tanto pela densidade do tema quanto pela minha dificuldade em entender o inglês do palestrante), mas acho que consegui entender alguns conceitos do pacote. É uma forma de conseguir representar, dentro do R, a estrutura relacional de um banco de dados, contendo as chaves primárias e chaves estrangeiras, além de ferramentas para melhorar a documentação. Pacote dbcooper: Essa foi a segunda palestra mais útil do dia! O pacote dbcooper, do David Robinson, tem apenas uma função, que transforma uma conexão de banco de dados em funções especializadas para ler cada uma das tabelas daquela base. Por exemplo, se você tem uma base relacional da receita federal, o dbcooper criará funções para pegar os dados de todas as tabelas da base, como estabelecimentos, socios, empresas, entre outros. Keynote: shiny para Python Para finalizar as palestras do primeiro dia, tivemos a keynote do Joe Cheng, sobre o passado e o futuro do shiny. O shiny está completando 10 anos de história, e o Joe Cheng apresentou um pouco de como ele surgiu, em uma fala super pessoal e emocionante.\nUma parte muito comovente foi ver ele contar que estava quase saindo da RStudio (naquele momento, eram somente ele e o JJ na empresa), quando numa bela manhã o ambiente do shiny simplesmente apareceu na cabeça dele. É muito interessante saber que essa ferramenta – que é uma parte importantíssima da carreira de muitas pessoas que trabalham com ciência de dados – poderia nunca ter acontecudo.\nEm seguida, Joe argumentou os motivos pelos quais o R era muito adequado para o desenvolvimento do shiny. O primeiro é que as pessoas de fato passaram usar o pacote e aprender como funcionam ferramentas web. O segundo é a comunidade R, que foi muito receptiva ao pacote, criando diversas extensões. O terceiro é mais técnico: o R era extremamente adaptado para o ambiente criado pelo shiny.\nPara finalizar, foi apresentado como será o shiny para python. A ideia é escrever um pacote que pareça natural para pessoas do python, não uma adaptação do pacote em R. A biblioteca ainda está em formato alpha, então pode mudar bastante.\nQuer saber mais? Bom, essas foram minhas anotações do dia 01 da rstudio::conf(2022). Todas as palestras ficarão online neste link. No futuro, eu e a Beatriz faremos mais posts sobre o tema!\n","permalink":"https://blog.curso-r.com/posts/2022-07-27-rstudio-conf-2022/","tags":["evento","rstudio::conf"],"title":"rstudio::conf 2022, primeiro dia de palestras"},{"author":["Caio"],"categories":["tutoriais"],"contents":" Uma das melhores funcionalidades do RStudio é, sem sombra de dúvidas, o RStudio Server. Para ser mais preciso, ele é a melhor versão do RStudio; esse produto é, na verdade, uma integração entre a IDE RStudio e um programa de acesso remoto.\nDe forma bem resumida, ao instalar o RStudio Server em um servidor, você nunca mais precisa se preocupar com SSH. Todo o acesso ao servidor passa a ser através de uma interface web que é idêntica ao RStudio normal, o que significa que você pode programar usando os recursos do seu servidor, mas com a mesmíssima IDE que você já usa no dia-a-dia.\nMesmo depois que eu passei a usar o VS Code para programar R no meu computador, eu continuei usando o RStudio Server no servidor da empresa. Mês passado, entretanto, eu descobri uma funcionalidade incrível do VS Code que me permitiu abandonar de uma vez por todas o RStudio: o VS Code Remote Development.\nDiferentemente do RStudio Server, o Remote Development não é instalado no servidor, mas sim no seu computador. Essa ferramenta usa SSH por trás dos panos para trazer todos os arquivos do servidor até o seu VS Code local e, quando você executa algum comando, ela faz o caminho reverso para rodar o código no servidor.\nIsso não é nada novo! Todo mundo que já usou SSH alguma vez na vida e precisou abrir um editor de texto remoto (vim ou nano), provavelmente não vai estranhar o modo como o Remote Development funciona. A única diferença é que o RD traz o código até o VS Code e não até o seu terminal.\nHá vantagens e desvantagens nessa abordagem: por um lado, o servidor não vai precisar gastar recursos renderizando a interface do RStudio Server, mas por outro, não tem um jeito muito simples de passar o seu acesso para outra pessoa (dado que não é só um site que você acessa de qualquer lugar). A melhor solução é sempre a que se encaixa melhor no seu fluxo de trabalho.\nO processo para configurar o RD é bastante simples e pode ser encontrado inteiramente na página do Remote development via SSH. Em resumo, você precisa instalar a extensão, conectar com o seu servidor via SSH e configurar as suas extensões para o ambiente remoto.\nO passo mais complexo é o último, mas mesmo ele é até que bastante intuitivo. Na época eu tive dificuldade de entender que as minhas extensões não funcionavam no modo remoto porque eu precisava instalá-las de novo! Essencialmente, eu precisava configurar a extensão do R para funcionar com os programas do servidor e não mais os da minha máquina.\nE isso é tudo! Espero que este post tenha sido útil para apresentar uma funcionalidade muito legal da IDE que eu tenho usado todos os dias pelos últimos meses. Até a próxima :)\nP.S.: Além da solução que eu apresentei, há ainda duas outras: o RStudio Workbench (um produto pago da RStudio) passou a permitir o uso da interface do VS Code e, recentemente, a Microsoft anunciou o VS Code Server, que eu ainda não tive oportunidade de testar.\n","permalink":"https://blog.curso-r.com/posts/2022-07-15-vscode-server/","tags":["rstudio","vscode"],"title":"Acesso Remoto ao VS Code"},{"author":["Tereza"],"categories":["tutoriais"],"contents":" Olá! Quem me conhece sabe que sou um pouco viciada em música, e no contexto do R não poderia ser diferente! hahaha\nFaz um tempo que venho mexendo com a API do Spotify no R. Primeiro comecei com o pacote Rspotify, e depois conheci o spotifyr (e foi aí que surtei de vez, descobri que dá pra fazer absolutamente tudo de dentro do R: criar playlists, adicionar músicas em playlists, ver quais foram as minhas últimas músicas escutadas, e tudo mais que você imaginar)\nComo o acesso a esses pacotes não é nada trivial, resolvi escrever esse post pra mostrar de uma vez por todas como isso é feito (eu sofri bastante na época que comecei a mexer com eles). Vou mostrar como obter os tokens de acesso do Spotify, e falar um pouco sobre as funções do Rspotify também.\nNesse post vou focar só nas funções do Rspotify (por serem mais simples), mas pretendo fazer um post dedicado para o spotifyr mais pra frente.\nObtendo os tokens de acesso do Spotify Primeiro, instale o pacote Rspotify através do comando\ninstall.packages(\u0026quot;Rspotify\u0026quot;) Agora, carregue o pacote\nlibrary(Rspotify) Note que todas as funções do Rspotify exigem o argumento token.\nPara obter esse token, siga os seguintes passos:\n(obs: estou assumindo que você possui uma conta no Spotify. Não precisa ser premium, então caso não possua ainda, é só criar uma)\n1 - Entre nesse link\nVocê será redirecionado para essa página:\n2 - Clique em Log in e faça o login com a sua conta do Spotify\n3 - Agora, clique em Create an app\n4 - Dê um nome e uma descrição para o seu app, assinale a caixinha de Termos de Uso, e então, clique em Create\n5 - Depois de criar o app, você será redirecionado para essa tela. Clique em edit settings\n6 - Preencha o campo Redirect URIs com “http://localhost:1410/” (sem as aspas), e clique em Add\nA tela deve ficar assim:\n7 - Agora, role até o final da página e clique em Save\n8 - Clique no botão Show client secret\nVeja que na tela estão o Client ID e o Client Secret. São esse dois tokens que usaremos para ter acesso às funções do Rspotify.\nAgora, com o R aberto e o pacote Rspotify carregado, rode o seguinte código, substituindo o segundo e terceito argumentos pelos tokens Client ID e Client Secret, respectivamente:\nkeys \u0026lt;- spotifyOAuth(\u0026quot;nome_qualquer_aqui\u0026quot;, \u0026quot;cole_aqui_seu_client_id\u0026quot;, \u0026quot;cole_aqui_seu_client_secret\u0026quot;) (obs: os três argumentos da função são do tipo character, então devem estar entre aspas; o primeiro argumento é livre)\nVocê será redirecionado para uma página de autorização do Spotify no seu navegador. Clique em Aceito e volte para o R.\nPronto! Agora é só usar as funções do Rspotify, usando token = keys como argumento em todas elas.\nConhecendo as funções do Rspotify O funcionamento das funções está explicado na documentação do pacote, então vou mostrar só alguns exemplos, sem me preocupar em explicar cada função detalhadamente.\nPara os exemplos, vou usar músicas e bandas brasileiras que gosto, então já fica aí a recomendação pra quem quiser! hahaha\nsearchArtist A função searchArtist pede como argumentos o nome do artista/banda, e o token (aquele que acabamos de obter). Ela retorna um data frame no qual cada linha é referente a um artista (relacionado com a busca feita). O id do artista é especialmente importante, pois algumas funções desse pacote exigem o id como argumento, como veremos em seguida.\nsearchArtist(\u0026quot;O Grilo\u0026quot;, token = keys) artist id popularity followers genres type O Grilo 22KEpOwThQ5q1DGochfayO 49 74930 rock alternativo brasileiro artist Banda Cumade Selvira \u0026amp; Os Cantiga de Grilo 504zftG42qkU6Gzw8Ai3mC 0 15 artist kamaitachi 1ISc8zhrqxd5WrJMkMMLSm 61 1031552 musica triste brasileira artist O Grito 0CzL84J7lB5sWycref43lN 1 291 artist Grilo 2s1cOPGtfbCeYvxibRHtew 1 39 artist O Gringo Sou Eu 7Da1dXVGp2bjqkVTKQ4W7L 1 52 artist El Grilo Ensamble Vocal 2NHB6zVcmBv8034ZNpBkzU 2 295 artist O. Grimm 59SQwI32qROXuzlHTu7RS8 1 1 artist O. Grinbergs 2E2MSLcsX2lYOTVMr2DKcq 1 668 artist getRelated A função getRelated pede como argumentos o nome do artista/banda, e o token. Ela retorna algumas informações sobre os artistas relacionados com o artista especificado.\ngetRelated(\u0026quot;O Grilo\u0026quot;, token = keys) name id popularity type followers Walfredo em Busca da Simbiose 6PzQC3kTcWcJcH8p4d7yJl 29 artist 8842 Abacaxepa 37YBTVhchsMr6ekdUHEKRy 40 artist 15088 Daparte 6D5hUhZncLiNMeE3gq9BhL 41 artist 17384 Lupa 7yct0AdXUgrHi83xwSt902 30 artist 7405 Nanasai 67PBrqP0nECUumF2AQ8G6S 36 artist 25626 Selvagens à Procura de Lei 093ybF4c81CndQe6qWtc8S 42 artist 125164 Scatolove 0icMce3ZQU1HdA8gGHo5Rg 37 artist 88050 Cidade Dormitório 3hnNIsiWatVyHen6mz79sf 32 artist 30869 Terno Rei 7c8kQb9AUntvapfnuC3IhF 50 artist 89239 Zimbra 11xX6bMCvpstEFOyx8lxYg 43 artist 111790 getArtist Essa função pede como argumentos o id do artista, e o token. Ela retorna algumas informações sobre esse artista. No caso, estou usando o id da banda O Grilo, que foi obtido acima.\ngetArtist(\u0026quot;22KEpOwThQ5q1DGochfayO\u0026quot;, token = keys) name id popularity followers genres O Grilo 22KEpOwThQ5q1DGochfayO 49 74930 rock alternativo brasileiro getTopTracks Essa função pede como argumentos o id do artista, a especificação de um país, e o token. Ela retorna as músicas mais populares daquele artista no país em questão (e algumas informações sobre elas)\ngetTopTracks(\u0026quot;22KEpOwThQ5q1DGochfayO\u0026quot;, country = \u0026quot;BR\u0026quot;, token = keys) artist_id name popularity duration_ms track_number available_markets 2DllnWTQNjh5NDN1A2ZtnH Serenata Existencialista 62 163209 5 61keC2MVAbhqm3RoAVSfI6 Sambinha 50 180202 3 43MdgqLdKQJl1Azdq0Shpc Trela 49 187482 1 6btBKvH1OlMbwW9lU1RGIo Herói do Futuro 47 232373 1 2JNXbqa8e0aQ7Trert8N8D Meu Amor 47 193440 8 6zHoaw5RcWAcaWCSDzKdlj Pra Você Gostar de Mim 46 184799 1 6aEQ365jPX55xfpA8OFfki Onde Flor 44 192546 12 2682XIBY8wcjL9UJ9voUa8 Contramão 44 172623 3 4aEDdWboWAKt5UYIFWqRwf Inês 43 173880 4 5ayHgfHVet0M47OC4kTrTK Tira a Roupa 43 191899 1 getAlbums A função getAlbums pede como argumentos o id do artista/banda, e o token. Ela retorna um data frame no qual cada linha é referente a um album do artista/banda. O id de cada álbum também é importante caso queira obter mais informações sobre o álbum.\ngetAlbums(\u0026quot;22KEpOwThQ5q1DGochfayO\u0026quot;, token = keys) id name album_type available_markets 0I7bofr4iucY3LXUPKv9cz Você Não Sabe de Nada album getAlbumInfo Essa função pede como argumentos o id do álbum, e o token. Ela retorna algumas informações sobre o álbum.\ngetAlbumInfo(\u0026quot;0I7bofr4iucY3LXUPKv9cz\u0026quot;, token = keys) id artist name label popularity release_date album_type track_total 0I7bofr4iucY3LXUPKv9cz O Grilo Você Não Sabe de Nada Rockambole 43 2021-03-26 album 13 getAlbum Essa função pede como argumentos o id do álbum, e o token. Ela retorna informações sobre cada música do álbum.\ngetAlbum(\u0026quot;0I7bofr4iucY3LXUPKv9cz\u0026quot;, token = keys) id name duration_ms track_number disc_number available_markets preview_url 43MdgqLdKQJl1Azdq0Shpc Trela 187482 1 1 AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, BB, BD, BE, BF, BG, BH, BI, BJ, BN, BO, BR, BS, BT, BW, BY, BZ, CA, CD, CG, CH, CI, CL, CM, CO, CR, CV, CW, CY, CZ, DE, DJ, DK, DM, DO, DZ, EC, EE, EG, ES, FI, FJ, FM, FR, GA, GB, GD, GE, GH, GM, GN, GQ, GR, GT, GW, GY, HK, HN, HR, HT, HU, ID, IE, IL, IN, IQ, IS, IT, JM, JO, JP, KE, KG, KH, KI, KM, KN, KR, KW, KZ, LA, LB, LC, LI, LK, LR, LS, LT, LU, LV, LY, MA, MC, MD, ME, MG, MH, MK, ML, MN, MO, MR, MT, MU, MV, MW, MX, MY, MZ, NA, NE, NG, NI, NL, NO, NP, NR, NZ, OM, PA, PE, PG, PH, PK, PL, PS, PT, PW, PY, QA, RO, RS, RW, SA, SB, SC, SE, SG, SI, SK, SL, SM, SN, SR, ST, SV, SZ, TD, TG, TH, TJ, TL, TN, TO, TR, TT, TV, TW, TZ, UA, UG, US, UY, UZ, VC, VE, VN, VU, WS, XK, ZA, ZM, ZW https://p.scdn.co/mp3-preview/7fecbb599cb7f86dfd86f2f145ce9b1979a49d70?cid=d31252a2880546b3ba640d5d64b7e652 7fT4wCxnKI1FFb0QnQ4nX3 Guitarrada 154912 2 1 AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, BB, BD, BE, BF, BG, BH, BI, BJ, BN, BO, BR, BS, BT, BW, BY, BZ, CA, CD, CG, CH, CI, CL, CM, CO, CR, CV, CW, CY, CZ, DE, DJ, DK, DM, DO, DZ, EC, EE, EG, ES, FI, FJ, FM, FR, GA, GB, GD, GE, GH, GM, GN, GQ, GR, GT, GW, GY, HK, HN, HR, HT, HU, ID, IE, IL, IN, IQ, IS, IT, JM, JO, JP, KE, KG, KH, KI, KM, KN, KR, KW, KZ, LA, LB, LC, LI, LK, LR, LS, LT, LU, LV, LY, MA, MC, MD, ME, MG, MH, MK, ML, MN, MO, MR, MT, MU, MV, MW, MX, MY, MZ, NA, NE, NG, NI, NL, NO, NP, NR, NZ, OM, PA, PE, PG, PH, PK, PL, PS, PT, PW, PY, QA, RO, RS, RW, SA, SB, SC, SE, SG, SI, SK, SL, SM, SN, SR, ST, SV, SZ, TD, TG, TH, TJ, TL, TN, TO, TR, TT, TV, TW, TZ, UA, UG, US, UY, UZ, VC, VE, VN, VU, WS, XK, ZA, ZM, ZW https://p.scdn.co/mp3-preview/c8ed5a257adac75a8fd7ef00202867baf71968cc?cid=d31252a2880546b3ba640d5d64b7e652 2682XIBY8wcjL9UJ9voUa8 Contramão 172623 3 1 AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, BB, BD, BE, BF, BG, BH, BI, BJ, BN, BO, BR, BS, BT, BW, BY, BZ, CA, CD, CG, CH, CI, CL, CM, CO, CR, CV, CW, CY, CZ, DE, DJ, DK, DM, DO, DZ, EC, EE, EG, ES, FI, FJ, FM, FR, GA, GB, GD, GE, GH, GM, GN, GQ, GR, GT, GW, GY, HK, HN, HR, HT, HU, ID, IE, IL, IN, IQ, IS, IT, JM, JO, JP, KE, KG, KH, KI, KM, KN, KR, KW, KZ, LA, LB, LC, LI, LK, LR, LS, LT, LU, LV, LY, MA, MC, MD, ME, MG, MH, MK, ML, MN, MO, MR, MT, MU, MV, MW, MX, MY, MZ, NA, NE, NG, NI, NL, NO, NP, NR, NZ, OM, PA, PE, PG, PH, PK, PL, PS, PT, PW, PY, QA, RO, RS, RW, SA, SB, SC, SE, SG, SI, SK, SL, SM, SN, SR, ST, SV, SZ, TD, TG, TH, TJ, TL, TN, TO, TR, TT, TV, TW, TZ, UA, UG, US, UY, UZ, VC, VE, VN, VU, WS, XK, ZA, ZM, ZW https://p.scdn.co/mp3-preview/0156d027e420d79acb24faa0775547cd2f779eb8?cid=d31252a2880546b3ba640d5d64b7e652 2XXBtwtgdfQ3rGvgweteRF Tudo e Mais um Pouco 166390 4 1 AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, BB, BD, BE, BF, BG, BH, BI, BJ, BN, BO, BR, BS, BT, BW, BY, BZ, CA, CD, CG, CH, CI, CL, CM, CO, CR, CV, CW, CY, CZ, DE, DJ, DK, DM, DO, DZ, EC, EE, EG, ES, FI, FJ, FM, FR, GA, GB, GD, GE, GH, GM, GN, GQ, GR, GT, GW, GY, HK, HN, HR, HT, HU, ID, IE, IL, IN, IQ, IS, IT, JM, JO, JP, KE, KG, KH, KI, KM, KN, KR, KW, KZ, LA, LB, LC, LI, LK, LR, LS, LT, LU, LV, LY, MA, MC, MD, ME, MG, MH, MK, ML, MN, MO, MR, MT, MU, MV, MW, MX, MY, MZ, NA, NE, NG, NI, NL, NO, NP, NR, NZ, OM, PA, PE, PG, PH, PK, PL, PS, PT, PW, PY, QA, RO, RS, RW, SA, SB, SC, SE, SG, SI, SK, SL, SM, SN, SR, ST, SV, SZ, TD, TG, TH, TJ, TL, TN, TO, TR, TT, TV, TW, TZ, UA, UG, US, UY, UZ, VC, VE, VN, VU, WS, XK, ZA, ZM, ZW https://p.scdn.co/mp3-preview/ef9a1f304dc853504e2867fb31c3a0acc114a232?cid=d31252a2880546b3ba640d5d64b7e652 5Vjhpp2vftIrYk3Fija7j7 Meu Pior Amigo 217737 5 1 AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, BB, BD, BE, BF, BG, BH, BI, BJ, BN, BO, BR, BS, BT, BW, BY, BZ, CA, CD, CG, CH, CI, CL, CM, CO, CR, CV, CW, CY, CZ, DE, DJ, DK, DM, DO, DZ, EC, EE, EG, ES, FI, FJ, FM, FR, GA, GB, GD, GE, GH, GM, GN, GQ, GR, GT, GW, GY, HK, HN, HR, HT, HU, ID, IE, IL, IN, IQ, IS, IT, JM, JO, JP, KE, KG, KH, KI, KM, KN, KR, KW, KZ, LA, LB, LC, LI, LK, LR, LS, LT, LU, LV, LY, MA, MC, MD, ME, MG, MH, MK, ML, MN, MO, MR, MT, MU, MV, MW, MX, MY, MZ, NA, NE, NG, NI, NL, NO, NP, NR, NZ, OM, PA, PE, PG, PH, PK, PL, PS, PT, PW, PY, QA, RO, RS, RW, SA, SB, SC, SE, SG, SI, SK, SL, SM, SN, SR, ST, SV, SZ, TD, TG, TH, TJ, TL, TN, TO, TR, TT, TV, TW, TZ, UA, UG, US, UY, UZ, VC, VE, VN, VU, WS, XK, ZA, ZM, ZW https://p.scdn.co/mp3-preview/4b6d85f4de13e716ea1f9983aec5e349c777e737?cid=d31252a2880546b3ba640d5d64b7e652 6tjsjuI6eZX1XpwrehfRRg Infinito (-1) 204792 6 1 AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, BB, BD, BE, BF, BG, BH, BI, BJ, BN, BO, BR, BS, BT, BW, BY, BZ, CA, CD, CG, CH, CI, CL, CM, CO, CR, CV, CW, CY, CZ, DE, DJ, DK, DM, DO, DZ, EC, EE, EG, ES, FI, FJ, FM, FR, GA, GB, GD, GE, GH, GM, GN, GQ, GR, GT, GW, GY, HK, HN, HR, HT, HU, ID, IE, IL, IN, IQ, IS, IT, JM, JO, JP, KE, KG, KH, KI, KM, KN, KR, KW, KZ, LA, LB, LC, LI, LK, LR, LS, LT, LU, LV, LY, MA, MC, MD, ME, MG, MH, MK, ML, MN, MO, MR, MT, MU, MV, MW, MX, MY, MZ, NA, NE, NG, NI, NL, NO, NP, NR, NZ, OM, PA, PE, PG, PH, PK, PL, PS, PT, PW, PY, QA, RO, RS, RW, SA, SB, SC, SE, SG, SI, SK, SL, SM, SN, SR, ST, SV, SZ, TD, TG, TH, TJ, TL, TN, TO, TR, TT, TV, TW, TZ, UA, UG, US, UY, UZ, VC, VE, VN, VU, WS, XK, ZA, ZM, ZW https://p.scdn.co/mp3-preview/46ee8ed72fdb8372b8d37ad206f4cf5b34ebd80e?cid=d31252a2880546b3ba640d5d64b7e652 6yPZTaETPJ6Juy00huOb55 Você Não Sabe de Nada 99378 7 1 AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, BB, BD, BE, BF, BG, BH, BI, BJ, BN, BO, BR, BS, BT, BW, BY, BZ, CA, CD, CG, CH, CI, CL, CM, CO, CR, CV, CW, CY, CZ, DE, DJ, DK, DM, DO, DZ, EC, EE, EG, ES, FI, FJ, FM, FR, GA, GB, GD, GE, GH, GM, GN, GQ, GR, GT, GW, GY, HK, HN, HR, HT, HU, ID, IE, IL, IN, IQ, IS, IT, JM, JO, JP, KE, KG, KH, KI, KM, KN, KR, KW, KZ, LA, LB, LC, LI, LK, LR, LS, LT, LU, LV, LY, MA, MC, MD, ME, MG, MH, MK, ML, MN, MO, MR, MT, MU, MV, MW, MX, MY, MZ, NA, NE, NG, NI, NL, NO, NP, NR, NZ, OM, PA, PE, PG, PH, PK, PL, PS, PT, PW, PY, QA, RO, RS, RW, SA, SB, SC, SE, SG, SI, SK, SL, SM, SN, SR, ST, SV, SZ, TD, TG, TH, TJ, TL, TN, TO, TR, TT, TV, TW, TZ, UA, UG, US, UY, UZ, VC, VE, VN, VU, WS, XK, ZA, ZM, ZW https://p.scdn.co/mp3-preview/eac1f0862b5aa377cba6831398fba9cdbbf7e517?cid=d31252a2880546b3ba640d5d64b7e652 2JNXbqa8e0aQ7Trert8N8D Meu Amor 193440 8 1 AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, BB, BD, BE, BF, BG, BH, BI, BJ, BN, BO, BR, BS, BT, BW, BY, BZ, CA, CD, CG, CH, CI, CL, CM, CO, CR, CV, CW, CY, CZ, DE, DJ, DK, DM, DO, DZ, EC, EE, EG, ES, FI, FJ, FM, FR, GA, GB, GD, GE, GH, GM, GN, GQ, GR, GT, GW, GY, HK, HN, HR, HT, HU, ID, IE, IL, IN, IQ, IS, IT, JM, JO, JP, KE, KG, KH, KI, KM, KN, KR, KW, KZ, LA, LB, LC, LI, LK, LR, LS, LT, LU, LV, LY, MA, MC, MD, ME, MG, MH, MK, ML, MN, MO, MR, MT, MU, MV, MW, MX, MY, MZ, NA, NE, NG, NI, NL, NO, NP, NR, NZ, OM, PA, PE, PG, PH, PK, PL, PS, PT, PW, PY, QA, RO, RS, RW, SA, SB, SC, SE, SG, SI, SK, SL, SM, SN, SR, ST, SV, SZ, TD, TG, TH, TJ, TL, TN, TO, TR, TT, TV, TW, TZ, UA, UG, US, UY, UZ, VC, VE, VN, VU, WS, XK, ZA, ZM, ZW https://p.scdn.co/mp3-preview/85df60a314644d579019ab11cd1d8574d9996a24?cid=d31252a2880546b3ba640d5d64b7e652 4KC49pnwukN8SVse2tzi7L Vou Levar 198197 9 1 AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, BB, BD, BE, BF, BG, BH, BI, BJ, BN, BO, BR, BS, BT, BW, BY, BZ, CA, CD, CG, CH, CI, CL, CM, CO, CR, CV, CW, CY, CZ, DE, DJ, DK, DM, DO, DZ, EC, EE, EG, ES, FI, FJ, FM, FR, GA, GB, GD, GE, GH, GM, GN, GQ, GR, GT, GW, GY, HK, HN, HR, HT, HU, ID, IE, IL, IN, IQ, IS, IT, JM, JO, JP, KE, KG, KH, KI, KM, KN, KR, KW, KZ, LA, LB, LC, LI, LK, LR, LS, LT, LU, LV, LY, MA, MC, MD, ME, MG, MH, MK, ML, MN, MO, MR, MT, MU, MV, MW, MX, MY, MZ, NA, NE, NG, NI, NL, NO, NP, NR, NZ, OM, PA, PE, PG, PH, PK, PL, PS, PT, PW, PY, QA, RO, RS, RW, SA, SB, SC, SE, SG, SI, SK, SL, SM, SN, SR, ST, SV, SZ, TD, TG, TH, TJ, TL, TN, TO, TR, TT, TV, TW, TZ, UA, UG, US, UY, UZ, VC, VE, VN, VU, WS, XK, ZA, ZM, ZW https://p.scdn.co/mp3-preview/699e8e652d0c1bac360eb256940e48b491565f14?cid=d31252a2880546b3ba640d5d64b7e652 1BcrtC8bB8D0Dj0waoD6uy e daí, eu sei lá ¯_(\u0026lt;U+30C4\u0026gt;)/¯ 46566 10 1 AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, BB, BD, BE, BF, BG, BH, BI, BJ, BN, BO, BR, BS, BT, BW, BY, BZ, CA, CD, CG, CH, CI, CL, CM, CO, CR, CV, CW, CY, CZ, DE, DJ, DK, DM, DO, DZ, EC, EE, EG, ES, FI, FJ, FM, FR, GA, GB, GD, GE, GH, GM, GN, GQ, GR, GT, GW, GY, HK, HN, HR, HT, HU, ID, IE, IL, IN, IQ, IS, IT, JM, JO, JP, KE, KG, KH, KI, KM, KN, KR, KW, KZ, LA, LB, LC, LI, LK, LR, LS, LT, LU, LV, LY, MA, MC, MD, ME, MG, MH, MK, ML, MN, MO, MR, MT, MU, MV, MW, MX, MY, MZ, NA, NE, NG, NI, NL, NO, NP, NR, NZ, OM, PA, PE, PG, PH, PK, PL, PS, PT, PW, PY, QA, RO, RS, RW, SA, SB, SC, SE, SG, SI, SK, SL, SM, SN, SR, ST, SV, SZ, TD, TG, TH, TJ, TL, TN, TO, TR, TT, TV, TW, TZ, UA, UG, US, UY, UZ, VC, VE, VN, VU, WS, XK, ZA, ZM, ZW https://p.scdn.co/mp3-preview/0c9b9c2db58981d27e6afb7c547e76ac64ae7d17?cid=d31252a2880546b3ba640d5d64b7e652 6e3BWdeIaXTt0mOkzKKHVA Adeus 262198 11 1 AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, BB, BD, BE, BF, BG, BH, BI, BJ, BN, BO, BR, BS, BT, BW, BY, BZ, CA, CD, CG, CH, CI, CL, CM, CO, CR, CV, CW, CY, CZ, DE, DJ, DK, DM, DO, DZ, EC, EE, EG, ES, FI, FJ, FM, FR, GA, GB, GD, GE, GH, GM, GN, GQ, GR, GT, GW, GY, HK, HN, HR, HT, HU, ID, IE, IL, IN, IQ, IS, IT, JM, JO, JP, KE, KG, KH, KI, KM, KN, KR, KW, KZ, LA, LB, LC, LI, LK, LR, LS, LT, LU, LV, LY, MA, MC, MD, ME, MG, MH, MK, ML, MN, MO, MR, MT, MU, MV, MW, MX, MY, MZ, NA, NE, NG, NI, NL, NO, NP, NR, NZ, OM, PA, PE, PG, PH, PK, PL, PS, PT, PW, PY, QA, RO, RS, RW, SA, SB, SC, SE, SG, SI, SK, SL, SM, SN, SR, ST, SV, SZ, TD, TG, TH, TJ, TL, TN, TO, TR, TT, TV, TW, TZ, UA, UG, US, UY, UZ, VC, VE, VN, VU, WS, XK, ZA, ZM, ZW https://p.scdn.co/mp3-preview/9d9995ab4e0bdee69a1031f49497db5d6f9b3fce?cid=d31252a2880546b3ba640d5d64b7e652 6aEQ365jPX55xfpA8OFfki Onde Flor 192546 12 1 AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, BB, BD, BE, BF, BG, BH, BI, BJ, BN, BO, BR, BS, BT, BW, BY, BZ, CA, CD, CG, CH, CI, CL, CM, CO, CR, CV, CW, CY, CZ, DE, DJ, DK, DM, DO, DZ, EC, EE, EG, ES, FI, FJ, FM, FR, GA, GB, GD, GE, GH, GM, GN, GQ, GR, GT, GW, GY, HK, HN, HR, HT, HU, ID, IE, IL, IN, IQ, IS, IT, JM, JO, JP, KE, KG, KH, KI, KM, KN, KR, KW, KZ, LA, LB, LC, LI, LK, LR, LS, LT, LU, LV, LY, MA, MC, MD, ME, MG, MH, MK, ML, MN, MO, MR, MT, MU, MV, MW, MX, MY, MZ, NA, NE, NG, NI, NL, NO, NP, NR, NZ, OM, PA, PE, PG, PH, PK, PL, PS, PT, PW, PY, QA, RO, RS, RW, SA, SB, SC, SE, SG, SI, SK, SL, SM, SN, SR, ST, SV, SZ, TD, TG, TH, TJ, TL, TN, TO, TR, TT, TV, TW, TZ, UA, UG, US, UY, UZ, VC, VE, VN, VU, WS, XK, ZA, ZM, ZW https://p.scdn.co/mp3-preview/d80e37677128ab1e0392904a241458e33036951f?cid=d31252a2880546b3ba640d5d64b7e652 70Sw1EWhOn9TUkUAAAFWAE Malabarista de Granadas - Bonus Track 176032 13 1 AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, BB, BD, BE, BF, BG, BH, BI, BJ, BN, BO, BR, BS, BT, BW, BY, BZ, CA, CD, CG, CH, CI, CL, CM, CO, CR, CV, CW, CY, CZ, DE, DJ, DK, DM, DO, DZ, EC, EE, EG, ES, FI, FJ, FM, FR, GA, GB, GD, GE, GH, GM, GN, GQ, GR, GT, GW, GY, HK, HN, HR, HT, HU, ID, IE, IL, IN, IQ, IS, IT, JM, JO, JP, KE, KG, KH, KI, KM, KN, KR, KW, KZ, LA, LB, LC, LI, LK, LR, LS, LT, LU, LV, LY, MA, MC, MD, ME, MG, MH, MK, ML, MN, MO, MR, MT, MU, MV, MW, MX, MY, MZ, NA, NE, NG, NI, NL, NO, NP, NR, NZ, OM, PA, PE, PG, PH, PK, PL, PS, PT, PW, PY, QA, RO, RS, RW, SA, SB, SC, SE, SG, SI, SK, SL, SM, SN, SR, ST, SV, SZ, TD, TG, TH, TJ, TL, TN, TO, TR, TT, TV, TW, TZ, UA, UG, US, UY, UZ, VC, VE, VN, VU, WS, XK, ZA, ZM, ZW https://p.scdn.co/mp3-preview/a3ba0939a9d55cc315c459cfdf875873a43f50da?cid=d31252a2880546b3ba640d5d64b7e652 searchPlaylist A função searchPlaylist pede como argumentos o nome de uma playlist, e o token. Ela retorna um data frame no qual cada linha é referente a uma playlist (relacionada com a busca feita). Assim como os outros casos, o id da playlist também é importante caso queira obter mais informações sobre ela.\nNo caso, vamos procurar a playlist “Uma Música Por Dia”, que é uma playlist que atualizo diariamente. (obs: meu id de usuário no spotify é “therezzza”)\nsearchPlaylist(\u0026quot;Uma Música Por Dia\u0026quot;, token = keys) id name tracks owner 3buJd0YAjAM0kuPnrQMFJ7 Uma Música por Dia 731 allanclaudino 4faz4FVeL328piox0lz7UF Uma Música Por Dia 179 therezzza 575JlxqDooZLZGVQyPO5QV Músicas para dormir, louvar, orar e devocional \u0026lt;U+0001F64F\u0026gt;\u0026lt;U+0001F3FB\u0026gt;\u0026lt;U+2728\u0026gt;\u0026lt;U+2764\u0026gt;\u0026lt;U+FE0F\u0026gt;#1 147 31slnxm43mhcf6lle5nqucq2inha 26ZQg5BBS4cSgCQpskRaiO Uma música por dia 47 oliveiraarnon 2OCnbwpAiu1j0i2tWUj3w2 Uma música por dia 288 1193062474 2sC8NL6bwlecjjtHokZYns Todo dia uma música em 2022 179 2c71zh1ergjwcfdilvypipwlx 2BX2mKiEzgtAcbcwLR89iD uma música por dia 21 nvymfjifcztrj905o6epne2at 1sqHtyK0w3zZ5POn6ZayAE Uma Musica Boa por Dia 591 thiagogsimonetti 0B5jPflKKB2xxpOA9nlGYH Uma música por dia 90 31lkfpkfdkhaci2644cbwlokaspe 0Wlh0n3NR4sDwWu1xqMemG uma música por dia 51 12167607547 Veja que a playlist que eu queria encontrar está na segunda linha do data frame, vou usá-la na próxima função!\ngetPlaylistSongs Essa função pede como argumentos o id do usuário, o id da playlist e o token, e retorna um data frame com as músicas da playlist (e algumas informações adicionais sobre elas)\ngetPlaylistSongs(\u0026quot;therezzza\u0026quot;, \u0026quot;4faz4FVeL328piox0lz7UF\u0026quot;, token = keys) tracks id popularity artist artist_full artist_id album album_id Por Supuesto 7xVSNhAUQhUIpwfR6lTOwA 72 Marina Sena Marina Sena 0nFdWpwl7h6fp3ADRyG14L De Primeira 6bN5oG5wKJJqO0j7OhXVNt Metamorfose Ambulante 2Yk0HvfTaijA47aM0Fj88u 62 Raul Seixas Raul Seixas 7jrRQZg4FZq6dwpi3baKcu “Krig-Ha, Bandolo” 2xRddGxRH0KdDWLNE5b0iu Police Station 6DfWymHzCYOH2ABUuHFaMe 51 Red Hot Chili Peppers Red Hot Chili Peppers 0L8ExT028jH3ddEcZwqJJ5 I’m with You 5wZtSIvijWCMc1vlPFqAyB Tremores 5cCFZeltndzRmZ5cjtQdXs 16 Lau e Eu Lau e Eu 2gh3qTCBKZ5LwVj5zQha9e Selma 0u3OZX362nmVG918dwDUK1 E.C.T. 4Wt9zi9hvsgiUACRmoN9Ak 40 Cássia Eller Cássia Eller 10naVTwNjE50daQVrN0bXh Cássia Eller (1994) 3JGRDTWHXpXNfoAPglI7Tr Bote 1ZNV4geiDow8Wks8sKDQCc 56 Julio Secchin Julio Secchin 0B0FipO3fLbLbHQz5RfTUs Bote 5IJwNADxj5Kew8Yjdq7psH Enemy - From the series Arcane League of Legends 5UlnuulVAVmmesw4VzqHdG 82 Imagine Dragons Imagine Dragons feat. Arcane feat. League of Legends 53XhwfbYqKCa1cC15pYq2q Enemy (from the series Arcane League of Legends) 2Y9Kal9IDz6JphsCfY6GzG 6h 34 2gNPwfmBZMo2xzZFFPIt8P 39 Visconde Visconde 0ScxxqjnVMeXYqPl5AavFd Visconde 6XiK7cPd6XqlyiPN7NjTJO Sinusite 1rEOoUHBbnmPnlFS4p5P9t 40 Rodrigo Alarcon Rodrigo Alarcon 6D3gtdoxrtRGO9ZDU5wWSQ Parte 1Q8UuqiYlAHTmkLlpVdACL Nossa Bossa 2YynWd3NpoQC6NId0HPTeq 30 O Cinza O Cinza 2hhRYX2Zdh4jrvMLghsqGX O Lugar Onde Envelhecemos 3CBTykgeyVSdjc4zm7Mra6 searchTrack A função searchTrack pede como argumentos o nome de uma música (funciona como a busca do Spotify mesmo), e o token. Ela retorna um data frame no qual cada linha é referente a uma música (relacionada com a busca feita).\nNo exemplo, estou procurando a música “Geminis” da Julia Mestre.\nsearchTrack(\u0026quot;Geminis Julia Mestre\u0026quot;, token = keys) display_name id popularity artists artists_IDs type 5 Agora Só Falta Você 5fGQXRu7c4gZ2lGO8zlL51 45 Julia Mestre 1FnGKreDca8xq3juSi5hAE track 1 Geminis 7w6IN6ebiY2AOQwZY50NOG 33 Julia Mestre 1FnGKreDca8xq3juSi5hAE track 10 KIMBALA 0tPTI9u5fsapoRRf5dc7xC 28 Julia Mestre 1FnGKreDca8xq3juSi5hAE track 8 Cores e Nomes 3OxGALD40vE750AQ7SLGZr 25 Julia Mestre 1FnGKreDca8xq3juSi5hAE track 2 Mudar o Mundo 5SDERPI78k0GcVov3vkrRn 21 Julia Mestre 1FnGKreDca8xq3juSi5hAE track 13 el fuego del amor 7w28YPENfQgVuqQxUeltL0 18 Julia Mestre 1FnGKreDca8xq3juSi5hAE track 4 Batom 0lVHrmfGdS2xoAkEKbEcCI 13 Julia Mestre 1FnGKreDca8xq3juSi5hAE track 12 Papai Me Empresta o Carro 3C1iarbfPZ2bhWUVPUilSc 13 Julia Mestre 1FnGKreDca8xq3juSi5hAE track 6 Canto de Saudade 1c39CBruWQ4EkY1npaNKwL 11 Julia Mestre 1FnGKreDca8xq3juSi5hAE track 7 Medo de Mim 1UaDO9pwz7pZsudleZhbHR 11 Julia Mestre 1FnGKreDca8xq3juSi5hAE track getTrack Essa função pede como argumentos o id da música, e o token. Ela retorna algumas informações sobre a música.\ngetTrack(\u0026quot;7w6IN6ebiY2AOQwZY50NOG\u0026quot;, token = keys) track_id name explicit popularity artists artists_id album album_id 7w6IN6ebiY2AOQwZY50NOG Geminis FALSE 33 Julia Mestre 1FnGKreDca8xq3juSi5hAE GEMINIS 4y85scLBaZV25lBqLn0DC2 getFeatures Essa função pede como argumentos o id da música, e o token. Ela retorna algumas informações mais específicas sobre a música (e é a parte que acho mais legal). Essa informações são algumas “métricas” da música, e dá pra fazer muitas coisas legais com elas, como por exemplo, ordenar uma playlist pelas músicas mais dançantes.\ngetFeatures(\u0026quot;7w6IN6ebiY2AOQwZY50NOG\u0026quot;, token = keys) id danceability energy key loudness mode speechiness acousticness instrumentalness liveness valence tempo duration_ms time_signature uri analysis_url 7w6IN6ebiY2AOQwZY50NOG 0.614 0.29 11 -10.721 0 0.0418 0.683 8.5e-06 0.115 0.385 148.273 141867 4 spotify:track:7w6IN6ebiY2AOQwZY50NOG https://api.spotify.com/v1/audio-analysis/7w6IN6ebiY2AOQwZY50NOG getUser Essa função pede como argumentos o id de um usuário, e o token. Ela retorna algumas informações sobre esse usuário.\nNo caso, estou usando o id de uma conta minha no spotify, chamada “guiadosmusiqueiros”, que tem playlists de recomendações mais bem organizadas :)\ngetUser(\u0026quot;bn5u195uotc5u95tfw24ct6uf\u0026quot;, token = keys) display_name id followers guiadosmusiqueiros bn5u195uotc5u95tfw24ct6uf 51 getPlaylists E por fim, a função getPlaylists pede como argumentos o id de um usuário e o token. Ela retorna informações sobre cada playlist desse usuário!\ngetPlaylists(\u0026quot;bn5u195uotc5u95tfw24ct6uf\u0026quot;, token = keys) id name ownerid tracks 3KGktfVPwVBJGBnoCrUQK9 Guia dos Musiqueiros bn5u195uotc5u95tfw24ct6uf 42 3RdNIPmldyUYDJvIyHk6oU DUDA BEAT bn5u195uotc5u95tfw24ct6uf 7 191YgBaCVYbTksKDhj2P4c Lau e Eu bn5u195uotc5u95tfw24ct6uf 7 5ONYYUGvooOe8OoGIqICg2 PLUMA bn5u195uotc5u95tfw24ct6uf 7 5kSoTjqYeRbYWDxJfGvyRU Marina Sena bn5u195uotc5u95tfw24ct6uf 7 54XmEFNjzqXRQULMRWSgEZ Der Baum bn5u195uotc5u95tfw24ct6uf 7 5ys3bqipFkmdqppTF8pZk4 Mariana Froes bn5u195uotc5u95tfw24ct6uf 7 0j3ZjUaasSzektT6nShKPz Toco bn5u195uotc5u95tfw24ct6uf 7 7LUYIRlOPsqD1gJa9jX3do Seu Pereira e Coletivo 401 bn5u195uotc5u95tfw24ct6uf 7 4I0EHR3LKqmZB9JUzSX0sn Letrux bn5u195uotc5u95tfw24ct6uf 7 Por hoje é só, pessoal! Espero que o tutorial tenha ficado claro :)\n","permalink":"https://blog.curso-r.com/posts/2022-06-28-rspotify/","tags":["tutoriais","spotify","música"],"title":"Tudo sobre o pacote {Rspotify}"},{"author":["Beatriz","Tereza"],"categories":["tutoriais"],"contents":" Introdução Neste post, listamos alguns lugares onde encontrar bases de dados interessantes para quem está aprendendo e quer praticar com novas bases, e também para quem quer ensinar R e procura bases de dados para criar exemplos!\nO post será separado em: bases em português (sejam elas traduzidas ou originais), e em inglês.\nBases de dados em Português Pacote dados O pacote está disponível no CRAN e pode ser instalado utilizando o seguinte código:\ninstall.packages(\u0026quot;dados\u0026quot;) Escrevemos um post recentemente focado neste pacote, então recomendamos checar lá! Para acessar as bases de dados, utilize a função library(dados) ou use com dados::nome_da_base. Por exemplo:\nlibrary(dados) dplyr::glimpse(dados::pixar_filmes) ## Rows: 27 ## Columns: 5 ## $ ordem_lancamento \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;6\u0026quot;, \u0026quot;7\u0026quot;, \u0026quot;8\u0026quot;, \u0026quot;9\u0026quot;, … ## $ filme \u0026lt;chr\u0026gt; \u0026quot;Toy Story - Um Mundo de Aventuras\u0026quot;, \u0026quot;Vida de… ## $ data_lancamento \u0026lt;date\u0026gt; 1995-11-22, 1998-11-25, 1999-11-24, 2001-11-… ## $ duracao \u0026lt;dbl\u0026gt; 81, 95, 92, 92, 100, 115, 117, 111, 98, 96, 1… ## $ classificacao_indicativa \u0026lt;chr\u0026gt; \u0026quot;Livre\u0026quot;, \u0026quot;Livre\u0026quot;, \u0026quot;Livre\u0026quot;, \u0026quot;Livre\u0026quot;, \u0026quot;Livre\u0026quot;, … Base dos dados A Base dos Dados é um projeto super interessante. Lá, são disponibilizadas diversas bases públicas, e várias delas já estão tratadas, como: Relação Anual de Informações Sociais (RAIS), Índice de Desenvolvimento da Educação Básica (Ideb), Censo Demográfico, entre outros!\nConsulte as bases tratadas neste link.\nOs dados são disponibilizados em um BigQuery, portanto o acesso tem algumas etapas a mais. Recomendamos olhar os seguintes links:\nAcessando bases públicas em R: o pacote basedosdados Live da Curso-R - Unboxing do pacote basedosdados Documentação do BD+ Pacote de bases usadas em cursos da Curso-R Algumas bases de dados foram traduzidas por pessoas da Curso-R, e disponibilizadas nesse pacote para serem usadas nos cursos. Esse pacote não está disponível no CRAN, então caso queira instalar, utilize os seguintes comandos:\ninstall.packages(\u0026quot;devtools\u0026quot;) devtools::install_github(\u0026quot;curso-r/basesCursoR\u0026quot;) Para verificar quais são as bases disponíveis no pacote, utilize a seguinte função: basesCursoR::bases_disponiveis().\nbasesCursoR::bases_disponiveis() ## [1] \u0026quot;adult\u0026quot; \u0026quot;airbnb_rj\u0026quot; \u0026quot;ames\u0026quot; \u0026quot;cetesb\u0026quot; ## [5] \u0026quot;covid\u0026quot; \u0026quot;credito\u0026quot; \u0026quot;imdb_avaliacoes\u0026quot; \u0026quot;imdb_completa\u0026quot; ## [9] \u0026quot;imdb_pessoas\u0026quot; \u0026quot;imdb_top_cast\u0026quot; \u0026quot;imdb\u0026quot; \u0026quot;pokemon\u0026quot; ## [13] \u0026quot;ssp\u0026quot; Para carregar alguma base do pacote, utilize a função basesCursoR::pegar_base(\"nome_da_base\"), por exemplo:\npokemon \u0026lt;- basesCursoR::pegar_base(\u0026quot;pokemon\u0026quot;) dplyr::glimpse(pokemon) ## Rows: 949 ## Columns: 20 ## $ id \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,… ## $ id_especie \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,… ## $ id_geracao \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ pokemon \u0026lt;chr\u0026gt; \u0026quot;bulbasaur\u0026quot;, \u0026quot;ivysaur\u0026quot;, \u0026quot;venusaur\u0026quot;, \u0026quot;charmander\u0026quot;, \u0026quot;cha… ## $ altura \u0026lt;dbl\u0026gt; 0.7, 1.0, 2.0, 0.6, 1.1, 1.7, 0.5, 1.0, 1.6, 0.3, 0.7,… ## $ peso \u0026lt;dbl\u0026gt; 6.9, 13.0, 100.0, 8.5, 19.0, 90.5, 9.0, 22.5, 85.5, 2.… ## $ exp_base \u0026lt;dbl\u0026gt; 64, 142, 236, 62, 142, 240, 63, 142, 239, 39, 72, 178,… ## $ tipo_1 \u0026lt;chr\u0026gt; \u0026quot;grama\u0026quot;, \u0026quot;grama\u0026quot;, \u0026quot;grama\u0026quot;, \u0026quot;fogo\u0026quot;, \u0026quot;fogo\u0026quot;, \u0026quot;fogo\u0026quot;, \u0026quot;ág… ## $ tipo_2 \u0026lt;chr\u0026gt; \u0026quot;venenoso\u0026quot;, \u0026quot;venenoso\u0026quot;, \u0026quot;venenoso\u0026quot;, NA, NA, \u0026quot;voador\u0026quot;, … ## $ hp \u0026lt;dbl\u0026gt; 45, 60, 80, 39, 58, 78, 44, 59, 79, 45, 50, 60, 40, 45… ## $ ataque \u0026lt;dbl\u0026gt; 49, 62, 82, 52, 64, 84, 48, 63, 83, 30, 20, 45, 35, 25… ## $ defesa \u0026lt;dbl\u0026gt; 49, 63, 83, 43, 58, 78, 65, 80, 100, 35, 55, 50, 30, 5… ## $ ataque_especial \u0026lt;dbl\u0026gt; 65, 80, 100, 60, 80, 109, 50, 65, 85, 20, 25, 90, 20, … ## $ defesa_especial \u0026lt;dbl\u0026gt; 65, 80, 100, 50, 65, 85, 64, 80, 105, 20, 25, 80, 20, … ## $ velocidade \u0026lt;dbl\u0026gt; 45, 60, 80, 65, 80, 100, 43, 58, 78, 45, 30, 70, 50, 3… ## $ cor_1 \u0026lt;chr\u0026gt; \u0026quot;#78C850\u0026quot;, \u0026quot;#78C850\u0026quot;, \u0026quot;#78C850\u0026quot;, \u0026quot;#F08030\u0026quot;, \u0026quot;#F08030\u0026quot;,… ## $ cor_2 \u0026lt;chr\u0026gt; \u0026quot;#A040A0\u0026quot;, \u0026quot;#A040A0\u0026quot;, \u0026quot;#A040A0\u0026quot;, NA, NA, \u0026quot;#A890F0\u0026quot;, NA… ## $ cor_final \u0026lt;chr\u0026gt; \u0026quot;#81A763\u0026quot;, \u0026quot;#81A763\u0026quot;, \u0026quot;#81A763\u0026quot;, NA, NA, \u0026quot;#DE835E\u0026quot;, NA… ## $ url_imagem \u0026lt;chr\u0026gt; \u0026quot;https://raw.githubusercontent.com/phalt/pokeapi/maste… ## $ url_icone \u0026lt;chr\u0026gt; \u0026quot;//cdn.bulbagarden.net/upload/e/ec/001MS.png\u0026quot;, \u0026quot;//cdn.… Bases de dados em Inglês Bases do tidytuesday O tidytuesday é um projeto bem legal da comunidade de R! Toda semana é disponibilizada uma base de dados. QUALQUER pessoa que queira participar pode participar, fazendo uma visualização com a base disponibilizada!\nUm lugar legal para ver as visualizações criadas pelas pessoas é no twitter, pesquisando a hashtag #tidytuesday.\nÉ possível consultar a lista de bases de dados neste link: tidytuesday datasets. Ao entrar na página de uma base de dados, o código para carregar a mesma é mostrado. Por exemplo: para usar os dados de Jogos de tabuleiro, podemos usar o seguinte código:\njogos_ratings \u0026lt;- readr::read_csv(\u0026#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv\u0026#39;) dplyr::glimpse(jogos_ratings) ## Rows: 21,831 ## Columns: 10 ## $ num \u0026lt;dbl\u0026gt; 105, 189, 428, 72, 103, 191, 100, 3, 15, 35, 30, 182, 13… ## $ id \u0026lt;dbl\u0026gt; 30549, 822, 13, 68448, 36218, 9209, 178900, 167791, 1733… ## $ name \u0026lt;chr\u0026gt; \u0026quot;Pandemic\u0026quot;, \u0026quot;Carcassonne\u0026quot;, \u0026quot;Catan\u0026quot;, \u0026quot;7 Wonders\u0026quot;, \u0026quot;Domini… ## $ year \u0026lt;dbl\u0026gt; 2008, 2000, 1995, 2010, 2008, 2004, 2015, 2016, 2015, 20… ## $ rank \u0026lt;dbl\u0026gt; 106, 190, 429, 73, 104, 192, 101, 4, 16, 36, 31, 183, 14… ## $ average \u0026lt;dbl\u0026gt; 7.59, 7.42, 7.14, 7.74, 7.61, 7.41, 7.60, 8.42, 8.11, 7.… ## $ bayes_average \u0026lt;dbl\u0026gt; 7.487, 7.309, 6.970, 7.634, 7.499, 7.305, 7.508, 8.274, … ## $ users_rated \u0026lt;dbl\u0026gt; 108975, 108738, 108024, 89982, 81561, 76171, 74419, 7421… ## $ url \u0026lt;chr\u0026gt; \u0026quot;/boardgame/30549/pandemic\u0026quot;, \u0026quot;/boardgame/822/carcassonne… ## $ thumbnail \u0026lt;chr\u0026gt; \u0026quot;https://cf.geekdo-images.com/S3ybV1LAp-8SnHIXLLjVqA__mi… Pacote ISLR O pacote ISLR acompanha o livro Introduction to Statistical Learning (disponível gratuitamente aqui). Apresenta bases de dados interessantes para treinar modelagem, acompanhando os exemplos do livro.\nVocê pode instalar o pacote através do CRAN, usando a função install.packages():\ninstall.packages(\u0026quot;ISLR\u0026quot;) Essas são as bases de dados disponíveis no pacote, que podem ser acessadas utilizando a função library(ISLR) ou use com ISLR::nome_da_base. :\nNome Descrição Auto Auto Data Set Caravan The Insurance Company (TIC) Benchmark Carseats Sales of Child Car Seats College U.S. News and World Report’s College Data Credit Credit Card Balance Data Default Credit Card Default Data Hitters Baseball Data Khan Khan Gene Data NCI60 NCI 60 Data OJ Orange Juice Data Portfolio Portfolio Data Smarket S\u0026amp;P Stock Market Data Wage Mid-Atlantic Wage Data Weekly Weekly S\u0026amp;P Stock Market Data Kaggle O Kaggle é uma plataforma onde é possível participar de competições de Machine Learning!\nPor lá, também estão disponíveis diversas bases de dados, na página de datasets. Um ponto importante para citar é que qualquer pessoa consegue fazer upload de uma base lá no Kaggle, então pesquise com calma e veja as bases mais bem avaliadas no tema que você quer explorar! Depois que escolher, é possível baixar a base de dados e importar para o R.\nConclusão Esperamos que este post seja interessante para quem está aprendendo e quer praticar com novas bases, e também para quem quer ensinar R e procura bases de dados para criar exemplos! Bons estudos e até a próxima!\n","permalink":"https://blog.curso-r.com/posts/2022-06-11-bases-de-dados/","tags":["ensino","bases de dados","importação"],"title":"Bases de dados: para praticar e ensinar"},{"author":["Tereza"],"categories":["tutoriais"],"contents":" Você já conhece o skimr?\nO skimr é um pacote que nos fornece medidas resumo de variáveis de uma base de dados de interesse. Ele pode ser visto como uma alternativa mais completa para a função summary() do R Base.\nAlém de incluir algumas informações a mais sobre cada coluna (como número de missings, por exemplo), ele lida com todos os tipos de dados, não só numéricos, fornecendo um conjunto diferente de medidas resumo a depender do tipo da coluna.\nGostou? Vamos dar uma olhada em como ele funciona!\nskimr Primeiramente, vamos instalar e carregar o pacote skimr\ninstall.packages(\u0026quot;skimr\u0026quot;) library(skimr) ## Warning: package \u0026#39;skimr\u0026#39; was built under R version 4.1.3 Agora, vamos aplicar a função skim em uma base de dados de interesse. No caso, estamos usando a base dados_starwars do pacote dados\ndados::dados_starwars |\u0026gt; skim() Tabela 1: Data summary Name dados::dados_starwars Number of rows 87 Number of columns 14 _______________________ Column type frequency: character 8 list 3 numeric 3 ________________________ Group variables None Variable type: character\nskim_variable n_missing complete_rate min max empty n_unique whitespace nome 0 1.00 3 21 0 87 0 cor_do_cabelo 5 0.94 5 20 0 11 0 cor_da_pele 0 1.00 4 28 0 31 0 cor_dos_olhos 0 1.00 4 19 0 14 0 sexo_biologico 4 0.95 5 12 0 4 0 genero 4 0.95 8 9 0 2 0 planeta_natal 10 0.89 4 14 0 48 0 especie 4 0.95 3 15 0 37 0 Variable type: list\nskim_variable n_missing complete_rate n_unique min_length max_length filmes 0 1 24 1 7 veiculos 0 1 11 0 2 naves_espaciais 0 1 17 0 5 Variable type: numeric\nskim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist altura 6 0.93 174.36 34.77 66 167.0 180 191.0 264 ▁▁▇▅▁ massa 28 0.68 97.31 169.46 15 55.6 79 84.5 1358 ▇▁▁▁▁ ano_nascimento 44 0.49 87.57 154.69 8 35.0 52 72.0 896 ▇▁▁▁▁ Veja que a saída da função skim mostra uma visão geral da base de dados, nos dando informações como número de linhas, número de colunas, e os tipos das colunas.\nEla fornece também informações individuais sobre cada coluna da base, separando as colunas por tipo: cada tipo nos dá um conjunto diferente de estatísticas, que façam sentido para aquele tipo de dado.\nAlém disso, se salvarmos a saída da função em um objeto, podemos obter um data frame no qual cada linha é referente a uma coluna da base de dados.\nIsso é muito útil quando estamos lidando com bases que tem muitas colunas.\ninfo_skim \u0026lt;- dados::dados_starwars |\u0026gt; skim() |\u0026gt; tibble::as_tibble() Podemos, por exemplo, ordenar pelas variáveis que tem maior número de NA:\ninfo_skim |\u0026gt; dplyr::arrange(desc(n_missing)) ## # A tibble: 14 x 20 ## skim_type skim_variable n_missing complete_rate character.min character.max ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 numeric ano_nascimento 44 0.494 NA NA ## 2 numeric massa 28 0.678 NA NA ## 3 character planeta_natal 10 0.885 4 14 ## 4 numeric altura 6 0.931 NA NA ## 5 character cor_do_cabelo 5 0.943 5 20 ## 6 character sexo_biologico 4 0.954 5 12 ## 7 character genero 4 0.954 8 9 ## 8 character especie 4 0.954 3 15 ## 9 character nome 0 1 3 21 ## 10 character cor_da_pele 0 1 4 28 ## 11 character cor_dos_olhos 0 1 4 19 ## 12 list filmes 0 1 NA NA ## 13 list veiculos 0 1 NA NA ## 14 list naves_espaciais 0 1 NA NA ## # ... with 14 more variables: character.empty \u0026lt;int\u0026gt;, character.n_unique \u0026lt;int\u0026gt;, ## # character.whitespace \u0026lt;int\u0026gt;, list.n_unique \u0026lt;int\u0026gt;, list.min_length \u0026lt;int\u0026gt;, ## # list.max_length \u0026lt;int\u0026gt;, numeric.mean \u0026lt;dbl\u0026gt;, numeric.sd \u0026lt;dbl\u0026gt;, ## # numeric.p0 \u0026lt;dbl\u0026gt;, numeric.p25 \u0026lt;dbl\u0026gt;, numeric.p50 \u0026lt;dbl\u0026gt;, numeric.p75 \u0026lt;dbl\u0026gt;, ## # numeric.p100 \u0026lt;dbl\u0026gt;, numeric.hist \u0026lt;chr\u0026gt; Você pode ler mais sobre as funcionalidades do skimr nesse vignette\n","permalink":"https://blog.curso-r.com/posts/2022-05-05-skimr/","tags":["tutoriais"],"title":"Conheça o pacote {skimr}"},{"author":["Beatriz"],"categories":["pacotes"],"contents":" Introdução O pacote dados disponibiliza a tradução de várias bases de dados que são originalmente disponíveis em outros pacotes de R. Recentemente o pacote foi disponibilizado no CRAN! Esse post foi escrito para que as pessoas conheçam mais sobre ele!\nPacote dados O pacote dados disponibiliza a tradução de várias bases de dados que são originalmente disponíveis em outros pacotes de R.\nO pacote começou a ser desenvolvido em Junho de 2020, e é um irmão do pacote datos. O datos foi desenvolvido para ser usado na tradução do livro R para Ciência de Dados em espanhol, feita volutariamente pela comunidade Latino-Americana de R.\nUm destaque importantíssimo é que as traduções do pacote dados foram feitas voluntariamente por pessoas da comunidade Latino-Americana de R, contando com a colaboração de pessoas que fazem parte da Latin-R, R-Ladies e Curso-R, sendo elas: Riva Quiroga, Sara Mortara, Beatriz Milz (professora na Curso-R), Andrea Sánchez-Tapia, Alejandra Andrea Tapia Silva, Beatriz Maurer Costa, Jean Prado, Renata Hirota, William Amorim (professor na Curso-R), e Emmanuelle Rodrigues Nunes.\nAlguns dos objetivos do pacote dados são:\nDisponibilizar bases de dados em português utilizadas para praticar e ensinar R (como em tutoriais, cursos, textos em blogs, livros). Por exemplo, estão disponíveis lá todas as bases usadas no livro R for Data Science, entre outras;\nIncentivar pessoas a ser tornarem desenvolvedoras em R (várias pessoas que participaram do desenvolvimento do pacote e realizaram traduções não tinham experiência anterior com colaboração via GitHub e desenvolvimento de pacotes em R, por exemplo);\nColaborar com a diminuição da lacuna linguística em materiais usados no ensino e prática de R.\nO pacote está disponível no CRAN e pode ser instalado utilizando o seguinte código:\ninstall.packages(\u0026quot;dados\u0026quot;) Para que as bases fiquem disponíveis, utilize a função library(dados) ou use com dados::nome_da_base.\nTodas as bases possuem a documentação traduzida também. Você pode conferir na aba Help, usando ?dados::nome_da_base ou conferindo na documentação online do pacote.\nExistem bases bem legais disponíveis, como:\nStar Wars A base de dados de personagens da franquia Star Wars, disponibilizada originalmente no pacote dplyr. Por isso, é esperado que seja bastante útil em exemplos de manipulação de dados. Porém podemos criar visualizações bem legais também usando esses dados! Aliás, essa é a base utilizada no desafio 3, focado em visualização de dados, com o objetivo de replicar esse gráfico: Espie a base por aqui:\ndplyr::glimpse(dados::dados_starwars) ## Rows: 87 ## Columns: 14 ## $ nome \u0026lt;chr\u0026gt; \u0026quot;Luke Skywalker\u0026quot;, \u0026quot;C-3PO\u0026quot;, \u0026quot;R2-D2\u0026quot;, \u0026quot;Darth Vader\u0026quot;, \u0026quot;Le… ## $ altura \u0026lt;int\u0026gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 1… ## $ massa \u0026lt;dbl\u0026gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0… ## $ cor_do_cabelo \u0026lt;chr\u0026gt; \u0026quot;Loiro\u0026quot;, NA, NA, \u0026quot;Nenhum\u0026quot;, \u0026quot;Castanho\u0026quot;, \u0026quot;Castanho, Cinz… ## $ cor_da_pele \u0026lt;chr\u0026gt; \u0026quot;Branca clara\u0026quot;, \u0026quot;Ouro\u0026quot;, \u0026quot;Branca, Azul\u0026quot;, \u0026quot;Branca\u0026quot;, \u0026quot;Cla… ## $ cor_dos_olhos \u0026lt;chr\u0026gt; \u0026quot;Azul\u0026quot;, \u0026quot;Amarelo\u0026quot;, \u0026quot;Vermelho\u0026quot;, \u0026quot;Amarelo\u0026quot;, \u0026quot;Castanho\u0026quot;, … ## $ ano_nascimento \u0026lt;dbl\u0026gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 5… ## $ sexo_biologico \u0026lt;chr\u0026gt; \u0026quot;Macho\u0026quot;, \u0026quot;Nenhum\u0026quot;, \u0026quot;Nenhum\u0026quot;, \u0026quot;Macho\u0026quot;, \u0026quot;Fêmea\u0026quot;, \u0026quot;Macho\u0026quot;… ## $ genero \u0026lt;chr\u0026gt; \u0026quot;Masculino\u0026quot;, \u0026quot;Masculino\u0026quot;, \u0026quot;Masculino\u0026quot;, \u0026quot;Masculino\u0026quot;, \u0026quot;F… ## $ planeta_natal \u0026lt;chr\u0026gt; \u0026quot;Tatooine\u0026quot;, \u0026quot;Tatooine\u0026quot;, \u0026quot;Naboo\u0026quot;, \u0026quot;Tatooine\u0026quot;, \u0026quot;Alderaan… ## $ especie \u0026lt;chr\u0026gt; \u0026quot;Humano\u0026quot;, \u0026quot;Droide\u0026quot;, \u0026quot;Droide\u0026quot;, \u0026quot;Humano\u0026quot;, \u0026quot;Humano\u0026quot;, \u0026quot;Hum… ## $ filmes \u0026lt;list\u0026gt; \u0026lt;\u0026quot;The Empire Strikes Back\u0026quot;, \u0026quot;Revenge of the Sith\u0026quot;, \u0026quot;R… ## $ veiculos \u0026lt;list\u0026gt; \u0026lt;\u0026quot;Snowspeeder\u0026quot;, \u0026quot;Imperial Speeder Bike\u0026quot;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;,… ## $ naves_espaciais \u0026lt;list\u0026gt; \u0026lt;\u0026quot;X-wing\u0026quot;, \u0026quot;Imperial shuttle\u0026quot;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026quot;TIE Advanced… Pinguins A base dos pinguins foi disponibilizada com o objetivo de ser uma alternativa à base de dados iris. Podemos utilizar essa base para ensinar e praticar principalmente temas de visualização e manipulação de dados.\nAlgo muito legal são as vignettes do pacote original, que apresentam tutoriais sobre como fazer visualizações interessantes com os dados. Que tal experimentar recriá-las utilizando os dados em português?\nEspie a base por aqui:\ndplyr::glimpse(dados::pinguins) ## Rows: 344 ## Columns: 8 ## $ especie \u0026lt;fct\u0026gt; Pinguim-de-adélia, Pinguim-de-adélia, Pinguim-de… ## $ ilha \u0026lt;fct\u0026gt; Torgersen, Torgersen, Torgersen, Torgersen, Torg… ## $ comprimento_bico \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34… ## $ profundidade_bico \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18… ## $ comprimento_nadadeira \u0026lt;int\u0026gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190,… ## $ massa_corporal \u0026lt;int\u0026gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 34… ## $ sexo \u0026lt;fct\u0026gt; macho, fêmea, fêmea, NA, fêmea, macho, fêmea, ma… ## $ ano \u0026lt;int\u0026gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, … Filmes da Pixar O pacote pixarfilms apresenta algumas bases de dados com informações sobre filmes da Pixar. Todas as bases foram traduzidas e disponibilizadas no pacote dados! Essas bases podem ser utilizadas para treinar manipulação e visualização de dados. Na minha opinião, essas bases são super interessantes para ensinar os concentos dos joins do dplyr!\nO post ‘Pixar: Uma História de dados’, foi escrito por Fernando Barbalho, e publicado no Blog da Escola de Dados, utilizando os dados deste pacote. No meu blog há uma versão do post com os códigos para gerar todas as visualizações.\nEspie as bases por aqui:\ndplyr::glimpse(dados::pixar_filmes) ## Rows: 27 ## Columns: 5 ## $ ordem_lancamento \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;6\u0026quot;, \u0026quot;7\u0026quot;, \u0026quot;8\u0026quot;, \u0026quot;9\u0026quot;, … ## $ filme \u0026lt;chr\u0026gt; \u0026quot;Toy Story - Um Mundo de Aventuras\u0026quot;, \u0026quot;Vida de… ## $ data_lancamento \u0026lt;date\u0026gt; 1995-11-22, 1998-11-25, 1999-11-24, 2001-11-… ## $ duracao \u0026lt;dbl\u0026gt; 81, 95, 92, 92, 100, 115, 117, 111, 98, 96, 1… ## $ classificacao_indicativa \u0026lt;chr\u0026gt; \u0026quot;Livre\u0026quot;, \u0026quot;Livre\u0026quot;, \u0026quot;Livre\u0026quot;, \u0026quot;Livre\u0026quot;, \u0026quot;Livre\u0026quot;, … dplyr::glimpse(dados::pixar_avalicao_publico) ## Rows: 24 ## Columns: 5 ## $ filme \u0026lt;chr\u0026gt; \u0026quot;Toy Story - Um Mundo de Aventuras\u0026quot;, \u0026quot;Vida de Ins… ## $ nota_rotten_tomatoes \u0026lt;dbl\u0026gt; 100, 92, 100, 96, 99, 97, 74, 96, 95, 98, 98, 40,… ## $ nota_metacritic \u0026lt;dbl\u0026gt; 95, 77, 88, 79, 90, 90, 73, 96, 95, 88, 92, 57, 6… ## $ nota_cinema_score \u0026lt;chr\u0026gt; \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;A+\u0026quot;, \u0026quot;A+\u0026quot;, \u0026quot;A+\u0026quot;, \u0026quot;A+\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, … ## $ nota_critics_choice \u0026lt;dbl\u0026gt; NA, NA, 100, 92, 97, 88, 89, 91, 90, 95, 97, 67, … dplyr::glimpse(dados::pixar_bilheteria) ## Rows: 24 ## Columns: 5 ## $ filme \u0026lt;chr\u0026gt; \u0026quot;Toy Story - Um Mundo de Aventuras\u0026quot;, \u0026quot;Vida de… ## $ orcamento \u0026lt;dbl\u0026gt; 3.00e+07, 1.20e+08, 9.00e+07, 1.15e+08, 9.40e… ## $ bilheteria_eua_canada \u0026lt;dbl\u0026gt; 191796233, 162798565, 245852179, 289916256, 3… ## $ bilheteria_outros_paises \u0026lt;dbl\u0026gt; 181757800, 200460294, 251522597, 342400393, 5… ## $ bilheteria_mundial \u0026lt;dbl\u0026gt; 373554033, 363258859, 497374776, 632316649, 8… dplyr::glimpse(dados::pixar_equipe) ## Rows: 220 ## Columns: 3 ## $ filme \u0026lt;chr\u0026gt; \u0026quot;Toy Story - Um Mundo de Aventuras\u0026quot;, \u0026quot;Toy Story - Um Mundo de Av… ## $ cargo \u0026lt;chr\u0026gt; \u0026quot;Diretor(a)\u0026quot;, \u0026quot;Roteirista\u0026quot;, \u0026quot;Roteirista\u0026quot;, \u0026quot;Roteirista\u0026quot;, \u0026quot;Roteiri… ## $ nome \u0026lt;chr\u0026gt; \u0026quot;John Lasseter\u0026quot;, \u0026quot;Joel Cohen\u0026quot;, \u0026quot;Alec Sokolow\u0026quot;, \u0026quot;Andrew Stanton\u0026quot;,… dplyr::glimpse(dados::pixar_generos) ## Rows: 128 ## Columns: 2 ## $ filme \u0026lt;chr\u0026gt; \u0026quot;Toy Story - Um Mundo de Aventuras\u0026quot;, \u0026quot;Toy Story - Um Mundo de A… ## $ genero \u0026lt;chr\u0026gt; \u0026quot;Animação\u0026quot;, \u0026quot;Aventura\u0026quot;, \u0026quot;Comédia\u0026quot;, \u0026quot;Família\u0026quot;, \u0026quot;Fantasia\u0026quot;, \u0026quot;Anim… dplyr::glimpse(dados::pixar_oscars) ## Rows: 80 ## Columns: 3 ## $ filme \u0026lt;chr\u0026gt; \u0026quot;Toy Story - Um Mundo de Aventuras\u0026quot;, \u0026quot;Toy Story -… ## $ tipo_premio_indicado \u0026lt;chr\u0026gt; \u0026quot;Melhor Filme de Animação\u0026quot;, \u0026quot;Melhor Roteiro Origi… ## $ resultado \u0026lt;chr\u0026gt; \u0026quot;Prêmio ainda não introduzido\u0026quot;, \u0026quot;Nomeado\u0026quot;, \u0026quot;Inele… Todas as bases disponíveis Veja o nome e a descrição de todas as bases disponíveis:\nNome Descrição aeroportos Aeroportos arremesadores Tabela de estatísticas de arremesadores avioes Dados de aviões bebes Nomes de bebês casas Dados de habitação em Ames clima Dados de clima companhias_aereas Nomes de companhias aéreas comuns Modelos comuns de carros dados_atmosfera Dados atmosféricos dados_gapminder Dados de Gapminder dados_iris Dados de espécies de flor de Íris por Edgar Anderson dados_oms Dados de tuberculose da Organização Mundial da Saúde dados_starwars Personagens de Starwars diamante Preço de 50 mil diamantes gerentes Tabela de gerentes jardineiros Tabela de estatísticas do jardineiro milhas Dados de economia de combustível de 1999 a 2008 para 38 modelos populares de carros mtcarros Testes de estrada para automóveis pessoas Tabela de pessoas pinguins Medidas de pinguins adultos perto da Estação Palmer, Antártida (Palmer Station) pixar_avalicao_publico Filmes da Pixar e avaliações pixar_bilheteria Bilheteria dos filmes da Pixar pixar_equipe Equipe dos filmes pixar_filmes Filmes da Pixar pixar_generos Gêneros dos filmes da Pixar pixar_oscars Filmes da Pixar com indicações ao Oscar premios_gerentes Tabela de prêmios dos gerentes presidentes_eua Período que engloba 11 presidentes, desde Eisenhower até Obama questionario Amostra de variáveis categóricas do questinário ‘General Social Survey’ (GSS) rebatedores Tabela de estatíticas de Beisebol salarios Tabela de salários tabela1 Registros de tuberculose da Organização Mundial da Saúde (primeira variante) tabela2 Registros de tuberculose da Organização Mundial da Saúde (segunda variante) tabela3 Registros de tuberculose da Organização Mundial da Saúde (terceira variante) tabela4a Registros de tuberculose da Organização Mundial da Saúde (variante 4a) tabela4b Registros de tuberculose da Organização Mundial da Saúde (variante 4a) tabela5 Registros de tuberculose da Organização Mundial da Saúde (quinta variante) veiculos Dados de economia de combustível velho_fiel Dados do gêiser Velho Fiel (Old Faithful) voos Dados de voos Conclusão Espero que o post tenha sido uma boa introdução ao pacote dados, seja você uma pessoa que deseja ensinar ou praticar R.\nCaso tenha sugestões de melhoria nas traduções, escreva uma issue no repositório do pacote!\nAté a próxima!\n","permalink":"https://blog.curso-r.com/posts/2022-04-06-pacote-dados/","tags":["ensino","bases de dados","importação"],"title":"Pacote dados: bases em português para praticar e ensinar"},{"author":["Tereza"],"categories":["Desafio"],"contents":" Os desafios da Curso-R são problemas práticos de análise de dados e programação envolvendo faxina de dados, construção de gráficos, relatórios em RMarkdown, modelagem, aplicativos Shiny e muito mais.\nO intuito é praticar as diversas etapas da Ciência de Dados, interagir com a comunidade compartilhando as suas soluções e gerar bastante repertório.\nVale sempre lembrar que:\nNão existe uma única solução para o desafio.\nVocê pode usar qualquer linguagem de programação e quaisquer ferramentas/pacotes da linguagem.\nNão existe a melhor solução. Toda solução é válida. No dia-a-dia o importante é resolver o problema.\nPara compartilhar a sua solução, vamos usar o Discourse da Curso-R. Basta acessar o tópico referente a esse desafio (no final deste post colocamos o link) e enviar o seu código. Você pode aproveitar também para pedir ajuda ou ver e comentar as soluções já enviadas.\nDesafio #3: Gráfico Temático Star Wars O objetivo desse desafio é escrever um código que reproduza o seguinte gráfico:\nAlgumas informações úteis:\nA base de dados utilizada é a dados_starwars do pacote dados (pode ser instalado com o comando install.packages(\"dados\"))\nO link da imagem de fundo do gráfico é: https://wallpaperaccess.com/full/11836.jpg\nA fonte utilizada pode ser baixada aqui\nPoste a sua resolução em nosso Discourse, respondendo o tópico criado para esse desafio!\n","permalink":"https://blog.curso-r.com/posts/2022-04-05-desafio-3/","tags":["desafio curso-r","visualização"],"title":"Desafio #3: Gráfico Temático Star Wars"},{"author":["Tereza"],"categories":["tutoriais"],"contents":" Você já conhece o esquisse?\nO esquisse é um pacote que permite a criação de gráficos em {ggplot2} de maneira point and click, o que torna a construção de gráficos uma tarefa bem mais simples, pois não há necessidade de digitar as linhas de código!\nAlém disso, é possível recuperar o código que gerou os gráficos, o que é ótimo para garantir a reprodutibilidade dos resultados!\nLegal, né?\nAqui vai um breve tutorial de como usar essa ferramenta:\nInstale o pacote {esquisse}: install.packages(\u0026quot;esquisse\u0026quot;) Carregue o pacote library(esquisse) ## Warning: package \u0026#39;esquisse\u0026#39; was built under R version 4.1.3 Carregue os seus dados. No nosso caso, estamos usando a base de dados pinguins, do pacote {dados} (Para instalar esse pacote, basta rodar o comando install.packages(\"dados\"))\ndados \u0026lt;- dados::pinguins Execute o seguinte comando: dados |\u0026gt; esquisser() E então, uma página como essa será aberta:\nÉ nessa interface/página que criaremos o nosso gráfico!\nCrie e edite o gráfico na maneira que desejar! Na imagem abaixo, podemos ver os botões para escolher o tipo de gráfico, e quais variáveis estarão mapeadas em cada atributo estético.\nNa aba “Labels \u0026amp; Title” você pode editar os títulos e legendas:\nNa aba “Appearance” você pode mudar as cores e aparência do gráfico, de maneira geral:\nAlém disso, na aba “Data” é possível filtrar algumas variáveis da base de dados, e isso é feito com a função filter do {dplyr}:\nE enfim, na aba “Code” você pode recuperar o código que gera o gráfico, para colocá-lo em sua análise de dados, por exemplo!\nAo copiar o código gerado e executar no R, o seguinte gráfico é gerado:\nlibrary(ggplot2) ggplot(dados) + aes(x = comprimento_bico, y = comprimento_nadadeira, colour = especie) + geom_point(shape = \u0026quot;circle\u0026quot;, size = 2.1) + scale_color_viridis_d(option = \u0026quot;plasma\u0026quot;, direction = 1) + labs(x = \u0026quot;legenda do eixo x\u0026quot;, y = \u0026quot;legenda do eixo y\u0026quot;, title = \u0026quot;Título do Gráfico!\u0026quot;, subtitle = \u0026quot;Subtítulo do Gráfico!\u0026quot;) + theme_minimal() ## Warning: Removed 2 rows containing missing values (geom_point). ","permalink":"https://blog.curso-r.com/posts/2022-03-22-esquisse/","tags":["tutoriais"],"title":"Criando gráficos ggplot2 com o pacote {esquisse}"},{"author":["Beatriz"],"categories":["pacotes"],"contents":" Introdução Recentemente, escrevi um post sobre como é possível acessar dados do Google Sheets com R, além de importar respostas do Google Forms. Algumas pessoas comentaram que utilizam o One Drive, e perguntaram como funcionaria para importar tabelas que estão por lá. Como eu não uso One Drive, dei uma explorada para responder, e neste post falarei um pouco sobre como podemos importar dados que estão na conta do One Drive usando o pacote Microsoft365R.\nO pacote Microsoft365R é uma interface para acessar o Microsoft 365, e segundo a documentação do pacote, possui funções para interagir com o Microsoft Teams, Outlook, SharePoint Online, e o OneDrive (que é o foco deste post!).\nPrimeiramente, é necessário instalar o pacote:\ninstall.packages(\u0026quot;Microsoft365R\u0026quot;) Então, podemos carregá-lo:\n# Carregar pacote Microsoft365R library(Microsoft365R) Ao carregar o pacote pela primeira vez, a seguinte mensagem aparecerá, dizendo que é possível armazenar as credenciais em uma pasta chamada AzureR e perguntando se permitimos que o pacote crie esse diretório. Eu aceitei respondendo com Yes:\nThe AzureR packages can save your authentication credentials in the directory: ~/Library/Application Support/AzureR This saves you having to re-authenticate with Azure in future sessions. Create this directory? (Yes/no/cancel) Depois de responder com Yes, nada aparentemente acontece, porém o pacote é carregado!\nAutenticação Ao acessar pela primeira vez, o pacote abrirá uma aba do navegador para fazer a autenticação.\nNeste exemplo, usarei a função Microsoft365R::get_personal_onedrive() que explicarei posteriormente. No console do R, aparecerá a seguinte mensagem ao utilizar alguma função pela primeira vez:\nMicrosoft365R::get_personal_onedrive() # Using authorization_code flow # Creating Microsoft Graph login for tenant \u0026#39;consumers\u0026#39; # Using authorization_code flow # Waiting for authentication in browser... # Press Esc/Ctrl + C to abort O navegador abrirá uma janela para que a autenticação seja feita:\nApós fazer a autenticação, a seguinte mensagem aparecerá no navegador:\nAuthenticated with Azure Active Directory. Please close this page and return to R. Isso significa que podemos fechar a aba do navegador e retornar ao R. No console do R, aparecerá a seguinte mensagem:\nAuthentication complete. Acesso de conta pessoal ou de trabalho? O primeiro passo após autenticar é acessar o One Drive, e a função para fazer isso depende do tipo de acesso da conta: pessoal ou de trabalho. Como o meu acesso é pessoal, vou usar a função get_personal_onedrive(). Mas caso o seu acesso seja de trabalho, utilize a função get_business_onedrive(). É necessário executar a função e salvar o resultado em um objeto.\nonedrive_da_bea \u0026lt;- Microsoft365R::get_personal_onedrive() Agora a forma de interagir com o pacote é um pouco diferente de como usamos na maior parte das vezes. Isso porque o pacote utiliza objetos que são da classe R6, e que permitem utilizar métodos. O foco deste post não é explicar o que são as classes R6, e escrevi este post pensando em ser útil para pessoas que também não conhecem as classes R6. Caso você tenha interesse em saber mais, deixei alguns links de referência no final do post.\nPara conferir a classe desse objeto, podemos usar a função class():\nclass(onedrive_da_bea) #\u0026gt; [1] \u0026quot;ms_drive\u0026quot; \u0026quot;ms_object\u0026quot; \u0026quot;R6\u0026quot; Ao executar apenas o objeto, o console apresenta os métodos possíveis:\ndrive_da_bea #\u0026gt; \u0026lt;Personal OneDrive of Beatriz Milz\u0026gt; #\u0026gt; directory id: df5a76afb6532845 #\u0026gt; --- #\u0026gt; Methods: #\u0026gt; create_folder, create_share_link, delete, #\u0026gt; delete_item, do_operation, download_file, #\u0026gt; get_item, get_item_properties, get_list_pager, #\u0026gt; list_files, list_items, list_shared_files, #\u0026gt; list_shared_items, open_item, #\u0026gt; set_item_properties, sync_fields, update, #\u0026gt; upload_file Sabendo o nome dos métodos, podemos usá-los usando o seguinte padrão: nome_do_objeto$nome_do_metodo()! Existem vários métodos, e vamos explorar os métodos relacionados à importação. Mas caso você use bastante o One Drive no seu dia-a-dia, recomendo experimentar os outros métodos, como os que servem para manipular arquivos (como criar, atualizar, deletar)!\nNavegar nos arquivos do Drive Para conseguir importar um arquivo, é necessário primeiramente encontrar o caminho até este arquivo. E isso funciona de forma similar aos caminhos que já conhecemos nos nossos projetos! Vamos usar o método list_files() para listar os arquivos presentes na pasta inicial do meu OneDrive. Como eu raramente uso ele, criei uma pasta de exemplo chamada pasta_de_exemplo, e dentro dela criei um arquivo Excel chamado pinguins. Veja que apareceu apenas a pasta, e não o conteúdo que ela armazena:\nonedrive_da_bea$list_files() |\u0026gt; dplyr::glimpse() #\u0026gt; Rows: 1 #\u0026gt; Columns: 4 #\u0026gt; $ name \u0026lt;chr\u0026gt; \u0026quot;pasta_de_exemplo\u0026quot; #\u0026gt; $ size \u0026lt;int\u0026gt; 23041 #\u0026gt; $ isdir \u0026lt;lgl\u0026gt; TRUE #\u0026gt; $ id \u0026lt;chr\u0026gt; \u0026quot;DF5A76AFB6532845!4368\u0026quot; Para ver os arquivos que estão dentro da pasta (e encontrar o excel pinguins), podemos usar o mesmo método utilizado acima, e informar o nome da pasta que queremos listar os arquivos.\nonedrive_da_bea$list_files(path = \u0026quot;pasta_de_exemplo/\u0026quot;) |\u0026gt; dplyr::glimpse() #\u0026gt; Rows: 1 #\u0026gt; Columns: 4 #\u0026gt; $ name \u0026lt;chr\u0026gt; \u0026quot;pinguins.xlsx\u0026quot; #\u0026gt; $ size \u0026lt;int\u0026gt; 23041 #\u0026gt; $ isdir \u0026lt;lgl\u0026gt; FALSE #\u0026gt; $ id \u0026lt;chr\u0026gt; \u0026quot;DF5A76AFB6532845!4369\u0026quot; Importar o arquivo Eu ainda não encontrei um método que permita ler o conteúdo de um Excel diretamente do OneDrive para o R (como fazemos com o Google Drive). Olhando nas issues do GitHub, parece que essa funcionalidade ainda não foi implementada até o momento, e algumas pessoas inclusive sugeriram melhorias neste sentido.\nA forma que encontrei para importar um Excel do Ondrive foi fazer o download do arquivo Excel que está no Ondrive, e depois importar o arquivo salvo localmente. Para fazer o download e abrir um arquivo Excel, podemos usar o método download_file() para baixar o arquivo, e então a função readxl::read_excel() para importar o arquivo:\n# baixar o arquivo onedrive_da_bea$download_file(\u0026quot;pasta_de_exemplo/pinguins.xlsx\u0026quot;, dest = \u0026quot;pinguins_onedrive.xlsx\u0026quot;) # ler o arquivo baixado pinguins_do_onedrive \u0026lt;- readxl::read_excel(\u0026quot;pinguins_onedrive.xlsx\u0026quot;) dplyr::glimpse(pinguins_do_onedrive) #\u0026gt; Rows: 344 #\u0026gt; Columns: 8 #\u0026gt; $ especie \u0026lt;chr\u0026gt; \u0026quot;Pinguim-de-adélia\u0026quot;, \u0026quot;Pinguim-de-adé… #\u0026gt; $ ilha \u0026lt;chr\u0026gt; \u0026quot;Torgersen\u0026quot;, \u0026quot;Torgersen\u0026quot;, \u0026quot;Torgersen… #\u0026gt; $ comprimento_bico \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38… #\u0026gt; $ profundidade_bico \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17… #\u0026gt; $ comprimento_nadadeira \u0026lt;dbl\u0026gt; 181, 186, 195, NA, 193, 190, 181, 19… #\u0026gt; $ massa_corporal \u0026lt;dbl\u0026gt; 3750, 3800, 3250, NA, 3450, 3650, 36… #\u0026gt; $ sexo \u0026lt;chr\u0026gt; \u0026quot;macho\u0026quot;, \u0026quot;fêmea\u0026quot;, \u0026quot;fêmea\u0026quot;, NA, \u0026quot;fême… #\u0026gt; $ ano \u0026lt;dbl\u0026gt; 2007, 2007, 2007, 2007, 2007, 2007, … Caso queira abrir o arquivo no navegador (para ver, compartilhar ou coisas similares), use o método open_item(), informando o caminho até o arquivo:\nonedrive_da_bea$open_item(path = \u0026quot;pasta_de_exemplo/pinguins.xlsx\u0026quot;) Conclusão Espero que este post seja útil para quem usa o One Drive também. Também espero que o pacote seja aprimorado para conseguir importar os dados sem que seja necessário baixá-los.\nCaso você encontre algum método melhor para importar os dados, conta pra gente!\nAté a próxima!\nReferências Documentação do pacote Microsoft365R - Vignette sobre o OneDrive\nGitHub do pacote Microsoft365R\nReferência da base dos pinguins\nAvançado: Referências sobre classes R6 em R\nDocumentação do pacote R6 Livro Advanced R ","permalink":"https://blog.curso-r.com/posts/2022-03-18-onedrive/","tags":["api","one drive","importação","excel","microsoft","azure","Microsoft365R"],"title":"Acessando dados do seu One Drive com R"},{"author":["William"],"categories":["Desafio"],"contents":" Os desafios da Curso-R são problemas práticos de análise de dados e programação envolvendo faxina de dados, construção de gráficos, relatórios em RMarkdown, modelagem, aplicativos Shiny e muito mais.\nO intuito é praticar as diversas etapas da Ciência de Dados, interagir com a comunidade compartilhando as suas soluções e gerar bastante repertório.\nVale sempre lembrar que:\nNão existe uma única solução para o desafio.\nVocê pode usar qualquer linguagem de programação e quaisquer ferramentas/pacotes da linguagem.\nNão existe a melhor solução. Toda solução é válida. No dia-a-dia o importante é resolver o problema.\nPara compartilhar a sua solução, vamos usar o Discourse da Curso-R. Basta acessar o tópico referente a esse desafio (no final deste post colocamos o link) e enviar o seu código. Você pode aproveitar também para pedir ajuda ou ver e comentar as soluções já enviadas.\nDesafio #2: app contador de cliques Este é o nosso primeiro desafio de Shiny! O objetivo é construir um app que conte o número de cliques seguidos em um botão. Veja o funcionamento a seguir:\nVeja que o contador começa no zero e aumenta em uma unidade para cada clique seguido. Cliques seguidos são cliques feitos em um intervalo muito curto de tempo. No exemplo acima, utilizamos 0.5 segundo. Assim, se o intervalo entre dois cliques for maior que 0.5 segundo, o contador deve voltar para 1. Você pode definir qualquer intervalo de tempo para a sua solução.\nA solução precisa conter apenas o comportamento descrito anteriormente. Você não precisa reproduzir o layout e a identidade visual do exemplo. Inclusive, incentivamos a criação de novas versões desse app. Explore a sua criatividade!\nPoste a sua resolução em nosso Discourse, respondendo o tópico criado para esse desafio!\n","permalink":"https://blog.curso-r.com/posts/2022-03-15-desafio-1/","tags":["desafio curso-r","shiny"],"title":"Desafio #2: contador de cliques [Shiny]"},{"author":["Beatriz"],"categories":["pacotes"],"contents":" Introdução O Google Sheets é um editor de planilhas da Google, que funciona vinculado ao Google Drive (ou seja, os arquivos ficam na nuvem). O pacote googlesheets4 permite interagir com o Google Sheets através do R. Neste post, vamos mostrar alguns exemplos sobre como podemos importar dados usando o pacote googlesheets4.\nPrimeiramente, é necessário instalar o pacote:\ninstall.packages(\u0026quot;googlesheets4\u0026quot;) Então, podemos carregá-lo:\n# Carregar pacote googlesheets4 library(googlesheets4) Neste exemplo, usaremos a tabela da base de dados Gapminder, que apresenta algumas informações por país e ano, como a população total, expectativa de vida ao nascer e o PIB per capita.\nPara importar uma tabela que está no Google Sheets, primeiramente precisamos identificá-la para que o R saiba qual arquivo queremos abrir. Uma forma de fazer isso é usando a URL (ou Uniform Resource Locator), que é também conhecido como o link que usamos no navegador.\nUm exemplo de url é o seguinte, e ele encaminha para essa tabela.\nhttps://docs.google.com/spreadsheets/d/1U6Cf_qEOhiR9AZqTqS3mbMF3zt2db48ZP5v3rkrAEJY/edit#gid=780868077 Sabendo a URL da planilha, podemos salvar em um objeto no R para facilitar o uso!\n# salvar a url/link da planilha que queremos acessar url_planilha \u0026lt;- \u0026quot;https://docs.google.com/spreadsheets/d/1U6Cf_qEOhiR9AZqTqS3mbMF3zt2db48ZP5v3rkrAEJY/edit#gid=780868077\u0026quot; Autenticação Para evitar que alguém que não tem acesso a uma Google Sheet consiga ler a mesma sem permissão, o pacote nos direciona para fazer uma autenticação com uma conta do Google. Esse direcionamento é feito quando usamos alguma função que necessita de autenticação pela primeira vez, e a cada sessão do R (ou seja, sempre que reiniciar), o R perguntará novamente com qual email do Google gostaríamos de autenticar.\nExemplo: No meu caso, eu já autentiquei anteriormente, então ele apresenta os emails já autenticados.\nrespostas_formulario \u0026lt;- read_sheet(url) # → The googlesheets4 package is requesting access to your Google account # Select a pre-authorised account or enter \u0026#39;0\u0026#39; to obtain a new token # Press Esc/Ctrl + C to cancel # # 1: bmilz@curso-r.com # 2: milz.bea@gmail.com Para fazer uma nova autenticação, basta escolher a opção 0, e uma janela do navegador abrirá para fazer a autenticação:\nPrecisamos então clicar no email desejado, selecionar todas as opções de acesso e clicar em “Continuar”:\nCaso funcione, a seguinte mensagem aparecerá no navegador: Authentication complete. Please close this page and return to R. Isso significa que podemos voltar ao R!\nDEU ERRO! Caso dê erro ao abrir o arquivo, se certifique que você tem acesso de leitura ou edição do Google Sheets desejado.\nAgora podemos experimenta as funções do pacote!\nFunção sheet_properties() A função sheet_properties() permite ver quais abas (chamaremos muitas vezes de sheets, pois é a forma usada no pacote) estão disponíveis na tabela:\n# ver as propriedades da planilha (conseguimos descobrir quais sheets # estão disponíveis) sheet_properties(url_planilha) ## # A tibble: 5 × 8 ## name index id type visible grid_rows grid_columns data ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;list\u0026gt; ## 1 Africa 0 780868077 GRID TRUE 625 6 \u0026lt;NULL\u0026gt; ## 2 Americas 1 45759261 GRID TRUE 301 6 \u0026lt;NULL\u0026gt; ## 3 Asia 2 1984823455 GRID TRUE 397 6 \u0026lt;NULL\u0026gt; ## 4 Europe 3 1503562052 GRID TRUE 361 6 \u0026lt;NULL\u0026gt; ## 5 Oceania 4 1796776040 GRID TRUE 25 6 \u0026lt;NULL\u0026gt; Com isso, sabemos que a tabela que informamos tem 5 abas, sendo elas: Africa, Americas, Asia, Europe e Oceania.\nFunção read_sheet() A função mais útil (na minha opinião) do pacote é a que usamos para ler os dados: read_sheet()!\nSe usarmos essa função sem nenhum argumento além do url da planilha, por padrão a primeira sheet será importada. Como vimos no resultado da função sheet_properties(), a primeira aba da base contém os dados dos países que estão no continente Africano. Então os dados que serão importados são referentes à este continente:\nplanilha \u0026lt;- read_sheet(url_planilha) dplyr::glimpse(planilha) ## Rows: 624 ## Columns: 6 ## $ country \u0026lt;chr\u0026gt; \u0026quot;Algeria\u0026quot;, \u0026quot;Algeria\u0026quot;, \u0026quot;Algeria\u0026quot;, \u0026quot;Algeria\u0026quot;, \u0026quot;Algeria\u0026quot;, \u0026quot;Alge… ## $ continent \u0026lt;chr\u0026gt; \u0026quot;Africa\u0026quot;, \u0026quot;Africa\u0026quot;, \u0026quot;Africa\u0026quot;, \u0026quot;Africa\u0026quot;, \u0026quot;Africa\u0026quot;, \u0026quot;Africa\u0026quot;, … ## $ year \u0026lt;dbl\u0026gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … ## $ lifeExp \u0026lt;dbl\u0026gt; 43.077, 45.685, 48.303, 51.407, 54.518, 58.014, 61.368, 65.7… ## $ pop \u0026lt;dbl\u0026gt; 9279525, 10270856, 11000948, 12760499, 14760787, 17152804, 2… ## $ gdpPercap \u0026lt;dbl\u0026gt; 2449.0082, 3013.9760, 2550.8169, 3246.9918, 4182.6638, 4910.… Podemos também usar o argumento sheet = para especificar qual aba queremos importar. Podemos fazer a busca pela posição (ou seja, informando um número), ou pelo nome da aba:\n# abrir uma sheet específica: pela posição planilha_sheet_3 \u0026lt;- read_sheet(url_planilha, sheet = 3) dplyr::glimpse(planilha_sheet_3) ## Rows: 396 ## Columns: 6 ## $ country \u0026lt;chr\u0026gt; \u0026quot;Afghanistan\u0026quot;, \u0026quot;Afghanistan\u0026quot;, \u0026quot;Afghanistan\u0026quot;, \u0026quot;Afghanistan\u0026quot;, … ## $ continent \u0026lt;chr\u0026gt; \u0026quot;Asia\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Asi… ## $ year \u0026lt;dbl\u0026gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … ## $ lifeExp \u0026lt;dbl\u0026gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8… ## $ pop \u0026lt;dbl\u0026gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12… ## $ gdpPercap \u0026lt;dbl\u0026gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, … # abrir uma sheet específica: pelo nome planilha_asia \u0026lt;- read_sheet(url_planilha, sheet = \u0026quot;Asia\u0026quot;) dplyr::glimpse(planilha_asia) ## Rows: 396 ## Columns: 6 ## $ country \u0026lt;chr\u0026gt; \u0026quot;Afghanistan\u0026quot;, \u0026quot;Afghanistan\u0026quot;, \u0026quot;Afghanistan\u0026quot;, \u0026quot;Afghanistan\u0026quot;, … ## $ continent \u0026lt;chr\u0026gt; \u0026quot;Asia\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Asi… ## $ year \u0026lt;dbl\u0026gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … ## $ lifeExp \u0026lt;dbl\u0026gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8… ## $ pop \u0026lt;dbl\u0026gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12… ## $ gdpPercap \u0026lt;dbl\u0026gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, … Como todas as tabelas da base tem a mesma estrutura, podemos usar o pacote purrr para importar todas as abas de uma vez:\n# a função sheet_names permite buscar apenas o nome das abas nome_abas \u0026lt;- sheet_names(url_planilha) # com o purrr, podemos abrir todas as abas de uma vez! gapminder_completo \u0026lt;- nome_abas |\u0026gt; purrr::map_dfr(~ read_sheet(url_planilha, sheet = .x)) dplyr::glimpse(gapminder_completo) ## Rows: 1,704 ## Columns: 6 ## $ country \u0026lt;chr\u0026gt; \u0026quot;Algeria\u0026quot;, \u0026quot;Algeria\u0026quot;, \u0026quot;Algeria\u0026quot;, \u0026quot;Algeria\u0026quot;, \u0026quot;Algeria\u0026quot;, \u0026quot;Alge… ## $ continent \u0026lt;chr\u0026gt; \u0026quot;Africa\u0026quot;, \u0026quot;Africa\u0026quot;, \u0026quot;Africa\u0026quot;, \u0026quot;Africa\u0026quot;, \u0026quot;Africa\u0026quot;, \u0026quot;Africa\u0026quot;, … ## $ year \u0026lt;dbl\u0026gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … ## $ lifeExp \u0026lt;dbl\u0026gt; 43.077, 45.685, 48.303, 51.407, 54.518, 58.014, 61.368, 65.7… ## $ pop \u0026lt;dbl\u0026gt; 9279525, 10270856, 11000948, 12760499, 14760787, 17152804, 2… ## $ gdpPercap \u0026lt;dbl\u0026gt; 2449.0082, 3013.9760, 2550.8169, 3246.9918, 4182.6638, 4910.… Outras funções Esse pacote tem muitas outras funções que interagem com o Googles Sheets, mas que não fazem parte do escopo deste post, como criar uma nova aba e escrever dados em uma aba. Essas funções também são muito úteis!\nE o Google Forms? Eu gosto muito de usar esse pacote para importar respostas de formulários do Google, pois o Google permite vincular respostas dos formulários em uma Google Sheets.\nAqui mostrarei como fazer isso! Primeiro precisamos criar um formulário (caso ele ainda não exista):\nAo criar o formulário, podemos clicar em “Respostas” e então clicar no logo do Google Sheets (um quadrado verde com uma linha branca horizontal e outra vertical):\nO Google Forms irá perguntar qual o nome da Google Sheet a ser criada (ou também podemos selecionar uma Google Sheets que já existe):\nDepois de selecionar qual a Google Sheet ficará vinculada ao formulário, a Google Sheet será aberta e podemos ver as respostas:\nCada coluna terá informações de uma pergunta do formulário. Inicialmente, a base terá 0 linhas, pois quando criamos o formulário ele tem 0 respostas. Porém, após alguém responder o formulário, a Google Sheet é automaticamente atualizada e apresentará a resposta. Podemos importar os dados usando a função read_sheet(), como mostrado anteriormente:\nurl \u0026lt;- \u0026quot;https://docs.google.com/spreadsheets/d/1Ch3aMraacwhNo1sU4Z9b54nWYG8gdl18UfLJMzquL8s/\u0026quot; respostas_formulario \u0026lt;- read_sheet(url) respostas_formulario ## # A tibble: 1 × 2 ## `Carimbo de data/hora` `Pergunta sem título` ## \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; ## 1 2022-03-08 21:29:01 Opção 1 É isso! Dúvidas, sugestões e críticas, mande aqui nos comentários. Postem também quais funções do pacote vocês gostariam de saber mais!!\nAté a próxima!\nReferências Documentação do pacote googlesheets4\nDocumentação do pacote purrr\nReferência da base Gapminder\n","permalink":"https://blog.curso-r.com/posts/2022-03-08-googlesheets4/","tags":["api","google","importação","google sheets"],"title":"Acessando dados do Google Sheets com R"},{"author":["Tereza"],"categories":["tutoriais"],"contents":" O prettydoc é um pacote que oferece alguns templates bem bonitos para serem utilizados em relatórios HTML construídos com R Markdown. Nesse tutorial, vamos mostrar como usar esses templates em seus relatórios!\nEntão, vamos lá:\nInstale o pacote {prettydoc}: install.packages(\u0026quot;prettydoc\u0026quot;) Crie um arquivo R Markdown. Selecione a opção HTML como formato de saída, e então, clique em From Template. Selecione a opção Lightweight and Pretty Document (HTML) e então, clique em OK. Um documento R Markdown como esse será criado: O importante aqui é o cabeçalho YAML (parte destacada na imagem), ele que vai garantir a personalização do seu relatório.\nPodemos ver na imagem que existe um campo chamado theme, e é nele que vamos especificar qual template queremos usar no nosso relatório!\nO prettydoc possui 5 templates, são eles: cayman, tactile, architect, leonids, hpstr. Para alterar o template, basta modificar o nome no campo “theme:”. Podemos ver que o template default é o architect.\nPara visualizar cada template, basta colocar o respectivo nome no campo “theme:”, e apertar o botão Knit. Vamos dar uma olhada em cada template:\ncayman: tactile: architect: leonids: hpstr: Você pode ver mais sobre os templates nessa página\nSe você já tem algum relatório em R Markdown e quer personalizá-lo, basta substituir o cabeçalho do seu relatório pelo cabeçalho que foi gerado no passo 4 (pelo prettydoc) Por exemplo, temos aqui esse relatório com a saída HTML:\nQue gera o seguinte HTML ao Knitar:\nAo substituir o cabeçalho (e alterar os campos de Título, Autor e Data), o nosso documento ficou assim:\nAqui estamos usando o template cayman, mas você pode escolher o que preferir!\nE voilà, ao Knitar esse documento, o seguinte HTML é gerado:\n","permalink":"https://blog.curso-r.com/posts/2022-02-24-prettydoc/","tags":["tutoriais"],"title":"Personalizando seus relatórios R Markdown em HTML com o {prettydoc}"},{"author":["Beatriz"],"categories":["pacotes"],"contents":" Introdução Esse post faz parte de uma série sobre acesso à APIs com R! O primeiro post foi uma introdução sobre como acessar APIs com R.\nNeste post mostraremos um exemplo usando a API do GitHub.\nO GitHub é uma plataforma onde conseguimos hospedar repositórios (pastas com nossos códigos e arquivos) com controle de versão usando o Git, e podemos fazer muitas coisas utilizando a sua API. E como dissemos no post anterior: “o primeiro passo para acessar qualquer API é procurar uma documentação”. A boa notícia é que a documentação da API do GitHub está disponível em Português e é bem detalhada!\nExistem muitas ações possíveis utilizando essa API. O que escolhemos para esse exemplo é buscar os repositórios que pertencem à uma organização.\nSegundo a documentação, para consultar os repositórios que pertencem à organização octokit, podemos utilizar a seguinte busca:\nGET /orgs/octokit/repos O equivalente a isso usando o pacote httr é:\n# url_base - nunca muda na mesma API url_base \u0026lt;- \u0026quot;https://api.github.com\u0026quot; # endpoint - é o que muda o resultado endpoint \u0026lt;- \u0026quot;/orgs/octokit/repos\u0026quot; # precisamos colar os textos para criar o link u_github \u0026lt;- paste0(url_base, endpoint) # ver como o texto ficou colado # u_github # \u0026gt; \u0026quot;https://api.github.com/orgs/octokit/repos\u0026quot; # fazer a requisição do tipo GET r_github \u0026lt;- httr::GET(u_github) r_github ## Response [https://api.github.com/orgs/octokit/repos] ## Date: 2022-02-19 16:45 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 181 kB ## [ ## { ## \u0026quot;id\u0026quot;: 417862, ## \u0026quot;node_id\u0026quot;: \u0026quot;MDEwOlJlcG9zaXRvcnk0MTc4NjI=\u0026quot;, ## \u0026quot;name\u0026quot;: \u0026quot;octokit.rb\u0026quot;, ## \u0026quot;full_name\u0026quot;: \u0026quot;octokit/octokit.rb\u0026quot;, ## \u0026quot;private\u0026quot;: false, ## \u0026quot;owner\u0026quot;: { ## \u0026quot;login\u0026quot;: \u0026quot;octokit\u0026quot;, ## \u0026quot;id\u0026quot;: 3430433, ## ... Podemos acessar o resultado usando a função httr::content(), porém não vamos colocar o resultado no post pois ficaria muito longo.\n# httr::content(r_github) O que é o pacote gh? O pacote gh permite acessar a API do GitHub. A lógica mostrada anteriormente se aplica para esse pacote também: precisaremos consultar a documentação para verificar como fazer alguma tarefa com a API.\nPrimeiro exemplo com o pacote gh Neste exemplo, vamos buscar as informações sobre os repositórios que são organização Curso-R no GitHub, gerar um dataframe, e ao final fazer uma visualização simples.\nInformações gerais da organização Curso-R Podemos buscar informações sobre a organização da Curso-R no GitHub:\ngh_curso_r \u0026lt;- gh::gh(\u0026quot;GET /orgs/{org}\u0026quot;, org = \u0026quot;curso-r\u0026quot;) A sintaxe do pacote gh é similar ao glue. Quando queremos buscar uma informação que está em uma variável (no caso “curso-r”), colocamos os {variavel} no primeiro argumento e escrevemos variavel= nos argumentos seguintes.\nComo vimos, o resultado é uma lista. Para consultar o número de repositórios públicos, podemos usar o $ para acessar essa informação dentro da lista:\ngh_curso_r$public_repos ## [1] 303 Olha só, a Curso-R tem atualmente 303 repositórios públicos no GitHub! Temos muitos repositórios pois criamos um diferente para cada curso, para que quem faz aula com a gente tenha sempre um lugar para olhar todos os materiais, de forma organizada e independente.\nAcessando informações de repositórios Podemos buscar informações sobre os repositórios que pertencem à organização Curso-R no GitHub:\nrepositorios_cursor \u0026lt;- gh::gh(\u0026quot;GET /orgs/{org}/repos\u0026quot;, org = \u0026quot;curso-r\u0026quot;) # A classe que retorna é uma lista class(repositorios_cursor) ## [1] \u0026quot;gh_response\u0026quot; \u0026quot;list\u0026quot; # É uma lista grande! length(repositorios_cursor) ## [1] 30 Esse código retornou informações de apenas 30 repositórios. Portanto, precisamos repetir o processo para obter informações de todos os repositórios.\nIterando com purrr e o pacote gh A documentação do pacote aponta que é possível buscar informações de 100 repositórios por vez. Se queremos buscar todos os repositórios, primeiro precisamos calcular quantas vezes vamos repetir o processo todo:\nnumero_repos_publicos \u0026lt;- gh_curso_r$public_repos # podemos buscar 100 repositórios por vez, # então podemos dividir o numero de repositorios # por 100, e arredondar \u0026quot;para cima\u0026quot; # (é para isso que a função ceiling() serve!) numero_paginas \u0026lt;- ceiling(numero_repos_publicos/100) numero_paginas ## [1] 4 Precisaremos repetir 4 vezes! Agora podemos usar a função purrr::map() para repetir o acesso à API quantas vezes forem necessárias para obter as informações de todos os repositórios da Curso-R:\nrepos_cursor \u0026lt;- purrr::map(1:numero_paginas, .f = ~gh::gh( \u0026quot;GET /orgs/{org}/repos\u0026quot;, org = \u0026quot;curso-r\u0026quot;, # organizacao type = \u0026quot;public\u0026quot;, # tipo de repositorio sort = \u0026quot;updated\u0026quot;, # forma de ordenar a busca per_page = 100, # numero de resultados por pagina page = .x # numero da pagina que será substituido )) O resultado ainda é uma lista… podemos usar a magia do pacote purrr e transformar essa lista em um dataframe:\nlista_repos \u0026lt;- repos_cursor |\u0026gt; purrr::flatten() |\u0026gt; purrr::map(unlist, recursive = TRUE) |\u0026gt; purrr::map_dfr(tibble::enframe, .id = \u0026quot;id_repo\u0026quot;) |\u0026gt; tidyr::pivot_wider() |\u0026gt; janitor::clean_names() lista_repos ## # A tibble: 303 × 108 ## id_repo id node_id name full_name private owner_login owner_id ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 311969160 MDEwOlJlcG9za… chess curso-r/… FALSE curso-r 10060716 ## 2 2 154844030 MDEwOlJlcG9za… auth0 curso-r/… FALSE curso-r 10060716 ## 3 3 316277552 MDEwOlJlcG9za… main… curso-r/… FALSE curso-r 10060716 ## 4 4 431987860 R_kgDOGb-clA 2022… curso-r/… FALSE curso-r 10060716 ## 5 5 249453848 MDEwOlJlcG9za… tree… curso-r/… FALSE curso-r 10060716 ## 6 6 272436141 MDEwOlJlcG9za… base… curso-r/… FALSE curso-r 10060716 ## 7 7 445626145 R_kgDOGo-3IQ main… curso-r/… FALSE curso-r 10060716 ## 8 8 163780826 MDEwOlJlcG9za… shin… curso-r/… FALSE curso-r 10060716 ## 9 9 436799548 R_kgDOGgkIPA scryr curso-r/… FALSE curso-r 10060716 ## 10 10 431978304 R_kgDOGb93QA 2022… curso-r/… FALSE curso-r 10060716 ## # … with 293 more rows, and 100 more variables: owner_node_id \u0026lt;chr\u0026gt;, ## # owner_avatar_url \u0026lt;chr\u0026gt;, owner_gravatar_id \u0026lt;chr\u0026gt;, owner_url \u0026lt;chr\u0026gt;, ## # owner_html_url \u0026lt;chr\u0026gt;, owner_followers_url \u0026lt;chr\u0026gt;, owner_following_url \u0026lt;chr\u0026gt;, ## # owner_gists_url \u0026lt;chr\u0026gt;, owner_starred_url \u0026lt;chr\u0026gt;, ## # owner_subscriptions_url \u0026lt;chr\u0026gt;, owner_organizations_url \u0026lt;chr\u0026gt;, ## # owner_repos_url \u0026lt;chr\u0026gt;, owner_events_url \u0026lt;chr\u0026gt;, ## # owner_received_events_url \u0026lt;chr\u0026gt;, owner_type \u0026lt;chr\u0026gt;, … Vamos fazer mais uma etapa de organização dos dados: são muitas colunas, e não precisaremos de todas para terminar o post. Também filtramos a base para remover os forks, já que não seriam repositórios da Curso-R originalmente.\ndf_repos_cursor \u0026lt;- lista_repos |\u0026gt; dplyr::filter(fork == FALSE) |\u0026gt; dplyr::select( name, created_at, default_branch ) |\u0026gt; dplyr::mutate( data_criacao = readr::parse_datetime(created_at), ano_criacao = as.Date(lubridate::floor_date(data_criacao, \u0026quot;year\u0026quot;)) ) Exemplo de visualização com os dados obtidos! Em 2020, o Caio escreveu um post sobre o uso do termo ‘master’ no GitHub. Lá no post é explicado sobre a questão da substituição do termo ‘master’. Em 2020 a GitHub anunciou que faria a transição para o termo main (principal), e desde então muitas pessoas e organizações estão renomeando a branch principal de seus repositórios para ‘main’(inclusive existe um post no blog da RStudio sobre isso).\nUsando os dados obtidos nesse post, vamos explorar os repositórios da Curso-R e averiguar qual é o nome da branch principal dos repositórios ao longo do tempo?\nlibrary(ggplot2) main_percent \u0026lt;- mean(df_repos_cursor$default_branch == \u0026quot;main\u0026quot;) main_percent \u0026lt;- scales::percent(main_percent) df_repos_cursor |\u0026gt; dplyr::count(ano_criacao, default_branch) |\u0026gt; ggplot() + geom_col(aes(y = n, x = ano_criacao, fill = default_branch)) + theme_bw() + scale_x_date(date_labels = \u0026quot;%Y\u0026quot;, date_breaks = \u0026quot;1 year\u0026quot;) + scale_fill_brewer(palette = \u0026quot;Pastel1\u0026quot;) + labs(x = \u0026quot;Ano de criação\u0026quot;, y = \u0026quot;Número de repositórios\u0026quot;, fill = \u0026quot;Nome da Branch\u0026quot;) É possível ver que em 2021 o uso do termo ‘main’ para nomear as branches principais foi muito mais usado! Atualmente, o percentual de repositórios main é de 25% e esperamos que isso aumente com o tempo. Outra coisa legal do gráfico é ver como a criação de repositórios na organização da Curso-R foi crescendo ao longo do tempo!\nÉ isso! Dúvidas, sugestões e críticas, mande aqui nos comentários. Postem também quais exemplos, dentre os que foram listados, vocês gostariam de saber mais!!\nSe você quiser saber mais sobre acessar APIs, o curso de Web Scraping é uma ótima oportunidade!\nAté a próxima!\nReferências Slides do curso de Web Scraping sobre APIs\nSlides do curso de Deploy\nPacote httr\n","permalink":"https://blog.curso-r.com/posts/2022-02-19-api-github-1/","tags":["api","github","git","purrr"],"title":"Acessando APIs com R: GitHub - Parte 1"},{"author":["Tereza"],"categories":["Desafio"],"contents":" Esse é o primeiro post de uma série de desafios de ciência de dados que postaremos aqui no blog!\nOs desafios serão bem variados, incluindo temas como Shiny, Manipulação, Visualização, etc…\nO intuito desses desafios é mostrar como um problema prático na ciência de dados pode ter as mais diversas soluções!\nPara isso, gostaríamos que você postasse a sua resolução em nosso Discourse, assim, você estaria contribuindo com a variedade de soluções do problema, além de poder receber dicas da comunidade de como melhorar o seu código :)\nEntão, vamos ao primeiro desafio!\nDesafio #1: Manipulando a base de filmes do IMDB Vamos começar com um desafio de manipulação. Para esse desafio, vamos utilizar a base de filmes do IMDB. Você pode baixá-la clicando aqui.\nDesafio: Escreva um código que transforme a base do IMDB em uma base nomeada IMDB_atores, que deve ser formada pelas seguintes colunas:\nator: nome de um(a) ator/atriz\n(cada nome deve aparecer uma vez só)\nnota_media_imdb: nota imdb média dos filmes que o(a) ator/atriz particiou\nmedia_lucro: lucro médio dos filmes que o(a) ator/atriz participou\n(obs: lucro = receita - orcamento)\ntop1_genero: gênero mais frequente entre os filmes que o(a) ator/atriz particiou\ntop2_genero: segundo gênero mais frequente entre os filmes que o(a) ator/atriz particiou\ntop3_genero: terceiro gênero mais frequente entre os filmes que o(a) ator/atriz particiou\nprimeiro_registro: ano do primeiro filme que o(a) ator/atriz participou\nultimo_registro: ano do último filme que o(a) ator/atriz particiou\nfilmes: um data frame com informações de todos os filmes que o(a) ator/atriz participou, as colunas desse data frame devem ser as seguintes:\ntítulo: nome do filme ano: ano do filme diretor: diretor(a) do filme duracao: duração do filme cor: cor do filme (Color ou Black and White) generos: generos do filme pais: país do filme classificacao: classificação etária do filme nota_imdb: nota imbd do filme orcamento: orcamento do filme receita: receita do filme lucro: lucro do filme contracenou: um vetor com os nomes de todos os(as) atores/atrizes que o(a) ator/atriz já contracenou (ou seja, participou do mesmo filme)\nPoste a sua resolução em nosso Discourse, respondendo o tópico criado para esse desafio!\n","permalink":"https://blog.curso-r.com/posts/2022-02-02-desafio-1/","tags":["desafio curso-r","manipulação"],"title":"Desafio #1: Manipulando a base de filmes do IMDB"},{"author":["Tereza"],"categories":["tutoriais"],"contents":" O clipr é um pacote composto por algumas funções que permitem importar/exportar objetos do R através da área de transferência!\nEssas funções são muito úteis para, por exemplo, copiar objetos do excel, powerpoint ou word para o R, e vice-versa.\nNesse post, vamos mostrar como copiar tabelas do R para o Excel, e como copiar tabelas do Excel para o R através desse pacote! Sem mais delongas, vamos aos tutoriais:\nCopiando tabelas do R para o Excel através da área de transferência Carregue o pacote clipr library(clipr) Agora, basta usar a tabela escolhida como argumento da função write_clip. No nosso caso, queremos copiar a tabela mtcars write_clip(mtcars) Abra um novo arquivo excel, use o atalho crtl + v e voilà! Note que por default, o separador decimal usado é o ponto. Para mudar isso, basta usar o argumento dec = \",\"\nwrite_clip(mtcars, dec = \u0026quot;,\u0026quot;) Usando o atalho ctrl + v no arquivo excel:\nTambém podemos usar o operador pipe, obtendo o mesmo resultado do passo anterior:\nmtcars %\u0026gt;% write_clip(dec = \u0026quot;,\u0026quot;) Além disso, a função também pode receber outros argumentos. Por exemplo, com o argumento col.names = FALSE a tabela será copiada sem os nomes das colunas\nmtcars %\u0026gt;% write_clip(dec = \u0026quot;,\u0026quot;, col.names = FALSE) Usando o atalho ctrl + v no arquivo excel:\nCopiando tabelas do Excel para o R através da área de transferência Carregue o pacote clipr library(clipr) No excel, selecione a tabela desejada, e a copie através do comando ctrl + c. No nosso caso, estamos copiando a base mtcars (no formato .xlsx) Agora, vá até o R e use o seguinte comando: base_de_dados \u0026lt;- read_clip_tbl() (base_de_dados é apenas um nome genérico que estamos dando para esse data.frame, você pode nomear como preferir)\nE prontinho! Vamos visualizar o objeto base_de_dados:\nbase_de_dados[1:11, ] ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 3 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## 4 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 5 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## 6 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 7 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## 8 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## 9 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## 10 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## 11 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 Também podemos usar alguns argumentos na função read_clip_tbl. Por exemplo, considere que temos uma base de dados no formato .csv:\nSe quisermos copiar a tabela mtcars no formato .csv do excel para o R, por exemplo, basta usar o argumento sep para especificar o separador usado, que é a vírgula:\nbase_de_dados2 \u0026lt;- read_clip_tbl(sep = \u0026quot;,\u0026quot;) Vamos visualizar o objeto base_de_dados2:\nbase_de_dados2[1:11, ] ## X mpg cyl disp hp drat wt qsec vs am gear carb ## 1 Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 2 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 3 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## 4 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 5 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## 6 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 7 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## 8 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## 9 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## 10 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## 11 Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 Obs: Para limpar a area de tranferencia, basta usar o comando clear_clip()\n","permalink":"https://blog.curso-r.com/posts/2021-12-27-clipr/","tags":["tutoriais"],"title":"Importando e exportando tabelas do R para o Excel através da área de transferência"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Sabe aquelas imagens que aparecem quando você está preenchendo um formulário ou quer acessar uma página específica, pedindo para você decifrar o texto? Isso é o que chamamos de CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart).\nFigura: Exemplo de captcha do site do Tribunal Regional do Trabalho (TRT). O Captcha é um desafio computacional criado para ser difícil de resolver por máquinas mas fácil de resolver por pessoas. No início, a maioria dos Captchas eram formados por letras aleatórias inseridas em imagens ou áudios com algum ruído. Com a popularização das técnicas de Deep Learning, esses desafios se tornaram fáceis de resolver por máquinas, desde que exista uma base suficientemente grande de exemplos classificados.\nEm posts anteriores aqui do blog, eu mostrei como resolvemos Captchas na mão ou usando redes neurais convolucionais. Todos esses posts usavam o {keras} como backend, que era a tecnologia de deep learning que eu conhecia na época.\nObservação: alguns dos posts anteriores de Captchas não estão visíveis. Algum dia vou revivê-los!\nNeste post, vou mostrar como resolver um Captcha de texto em imagem usando o pacote {luz}. O pacote surgiu para facilitar a vida de quem constrói modelos de Deep Learning usando o {torch}.\nO que é {torch}? O que é {luz}? O {torch} é um pacote do R criado pelo nosso sócio Daniel Falbel, que também trabalha na RStudio. O pacote é uma adaptação do pytorch, biblioteca de python muito popular para fazer contas de álgebra matricial e diferenciação automática utilizando GPUs, que são os principais ingredientes utilizados na construção e ajuste de modelos de Deep Learning. A empresa por trás do projeto Torch é o Facebook (Meta?).\nA dificuldade do pacote {torch} para pessoas que estão começando é a complexidade da sintaxe. A ferramenta é super flexível, permitindo o ajuste de modelos customizados, mas a construção desses modelos está longe de ser trivial. Por exemplo, é comum precisar implementar os passos de atualização dos parâmetros manualmente a partir de algumas funções básicas do pacote.\nO pacote {luz} – sim, o nome é uma brincadeira com a luz que sai de uma tocha… 🥁 – vem com a ideia de facilitar um pouco esse processo. Para conseguir rodar um modelo de deep learning usando o luz, precisamos de apenas i) um dataset/dataloader construído para gerar amostras das bases de dados de treino/validação e ii) um modelo, que deve ser um módulo do torch.\nVamos investigar os elementos do luz a seguir. Como exemplo, utilizaremos os nossos queridos Captchas, para não ficar na mesmisse do MNIST 😉\nO exemplo da vez será o TRT, pois a base é pequenininha e é um Captcha relativamente fácil de resolver.\nDados, datasets e dataloaders Uma característica importante de modelos de deep learning é que as bases de dados costumam ser bem grandes. Por isso, foram desenvolvidas técnicas que atualizam os parâmetros do modelo com base apenas em uma amostra dos dados, sem a necessidade de carregar todas as observações na memória do computador. Essa estratégia parece um pouco estranha de início, mas existem muitos estudos que demonstram que elas não só funcionam como também têm chances de trazer propriedades de regularização ao modelo.\nNesse contexto surgem os dataset()s e dataloader()s do torch. O dataset é um objeto responsável por informar como uma base de dados deve ser acessada (por exemplo, lendo uma imagem, um arquivo csv ou um áudio), e de que forma podemos obter um item dessa base. O dataset é um objeto de classe R6, que é uma espécie de orientação a objetos do R desenvolvida pela RStudio.\nAbaixo, temos o esqueleto de um dataset do torch:\n# isso cria um dataset generator ds_generator \u0026lt;- torch::dataset( name = \u0026quot;meu nome!\u0026quot;, objeto_arbitrario = \u0026quot;você pode criar objetos arbitrários aqui\u0026quot;, # isso é rodado quando você cria o dataset(), com parâmetros initialize = function(parametro1, parametro2, repeticoes) { # código arbitrário dados \u0026lt;- matrix( c(parametro1, parametro2), ncol = 2, nrow = repeticoes, byrow = TRUE ) ## registra os dados para ser usado em outro lugar self$dados \u0026lt;- dados }, # função que pega um item da base .getitem = function(index) { self$dados[index, ] }, # função que mede o tamanho da base .length = function() { nrow(self$dados) } ) # para criar o dataset mesmo: ds \u0026lt;- ds_generator(\u0026quot;ola\u0026quot;, \u0026quot;mundo\u0026quot;, 5) \u0026lt;meu nome!\u0026gt; Inherits from: \u0026lt;dataset\u0026gt; Public: .getitem: function (ii) .length: function () clone: function (deep = FALSE) dados: ola ola ola ola ola mundo mundo mundo mundo mundo initialize: function (parametro1, parametro2, repeticoes) objeto_arbitrario: você pode criar objetos arbitrários aqui Os objetos registrados podem ser acessados dessa forma:\nds$dados Legal! agora temos nosso dataset. Agora precisamos de um método para gerar amostras desse dataset. Esse é o dataloader, que pode ser criado assim:\n[,1] [,2] [1,] \u0026quot;ola\u0026quot; \u0026quot;mundo\u0026quot; [2,] \u0026quot;ola\u0026quot; \u0026quot;mundo\u0026quot; [3,] \u0026quot;ola\u0026quot; \u0026quot;mundo\u0026quot; [4,] \u0026quot;ola\u0026quot; \u0026quot;mundo\u0026quot; [5,] \u0026quot;ola\u0026quot; \u0026quot;mundo\u0026quot; dl \u0026lt;- torch::dataloader(ds, batch_size = 2) dl \u0026lt;dataloader\u0026gt; Public: .auto_collation: active binding .dataset_kind: map .has_getbatch: FALSE .index_sampler: active binding .iter: function () .length: function () batch_sampler: utils_sampler_batch, utils_sampler, R6 batch_size: 2 clone: function (deep = FALSE) collate_fn: function (batch) dataset: meu nome!, dataset, R6 drop_last: FALSE generator: NULL initialize: function (dataset, batch_size = 1, shuffle = FALSE, sampler = NULL, multiprocessing_context: NULL num_workers: 0 pin_memory: FALSE sampler: utils_sampler_sequential, utils_sampler, R6 timeout: -1 worker_globals: NULL worker_init_fn: NULL worker_packages: NULL Isso pode ser usado para gerar amostras da base de dados, o que é importante para a etapa de ajuste de modelos realizadas pelo {torch} e pelo {luz}.\nNo caso dos captchas, eu montei uma função do pacote {captcha} que cria os dataloaders de treino e validação a partir de uma função que faz o download dos arquivos brutos.\npath \u0026lt;- \u0026quot;caminho/para/arquivos/brutos\u0026quot; # cria o dataset. No caso, estamos usando a base do TRT, # que tem apenas 400 Captchas. captcha_ds \u0026lt;- captcha::captcha_dataset( root = path, captcha = \u0026quot;trt2\u0026quot;, download = TRUE ) # cria os dataloaders de treino e validação ## amostra de indices id_train \u0026lt;- sample(1:length(captcha_ds), .8 * length(captcha_ds)) ## cria o dataloader de treino captcha_dl_train \u0026lt;- torch::dataloader( torch::dataset_subset(captcha_ds, id_train), batch_size = 32, shuffle = TRUE ) ## cria o dataloader de validação captcha_dl_valid \u0026lt;- torch::dataloader( torch::dataset_subset(captcha_ds, -id_train), batch_size = 32 ) Modelo Um modelo do torch também é um objeto de classe R6, mas que precisa ter os parâmetros initialize= e forward= implementados. Aqui um exemplo que soma um número ao valor de entrada:\nmodel_generator \u0026lt;- torch::nn_module( \u0026quot;meu modelo!\u0026quot;, # aqui inicializamos nosso modelo initialize = function(parametro) { self$valor \u0026lt;- parametro }, # aqui calculamos o que o modelo deve calcular forward = function(x) { x + self$valor } ) model_generator \u0026lt;meu modelo!\u0026gt; object generator Inherits from: \u0026lt;inherit\u0026gt; Public: .classes: meu modelo! nn_module initialize: function (parametro) forward: function (x) clone: function (deep = FALSE) Parent env: \u0026lt;environment: 0x5594bc41f3c0\u0026gt; Locked objects: FALSE Locked class: FALSE Portable: TRUE Por exemplo, o modelo abaixo somará 10 ao valor de entrada\nmodelo \u0026lt;- model_generator(10) modelo(1) [1] 11 No nosso caso, utilizaremos um modelo já implementado no pacote {captcha}, que utiliza redes neurais convolucionais. É importante mencionar que não é necessário criar o modelo: utilizaremos apenas o model generator.\ncaptcha::net_captcha \u0026lt;CAPTCHA-CNN\u0026gt; object generator Inherits from: \u0026lt;inherit\u0026gt; Public: .classes: CAPTCHA-CNN nn_module initialize: function (input_dim, output_ndigits, output_vocab_size) forward: function (x) clone: function (deep = FALSE) Parent env: \u0026lt;environment: 0x557dc107da60\u0026gt; Locked objects: FALSE Locked class: FALSE Portable: TRUE Treino E agora chegamos na parte mais legal. Com o {luz}, temos uma sintaxe parecida com o {keras}, em que fazemos o setup do modelo e depois ajustamos, com direito a uma barrinha de progresso que já vem automaticamente 🤩\nfitted \u0026lt;- captcha::net_captcha |\u0026gt; # aqui colocamos a função de perda e o otimizador luz::setup( loss = torch::nn_multilabel_soft_margin_loss(), optimizer = torch::optim_adam ) |\u0026gt; # aqui nós colocamos os hiperparâmetros do modelo # no caso, precisamos passar informações sobre a dimensão # da imagem, o tamanho do vocabulário e a quantidade de letras # em um captcha luz::set_hparams( input_dim = dim(captcha_ds$data)[-1], output_vocab_size = dim(captcha_ds$target)[3], output_ndigits = dim(captcha_ds$target)[2] ) |\u0026gt; # aqui nós colocamos os hiperparâmetros de otimização luz::set_opt_hparams( lr = .1 ) |\u0026gt; # ajustar o modelo luz::fit( captcha_dl_train, valid_data = captcha_dl_valid, epochs = 30 ) E pronto! Agora é só ver o modelo rodar e mexer nos hiperparâmetros para que o modelo alcance a acurácia desejada.\nExtra: métrica customizada Uma coisa que ficou faltando no código anterior é uma ideia de como fica a acurácia do modelo ao longo do ajuste. Como não existe uma métrica já pronta para calcular o quanto acertamos das 6 letras ao mesmo tempo, precisamos implementar uma métrica customizada para o Captcha.\nIsso não é um grande desafio para o {luz}! Assim como nos datasets e modelos, podemos criar uma métrica com uma classe R6, usando a função luz::luz_metric(). Essa classe deve ter 4 elementos: abbrev=, o nome da métrica, initialize= que seta os parâmetros iniciais, update= que atualiza a métrica a partir de um conjunto novo de outputs e predições e compute= que calcula a métrica atualizada.\nNo caso dos Captchas, a métrica fica assim:\ncaptcha_accuracy \u0026lt;- luz::luz_metric( abbrev = \u0026quot;Captcha Acc\u0026quot;, initialize = function() { # inicializa os parâmetros self$correct \u0026lt;- 0 self$total \u0026lt;- 0 }, update = function(preds, target) { # extraímos o token com maior probabilidade associada, para cada letra pred \u0026lt;- torch::torch_argmax(preds, dim = 3) # fazemos o mesmo para a variável resposta tgt \u0026lt;- torch::torch_argmax(target$squeeze(), dim = 3) # comparamos os dois resultados e somamos a quantidade de resultados iguais new_correct \u0026lt;- torch::torch_sum(pred == tgt, 2) == dim(pred)[2] new_correct \u0026lt;- new_correct$to(dtype = torch::torch_float())$sum()$item() # atualizamos os valores self$correct \u0026lt;- self$correct + new_correct self$total \u0026lt;- self$total + dim(pred)[1] }, compute = function() { # calcula a proporção de acertos self$correct / self$total } ) Resultados O código completo de ajuste do modelo pode ser encontrado aqui. A sintaxe do pacote {captcha} ainda está instável, então pode ser que o código quebre no futuro.\nCom o código, consegui chegar em uma acurácia de 90% com apenas 400 exemplos classificados!\nAbaixo, um exemplinho de aplicação:\ncaptcha \u0026lt;- captcha_ds$files[1] captcha |\u0026gt; captcha::read_captcha() |\u0026gt; plot() fitted \u0026lt;- luz::luz_load(\u0026quot;caminho/para/trt.pt\u0026quot;) captcha::decrypt(captcha, fitted) [1] \u0026quot;2n73k7\u0026quot; Wrap-up Neste post, vimos que\nO pacote {luz} é uma alterativa ao {keras} que usa o {torch} como backend. Para trabalhar com o {luz}, é necessário criar datasets, dataloaders e modelos na forma de classes R6. Com o {luz}, é possível criar métricas customizadas para acompanhar o desempenho de um modelo. Os modelos de redes neurais convolucionais funcionam bem em tarefas de visão computacional, como os Captchas. É isso. Happy coding ;)\nAgradecimentos Um agradecimento especial à Beatriz Milz pela revisão do post!\n","permalink":"https://blog.curso-r.com/posts/2021-12-27-captcha-luz/","tags":["torch","luz"],"title":"Resolvendo Captchas com o {luz}"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nPepino-do-mar (A) Finalmente chegamos ao último dia do AoC deste ano! O problema de hoje foi um verdadeiro presente de Natal: bem mais simples que todos os dias anteriores. Nossa missão era acompanhar os movimentos de dois grupos de pepinos-do-mar e encontrar o momento em que eles não poderiam mais se mover.\nOs pepinos estavam dispostos em uma matriz retangular e se moviam na direção para a qual estavam apontando. Se o espaço em frente ao pepino estivesse vago (.), então ele se movia.\n# Estado inicial: # ...\u0026gt;... # ....... # ......\u0026gt; # v.....\u0026gt; # ......\u0026gt; # ....... # ..vvv.. # # Depis de 1 passo: # ..vv\u0026gt;.. # ....... # \u0026gt;...... # v.....\u0026gt; # \u0026gt;...... # ....... # ....v.. # # Depois de 58 passos (todos travados): # ..\u0026gt;\u0026gt;v\u0026gt;vv.. # ..v.\u0026gt;\u0026gt;vv.. # ..\u0026gt;\u0026gt;v\u0026gt;\u0026gt;vv. # ..\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;vv. # v......\u0026gt;vv # v\u0026gt;v....\u0026gt;\u0026gt;v # vvv.....\u0026gt;\u0026gt; # \u0026gt;vv......\u0026gt; # .\u0026gt;v.vv.v.. Meu código ficou simples. Eu li o mapa do fundo do mar como uma matriz e calculei todos os pepinos que podiam se mover; quando nenhum mais pudesse, eu retornava o número de passos transcorridos.\n# Ler fundo do mar como matriz seafloor \u0026lt;- \u0026quot;data-raw/25a_sea_cucumber.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; stringr::str_split(\u0026quot;\u0026quot;) |\u0026gt; purrr::flatten_chr() |\u0026gt; matrix(nrow = 137, ncol = 139, byrow = TRUE) # Iterar enquanto ainda há movimentos i \u0026lt;- 0 while (TRUE) { i \u0026lt;- i + 1 # Todos os pepinos e \u0026lt;- which(seafloor == \u0026quot;\u0026gt;\u0026quot;) s \u0026lt;- which(seafloor == \u0026quot;v\u0026quot;) # As suas próximas posições next_e \u0026lt;- ((e + 137) %% 19043) + ((e + 137) %% 19043 == 0) * 19043 next_s \u0026lt;- s + 1 - (s %% 137 == 0) * 137 # Mover todos os pepinos virados para a esquerda allowed_e \u0026lt;- seafloor[next_e] == \u0026quot;.\u0026quot; seafloor[next_e[allowed_e]] \u0026lt;- seafloor[e[allowed_e]] seafloor[e[allowed_e]] \u0026lt;- \u0026quot;.\u0026quot; # Mover todos os pepinos virados para baixo allowed_s \u0026lt;- seafloor[next_s] == \u0026quot;.\u0026quot; seafloor[next_s[allowed_s]] \u0026lt;- seafloor[s[allowed_s]] seafloor[s[allowed_s]] \u0026lt;- \u0026quot;.\u0026quot; # Verificar condição de parada if (all(!allowed_e) \u0026amp;\u0026amp; all(!allowed_s)) break } # Imprimir print(i) #\u0026gt; [1] 518 Pepino-do-mar (B) O segundo item me pegou de surpresa porque… Não havia segundo item! A historinha que estava sendo contada ao longo do AoC foi finalmente concluída e ganhamos a última estrela de graça.\nE esse foi o fim da aventura. Muito obrigado por me acompanhar nesses últimos 25 dias de programação intensa! Espero que tenham gostado e, até que enfim, boas festas!\n","permalink":"https://blog.curso-r.com/posts/2021-12-25-advent-of-r-25/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 25"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nUnidade lógica e aritmética (A e B) O penúltimo dia do AoC de 2021 chegou e, com ele, mais um problema que era mais fácil de resolver na mão! Hoje e ontem vão ficar na história como exercícios de lógica e não de programação.\nSem mais delongas, deixo vocês com outra tirinha do XKCD:\n","permalink":"https://blog.curso-r.com/posts/2021-12-24-advent-of-r-24/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 24"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nAnfípodes (A e B) O dia 23 do AoC foi… Estranho. O enunciado era fácil de entender, mas o código foi impossível de fazer. E não estou exagerando: eu literamente não consegui fazer o código para resolver o exercício. É verdade que eu fiquei doente hoje, então não sei se meus neurônios estavam de cama.\nNo meu desespero, fui olhar o subreddit do Advent em busca de sugestões de outros programadores e, quando cheguei lá, descobri que várias pessoas estavam resolvendo o problema na mão! Uma boa alma tinha até criado um helper online!\nNo final, a minha lição do dia de hoje é que nem sempre o jeito mais rápido de resolver um problema é programando; às vezes é mais fácil usar a cabeça mesmo. No caso, a cabeça da Renata Hirota, que resolveu o problema na mão em 10 minutos depois de eu ter passado o dia inteiro na frente do computador tentando achar uma solução.\nSendo assim, deixo vocês com uma tirinha do XKCD:\n","permalink":"https://blog.curso-r.com/posts/2021-12-23-advent-of-r-23/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 23"},{"author":["Tereza"],"categories":["tutoriais"],"contents":" Este tutorial é para você que quer usar o R Markdown para criar arquivos em Word e quer ter maior controle sobre a formatação desses documentos!\nÉ uma dica muito útil, pois você não terá mais que perder tempo formatando o seu relatório depois de escrevê-lo, ele já sairá estilizado e formatado do jeito que você quiser!\nEntão, vamos ao tutorial:\nAbra o RStudio e crie um arquivo R Markdown. Selecione a opção Word como formato de saída. Salve o arquivo.\nDê um Knit, apertando no botão do canto superior esquerdo (ou Knit to Word).\nUm arquivo Word aparecerá na sua tela, como esse: Agora, edite esse arquivo com a formatação que deseja ter em seu relatório!\nClique no botão do canto inferior direito da aba Estilos. Deixe essa aba aberta.\nClique em cima do título (“Untitled”, na imagem), e note que na aba Estilos a caixa Título será automaticamente selecionada. Clique na seta à direita da caixa Título, e então, em Modificar. Edite a formatação do jeito que preferir. No nosso exemplo, ficou assim: Quando terminar a edição, clique em Ok no canto inferior direito.\nPara editar outro elemento do texto, basta clicar em cima desse elemento, e a caixa do elemento será automaticamente selecionada na aba Estilos. (na imagem exemplo, clicamos no link “http://rmarkdown.rstudio.com” e na aba “Estilos” a caixa de Hiperlink foi automaticamente selecionada)\nProssiga como anteriormente, clicando na seta à direita da caixa, e então, em Modificar.\nQuando a formatação estiver do jeito que você quiser, salve o arquivo. Aqui estamos usando o nome Template_1.docx. Feche o arquivo.\nVoltando ao RStudio, crie outro arquivo R Markdown, com Word como formato de saída.\nEscreva o seu relatório.\nEdite o começo do texto, especificando o seu arquivo de referência (sim, o arquivo que você acabou de editar!), ele deve ficar assim:\nSalve o arquivo.\nDê um Knit, apertando no botão do canto superior esquerdo (ou Knit to Word).\nProntinho! Aparecerá na tela o arquivo Word com o seu relatório personalizado como no arquivo de referência!\nObs: Antes de apertar o botão Knit, certifique-se de que o arquivo Word está fechado. Se não, aparecerá um erro na tela do R.\n","permalink":"https://blog.curso-r.com/posts/2021-12-23-word-rmarkdown/","tags":["tutoriais"],"title":"Criando um relatório personalizado no Word através do R Markdown"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nReinicialização do Reator (A) O dia 22 do AoC foi mais um cujo enunciado não apresentou dificuldades. Não que a resolução tenha sido fácil, mas pelo menos o problema foi fácil de entender.\nEssencialmente tínhamos que reiniciar o reator do submarino seguindo uma série de instruções (a entrada do problema). O reator era composto por uma grade gigantesca feita de cubos 1x1x1 que começavam todos desligados; cada instrução nos dava uma região do reator que precisava ser desligada ou ligada:\n# on x=10..12,y=10..12,z=10..12 # on x=11..13,y=11..13,z=11..13 # off x=9..11,y=9..11,z=9..11 # on x=10..10,y=10..10,z=10..10 O primeiro comando da lista acima, por exemplo, ligava todos os cubos dentro da matrix reator[10:12, 10:12, 10:12]. Nosso objetivo no primeiro item era contar todos os cubos que estariam acessos no final do processo de reinicialização, mas levando em conta apenas os cubos dentro da região denotada por x=-50..50,y=-50..50,z=-50..50.\nO código era bastante simples de escrever usando a função array() do R, prestando atenção apenas ao fato de que as coordenadas da array deveríam ir de 1 a 101 e não de -50 a 50.\n# Ler todos os passos como uma tabela steps \u0026lt;- \u0026quot;data-raw/22a_reactor_reboot.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; stringr::str_split(\u0026quot;[ ,]|(\\\\.\\\\.)\u0026quot;) |\u0026gt; purrr::transpose() |\u0026gt; purrr::set_names(\u0026quot;state\u0026quot;, \u0026quot;x1\u0026quot;, \u0026quot;x2\u0026quot;, \u0026quot;y1\u0026quot;, \u0026quot;y2\u0026quot;, \u0026quot;z1\u0026quot;, \u0026quot;z2\u0026quot;) |\u0026gt; purrr::map(purrr::flatten_chr) |\u0026gt; tibble::as_tibble() |\u0026gt; dplyr::mutate( dplyr::across(dplyr::ends_with(\u0026quot;1\u0026quot;), stringr::str_remove, \u0026quot;[a-z]=\u0026quot;), dplyr::across(c(-state), as.integer), x = purrr::map2(x1, x2, `:`), y = purrr::map2(y1, y2, `:`), z = purrr::map2(z1, z2, `:`) ) |\u0026gt; dplyr::select(state, x, y, z) # Criar reator como uma array 3D reactor \u0026lt;- array(rep(\u0026quot;off\u0026quot;, 303), dim = c(101, 101, 101)) # Iterar nos passos for (i in seq_len(nrow(steps))) { # Coordenadas do cubóide x \u0026lt;- steps$x[[i]] + 51 y \u0026lt;- steps$y[[i]] + 51 z \u0026lt;- steps$z[[i]] + 51 # Eliminar o que estiver fora do cubo -50:50 x \u0026lt;- x[x \u0026gt;= 1 \u0026amp; x \u0026lt;= 101] y \u0026lt;- y[y \u0026gt;= 1 \u0026amp; y \u0026lt;= 101] z \u0026lt;- z[z \u0026gt;= 1 \u0026amp; z \u0026lt;= 101] # Atribuir estado reactor[x, y, z] \u0026lt;- steps$state[i] } # Contar cubos ligados sum(reactor == \u0026quot;on\u0026quot;) #\u0026gt; [1] 647076 Reinicialização do Reator (B) Sem muita surpresa, o item 2 pedia para contarmos o número de cubos ligados ao final do processo de reinicialização em todo o reator. Olhando o código acima, parece que só seria necessário mudar as dimensões da array e tirar os filtros dentro do loop, certo? Infelizmente não, pois com esse algoritmo ineficiente precisaríamos contar aproximadamente 2 quadrilhões de cubos…\nA solução foi, então, calcular apenas os limites das regiões e lidar com as suas intersecções. Ou seja, se dois cubóides tiverem que ser ligados, então podemos tomar nota das suas coordenadas e adicionar um novo cubóide de “subtração” na nossa lista que servirá para remover uma cópia da intersecção que foi ligada “duas vezes”. Resumidamente, estaremos contando apenas os volumes de cada cubóide ligado e subtraíndo o volume de cada intersecção para não contar nada duas vezes.\n# Ler todos os passos como uma tabela steps \u0026lt;- \u0026quot;data-raw/22b_reactor_reboot.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; stringr::str_split(\u0026quot;[ ,]|(\\\\.\\\\.)\u0026quot;) |\u0026gt; purrr::transpose() |\u0026gt; purrr::set_names(\u0026quot;state\u0026quot;, \u0026quot;x1\u0026quot;, \u0026quot;x2\u0026quot;, \u0026quot;y1\u0026quot;, \u0026quot;y2\u0026quot;, \u0026quot;z1\u0026quot;, \u0026quot;z2\u0026quot;) |\u0026gt; purrr::map(purrr::flatten_chr) |\u0026gt; tibble::as_tibble() |\u0026gt; dplyr::mutate( dplyr::across(dplyr::ends_with(\u0026quot;1\u0026quot;), stringr::str_remove, \u0026quot;[a-z]=\u0026quot;), dplyr::across(c(-state), as.integer), state = ifelse(state == \u0026quot;on\u0026quot;, 1L, -1L), ) # Iterar nos passos Iterate over steps cuboids \u0026lt;- dplyr::slice_head(steps, n = 1) for (i in 2:nrow(steps)) { # Iterar nos cubóides que já vimos for (j in seq_len(nrow(cuboids))) { # Calcular intersecção x1_inter \u0026lt;- max(steps$x1[i], cuboids$x1[j]) x2_inter \u0026lt;- min(steps$x2[i], cuboids$x2[j]) y1_inter \u0026lt;- max(steps$y1[i], cuboids$y1[j]) y2_inter \u0026lt;- min(steps$y2[i], cuboids$y2[j]) z1_inter \u0026lt;- max(steps$z1[i], cuboids$z1[j]) z2_inter \u0026lt;- min(steps$z2[i], cuboids$z2[j]) # Adicionar intersecção à lista (com sinal virado) if (x1_inter \u0026lt;= x2_inter \u0026amp;\u0026amp; y1_inter \u0026lt;= y2_inter \u0026amp;\u0026amp; z1_inter \u0026lt;= z2_inter) { cuboids \u0026lt;- tibble::add_row(cuboids, state = cuboids$state[j] * -1L, x1 = x1_inter, x2 = x2_inter, y1 = y1_inter, y2 = y2_inter, z1 = z1_inter, z2 = z2_inter, ) } } # Adicionar cubóide à lista se ele estiver ligado if (steps$state[i] == 1) { cuboids \u0026lt;- tibble::add_row(cuboids, state = steps$state[i], x1 = steps$x1[i], x2 = steps$x2[i], y1 = steps$y1[i], y2 = steps$y2[i], z1 = steps$z1[i], z2 = steps$z2[i], ) } } # Contar cubos ligados on \u0026lt;- 0 for (i in seq_len(nrow(cuboids))) { # Calcular volume x \u0026lt;- cuboids$x2[i] - cuboids$x1[i] + 1 y \u0026lt;- cuboids$y2[i] - cuboids$y1[i] + 1 z \u0026lt;- cuboids$z2[i] - cuboids$z1[i] + 1 # Adicionar/remover à/da conta on \u0026lt;- on + (x * y * z * cuboids$state[i]) } # Imprimir format(on, scientific = FALSE) #\u0026gt; [1] 1233304599156793 ","permalink":"https://blog.curso-r.com/posts/2021-12-22-advent-of-r-22/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 22"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nDados de Dirac (A) O dia 21 do AoC começou bem. O primeiro item foi bastante direto e tranquilo… O que complicou tudo foi o segundo.\nComeçamos aprendendo as regras de um jogo chamado Dados de Dirac. Ele é composto um tabuleiro circular que vai de 1 a 10, um dado e dois peões para representar os dois jogadores. Cada jogador rola o dado 3 vezes, soma os resultados e anda aquele número de casas no tabuleiro; o número da casa em que ele caiu é então adicionado à pontuação do jogador. Cada jogador começa em uma casa escolhida aleatoriamente e ganha o primeiro a atingir 1000 ou mais pontos.\nO primeiro item pedia para simularmos um jogo de Dados de Dirac com um dado determinístico antes de partirmos para a versão oficial. Nós recebemos como entrada a posição de início de cada jogador e a mecânica de funcionamento do dado: ele ia de 1 a 100 e seu resultado sempre vinha nessa ordem (ou seja, o primeiro jogador rolaria 1, 2, 3, o segundo rolaria 4, 5, 6, etc.). Nosso objetivo era simular o jogo até que alguém ganhasse e retornar a pontuação do jogador perdedor multiplicada pelo número de vezes que o dado foi rolado naquele jogo.\n# Ler posições iniciais pos \u0026lt;- \u0026quot;data-raw/21a_dirac_dice.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; stringr::str_extract(\u0026quot;[0-9]+$\u0026quot;) |\u0026gt; as.numeric() # Posições iniciais p1_pos \u0026lt;- pos[1] p2_pos \u0026lt;- pos[2] # Pontuações iniciais p1_pts \u0026lt;- 0 p2_pts \u0026lt;- 0 # Fazer os dados irem do valor máximo para 1 die_mod \u0026lt;- function(e1, e2) ((e1 - 1) %% e2) + 1 # Iterar até o jogo acabar die \u0026lt;- 1 counter \u0026lt;- 0 while (TRUE) { # J1 rola 3 vezes p1_rolls \u0026lt;- die:(die + 2) p1_rolls \u0026lt;- die_mod(p1_rolls, 100) # Atualizar estado do dado e contador de rolagem die \u0026lt;- die_mod(p1_rolls[3] + 1, 100) counter \u0026lt;- counter + 3 # Atualizar pontuação do J1 p1_pos \u0026lt;- p1_pos + sum(p1_rolls) p1_pos \u0026lt;- die_mod(p1_pos, 10) p1_pts \u0026lt;- p1_pts + p1_pos # Parar se J1 ganhou if (p1_pts \u0026gt;= 1000) break # J2 rola 3 vezes p2_rolls \u0026lt;- die:(die + 2) p2_rolls \u0026lt;- die_mod(p2_rolls, 100) # Atualizar estado do dado e contador de rolagem die \u0026lt;- die_mod(p2_rolls[3] + 1, 100) counter \u0026lt;- counter + 3 # Atualizar pontuação do J2 p2_pos \u0026lt;- p2_pos + sum(p2_rolls) p2_pos \u0026lt;- die_mod(p2_pos, 10) p2_pts \u0026lt;- p2_pts + p2_pos # Parar se J2 ganhou if (p2_pts \u0026gt;= 1000) break } # Contador * pontuação do perdedor min(p1_pts, p2_pts) * counter #\u0026gt; [1] 597600 Dados de Dirac (B) Bem direto, certo? Uma pena que o segundo item não tinha nada a ver… Agora deveríamos simular o jogo com o epônimo Dado de Dirac. Ele tem 3 lados (de 1 a 3) e, cada vez que ele é rolado, um universo paralelo é criado para cada possível resultado. Em suma, no final do jogo haveria um universo para cada caminho que o jogo poderia hipoteticamente tomar. Felizmente, com o Dado de Dirac, o jogo ia só até 21 pontos.\nNossa missão era, dadas as posições iniciais, calcular em quantos universos ganhava o jogador que ganhava mais vezes. Não parece tão difícil até você perceber que teremos algo em torno de 700 trilhões de universos para considerar. Espero que esteja claro que tentar gerar todas as rodadas não vai funcionar.\nA solução ideal para esse problema é programação dinâmica (PD) que, apesar do nome esotérico, não é tão misteriosa assim. De forma bem superficial, um algoritmo que usa PD começa dividindo o problema principal em sub-problemas mais simples e armazenando seus resultados; a parte vital é, então, utilizar esses resultados já calculados para evitar contas desnecessárias mais para frente.\nConcretamente, queremos dividir o jogo em estados distintos definidos pelos quartetos (p1_pos, p2_pos, p1_pts, p2_pts). Vejamos como funcionaria um trecho desse algoritmo:\nComeçamos por um estado no final do jogo: (3, 8, 19, 21). Neste universo, sabemos que o J2 ganhou, então salvamos a informação (3, 8, 19, 21) = (0, 1).\nMais para frente, encontramos o estado (3, 5, 19, 13). O J2 pode rolar uma série de valores aqui que precisamos verificar, mas, se ele rolar 1 + 1 + 1, sabemos que cairemos no estado (3, 8, 19, 21)! Sendo assim, podemos pular este cálculo e verificar apenas as outras rolagens possíveis.\nCom PD, calcularemos primeiro estados mais fáceis e, conforme formos evoluindo para o começo do jogo, já teremos calculado o número de vitórias de cada jogador em cada futuro. Assim, basta somar esses futuros e passar para um estado anterior.\n# Ler posições iniciais pos \u0026lt;- \u0026quot;data-raw/21b_dirac_dice.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; stringr::str_extract(\u0026quot;[0-9]+$\u0026quot;) |\u0026gt; as.numeric() # Posições iniciais p1_pos \u0026lt;- pos[1] p2_pos \u0026lt;- pos[2] # Fazer os dados irem do valor máximo para 1 die_mod \u0026lt;- function(e1, e2) ((e1 - 1) %% e2) + 1 # Criar um identificar para `states` id \u0026lt;- function(a, b, c, d) paste0(a, \u0026quot;,\u0026quot;, b, \u0026quot;,\u0026quot;, c, \u0026quot;,\u0026quot;, d) # Contar vitórias de cada jogador a partir de cada estado do jogo states \u0026lt;- list() count_states \u0026lt;- function(p1_pos, p2_pos, p1_pts = 0, p2_pts = 0) { this_id \u0026lt;- id(p1_pos, p2_pos, p1_pts, p2_pts) # Condições de parada if (p1_pts \u0026gt;= 21) return(c(1, 0)) if (p2_pts \u0026gt;= 21) return(c(0, 1)) if (this_id %in% names(states)) return(states[[this_id]]) # Todas as combinações possíveis de rolagens rolls \u0026lt;- list(1:3, 1:3, 1:3) |\u0026gt; purrr::cross() |\u0026gt; purrr::map(purrr::flatten_int) |\u0026gt; purrr::map_int(sum) # Iterar nas rolagens e fazer a recursão para os próximos estados wins_total \u0026lt;- c(0, 0) for (roll in rolls) { p1_pos_ \u0026lt;- die_mod(p1_pos + roll, 10) # Ir para o próximo estado e somar vitórias wins \u0026lt;- count_states(p2_pos, p1_pos_, p2_pts, p1_pts + p1_pos_) wins_total \u0026lt;- wins_total + rev(wins) } # Atualizar `states` e retornar states[[this_id]] \u0026lt;\u0026lt;- wins_total return(wins_total) } # Rodar programação dinâmica count_states(p1_pos, p2_pos) |\u0026gt; max() |\u0026gt; format(scientific = FALSE) #\u0026gt; [1] 634769613696613 ","permalink":"https://blog.curso-r.com/posts/2021-12-21-advent-of-r-21/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 21"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nMapa da Fossa (A) Depois de um domingo assustadoramente difícil, o problema do dia 20 do AoC foi bastante tranquilo de resolver. Tanto o enunciado quanto a solução me pareceram simples (apesar de algumas reclamações na internet sobre uma pegadinha que vou explicar em breve).\nHoje nós recebemos uma imagem na forma de uma matriz composta por pontos luminosos # e pontos escuros .. O outro componente da entrada era uma lista de “conversões”: nós deveríamos converter cada quadrado 3x3 da imagem em um número binário onde # = 1 e . = 0 e encontrar o elemento de índice correspondente da lista de conversões; o ponto do centro do quadrado deveria ser substituido por esse elemento da lista.\n# Um quadrado 3x3 # # . . # . # #[. . .]. # #[# . .]# # .[. # .]. # . . # # # # # Número correspondente # ...#...#. = 000100010 = 34 # # 34o elemento da lista de conversões # 0 10 20 30 [34] 40 50 60 70 # | | | | | | | | | # ..#.#..#####.#.#.#.###.##.....###.##.#..###.####..#####..#....#..#..##..## Entretanto, essa operação, denominada realce, tinha um detalhe a mais. A nossa imagem de entrada era, na verdade, infinita! Em todas as direções, a imagem deveria ser completa por infinitos pontos escuros. Nosso objetivo era contar o número de pontos luminosos que restavam na nossa imagem após 2 aplicações do realce.\nComo é possível imaginar, os pontos escuros infinitos não podem fazer diferença nessa contagem (senão a resposta seria incalculável). Note que um quadrado composto só por pontos escuros equivale ao índice 0 da lista e, no exemplo acima, isso é convertido para um novo ponto escuro; ou seja, as bordas infinitas continuam sendo escuras após o realce.\nA pegadinha, porém, era que a lista de conversões na entrada do problema começava com # e não ., ou seja, os infinitos pontos escuros iam virar infinitos pontos luminosos depois de um realce. Felizmente, na segunda aplicação, todos os quadrados luminosos apontariam para o 511º elemento da lista e esse sim era um .. Em conclusão, desde que aplicássemos um número par de realces, as fronteiras infinitas da imagem seriam escuras e o número de pontos luminosos poderia ser contado.\nSendo assim, o código que resolvia o problema era bem simples, bastava adicionar uma borda escura à imagem para levar em conta a fronteira infinita e seguir em frente.\n# Converter uma região 3x3 em um número img_to_int \u0026lt;- function(image) { # Achatar a matriz para uma só coluna bits \u0026lt;- ifelse(image == \u0026quot;.\u0026quot;, 0, 1) binary \u0026lt;- paste0(as.vector(t(bits)), collapse = \u0026quot;\u0026quot;) # String para inteiro strtoi(binary, base = 2) } # Aplicar realce enhance \u0026lt;- function(image, algo) { # Iterar nas linhas e colunas, sem passar pela borda new_image \u0026lt;- image for (i in 2:(nrow(image) - 1)) { for (j in 2:(ncol(image) - 1)) { # Trocar [i,j] pelo índice correspondente em `algo` ind \u0026lt;- img_to_int(image[(-1:1 + i), (-1:1 + j)]) new_image[i, j] \u0026lt;- algo[ind + 1] } } # Remover borda e retornar new_image[2:(nrow(image) - 1), 2:(ncol(image) - 1)] } # Adicionar borda add_padding \u0026lt;- function(image) { # Adicionar mais 2 linhas em cima e embaixo image \u0026lt;- rbind( image[1, ], image[1, ], image, image[nrow(image), ], image[nrow(image), ] ) # Adicionar 2 colunas na esquerda e na direita image \u0026lt;- cbind( image[, 1], image[, 1], image, image[, ncol(image)], image[, ncol(image)] ) return(image) } # Ler lista de realce como um vetor de strings algo \u0026lt;- \u0026quot;data-raw/20a_trench_map.txt\u0026quot; |\u0026gt; readr::read_lines(n_max = 1) |\u0026gt; stringr::str_split(\u0026quot;\u0026quot;) |\u0026gt; purrr::pluck(1) # Ler imagem como uma matriz (e adicionar bordas) image \u0026lt;- \u0026quot;data-raw/20a_trench_map.txt\u0026quot; |\u0026gt; readr::read_lines(skip = 2) |\u0026gt; purrr::prepend(rep(paste0(rep(\u0026quot;.\u0026quot;, 100), collapse = \u0026quot;\u0026quot;), 3)) |\u0026gt; append(rep(paste0(rep(\u0026quot;.\u0026quot;, 100), collapse = \u0026quot;\u0026quot;), 3)) |\u0026gt; {\\(s) stringr::str_c(\u0026quot;...\u0026quot;, s, \u0026quot;...\u0026quot;)}() |\u0026gt; stringr::str_split(\u0026quot;\u0026quot;) |\u0026gt; purrr::flatten_chr() |\u0026gt; matrix(106, 106, byrow = TRUE) # Aplicar o realce duas vezes e contar pontos luminosos image |\u0026gt; enhance(algo) |\u0026gt; add_padding() |\u0026gt; enhance(algo) |\u0026gt; magrittr::equals(\u0026quot;#\u0026quot;) |\u0026gt; sum() #\u0026gt; [1] 5498 Mapa da Fossa (B) O segundo item pedia apenas para aplicarmos o algoritmo 50 ao invés de 2 vezes e contar o número de pontos luminosos. Como o nosso algoritmo generaliza as bordas, podemos simplesmente aplicá-lo mais vezes.\n# Aplicar o realce 50 vezes image \u0026lt;- enhance(image, algo) for (i in seq_len(49)) { image \u0026lt;- enhance(add_padding(image), algo) } # Contar pontos luminosos image |\u0026gt; magrittr::equals(\u0026quot;#\u0026quot;) |\u0026gt; sum() #\u0026gt; [1] 16014 ","permalink":"https://blog.curso-r.com/posts/2021-12-20-advent-of-r-20/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 20"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nDetectores de Sinalizadores (A) Neville Chamberlain tem uma frase que eu gosto muito: “na guerra não há vencedores, todos são perdedores.” É assim que eu me senti com o dia 19 do AoC. No total eu demorei mais de 6 horas de programação intensa para resolver o problema de hoje. Joguei meu código fora múltiplas vezes, quase desisti, mas no final perseverei. Não acho que eu tenha resolvido o problema; assim como Chamberlain, acredito que o problema só perdeu antes.\nPor esse motivo, o post de hoje vai ser um pouco diferente. Em primeiro lugar, é impossível resumir as mais de 400 linhas do enunciado de forma efetiva e, em segundo, explicar o raciocínio por trás da minha solução seria tão exaustivo quanto. Sendo assim, vou fazer um super resumo do enunciado e deixar a explicação do código a cargo dos comentários. Quem sabe um dia eu não revisito esse exercício para dar um passo-a-passo melhor.\nO grosso da pergunta é o seguinte: temos 36 detectores e uma série de sinalizadores espalhados pelo oceano em posições fixas. A entrada são as coordenadas dos sinalizadores que são vistos por cada detector relativas à posição desse detector. Cada detector também pode estar em uma de 24 orientações (olhando para +x com o topo apontado para +y, olhando para -y com o topo apontado para +z, etc.). Se dois detectores tiverem uma intersecção entre os seus cubos de detecção, então deve haver pelo menos 12 sinalizadores nesse volume. A pergunta pede para calcularmos o número de sinalizadores que estão nessa região do mar.\n# Converter c(x,y,z) para \u0026quot;x,y,z\u0026quot; vec_to_str \u0026lt;- function(vec) { stringr::str_c(vec, collapse = \u0026quot;,\u0026quot;) } # Converter \u0026quot;x,y,z\u0026quot; para c(x,y,z) str_to_vec \u0026lt;- function(str) { as.integer(stringr::str_split(str, \u0026quot;,\u0026quot;)[[1]]) } # Atalho para escolhe(n,2) de uma lista choose_pairs \u0026lt;- function(l) { seq_along(l) |\u0026gt; list(seq_along(l)) |\u0026gt; purrr::cross(`==`) |\u0026gt; purrr::transpose() |\u0026gt; purrr::map(purrr::flatten_int) |\u0026gt; purrr::set_names(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;) |\u0026gt; dplyr::as_tibble() |\u0026gt; dplyr::rowwise() |\u0026gt; dplyr::mutate(ordered = paste0(sort(c(a, b)), collapse = \u0026quot;,\u0026quot;)) |\u0026gt; dplyr::group_by(ordered) |\u0026gt; dplyr::slice_head(n = 1) |\u0026gt; dplyr::ungroup() |\u0026gt; dplyr::select(-ordered) |\u0026gt; dplyr::mutate( a = purrr::map(a, ~l[[.x]]), b = purrr::map(b, ~l[[.x]]) ) } # Aplicar todas as rotações de um ponto apply_rotations \u0026lt;- function(point) { rotations \u0026lt;- list( list(c(-1, 0, 0), c(0, -1, 0), c(0, 0, 1)), list(c(-1, 0, 0), c(0, 0, -1), c(0, -1, 0)), list(c(-1, 0, 0), c(0, 0, 1), c(0, 1, 0)), list(c(-1, 0, 0), c(0, 1, 0), c(0, 0, -1)), list(c(0, -1, 0), c(-1, 0, 0), c(0, 0, -1)), list(c(0, -1, 0), c(0, 0, -1), c(1, 0, 0)), list(c(0, -1, 0), c(0, 0, 1), c(-1, 0, 0)), list(c(0, -1, 0), c(1, 0, 0), c(0, 0, 1)), list(c(0, 0, -1), c(-1, 0, 0), c(0, 1, 0)), list(c(0, 0, -1), c(0, -1, 0), c(-1, 0, 0)), list(c(0, 0, -1), c(0, 1, 0), c(1, 0, 0)), list(c(0, 0, -1), c(1, 0, 0), c(0, -1, 0)), list(c(0, 0, 1), c(-1, 0, 0), c(0, -1, 0)), list(c(0, 0, 1), c(0, -1, 0), c(1, 0, 0)), list(c(0, 0, 1), c(0, 1, 0), c(-1, 0, 0)), list(c(0, 0, 1), c(1, 0, 0), c(0, 1, 0)), list(c(0, 1, 0), c(-1, 0, 0), c(0, 0, 1)), list(c(0, 1, 0), c(0, 0, -1), c(-1, 0, 0)), list(c(0, 1, 0), c(0, 0, 1), c(1, 0, 0)), list(c(0, 1, 0), c(1, 0, 0), c(0, 0, -1)), list(c(1, 0, 0), c(0, -1, 0), c(0, 0, -1)), list(c(1, 0, 0), c(0, 0, -1), c(0, 1, 0)), list(c(1, 0, 0), c(0, 0, 1), c(0, -1, 0)), list(c(1, 0, 0), c(0, 1, 0), c(0, 0, 1)) ) # Criar uma tabela com (x, y, z) rotacionados e um ID de rotação rotations |\u0026gt; purrr::map(purrr::map, `*`, point) |\u0026gt; purrr::map(purrr::map, sum) |\u0026gt; purrr::map(purrr::flatten_dbl) |\u0026gt; dplyr::tibble() |\u0026gt; purrr::set_names(\u0026quot;point\u0026quot;) |\u0026gt; dplyr::mutate(rotation = rotations) |\u0026gt; tibble::rowid_to_column() |\u0026gt; tidyr::unnest(point) |\u0026gt; dplyr::mutate(coord = rep(c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;, \u0026quot;z\u0026quot;), dplyr::n() / 3)) |\u0026gt; tidyr::pivot_wider(names_from = coord, values_from = point) |\u0026gt; dplyr::mutate(rotation = purrr::map_chr(rotation, paste, collapse = \u0026quot;,\u0026quot;)) |\u0026gt; dplyr::select(x, y, z, rotation) } # Fábrica de função para transformar um ponto com rotação + translação factory_transform \u0026lt;- function(df) { # Extrair a operação de rotação da df rot \u0026lt;- df$rotation |\u0026gt; stringr::str_split(\u0026quot;c\\\\(\u0026quot;) |\u0026gt; purrr::pluck(1) |\u0026gt; stringr::str_remove(\u0026quot;\\\\),?\u0026quot;) |\u0026gt; stringr::str_subset(\u0026quot;,\u0026quot;) |\u0026gt; stringr::str_split(\u0026quot;, \u0026quot;) |\u0026gt; purrr::map(as.numeric) # Extrair a operação de translação da df trans \u0026lt;- c(df$dif_x, df$dif_y, df$dif_z) # Retornar função que aplica a transformação function(vec) { rot |\u0026gt; purrr::map(`*`, vec) |\u0026gt; purrr::map(sum) |\u0026gt; purrr::flatten_dbl() |\u0026gt; magrittr::add(trans) } } # Pegar todas as intersecções entre detectores get_intersections \u0026lt;- function(points) { # Parear os detectores e retornar as suas intersecções points |\u0026gt; purrr::map(choose_pairs) |\u0026gt; purrr::map( dplyr::mutate, # Intersecções são baseadas nas distâncias entre pontos dist = purrr::map2_dbl(a, b, ~sum((.x - .y)**2)) ) |\u0026gt; choose_pairs() |\u0026gt; dplyr::rowwise() |\u0026gt; dplyr::group_split() |\u0026gt; purrr::map(~dplyr::inner_join(.x[[\u0026quot;a\u0026quot;]][[1]], .x[[\u0026quot;b\u0026quot;]][[1]], \u0026quot;dist\u0026quot;)) |\u0026gt; purrr::keep(~nrow(.x) \u0026gt;= 66) # 66 = C(12, 2) = 12 pontos na intersec. } # Pegar todas as transformações que podem converter pairs1 em pairs2 get_transforms \u0026lt;- function(pairs1, pairs2) { # Criar uma função que leva pairs1[2] a pairs2[2a] ou pairs2[2b] dplyr::bind_rows( dplyr::mutate( apply_rotations(pairs1$a.x[[2]]), ref_x = pairs2$a.y[[2]][1], ref_y = pairs2$a.y[[2]][2], ref_z = pairs2$a.y[[2]][3] ), dplyr::mutate( apply_rotations(pairs1$a.x[[2]]), ref_x = pairs2$b.y[[2]][1], ref_y = pairs2$b.y[[2]][2], ref_z = pairs2$b.y[[2]][3] ) ) |\u0026gt; dplyr::mutate( dif_x = ref_x - x, dif_y = ref_y - y, dif_z = ref_z - z ) |\u0026gt; dplyr::rowwise() |\u0026gt; dplyr::group_split() |\u0026gt; purrr::map(factory_transform) } # Encontrar a função correta de transformação find_transform \u0026lt;- function(df, funs) { # Dadas as funções de transformação, encontrar uma que converte os pontos de # df (conjunto de intersecções) corretamente df |\u0026gt; tibble::rowid_to_column(\u0026quot;pair_id\u0026quot;) |\u0026gt; dplyr::rowwise() |\u0026gt; dplyr::group_split() |\u0026gt; purrr::map(~{ .x |\u0026gt; dplyr::mutate(, fun_a.x = list(purrr::map(funs, ~.x(a.x[[1]]))), fun_id = list(seq_along(funs)) ) |\u0026gt; tidyr::unnest(dplyr::starts_with(\u0026quot;fun\u0026quot;)) |\u0026gt; dplyr::select(-dist) |\u0026gt; tidyr::unnest(dplyr::everything()) }) |\u0026gt; dplyr::bind_rows() |\u0026gt; dplyr::mutate( a_works = a.y == fun_a.x, b_works = b.y == fun_a.x ) |\u0026gt; dplyr::group_by(pair_id, fun_id) |\u0026gt; dplyr::summarise( some_works = all(a_works) || all(b_works), .groups = \u0026quot;drop\u0026quot; ) |\u0026gt; dplyr::ungroup() |\u0026gt; dplyr::group_by(fun_id) |\u0026gt; dplyr::summarise(works = sum(some_works)) |\u0026gt; dplyr::slice_max(works) |\u0026gt; dplyr::pull(fun_id) } # Ler pontos como uma lista de vetores points \u0026lt;- \u0026quot;data-raw/19a_beacon_scanner.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; tibble::tibble() |\u0026gt; purrr::set_names(\u0026quot;point\u0026quot;) |\u0026gt; dplyr::mutate( scanner = as.integer(stringr::str_detect(point, \u0026quot;scanner\u0026quot;)), scanner = cumsum(scanner) - 1 ) |\u0026gt; dplyr::filter(!stringr::str_detect(point, \u0026quot;scanner\u0026quot;)) |\u0026gt; dplyr::filter(point != \u0026quot;\u0026quot;) |\u0026gt; dplyr::group_split(scanner) |\u0026gt; purrr::map(dplyr::pull, point) |\u0026gt; purrr::map(purrr::map, str_to_vec) # Reduzir detectores a uma única região while (length(points) \u0026gt; 1) { # Pegar um par de detectores que tem uma intersecção pairs \u0026lt;- get_intersections(points)[[1]] # Pegar todas as funções de transformação funs \u0026lt;- get_transforms( dplyr::select(pairs, a.x, b.x), dplyr::select(pairs, a.y, b.y) ) # Encontrar a função correta transformation \u0026lt;- funs[[find_transform(pairs, funs)]] # Converter pontos para strings pairs \u0026lt;- pairs |\u0026gt; dplyr::select(-dist) |\u0026gt; dplyr::mutate_all(purrr::map_chr, vec_to_str) # Criar uma cópia dos pontos que também é strings points_ \u0026lt;- purrr::map(points, purrr::map_chr, vec_to_str) # Encontrar detector usado como referência por transformation() for (i in seq_along(points_)) { ref \u0026lt;- all(c(pairs$a.y, pairs$b.y) %in% points_[[i]]) if (ref) reference \u0026lt;- i } # Encontrar detector que foi transformado por transformation() for (i in seq_along(points_)) { trns \u0026lt;- all(c(pairs$a.x, pairs$b.x) %in% points_[[i]]) if (trns) transformed \u0026lt;- i } # Aplicar transformation() em todos os pontos do detector e adicionar pontos # transformados ao detector de referência points_[[reference]] \u0026lt;- points[[transformed]] |\u0026gt; purrr::map(transformation) |\u0026gt; purrr::map_chr(vec_to_str) |\u0026gt; c(points_[[reference]]) |\u0026gt; unique() # Atualizar lista de pontos points_[[transformed]] \u0026lt;- NULL points \u0026lt;- purrr::map(points_, purrr::map, str_to_vec) } # Calcular o número de pontos em uma única região contígua sum(lengths(points)) #\u0026gt; [1] 408 Detectores de Sinalizadores (B) O segundo item pedia para que encontrássemos a maior distância de Manhattan entre detectores distintos.\n# Reduzir detectores a uma única região, guardando as funções de tranform. save_funs \u0026lt;- list() while (length(points) \u0026gt; 1) { # Pegar um par de detectores que tem uma intersecção pairs \u0026lt;- get_intersections(points)[[1]] # Pegar todas as funções de transformação funs \u0026lt;- get_transforms( dplyr::select(pairs, a.x, b.x), dplyr::select(pairs, a.y, b.y) ) # Encontrar a função correta transformation \u0026lt;- funs[[find_transform(pairs, funs)]] save_funs \u0026lt;- c(save_funs, transformation) # Converter pontos para strings pairs \u0026lt;- pairs |\u0026gt; dplyr::select(-dist) |\u0026gt; dplyr::mutate_all(purrr::map_chr, vec_to_str) # Criar uma cópia dos pontos que também é strings points_ \u0026lt;- purrr::map(points, purrr::map_chr, vec_to_str) # Encontrar detector usado como referência por transformation() for (i in seq_along(points_)) { ref \u0026lt;- all(c(pairs$a.y, pairs$b.y) %in% points_[[i]]) if (ref) reference \u0026lt;- i } # Encontrar detector que foi transformado por transformation() for (i in seq_along(points_)) { trns \u0026lt;- all(c(pairs$a.x, pairs$b.x) %in% points_[[i]]) if (trns) transformed \u0026lt;- i } # Aplicar transformation() em todos os pontos do detector e adicionar pontos # transformados ao detector de referência points_[[reference]] \u0026lt;- points[[transformed]] |\u0026gt; purrr::map(transformation) |\u0026gt; purrr::map_chr(vec_to_str) |\u0026gt; c(points_[[reference]]) |\u0026gt; unique() # Atualizar lista de pontos points_[[transformed]] \u0026lt;- NULL points \u0026lt;- purrr::map(points_, purrr::map, str_to_vec) } # Aplicar transformações aos detectores e tirar distância de Manhattan save_funs |\u0026gt; purrr::map(~.x(c(0, 0, 0))) |\u0026gt; choose_pairs() |\u0026gt; dplyr::mutate(dist = purrr::map2_dbl(a, b, ~sum(abs(.x - .y)))) |\u0026gt; dplyr::slice_max(dist) |\u0026gt; dplyr::pull(dist) ","permalink":"https://blog.curso-r.com/posts/2021-12-19-advent-of-r-19/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 19"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nPeixe-Caracol (A) Chegou o dia 18 do AoC e mais uma vez o problema não foi muito difícil apesar do enunciado monstruoso. Uma coisa que notei hoje é que havia vários caminhos para resolver o exercício que pareciam igualmente razoáveis. No final eu decidi usar regex, uma das melhores e mais temidas funcionalidades de qualquer linguagem de programação.\nO enunciado pedia para aprendermos a fazer somas usando os números dos peixes-caracol… A primeira característica desse sistema aritmético é que um número é representado por pares de elementos na forma [x,y], que podem ser números normais ou outros pares; por exemplo [[1,2],3]. Além disso, há duas limitações para os números: nunca pode haver um par dentro de 4 ou mais pares e nenhum número normal pode ser maior que 9.\nA soma dos peixes-caracol coloca cada um dos dois números como elementos de um novo par. Se o primeiro número for [a,b] e o segundo [x,y], então a soma deles é [[a,b],[x,y]]. Obviamente isso pode criar um número que viola a as limitações acima, então precisamos aplicar as regras da explosão e da quebra. Abaixo eu descrevo as regras e as funções que criei para implementar cada uma:\nA regra da explosão sempre vem primeiro e ela deve ser aplicada o maior número possível de vezes antes de partirmos para a regra da quebra.\n# Exemplo: # [[6,[5,[4,[3,2]]]],1] # # Passos da explosão: # 1. Encontrar o primeiro par simples que está dentro de 4 ou mais pares # [3,2] # # 2. Denominar as partes do par com x e y: # [x,y] = [3,2] # # 3. Somar x ao número normal mais próximo à esquerda (se houver) # [[6,[5,[4 + 3,[3,2]]]],1] # [[6,[5,[7,[3,2]]]],1] # # 4. Somar y ao número normal mais próximo à direita (se houver) # [[6,[5,[7,[3,2]]]],1 + 2] # [[6,[5,[7,[3,2]]]],3] # # 5. Substituir o par por 0 # [[6,[5,[7,0]]],3] # Encontrar posição de um par que precisa ser explodido find_explode \u0026lt;- function(num) { chrs \u0026lt;- stringr::str_split(num, \u0026quot;\u0026quot;)[[1]] # Iterar nos caracteres para encontrar um par profundo demais counter \u0026lt;- 0 for (i in seq_along(chrs)) { if (chrs[i] == \u0026quot;[\u0026quot;) { counter \u0026lt;- counter + 1 } else if (chrs[i] == \u0026quot;]\u0026quot;) { counter \u0026lt;- counter - 1 # Se o par for profundo demais, retornar if (counter \u0026gt;= 4) { # Encontrar o começo do par len \u0026lt;- num |\u0026gt; stringr::str_sub(end = i) |\u0026gt; stringr::str_extract(\u0026quot;\\\\[[^\\\\[]*?$\u0026quot;) |\u0026gt; stringr::str_length() |\u0026gt; magrittr::subtract(1) # Retornar \u0026quot;coordenadas\u0026quot; do par return(c(i - len, i)) } } } # Se não ouver par para explodir, returnar NULL return(NULL) } # Aplicar o algoritmo da explosão explode \u0026lt;- function(num) { # Encontrar um par para explodir pos \u0026lt;- find_explode(num) # Se não houver par, retornar o número if (is.null(pos)) return(num) # Extrair números normais do par pair \u0026lt;- num |\u0026gt; stringr::str_sub(pos[1], pos[2]) |\u0026gt; stringr::str_extract_all(\u0026quot;[0-9]+\u0026quot;) |\u0026gt; purrr::pluck(1) |\u0026gt; as.numeric() # Pegar a parte esquerda do número (até o par que vai explodir) lhs \u0026lt;- stringr::str_sub(num, end = pos[1] - 1) # Encontrar o número normal mais próximo de pair[1] e somar left_num \u0026lt;- lhs |\u0026gt; stringr::str_extract(\u0026quot;[0-9]+(?=[^0-9]+$)\u0026quot;) |\u0026gt; as.numeric() |\u0026gt; magrittr::add(pair[1]) # Pegar a parte direita do número (a partir do par que vai explodir) rhs \u0026lt;- stringr::str_sub(num, pos[2] + 1) # Encontrar o número normal mais próximo de pair[2] e somar right_num \u0026lt;- rhs |\u0026gt; stringr::str_extract(\u0026quot;^[^0-9]+[0-9]+\u0026quot;) |\u0026gt; stringr::str_remove(\u0026quot;^[^0-9]+\u0026quot;) |\u0026gt; as.numeric() |\u0026gt; magrittr::add(pair[2]) # Substituir os números normais que mudamos lhs \u0026lt;- stringr::str_replace(lhs, \u0026quot;[0-9]+([^0-9]+)$\u0026quot;, paste0(left_num, \u0026quot;\\\\1\u0026quot;)) rhs \u0026lt;- stringr::str_replace(rhs, \u0026quot;^([^0-9]+)[0-9]+\u0026quot;, paste0(\u0026quot;\\\\1\u0026quot;, right_num)) # Colar as partes esquerda e direita de volta return(paste0(lhs, \u0026quot;0\u0026quot;, rhs)) } Se não houver mais como aplicar a explosão, então podemos fazer uma quebra e voltar para o começo do algoritmo: aplicar quantas explosões forem possíveis e depois tentar uma quebra. Quando nenhuma regra puder ser aplicada, então encontramos o resultado da soma.\n# Exemplo: # [11,1] # # Passos da quebra: # 1. Encontrar o primeiro número normal maior que 9 # 11 # # 2. Criar um novo par onde o elemento da esquerda é o número dividido por 2 # arredondado para baixo e o elemento da direita é o número dividido por 2 # arredondado para cima. # [5,6] # # 3. Substituir o número normal pelo par criado # [[5,6],1] # Aplicar o algoritmo da quebra split \u0026lt;- function(num) { # Verificar se algo precisa ser quebrado e retornar o número se não if (!stringr::str_detect(num, \u0026quot;[0-9]{2,}\u0026quot;)) return(num) # Criar um par a partir das metades do primeiro número normal \u0026gt; 9 pair \u0026lt;- num |\u0026gt; stringr::str_extract(\u0026quot;[0-9]{2,}\u0026quot;) |\u0026gt; as.numeric() |\u0026gt; {\\(n) paste0(\u0026quot;[\u0026quot;, floor(n / 2), \u0026quot;,\u0026quot;, ceiling(n / 2), \u0026quot;]\u0026quot;)}() # Substituir o número normal pelo par criado stringr::str_replace(num, \u0026quot;[0-9]{2,}\u0026quot;, pair) } Agora que sabemos como explodir e qubrar, podemos implementar o algoritmo completo da soma dos peixes-caracol. Notem o next no loop; ele é essencial por causa da exigência de aplicarmos a explosão quantas vezes forem necessárias.\n# Soma dos peixes-caracol snailfish_sum \u0026lt;- function(num1, num2) { # Juntar números como elementos de um novo par num \u0026lt;- paste0(\u0026quot;[\u0026quot;, num1, \u0026quot;,\u0026quot;, num2, \u0026quot;]\u0026quot;) # Aplicar explosão e quebra até o número não mudar mais num_ \u0026lt;- \u0026quot;\u0026quot; while (num_ != num) { num_ \u0026lt;- num # Explodir e, se o número tiver mudado, voltar num \u0026lt;- explode(num) if (num_ != num) next # Qubrar num \u0026lt;- split(num) } return(num) } Mas o enunciado não pedia para simplesmente implementarmos a soma dos peixes-caracol… A resposta final deveria ser a magnitude do número obtido a partir de somas sucessivas. Essencialmente, a nossa entrada era uma sequência de números A, B, C, D, etc. e devíamos calcular (((A + B) + C) + D) + .... Já a magnitude de um número envolve outro algoritmo; a magnitude de um [x,y] qualquer é 3*x + 2*y, mas devemos aplicar isso recursivamente, entrando nas camadas mais profundas do número e voltando para a superfície.\n# Fazer uma rodada do algoritmo da magnitude get_one_magnitude \u0026lt;- function(num) { # Pegar a magnitude do par mais à esquerda val \u0026lt;- num |\u0026gt; stringr::str_extract(\u0026quot;\\\\[[^\\\\[\\\\]]+\\\\]\u0026quot;) |\u0026gt; stringr::str_extract_all(\u0026quot;[0-9]+\u0026quot;) |\u0026gt; purrr::pluck(1) |\u0026gt; as.numeric() |\u0026gt; {\\(n) 3 * n[1] + 2 * n[2]}() |\u0026gt; as.character() # Trocar o par pela sua magnitude stringr::str_replace(num, \u0026quot;\\\\[[^\\\\[\\\\]]+\\\\]\u0026quot;, val) } # Aplicar o algoritmo completo da magnitude get_magnitude \u0026lt;- function(num) { # Enquanto ainda houver pares, fazer uma rodada do cálculo while (stringr::str_detect(num, \u0026quot;\\\\[\u0026quot;)) { num \u0026lt;- get_one_magnitude(num) } # Retornar magnitude convertida para um valor numérico return(as.numeric(num)) } Enfim, depois de uma parede de texto e uma parede de código, podemos finalmente juntar tudo na solução do primeiro item.\n# Reduce list of numbers with snalfish addition and get magnitude \u0026quot;data-raw/18a_snailfish.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; purrr::reduce(snailfish_sum) |\u0026gt; get_magnitude() #\u0026gt; [1] 4124 Peixes-Caracol (B) Em um ato de bondade, o autor do Advent of Code fez um item 2 bem simples. Dados todos os números A, B, C, D, etc. que recebemos como entrada, precisávamos combinar todos para encontrar a maior magnitude possível. Minha solução foi gerar todas as somas possíveis (A + B, B + A, A + C, C + A, etc., notando que A + B != B + A) e simplesmente calcular a magnitude de todas. A resposta do item devia ser justamente essa maior magnitude possível.\n# Cruzar os números consigo mesmos e somar toda combinação \u0026quot;data-raw/18b_snailfish.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; {\\(ns) list(ns, ns)}() |\u0026gt; purrr::cross(`==`) |\u0026gt; purrr::map_dbl(~get_magnitude(snailfish_sum(.x[[1]], .x[[2]]))) |\u0026gt; max() #\u0026gt; [1] 4673 ","permalink":"https://blog.curso-r.com/posts/2021-12-18-advent-of-r-18/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 18"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nCesta de Três (A) O 17º dia do AoC foi uma ótima quebra em relação aos últimos. O enunciado era simples de entender e a solução foi fácil de criar, tudo que eu precisava depois de uma semana cansativa.\nHoje precisávamos tentar encontrar a chave do nosso submarino em uma fossa marinha. A sonda que tínhamos a bordo podia ser arremessada a partir do ponto (0, 0) com qualquer velocidade inteira tanto no eixo x quanto no y. A entrada do problema era a posição do alvo e a saída do primeiro item deveria ser a altura máxima que podíamos arremessar a sonda de modo que ela ainda atingisse o alvo.\nAs regras para a aceleração da sonda a cada passo eram as seguintes:\nA posição x da sonda aumenta um valor igual à sua velocidade x.\nA posição y da sonda aumenta um valor igual à sua velocidade y.\nPor causa do atrito, a velocidade x da sonda muda em 1 um direção a 0 (ou seja, ela diminui em 1 se a velocidade for maior que 0 e aumenta em 1 caso contrário).\nPor causa da gravidade, a velocidade y da sonda diminui em 1.\nO grande truque do exercício era identificar todas as velocidades possíveis da sonda e depois verificar qual o levava à maior altura. Como o alvo estava sempre abaixo e à direita do (0, 0), podíamos estabelecer os limites inferiores e superiores para as velocidades x e y:\nA velocidade x necessariamente tem que ser maior que 0, já que precisamos que a sonda se mova para frente. Adicionalmente, velocidade x máxima não pode ser maior que a fronteira direita do alvo; se o alvo terminar, por exemplo, em x = 10, nunca vamos acertá-lo jogando o módulo para frente com velocidade maior que 10.\nOs limites da velocidade y são mais difícil de entender. Em primeiro lugar, ela nunca pode ser menor do que a fronteira inferior do alvo (pensando na mesma lógica que usamos antes, se o alvo terminar, por exemplo, em y = -10, nunca vamos acertá-lo jogando a sonda para baixo com velocidade menor que -10). O limite superior vem do fato de que se jogarmos a sonda para cima, não importando a velocidade, ela eventualmente vai voltar a y = 0 com velocidade igual à velocidade inicial menos 1, mas com sinal negativo; sendo assim, a velocidade y máxima é igual ao valor absoluto do limite inferior do alvo.\n# Velocidade inicial: (6,3) # .................................. # .........(3,0).#..#.(2,-1)........ # .....(4,1).#........#.(1,-2)...... # .................................. # (5,2).#..............#.(0,-3)..... # .................................. # .................................. # S.(6,3)..............#.(0,-4)..... # .................................. # .................................. # .................................. # .....................#.(0,-5)..... # ....................TTTTTTTTTTT... # ....................TTTTTTTTTTT... # ....................TTTTTTTTTTT... # ....................TTTTTTTTTTT... # ....................T#T(0,-6)TT... # .................................. # .................................. Note no diagrama acima a simetria da trajetória no eixo y. Assim fica mais fácil entender porque, por exemplo, se o limite inferior do alvo for y = -10, então nunca podemos jogar a sonda para cima com velocidade maior que 9; ela voltará para y = 0 com velocidade -10 e acertará exatamente a fronteira de baixo do alvo.\n# Ler alvo como uma tabela de coordenadas target \u0026lt;- \u0026quot;data-raw/17a_trick_shot.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; stringr::str_split(\u0026quot;[=,]\u0026quot;) |\u0026gt; purrr::pluck(1) |\u0026gt; stringr::str_subset(\u0026quot;^[0-9-]\u0026quot;) |\u0026gt; stringr::str_replace(\u0026quot;\\\\.\\\\.\u0026quot;, \u0026quot;:\u0026quot;) |\u0026gt; purrr::map(~eval(parse(text = .x))) |\u0026gt; purrr::cross() |\u0026gt; purrr::transpose() |\u0026gt; purrr::set_names(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;) |\u0026gt; tibble::as_tibble() |\u0026gt; tidyr::unnest(c(x, y)) # Todas as possíveis combinações de velocidades x e y válidas vels \u0026lt;- purrr::cross(list( 1:max(target$x), min(target$y):abs(min(target$y)) )) Para calcular a altura máxima que poderíamos arremessar a sonda, eu simulei a trajetória a partir de cada um dos pares de velocidades válidas e guardei a altura máxima à qual a sonda chegava. No final da iteração, se a sonda de fato atingisse o alvo, então eu comparava a altura máxima dessa combinação com a altura máxima global e mantinha a maior.\n# Verificar quais pares de velocidades funcionam e pegar a altura máxima max_height \u0026lt;- 0 for (vel in vels) { # Posição inicial x_pos \u0026lt;- 0 y_pos \u0026lt;- 0 # Velocidades iniciais x_vel \u0026lt;- vel[[1]] y_vel \u0026lt;- vel[[2]] # Encontrar a altura máxima deste par de velocidades max_height_ \u0026lt;- 0 while (y_pos \u0026gt;= min(target$y) \u0026amp;\u0026amp; x_pos \u0026lt;= max(target$x)) { # Atualizar posições x_pos \u0026lt;- x_pos + x_vel y_pos \u0026lt;- y_pos + y_vel # Atualizar altura máxima local if (y_pos \u0026gt; max_height_) max_height_ \u0026lt;- y_pos # Se o par de fato leva ao alvo, atualizar altura máxima global if (x_pos %in% target$x \u0026amp;\u0026amp; y_pos %in% target$y) { if (max_height_ \u0026gt; max_height) max_height \u0026lt;- max_height_ } # Atualizar velocidades x_vel \u0026lt;- if (x_vel \u0026gt; 0) x_vel - 1 else 0 y_vel \u0026lt;- y_vel - 1 } } # Retornar a altura máxima global max_height #\u0026gt; [1] 4753 Cesta de Três (B) Chegando no item 2, eu percebi que tinha dado muita sorte. O enunciado aqui pedia para encontrarmos o número de velocidades iniciais da sonda que a faziam chegar ao alvo. Meu código anterior já encontrava todos os pares válidos, mas utilizava isso para atualizar a altura máxima; só era necessário trocar a variável sendo atualizada dentro do while.\n# Verificar pares de velocidades que funcionam e contá-los n_works \u0026lt;- 0 for (vel in vels) { # Posição inicial x_pos \u0026lt;- 0 y_pos \u0026lt;- 0 # Velocidades iniciais x_vel \u0026lt;- vel[[1]] y_vel \u0026lt;- vel[[2]] # Encontrar a altura máxima deste par de velocidades max_height_ \u0026lt;- 0 while (y_pos \u0026gt;= min(target$y) \u0026amp;\u0026amp; x_pos \u0026lt;= max(target$x)) { # Atualizar posições x_pos \u0026lt;- x_pos + x_vel y_pos \u0026lt;- y_pos + y_vel # Se o par de fato leva ao alvo, atualizar contador if (x_pos %in% target$x \u0026amp;\u0026amp; y_pos %in% target$y) { n_works \u0026lt;- n_works + 1 break } # Atualizar velocidades x_vel \u0026lt;- if (x_vel \u0026gt; 0) x_vel - 1 else 0 y_vel \u0026lt;- y_vel - 1 } } # Retornar número de velocidades que funcionam n_works ","permalink":"https://blog.curso-r.com/posts/2021-12-17-advent-of-r-17/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 17"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nDecodificador de Pacotes (A) O 16º problema do AoC foi bastante diverido. O enunciado era extremamente longo e cheio de detalhes, mas consegui fazer uma implementação direta e eficiente que só não funcionou de primeira por causa de um detalhe obscuro da função strtoi().\nHoje nosso objetivos era decodificar pacotes binários. Eles chegavam ao nosso submarino em hexadecimal e, depois de convertidos para binário eles tinham as seguintes características:\nOs 3 primeiros bits representavam a versão do pacote;\nOs 3 bits seguintes representavam o tipo do pacote, que podia cair em dois casos:\nSe o tipo (na forma decimal) fosse igual a 4, então o pacote representaria um valor. Isso queria dizer que o resto do pacote poderia ser quebrado em pedaços de 5 bits com a seguinte configuração:\nSe o pedaço começasse com 1, então os 4 bits a seguir eram parte do valor e deveríamos continuar lendo o pacote;\nSe o pedaço começassem em 0, então os 4 bits a seguir eram o final do valor e poderíamos parar de ler o pacote.\nSe o tipo do pacote fosse diferente de 4, então o pacote representaria um operador. Isso queria dizer que o bit de número 7 indicava o modo do pacote:\nSe o indicador fosse 1, então os próximos 15 bits seriam iguais à soma dos comprimentos de todos os sub-pacotes contidos naquele pacote operador;\nSe o indicador fosse 0, então os próximos 11 bits seriam iguais ao número de sub-pacotes contidos naquele pacote operador.\nSimples? Longe disso. Vejamos alguns exemplos:\n# Pacote literal (valor) # D2FE28 # 110100101111111000101000 # VVVTTTAaaaaBbbbbCcccc # # - VVV são a versão do pacote, 6. # - TTT são o tipo, 4. Então este pacote carrega um valor. # - A é 1 (continuar lendo), então aaaa são o primeiro pedaço do valor. # - B é 1 (continuar lendo), então bbbb são o segundo pedaço do valor. # - C é 0 (parar de ler), então cccc são o último pedaço do valor. # - O resto são bits extras. # - Portanto, o valor carregado por este pacote é 011111100101 = 2021. # # Pacote operador com indicador 0 # 38006F45291200 # 00111000000000000110111101000101001010010001001000000000 # VVVTTTILLLLLLLLLLLLLLLAAAAAAAAAAABBBBBBBBBBBBBBBB # # - VVV são a versão do pacote, 1. # - TTT são o tipo, 6. Então este pacote carrega um operador. # - I é o indicador, 0. Então este pacote tem 15 bits com os comprimentos # dos sub-pacotes. # - LLLLLLLLLLLLLLL contêm a soma dos comprimentos dos sub-pacotes, 27. # - AAAAAAAAAAA são um sub-pacote carregando um valor, 10. # - BBBBBBBBBBBBBBBB são um sub-pacote carregando um valor, 20. # # Pacote operador com indicador 1 # EE00D40C823060 # 11101110000000001101010000001100100000100011000001100000 # VVVTTTILLLLLLLLLLLAAAAAAAAAAABBBBBBBBBBBCCCCCCCCCCC # - VVV são a versão do pacote, 7. # - TTT são o tipo, 3. Então este pacote carrega um operador. # - I é o indicador, 1. Então este pacote tem 11 bits com os número de # sub-pacotes. # - LLLLLLLLLLL contêm o número de sub-pacotes, 3. # - AAAAAAAAAAA são um sub-pacote carregando um valor, 1. # - BBBBBBBBBBB são um sub-pacote carregando um valor, 2. # - CCCCCCCCCCC são um sub-pacote carregando um valor, 3. O ponto positivo desse enunciado enorme é que conseguimos implementar os recursos necessários quase em sequência.\n# Converter string hexadecimal para string binária hex_to_bits \u0026lt;- function(hex) { hex |\u0026gt; stringr::str_split(\u0026quot;\u0026quot;) |\u0026gt; purrr::pluck(1) |\u0026gt; purrr::map(~paste(rev(as.integer(intToBits(strtoi(.x, 16)))))) |\u0026gt; purrr::map(magrittr::extract, 29:32) |\u0026gt; purrr::flatten_chr() |\u0026gt; stringr::str_c(collapse = \u0026quot;\u0026quot;) } # Pegar a versão de um pacote get_version \u0026lt;- function(pkt) { strtoi(stringr::str_sub(pkt, 1, 3), 2) } # Pegar o tipo de um pacote get_type \u0026lt;- function(pkt) { strtoi(stringr::str_sub(pkt, 4, 6), 2) } O objetivo final deste item era parsear a hierarquia de pacotes da nossa entrada e somar as versões de todos. Minha solução envolveu, desta forma, cirar uma “classe” que podia conter a versão e o comprimento de um pacote. O comprimento era importante para descartar o número certo de bits do pacote quando tivéssemos terminado de processar um sub-pacote.\nSe um pacote fosse do tipo operador, então sua “classe” também conteria todos os seus sub-pacotes como elementos sem nome. O código abaixo implementa o processamento dos dois tipos de pacotes; note como foram implementadas as “classes”:\n# Pegar o valor de um pacote literal get_literal \u0026lt;- function(pkt) { interval \u0026lt;- c(7, 11) # Iterar até o último pedaço ser encontrado literal \u0026lt;- \u0026quot;\u0026quot; flag \u0026lt;- FALSE while (!flag) { # Pegar o grupo especificado pelo intervalo group \u0026lt;- stringr::str_sub(pkt, interval[1], interval[2]) literal \u0026lt;- stringr::str_c(literal, stringr::str_sub(group, 2)) # Parar se este é o último pedaço, caso contrário somar 5 ao intervalo if (!as.integer(stringr::str_sub(group, 1, 1))) { flag \u0026lt;- TRUE } else { interval \u0026lt;- interval + 5 } } # Retornar a \u0026quot;classe\u0026quot; que descreve o pacote return(list( version = get_version(pkt), len = interval[2], value = strtoi(literal, 2) )) } # Processar um pacote operador get_operator \u0026lt;- function(pkt) { indicator \u0026lt;- stringr::str_sub(pkt, 7, 7) # Inicializar \u0026quot;classe\u0026quot; out \u0026lt;- list( version = get_version(pkt) ) # Lidar com os 2 indicadores if (as.integer(indicator)) { # Pegar o número de sub-pacotes e separar a cauda do pacote num \u0026lt;- strtoi(stringr::str_sub(pkt, 8, 18), 2) rest \u0026lt;- stringr::str_sub(pkt, 19) out$len \u0026lt;- 18 # Iterar no número de pacotes for (i in seq_len(num)) { # Processar sub-pacote sub \u0026lt;- if (get_type(rest) == 4) get_literal(rest) else get_operator(rest) out$len \u0026lt;- out$len + sub$len out \u0026lt;- c(out, list(sub)) # Atualizar a cauda dado o compimento do último sub-pacote rest \u0026lt;- stringr::str_sub(rest, sub$len + 1) } } else { # Pegar o limite de comprimento dos sub-pacotes e separar a cauda lim \u0026lt;- strtoi(stringr::str_sub(pkt, 8, 22), 2) rest \u0026lt;- stringr::str_sub(pkt, 23) out$len \u0026lt;- 22 # Iterar enquanto os sub-pacotes não tiverem passado do limite while (lim \u0026gt; 0) { # Processar sub-pacote sub \u0026lt;- if (get_type(rest) == 4) get_literal(rest) else get_operator(rest) out$len \u0026lt;- out$len + sub$len out \u0026lt;- c(out, list(sub)) # Atualizar a cauda dado o compimento do último sub-pacote rest \u0026lt;- stringr::str_sub(rest, sub$len + 1) lim \u0026lt;- lim - sub$len } } return(out) } O último passo do meu código era achatar toda a estrutura de árvore que seria devolvida pelas funções acima e somar todos os comprimentos.\n# Somar todas as versões do pacote representado por um hex sum_versions \u0026lt;- function(hex) { # Pegar a árvore de pacotes representada pelo hex pkt \u0026lt;- hex_to_bits(hex) pkts \u0026lt;- if (get_type(pkt) == 4) get_literal(pkt) else get_operator(pkt) # Achatar árvore while (purrr::vec_depth(pkts) \u0026gt; 2) { pkts \u0026lt;- purrr::flatten(pkts) } # Somar versões pkts |\u0026gt; magrittr::extract(names(pkts) == \u0026quot;version\u0026quot;) |\u0026gt; purrr::reduce(sum) } # Ler pacotes de um hex e somar versões \u0026quot;data-raw/16a_packet_decoder.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; sum_versions() #\u0026gt; [1] 991 Decodificador de Pacotes (B) O segundo item era mais ou menos o que eu já esperava. Os tipos dos pacotes tinham um significado maior, ou seja, cada sub-tipo de pacote operador indicava uma operação matemática que deveria ser aplicada no valor dos seus sub-pacotes.\nA operação 0 é soma (sum()).\nA operação 1 é produto (prod()).\nA operação 2 é mínimo (min()).\nA operação 3 é máximo (max()).\nA operação 5 é maior que (\u0026gt;).\nA operação 6 é menor que (\u0026lt;).\nA operação 7 é igual (==).\nOu seja, se um pacote tiver a estrutura (operador + (operador * (valor 1) (valor 2)) (valor 3)), então a expressão aritmética resultante seria (1 * 2) + 3). Nosso objetivo final era calcular o valor da expressão que o nosso pacote representava. Felizmente, o meu script anterior funcionava muito bem com essa alteração!\nEu troquei o elemento version da “classe” por type (o tipo do operador) e adicionei o seguinte no final do código:\n# Avaliar a árvore de pacotes get_value \u0026lt;- function(tree) { # Funções correspondentes aos tipos fun \u0026lt;- switch(as.character(tree$type), \u0026quot;0\u0026quot; = sum, \u0026quot;1\u0026quot; = prod, \u0026quot;2\u0026quot; = min, \u0026quot;3\u0026quot; = max, \u0026quot;5\u0026quot; = `\u0026gt;`, \u0026quot;6\u0026quot; = `\u0026lt;`, \u0026quot;7\u0026quot; = `==`, ) # Aplicar função aos sub-pacotes apply_fun \u0026lt;- function(tree) { tree |\u0026gt; purrr::keep(names(tree) == \u0026quot;\u0026quot;) |\u0026gt; purrr::map(get_value) |\u0026gt; purrr::reduce(fun) } # Aplicar recursivamente if (tree$type == 4) tree$value else as.numeric(apply_fun(tree)) } # Decodificar a expressão de um pacote hex decode \u0026lt;- function(hex) { pkt \u0026lt;- hex_to_bits(hex) tree \u0026lt;- if (get_type(pkt) == 4) get_literal(pkt) else get_operator(pkt) get_value(tree) } # Ler pacotes de um hex e calcular o valor da expressão \u0026quot;data-raw/16b_packet_decoder.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; decode() |\u0026gt; format(scientific = FALSE) #\u0026gt; [1] 1264485568252 P.S.: Mas isso não funcionou de primeira! Eu recebi um belo NA ao final da execução e demorei para entender a causa… No final eu descobri que a função strtoi() retorna NA quando o resultado é grande demais. A solução foi trocá-la por uma função própria:\nstrton \u0026lt;- function(x) { y \u0026lt;- as.numeric(strsplit(x, \u0026quot;\u0026quot;)[[1]]) sum(y * 2^rev((seq_along(y) - 1))) } ","permalink":"https://blog.curso-r.com/posts/2021-12-16-advent-of-r-16/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 16"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nQuítons (A) No 15º dia do AoC eu demorei muito mais do que deveria. Apesar de ter entendido bem o enunciado e ter identificado rapidamente o caminho para a solução, eu empaquei na implementação do algoritmo. No final achei melhor pegar uma versão pronta do algoritmo para não perder mais horas com isso.\nNovamente o enunciado envolvia um submarino, uma caverna, etc. Passar por cada ponto da caverna vinha com um certo risco que variava entre 1 e 9 e nosso objetivo era levar o submarino do ponto esquerdo superior até o ponto esquerdo inferior passando pelo caminho com menor risco total. A saída do programa deveria ser a soma do risco de todos os pontos do caminho (sem incluir o ponto de entrada, pois já começávamos nele).\nA esse ponto, qualquer um que tenha aprendido sobre grafos já deve estar com o sentido aranha ativado. Esse é um problema clássico da Computação que pode ser facilmente solucionado pelo algoritmo de Dijkstra. Algumas alterações são necessárias, mas todas podem ser feitas antes de executar o algoritmo.\nO passo-a-passo do código é mais ou menos o seguinte:\nMarcar todos os pontos como não visitados. Criar um conjunto com todos os pontos não visitador chamado conjunto não visitado.\nAtribuir a todos os pontos um risco temporário: ele deve ser 0 para o nó inicial e infinito para o resto. O risco temporário de um ponto v é o risco total do caminho de menor risco já descoberto entre v e o ponto inicial. Como no começo não conhecemos nenhum outro ponto além do inicial, todos os riscos temporários começam como infinito. Fixar o ponto inicial como o atual.\nPara o ponto atual, considerar todos os seus vizinhos não vizitados e calcular os seus riscos temporários através do ponto atual. Comparar o novo risco temporário ao risco atual e atribuir o menor dos dois. Por exemplo, se o risco do ponto atual A é 6 e o seu vizinho B tem risco 2, então o risco de chegar em B por A é 6 + 2 = 8. Se o risco temporário de B até agora era maior que 8, então ele deve virar 8. Caso contrário, nada muda.\nQuando já tivermos considerado todos os vizinhos não visitados do ponto atual, marcar o ponto atual como visitado e removê-lo do conjunto não visitado. Um ponto visitado nunca será checado de novo.\nSe o ponto final houver sido marcado como visitado, então parar. O algoritmo terminou e o risco total do melhor caminho até o distino é igual ao risco temporário que foi atribuido ao destino.\nCaso contrário, selecionar o ponto não visitado que tem o menor risco temporário e torná-lo o ponto atual. Voltar ao passo 3.\nNormalmente o algoritmo de Dijkstra é aplicado em grafos nos quais os custos de cada passo do caminho são atribuídos à arestas do grafo e não aos nós, como é o nosso caso. Para resolver esse problema, temos que fazer uma certa ginástica para que os custos sejam transferidos para as arestas. Cada par de nós vizinhos ganham duas arestas direcionadas, cada uma com o risco do nó para o qual ela aponta:\n# Ponto atual com seus 4 vizinhos # 7 # 9 3 1 # 6 # # Arestas indo para o ponto atual (todas têm risco 3) # o # 3 # ↓ # o 3 → x ← 3 o # ↑ # 3 # o # # Arestas saindo do ponto atual (todas têm o risco do vizinho) # o # ↑ # 7 # o ← 9 x 1 → o # 6 # ↓ # o Eu queria ter de fato implementado o algoritmo de Dijkstra no R por conta própria, mas eu cometi vários erros pelo caminho (eram 7:30, não me julgue) e, para não passar a manhã toda nisso, resolvi usar o pacote cppRouting para aplicar o algoritmo.\n# Ler os riscos da caverna como uma matriz cave \u0026lt;- \u0026quot;data-raw/15a_chiton.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; stringr::str_split(\u0026quot;\u0026quot;) |\u0026gt; purrr::flatten_chr() |\u0026gt; as.integer() |\u0026gt; matrix(100, 100, byrow = TRUE) # Criar uma tabela com os custos entre vizinhos graph \u0026lt;- tibble::tibble() for (i in 1:prod(dim(cave))) { vals \u0026lt;- c() if (i %% 100 != 0) vals \u0026lt;- append(vals, i + 1L) if (i %% 100 != 1) vals \u0026lt;- append(vals, i - 1L) if (i \u0026gt; 100) vals \u0026lt;- append(vals, i - 100L) if (i \u0026lt; 9901) vals \u0026lt;- append(vals, i + 100L) node \u0026lt;- tibble::tibble(from_vertex = i, to_vertex = vals, cost = cave[vals]) graph \u0026lt;- dplyr::bind_rows(graph, node) } # Criar grafo e executar o algoritmo de Dijkstra path \u0026lt;- graph |\u0026gt; cppRouting::makegraph(directed = TRUE) |\u0026gt; cppRouting::get_path_pair(from = 1L, to = 10000L) |\u0026gt; purrr::pluck(1) |\u0026gt; as.integer() # Calcular o risco total do caminho (subtraíndo o custo da entrada) graph |\u0026gt; dplyr::filter(to_vertex %in% path) |\u0026gt; dplyr::group_by(to_vertex) |\u0026gt; dplyr::summarise(cost = cost[1]) |\u0026gt; dplyr::summarise(risk = sum(cost)) |\u0026gt; dplyr::pull(risk) |\u0026gt; magrittr::subtract(cave[1]) #\u0026gt; [1] 811 Quítons (B) O segundo item seguia a mesma lógica de outros problemas desse ano: igual ao primeiro item, mas maior. Como eu estava usando um algoritmo bastante eficiente, não tive problema nenhum nessa parte.\nAqui descobríamos que, na verdade, a caverna era 5 vezes maior em cada dimensão (ou seja, 25 vezes mais pontos). A caverna completa era, entretanto, composta por cópias da sessão original com riscos mais elevados; para obter a versão final da caverna era necessário juntar 25 cópias da original somando um certo fator a cada cópia.\n# +0 +1 +2 +3 +4 # +1 +2 +3 +4 +5 # +2 +3 +4 +5 +6 # +3 +4 +5 +6 +7 # +4 +5 +6 +7 +8 Seguindo o guia acima, vemos que o canto superior esquerdo da caverna maior era igual à sessão original e, sucessivamente, chegávamos ao canto direito inferior, que era igual à sessão original, mas o risco de cada ponto era acrescido de 8. O único detalhe é que, quando o risco de um ponto passava de 9, ele voltava para 1 (igual aos polvos-dumbo que vimos anteriormente). O resto da solução era igual.\n# Criar clones da caverna, somar fator de risco e juntar cave \u0026lt;- cbind( rbind(cave + 0L, cave + 1L, cave + 2L, cave + 3L, cave + 4L), rbind(cave + 1L, cave + 2L, cave + 3L, cave + 4L, cave + 5L), rbind(cave + 2L, cave + 3L, cave + 4L, cave + 5L, cave + 6L), rbind(cave + 3L, cave + 4L, cave + 5L, cave + 6L, cave + 7L), rbind(cave + 4L, cave + 5L, cave + 6L, cave + 7L, cave + 8L) ) # Reduzir pontos que passaram de 9 cave[cave \u0026gt; 9] \u0026lt;- cave[cave \u0026gt; 9] - 9 ","permalink":"https://blog.curso-r.com/posts/2021-12-15-advent-of-r-15/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 15"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nPolimerização Estendida (A) O 14º dia do AoC foi muito demorado de resolver para mim. Apesar de ambas as soluções abaixo serem “simples”, levei horas para encontrar um jeito razoável de resolver o segundo item e, infelizmente, só consegui depois de olhar uma dica na internet que deixou tudo mais simples.\nDesta vez nossa missão era estender um molde de polímero através de um conjunto de regras de reação. A primeira linha da entrada era o molde e, a partir daí, tínhamos as regras de inserção:\n# NNCB # # CH -\u0026gt; B # HH -\u0026gt; N # CB -\u0026gt; H # NH -\u0026gt; C # HB -\u0026gt; C # HC -\u0026gt; B # HN -\u0026gt; C # NN -\u0026gt; C # BH -\u0026gt; H # NC -\u0026gt; B # NB -\u0026gt; B # BN -\u0026gt; B # BB -\u0026gt; N # BC -\u0026gt; B # CC -\u0026gt; N # CN -\u0026gt; C As regras eram fáceis de entender. Cada uma delas indicava que, quando os dois elementos da esquerda se encontravam, entre eles apareceria o elemento da direita. Uma rodada de reação envolvia estender todos os pares da cadeia polimérica; no caso do exemplo, isso transformaria NNCB em NCNBCHB.\nApós 10 iterações, deveríamos contar o número de ocorrências do elemento mais comum da cadeia e subtrair dele o número de ocorrências do elemento menos comum da cadeia. Esta seria a resposta do problema.\nMeu código para o primeiro item acabou seguindo o que chamamos de estratégia de força bruta. A cada iteração, eu quebrava o polímero nos seus pares de elementos e fazia um join com a tabela de regras; depois era só colar tudo em uma string só e seguir em frente. No final eu só precisava encontrar as letras mais e menos comuns da string e subtraí-las.\n# Ler modelo como string poly \u0026lt;- readr::read_lines(\u0026quot;data-raw/14a_extended_polymerization.txt\u0026quot;, n_max = 1) # Ler regras como tabela rules \u0026lt;- \u0026quot;data-raw/14a_extended_polymerization.txt\u0026quot; |\u0026gt; readr::read_table(skip = 1, col_names = FALSE) |\u0026gt; purrr::set_names(\u0026quot;pair\u0026quot;, \u0026quot;rm\u0026quot;, \u0026quot;insertion\u0026quot;) |\u0026gt; dplyr::select(-rm) |\u0026gt; dplyr::mutate(insertion = stringr::str_replace( pair, \u0026quot;(.)(.)\u0026quot;, paste0(\u0026quot;\\\\1\u0026quot;, insertion, \u0026quot;\\\\2\u0026quot;) )) # Executar uma rodada de inserções do_insertions \u0026lt;- function(poly) { poly |\u0026gt; stringr::str_split(\u0026quot;\u0026quot;) |\u0026gt; purrr::pluck(1) |\u0026gt; purrr::accumulate(~paste0(stringr::str_sub(.x, -1), .y)) |\u0026gt; utils::tail(-1) |\u0026gt; purrr::map_chr(~rules[rules$pair == .x, ]$insertion) |\u0026gt; purrr::reduce(~paste0(.x, stringr::str_sub(.y, -2))) |\u0026gt; stringr::str_c(collapse = \u0026quot;\u0026quot;) } # Rodar do_insertions() 10 vezes e fazer el. mais comum - el. menos comum 10 |\u0026gt; seq_len() |\u0026gt; purrr::reduce(~do_insertions(.x), .init = poly) |\u0026gt; stringr::str_split(\u0026quot;\u0026quot;) |\u0026gt; table() |\u0026gt; {\\(t) list(t[which.max(t)], t[which.min(t)])}() |\u0026gt; purrr::reduce(`-`) |\u0026gt; abs() |\u0026gt; unname() #\u0026gt; [1] 2584 Polimerização Estendida (B) O segundo item parecia suspeitamente simples, mas eu estava redondamente enganado. A única instrução era repetir o problema do primeiro item para 40 iterações ao invés de 10. Pode parecer que eu não precisaria nem mudar meu código, mas note que no primeiro item a minha cadeia polimérica só chegou a ter 19457 letras. No segundo item a cadeia chegaria a… mais de 20 trilhões.\nSeria necessário mudar de estratégia e foi aí que eu empaquei. Tentei diversas formas de manter apenas o número de letras na cadeia, sem armazenar a cadeia em si, mas nada funcionava. Eu até notei que a primeira e a última letras da cadeia nunca mudavam, mas isso não me ajudou.\nDepois de procurar por dicas no subreddit do AoC, finalmente achei uma boa alma que havia feito uma observação incrível:\nSempre podemos manter apenas a contagem de pares distintos na cadeia. Se tivermos, por exemplo, um par AC aparecendo n = 10 vezes na cadeia e uma regra AC -\u0026gt; B, então na próxima iteração podemos adicionar à nossa contagem AB e BC, cada uma aparecendo n = 10 vezes.\nAté aí eu já sabia, era essencialmente o que eu fazia manualmente no item 1. O problema é que, mantendo apenas as contagens dos pares, isso repetiria a letra B duas vezes, totalizando A 10 vezes, C 10 vezes e B 20 vezes. A ideia que veio a seguir, entretanto, foi o que realmente resolveu o problema:\nSe pensarmos na cadeia como um todo, todos as letras serão contadas 2 vezes, exceto pela primeira e pela última, pois elas nunca ficam no meio de uma reação. O número de ocorrências de cada letra é, portanto, n / 2, exceto pelas letras que aparecem no início e no fim, paras quais a fórmula é (n + 1) / 2.\nDepois disso o item 2 podia ser solucionado facilmente.\n# Registrar a primeira e a última letras da cadeia original orig \u0026lt;- \u0026quot;data-raw/14b_extended_polymerization.txt\u0026quot; |\u0026gt; readr::read_lines(n_max = 1) |\u0026gt; stringr::str_replace(\u0026quot;^(.).*?(.)$\u0026quot;, \u0026quot;\\\\1\\\\2\u0026quot;) |\u0026gt; stringr::str_split(\u0026quot;\u0026quot;) |\u0026gt; purrr::pluck(1) # Ler modelo já no formato de contagem de pares poly \u0026lt;- \u0026quot;data-raw/14b_extended_polymerization.txt\u0026quot; |\u0026gt; readr::read_lines(n_max = 1) |\u0026gt; stringr::str_split(\u0026quot;\u0026quot;) |\u0026gt; purrr::pluck(1) |\u0026gt; purrr::accumulate(~paste0(stringr::str_sub(.x, -1), .y)) |\u0026gt; utils::tail(-1) |\u0026gt; tibble::tibble() |\u0026gt; purrr::set_names(\u0026quot;pair\u0026quot;) |\u0026gt; dplyr::count(pair) # Ler regras como tabela rules \u0026lt;- \u0026quot;data-raw/14b_extended_polymerization.txt\u0026quot; |\u0026gt; readr::read_table(skip = 1, col_names = FALSE) |\u0026gt; purrr::set_names(\u0026quot;pair\u0026quot;, \u0026quot;rm\u0026quot;, \u0026quot;insertion\u0026quot;) |\u0026gt; dplyr::select(-rm) |\u0026gt; dplyr::mutate(insertion = stringr::str_replace( pair, \u0026quot;(.)(.)\u0026quot;, paste0(\u0026quot;\\\\1\u0026quot;, insertion, \u0026quot;\\\\2\u0026quot;) )) # Executar uma rodada de inserções do_insertions \u0026lt;- function(poly) { poly |\u0026gt; dplyr::left_join(rules, \u0026quot;pair\u0026quot;) |\u0026gt; dplyr::mutate( insertion = purrr::map(insertion, stringr::str_extract, c(\u0026quot;^..\u0026quot;, \u0026quot;..$\u0026quot;)) ) |\u0026gt; tidyr::unnest(insertion) |\u0026gt; dplyr::group_by(pair = insertion) |\u0026gt; dplyr::summarise(n = sum(n)) } # Rodar do_insertions() 40 vezes e fazer el. mais comum - el. menos comum 40 |\u0026gt; seq_len() |\u0026gt; purrr::reduce(~do_insertions(.x), .init = poly) |\u0026gt; dplyr::mutate(elem = stringr::str_split(pair, \u0026quot;\u0026quot;)) |\u0026gt; tidyr::unnest(elem) |\u0026gt; dplyr::group_by(elem) |\u0026gt; dplyr::summarise(n = sum(n)) |\u0026gt; dplyr::mutate( n = ifelse(elem %in% orig, n + 1, n), n = n / 2 ) |\u0026gt; dplyr::filter(n == max(n) | n == min(n)) |\u0026gt; dplyr::pull(n) |\u0026gt; purrr::reduce(`-`) |\u0026gt; abs() |\u0026gt; format(scientific = FALSE) #\u0026gt; [1] 3816397135460 ","permalink":"https://blog.curso-r.com/posts/2021-12-14-advent-of-r-14/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 14"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nOrigami Transparente (A) O dia 13 foi um belo alívio comparado com o dia anterior. Nossa missão hoje era descobrir o código de um sensor a partir de um código escrito em papel transparente. A entrada era uma série de coordenadas de pontos no papel e uma sequência de instruções de como dobrar o papel para obter o código final.\nPartindo do princípio de que a matriz começava no ponto (0, 0) na esqueda superior, o primeiro item pedia para que lêssemos a nossa lista de coordenadas e contasse o número de pontos (#) visíveis depois de realizar a primeira instrução que nos era dada. Para ilustrar como as dobras ocorriam, veja os resultados de uma dobra em y = 7 e, depois, de uma dobra em x = 5:\n# Papel inicial # ...#..#..#. # ....#...... # ........... # #.......... # ...#....#.# # ........... # ........... # ........... # ........... # ........... # .#....#.##. # ....#...... # ......#...# # #.......... # #.#........ # Linha em y = 7 # ...#..#..#. # ....#...... # ........... # #.......... # ...#....#.# # ........... # ........... # ----------- # ........... # ........... # .#....#.##. # ....#...... # ......#...# # #.......... # #.#........ # Resultado da primeira dobra # #.##..#..#. # #...#...... # ......#...# # #...#...... # .#.#..#.### # ........... # ........... # Linha em x = 5 # #.##.|#..#. # #...#|..... # .....|#...# # #...#|..... # .#.#.|#.### # .....|..... # .....|..... # Resultado final # ##### # #...# # #...# # #...# # ##### # ..... # ..... O maior desafio no código em R foi arrumar todas as coordenadas e sub-matrizes para um sistema que começa em 1 e não em 0. Eu também resolvi fazer uma aposta: o primeiro item pedia para fazer apenas a primeira dobra, então eu imaginei que o segundo item pediria para fazer todas. Minha decisão, portanto, foi tentar já generalizar meu algortimo para que ele funcionasse com o mínimo de alterações possíveis para realizar várias dobras.\n# Ler tabela de onde os pontos estão dots \u0026lt;- \u0026quot;data-raw/13a_transparent_origami.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; stringr::str_subset(\u0026quot;^[0-9]\u0026quot;) |\u0026gt; tibble::tibble() |\u0026gt; purrr::set_names(\u0026quot;dot\u0026quot;) |\u0026gt; tidyr::separate(dot, c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;), \u0026quot;,\u0026quot;) |\u0026gt; dplyr::mutate_all(as.integer) |\u0026gt; dplyr::mutate_all(`+`, 1L) # Ler instruções das dobras instructions \u0026lt;- \u0026quot;data-raw/13a_transparent_origami.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; stringr::str_subset(\u0026quot;^[^0-9]\u0026quot;) |\u0026gt; tibble::tibble() |\u0026gt; purrr::set_names(\u0026quot;fold\u0026quot;) |\u0026gt; tidyr::separate(fold, c(\u0026quot;axis\u0026quot;, \u0026quot;line\u0026quot;), \u0026quot;=\u0026quot;) |\u0026gt; dplyr::mutate( axis = stringr::str_sub(axis, -1), line = as.integer(line) + 1L ) # Colocar os pontos no papel paper \u0026lt;- matrix(FALSE, nrow = max(dots$y), ncol = max(dots$x)) for (i in seq_len(nrow(dots))) { paper[dots$y[i], dots$x[i]] \u0026lt;- TRUE } # Rodar apenas a primeira instrução for (i in 1) { # Achar o eixo e o ponto da dobra axis \u0026lt;- instructions$axis[i] line \u0026lt;- instructions$line[i] # Dobras de acordo com o eixo if (axis == \u0026quot;x\u0026quot;) { # Número de colunas à direita da dobra size \u0026lt;- length((line + 1):dim(paper)[2]) # Pegar colunas à direita, invertê-las e fazer um OR com o lado esquerdo paper[, (line - size):(line - 1)] \u0026lt;- paper[, (line + 1):(line + size)][, size:1] | paper[, (line - size):(line - 1)] # Descartar colunas representando o papel dobrado paper \u0026lt;- paper[, 1:(line - 1)] } else { # Número de linhas abaixo da dobra size \u0026lt;- length((line + 1):dim(paper)[1]) # Pegar linhas abaixo da dobra, invertê-las e fazer um AND com as acima paper[(line - size):(line - 1), ] \u0026lt;- paper[(line + 1):(line + size), ][size:1, ] | paper[(line - size):(line - 1), ] # Descartar linhas representando o papel dobrado paper \u0026lt;- paper[1:(line - 1), ] } } # Contar pontos no papel sum(paper) #\u0026gt; [1] 765 Origami Transparente (B) E minha aposta valeu à pena! De fato o enunciado da parte 2 pedia para que realizássemos todas as dobras do nosso conjunto de instruções. No final, se tudo estivesse correto, os # e . do papel deveriam formar 8 letras maiúsculas.\nA única alteração no código foi trocar a condição do for:\n# Iterar por todas as instruções for (i in seq_len(nrow(instructions))) E, no final, também foi necessário fazer um print melhor da matriz:\n# Imprimir os pontos de um jeito mais amigável paper \u0026lt;- ifelse(paper, \u0026quot;#\u0026quot;, \u0026quot;.\u0026quot;) for (i in seq_len(nrow(paper))) { cat(paper[i, ]) cat(\u0026quot;\\n\u0026quot;) } # # # # . . # # # # . # . . # . # # # # . # . . . . # # # . . . # # . . # . . # . # # . . # . . . . # . # . # . . . . . # . # . . . . # . . # . # . . # . # . . # . # # . . # . . . # . . # # . . . . . # . . # . . . . # . . # . # . . . . # # # # . # # # # . . . # . . . # . # . . . # . . . # . . . . # # # . . # . # # . # . . # . # # . # . . # . . . . # . # . . # . . . . # . . . . # . . . . # . . # . # . . # . # # . . # . # # # # . # . . # . # # # # . # # # # . # . . . . . # # # . # . . # . ","permalink":"https://blog.curso-r.com/posts/2021-12-13-advent-of-r-13/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 13"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nBusca de Caminho (A) O dia 12, juntamente com os anteriores, começou a me deixar preocupado com os próximos exercícios do Advent of Code. Aparentemente a dificuldade vai aumentando conforme o passar dos dias, mas já estou chegando no limite do meu conhecimento.\nMais uma vez temos um enunciado complicado, então leia a versão original se ficar difícil de entender aqui. Nosso objetivo esta vez era contar o número de caminhos que o nosso submarino podia tomar em um sistema de cavernas.\nA entrada era uma lista de arestas nomeadas em um grafo. Os nossos caminhos deveriam sempre começar na caverna chamada “start” e terminar na chamada “end”, sendo que todas as outras eram divididas em dois grupos: grandes e pequenas. Uma caverna grande era demarcada por uma letra maiúscula e podia ser utilizada pelo nosso caminho qualquer número de vezes. Já uma caverna pequena (demarcada por uma letra minúscula), só podia ser utilizada uma vez no caminho.\nVeja o exemplo abaixo. A primeira parte seria a entrada do problema, a segunda, o diagrama das cavernas e a terceira, os 10 possíveis caminhos para o nosso submarino.\n# start-A # start-b # A-c # A-b # b-d # A-end # b-end # start # / \\ # c--A-----b--d # \\ / # end # start,A,b,A,c,A,end # start,A,b,A,end # start,A,b,end # start,A,c,A,b,A,end # start,A,c,A,b,end # start,A,c,A,end # start,A,end # start,b,A,c,A,end # start,b,A,end # start,b,end Minha solução envolvia uma tabela que representava todas as arestas do grafo do sistema de cavernas. A cada nova recursão, a última caverna poderia ser mantida na tabela ou removida (no caso das cavernas pequenas); toda vez que um caminho chegasse ao “end”, um contador global era incrementado.\n# Contar caminhos distintos em um grafo count \u0026lt;- 0 count_paths \u0026lt;- function(graph, path = \u0026quot;start\u0026quot;) { # Verificar se o nó atual é \u0026quot;pequeno\u0026quot; cave \u0026lt;- tail(path, 1) is_small \u0026lt;- stringr::str_to_lower(cave) == cave # Condições de parada if (cave == \u0026quot;end\u0026quot;) {count \u0026lt;\u0026lt;- count + 1; return(1)} if (!any(graph$orig == cave)) return(0) # Encontrar próximo nó do caminho searches \u0026lt;- graph |\u0026gt; dplyr::filter(orig == cave) |\u0026gt; dplyr::pull(dest) |\u0026gt; purrr::map(purrr::prepend, path) # Atualizar nós disponíveis graph \u0026lt;- if (is_small) dplyr::filter(graph, orig != cave) else graph # Iterar nos possíveis caminhos for (search in searches) { count_paths(graph, search) } # Retornar contador global return(count) } # Ler arestas do grafo e retornar conta dos caminhos \u0026quot;data-raw/12a_passage_pathing.txt\u0026quot; |\u0026gt; readr::read_table(col_names = \u0026quot;path\u0026quot;) |\u0026gt; tidyr::separate(path, c(\u0026quot;orig\u0026quot;, \u0026quot;dest\u0026quot;), \u0026quot;-\u0026quot;) |\u0026gt; {\\(d) dplyr::bind_rows(d, purrr::set_names(d, rev(names(d))))}() |\u0026gt; dplyr::filter(dest != \u0026quot;start\u0026quot;, orig != \u0026quot;end\u0026quot;) |\u0026gt; count_paths() #\u0026gt; [1] 4792 Busca de Caminho (B) O segundo item do problema mudava muito pouco o enunciado. Agora, ao invés de cada caverna pequena poder ser visitada apenas 1 vez, tínhamos um pequeno acréscimo de tempo. Isso queria dizer que, em cada caminho até o final do sistema de cavernas, podíamos visitar apenas 1 das cavernas pequenas até 2 vezes.\nMinha solução foi criar um argumento chamado boost que indicava se já tínhamos usado o nosso excedente de tempo naquele caminho expecífico. Se não tivéssemos, poderíamos não retirar uma das cavernas pequenas da lista imediatamente. Esta estratégia funcionou, mas gerou caminhos repetidos (usando e não usando o boost), então, ao invés de contar os caminhos, passei a salvar os caminhos e contar o número de caminhos distintos no final.\n# Pegar todos os caminhos distintos em um grafo all_paths \u0026lt;- list() get_paths \u0026lt;- function(graph, path = \u0026quot;start\u0026quot;, boost = FALSE) { # Verificar se o nó atual é \u0026quot;pequeno\u0026quot; cave \u0026lt;- tail(path, 1) is_small \u0026lt;- stringr::str_to_lower(cave) == cave # Condições de parada if (cave == \u0026quot;end\u0026quot;) {all_paths \u0026lt;\u0026lt;- append(all_paths, list(path)); return(1)} if (!any(graph$orig == cave)) return(0) # Encontrar próximo nó do caminho searches \u0026lt;- graph |\u0026gt; dplyr::filter(orig == cave) |\u0026gt; dplyr::pull(dest) |\u0026gt; purrr::map(purrr::prepend, path) # Atualizar nós disponíveis graph_ \u0026lt;- if (is_small) dplyr::filter(graph, orig != cave) else graph # Iterar nos possíveis caminhos for (search in searches) { get_paths(graph_, search, boost = boost) # Uma opção é não remover o nó do grafo e usar o boost if (!boost \u0026amp;\u0026amp; is_small \u0026amp;\u0026amp; cave != \u0026quot;start\u0026quot;) { get_paths(graph, search, boost = TRUE) } } # Retornar lista global return(all_paths) } # Ler arestas do grafo e retornar conta dos caminhos distintos \u0026quot;data-raw/12b_passage_pathing.txt\u0026quot; |\u0026gt; readr::read_table(col_names = \u0026quot;path\u0026quot;) |\u0026gt; tidyr::separate(path, c(\u0026quot;orig\u0026quot;, \u0026quot;dest\u0026quot;), \u0026quot;-\u0026quot;) |\u0026gt; {\\(d) dplyr::bind_rows(d, purrr::set_names(d, rev(names(d))))}() |\u0026gt; dplyr::filter(dest != \u0026quot;start\u0026quot;, orig != \u0026quot;end\u0026quot;) |\u0026gt; get_paths() |\u0026gt; purrr::map_chr(stringr::str_c, collapse = \u0026quot;|\u0026quot;) |\u0026gt; unique() |\u0026gt; length() #\u0026gt; [1] 133360 ","permalink":"https://blog.curso-r.com/posts/2021-12-12-advent-of-r-12/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 12"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nPolvo-dumbo (A) O dia 11 do AoC foi bastante complicado e o meu código talvez tenha ficado pior ainda. As instruções eram até simples: recebemos uma matriz 10x10 com os níveis de energia de 100 polvos-dumbo e precisávamos acompanhar seus níveis de energia ao longo de 100 iterações. As regras eram as seguintes:\nPrimeiro, o nível de energia de cada polvo sobe em 1.\nDepois, qualquer polvo com nível de energia maior que 9 emite luz (pisca). Isso aumenta o nível de energia de todos os polvos adjacentes em 1, incluindo os adjacentes diagonalmente. Se isso causar um polvo a atingir um nível de energia maior que 9, ele também pisca. Esse processo continua conforme mais polvos passam do nível de energia 9. Um polvo só pode piscar uma vez por passo e não pode subir mais nenhum nível de energia a partir daí.\nFinalmente, todos os polvos que piscaram durante este passo têm seus níveis de energia ajustados para 0 (já que ele usou toda a sua energia para piscar).\nMeu código seguia esse procedimento à risca e precisou de 3 loops aninhados para funcionar. O truque mais importante foi criar um clone dos polvos que marcava todos os polvos que já tinham piscado para garantir que nenhum deles ganharia mais energia durante aquele passo; este mecanismo envolvia marcar um polvo que piscava com 0 e um polvo que tinha piscado em qualquer ponto anterior do loop com -1 (para que ele não fosse contado duas vezes). O resultado final deveria ser o número de piscadas totais depois dos 100 passos.\n# Ler matriz dumbo \u0026lt;- \u0026quot;data-raw/11a_dumbo_octopus.txt\u0026quot; |\u0026gt; readr::read_table(col_names = FALSE) |\u0026gt; tidyr::separate(X1, paste0(\u0026quot;C\u0026quot;, 0:10), \u0026quot;\u0026quot;) |\u0026gt; dplyr::select(-C0) |\u0026gt; dplyr::mutate_all(as.numeric) |\u0026gt; as.matrix() # Iterar nos 100 passos flashes \u0026lt;- 0 for (k in 1:100) { # Aumentar níveis de energia dumbo \u0026lt;- (dumbo + 1) %% 10 # Adicionar energia aos polvos cujos vizinhos piscaram flag \u0026lt;- FALSE while(!flag) { # Contar piscadas flashes \u0026lt;- flashes + sum(dumbo == 0) # Adicionar energia aos polvos adjacentes a piscadas dumbo_ \u0026lt;- dumbo for (i in 1:10) { for (j in 1:10) { # Índices dos vizinhos i1 \u0026lt;- i - 1 i2 \u0026lt;- min(i + 1, 10) j1 \u0026lt;- j - 1 j2 \u0026lt;- min(j + 1, 10) # Adicionar energia nos índices (exceto no centro) if (dumbo[i, j] == 0) { dumbo_[i1:i2, j1:j2] \u0026lt;- dumbo_[i1:i2, j1:j2] + 1 dumbo_[i, j] \u0026lt;- dumbo_[i, j] - 1 } } } # Separar piscadas anteriores dos que piscaram na última iteração dumbo \u0026lt;- ifelse(dumbo == -1, 0, dumbo) # Sobrescrever as piscadas com 0 (eles não podem receber mais energia) dumbo \u0026lt;- ifelse(dumbo == 0, 0, dumbo_) # Verificar se o passo atual acabou if (!any(dumbo \u0026gt; 9)) { flag \u0026lt;- TRUE } else { # Prevenir piscadas antigas de serem contadas de novo dumbo \u0026lt;- ifelse(dumbo == 0, -1, dumbo) dumbo \u0026lt;- ifelse(dumbo \u0026gt; 9, 0, dumbo) } } } # Imprimir flashes #\u0026gt; [1] 1681 Polvo-dumbo (B) Felizmente o segundo item o exercício de hoje foi bem mais simples. Eventualmente todos os polvos entram em sincronia, ou seja, passam a piscar todos juntos; o nosso objetivo era descobrir em que passo isso acontecia. A única coisa que precisei fazer com o código do item anterior foi ignorar o limite de passos e criar uma verificação para quando todos os polvos atingiam 0 de energia juntos.\n# Ler matriz dumbo \u0026lt;- \u0026quot;data-raw/11b_dumbo_octopus.txt\u0026quot; |\u0026gt; readr::read_table(col_names = FALSE) |\u0026gt; tidyr::separate(X1, paste0(\u0026quot;C\u0026quot;, 0:10), \u0026quot;\u0026quot;) |\u0026gt; dplyr::select(-C0) |\u0026gt; dplyr::mutate_all(as.numeric) |\u0026gt; as.matrix() # Iterar em 1000 passos for (k in 1:1000) { print(k) # Aumentar níveis de energia dumbo \u0026lt;- (dumbo + 1) %% 10 # Adicionar energia aos polvos cujos vizinhos piscaram flag \u0026lt;- FALSE while(!flag) { # Adicionar energia aos polvos adjacentes a piscadas dumbo_ \u0026lt;- dumbo for (i in 1:10) { for (j in 1:10) { # Índices dos vizinhos i1 \u0026lt;- i - 1 i2 \u0026lt;- min(i + 1, 10) j1 \u0026lt;- j - 1 j2 \u0026lt;- min(j + 1, 10) # Adicionar energia nos índices (exceto no centro) if (dumbo[i, j] == 0) { dumbo_[i1:i2, j1:j2] \u0026lt;- dumbo_[i1:i2, j1:j2] + 1 dumbo_[i, j] \u0026lt;- dumbo_[i, j] - 1 } } } # Separar piscadas anteriores dos que piscaram na última iteração dumbo \u0026lt;- ifelse(dumbo == -1, 0, dumbo) # Sobrescrever as piscadas com 0 (eles não podem receber mais energia) dumbo \u0026lt;- ifelse(dumbo == 0, 0, dumbo_) # Verificar se o passo atual acabou if (!any(dumbo \u0026gt; 9)) { flag \u0026lt;- TRUE } else { # Prevenir piscadas antigas de serem contadas de novo dumbo \u0026lt;- ifelse(dumbo == 0, -1, dumbo) dumbo \u0026lt;- ifelse(dumbo \u0026gt; 9, 0, dumbo) } } # Parar se todos os polvos tiverem piscado if (all(dumbo %in% c(0, -1))) { break() } } # Imprimir k #\u0026gt; [1] 276 ","permalink":"https://blog.curso-r.com/posts/2021-12-11-advent-of-r-11/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 11"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nPontuação de Sintaxe (A) O dia 10 do AoC pedia para que resolvessemos um clássico problema de parentização com alguns facilitadores. Em resumo, recebíamos uma string composta por parênteses e seus amigos (“(”, “[”, ”{”, ”\u0026lt;”, ”\u0026gt;”, ”}”, ”]”, “)”) e precisávamos identificar se o fechamento de algum deles estava errado, por exemplo, “[}”, “{()()()\u0026gt;”, etc. Para cada string que tivesse um fechamento ilegal recebia uma quantidade de pontos de acordo com a tabela abaixo e, finalmente, a saída do exercício era a soma de todas as pontuações.\n# ): 3 pontos. # ]: 57 pontos. # }: 1197 pontos. # \u0026gt;: 25137 pontos. Para quem nunca viu um problema desse tipo, a solução pode ser alcançada facilmente usando uma pilha. Cada caractere que abre um bloco é colocado na pilha e, para cada caractere que fecha um bloco, removemos o elemento do topo da pilha. Se os caracteres se complementam corretamente o algoritmo segue em frente, caso contrário ele busca a pontuação na tabela e retorna.\n# Correspondência de valores scores \u0026lt;- list( \u0026quot;)\u0026quot; = 3, \u0026quot;]\u0026quot; = 57, \u0026quot;}\u0026quot; = 1197, \u0026quot;\u0026gt;\u0026quot; = 25137 ) # Calcular a pontuação por caractere ilegal em uma linha score_ilegal \u0026lt;- function(line) { stack \u0026lt;- flifo::lifo() # Iterar na linha até um elemento não corresponder symbols \u0026lt;- stringr::str_split(line, \u0026quot;\u0026quot;)[[1]] for (symbol in symbols) { # Empilhar ou desempilhar (e calcular pontuação se necessário) if (symbol %in% c(\u0026quot;(\u0026quot;, \u0026quot;[\u0026quot;, \u0026quot;{\u0026quot;, \u0026quot;\u0026lt;\u0026quot;)) { flifo::push(stack, symbol) } else { check \u0026lt;- flifo::pop(stack) if ( (check == \u0026quot;{\u0026quot; \u0026amp;\u0026amp; symbol != \u0026quot;}\u0026quot;) || (check == \u0026quot;(\u0026quot; \u0026amp;\u0026amp; symbol != \u0026quot;)\u0026quot;) || (check == \u0026quot;[\u0026quot; \u0026amp;\u0026amp; symbol != \u0026quot;]\u0026quot;) || (check == \u0026quot;\u0026lt;\u0026quot; \u0026amp;\u0026amp; symbol != \u0026quot;\u0026gt;\u0026quot;) ) { return(scores[names(scores) == symbol][[1]]) } } } return(0) } # Iterar nas linhas e calcular pontuações \u0026quot;data-raw/10a_syntax_scoring.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; purrr::map_dbl(score_ilegal) |\u0026gt; sum() #\u0026gt; [1] 216297 Pontuação de Sintaxe (B) O segundo item do problema pedia para que começássemos removendo as linhas que tinham pontuação maior que 0 (então só foi necessário filtrar isso no código, que vou omitir). Depois disso o objetivo era completar as linhas que restavam.\nO fato é que as linhas restantes estavam todas com um pedaço faltando, por exemplo, “[({(\u0026lt;(())[]\u0026gt;[[{[]{\u0026lt;()\u0026lt;\u0026gt;\u0026gt;” precisa ainda de ”}}]])})]” para ficar correta. Usando a lógica do item anterior, só precisávamos seguir o mesmo roteiro e, ao final da linha, contar os pontos dos caracteres que ainda haviam sobrado na pilha.\nDesta vez a regra de pontuação era diferente: para cada caractere faltante, precisávamos multiplicar a pontuação corrente por 5 e então somar o valor do caractere de acordo com uma nova tabelha. A resposta final era a mediana da pontuação de todoas as linhas. Enfim, o código vai a seguir:\n# Ler linhas e remover corrompidas lines \u0026lt;- readr::read_lines(\u0026quot;data-raw/10b_syntax_scoring.txt\u0026quot;) lines \u0026lt;- lines[purrr::map_dbl(lines, score_ilegal) == 0] # Correspondência de valores scores \u0026lt;- list( \u0026quot;(\u0026quot; = 1, \u0026quot;[\u0026quot; = 2, \u0026quot;{\u0026quot; = 3, \u0026quot;\u0026lt;\u0026quot; = 4 ) # Calcular a pontuação por caractere faltante em uma linha score_complete \u0026lt;- function(line) { stack \u0026lt;- flifo::lifo() # Iterar na linha e remover parte completa symbols \u0026lt;- stringr::str_split(line, \u0026quot;\u0026quot;)[[1]] for (symbol in symbols) { # Empilhar ou desempilhar if (symbol %in% c(\u0026quot;(\u0026quot;, \u0026quot;[\u0026quot;, \u0026quot;{\u0026quot;, \u0026quot;\u0026lt;\u0026quot;)) { flifo::push(stack, symbol) } else { flifo::pop(stack) } } # Iterar no resto da pilha e calcular pontos score \u0026lt;- 0 while (flifo::size(stack) \u0026gt; 0) { check \u0026lt;- flifo::pop(stack) score \u0026lt;- (score * 5) + scores[names(scores) == check][[1]] } return(score) } # Pegar mediana das pontuações lines |\u0026gt; purrr::map_dbl(score_complete) |\u0026gt; median() #\u0026gt; [1] 2165057169 ","permalink":"https://blog.curso-r.com/posts/2021-12-10-advent-of-r-10/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 10"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nBacia de Fumaça (A) O dia 9 do AoC foi desafiador, apesar de não tanto quanto o anterior. Como sempre o problema envolvia uma historinha que não contribui muito para o entendimento do enunciado, então vamos direto ao ponto: recebemos uma matriz 100x100 que representa um mapa de alturas e precisávamos encontrar todos os pontos que eram cercados (em cima, embaixo, na esquerda e na direita) por pontos mais altos. Ademais sabíamos que as alturas iam de 0 a 9 e que as fronteiras fora do mapa podiam ser todas consideradas mais altas que o resto do mapa. A resposta do problema seria o risco total de todos os pontos baixos, onde o risco de um ponto é igual à sua altura + 1.\nO problema não é tão complicado, pois bastaria iterar em todos os pontos da matriz e comparar cada um com seus vizinhos. O maior dezafio era lidar com as fronteiras do mapa. Para isso, resolvi cercar toda a matriz por noves e iterar no quadrado 2:101x2:101.\n# Ler o mapa de alturas e estofar as fronteiras com 9 height \u0026lt;- \u0026quot;data-raw/09a_smoke_basin.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; stringr::str_split(\u0026quot;\u0026quot;) |\u0026gt; purrr::flatten_chr() |\u0026gt; as.integer() |\u0026gt; matrix(nrow = 100, ncol = 100, byrow = TRUE) |\u0026gt; rbind(rep(9, 100)) |\u0026gt; {\\(m) rbind(rep(9, 100), m)}() |\u0026gt; cbind(rep(9, 102)) |\u0026gt; {\\(m) cbind(rep(9, 102), m)}() # Iterar por todos os pontos risk \u0026lt;- 0 for (i in 2:101) { for (j in 2:101) { # Verificar se é um ponto baixo e somar o risco ao total if ( height[i, j] \u0026lt; height[i - 1, j] \u0026amp;\u0026amp; height[i, j] \u0026lt; height[i + 1, j] \u0026amp;\u0026amp; height[i, j] \u0026lt; height[i, j - 1] \u0026amp;\u0026amp; height[i, j] \u0026lt; height[i, j + 1] ) { risk \u0026lt;- risk + height[i, j] + 1 } } } # Imprimir risk #\u0026gt; [1] 494 Bacia de Fumaça (B) O segundo item já era mais complicado. Considerando que os pontos com altura 9 não pertencem a nenhuma bacia, precisávamos encontrar as 3 maiores bacias no nosso mapa. Uma bacia é definida por toda uma região cercada por noves e seu tamanho é igual ao número de pontos contíguos contidos nessa área.\nO diagrama abaixo não estava no enunciado, mas ele me ajudou muito a entender o que era uma bacia. Para criá-lo, eu peguei um retângulo na ponta do meu mapa e substituí todos os números menores que 9 por um ., representando assim as bacias. Cada região cercada por noves é uma bacia diferente.\n# ....999.........9.9....99......9....9..........9 # ...9.9.9.......9...9..9.......9.9...99.99.9..... # ..9.....9.9.....9...99...........999.999.9.9.... # ..9......9.9...9....999.........9..9..9.....999. # 99..........999......9999......9..9..9.....9...9 # ...........9..9........99...9.9.......9......... # 9..............9...9..9..9.99.9......9.......... # ...........99.9...9.99....9..99.....9........... # ............99....9..9.......9.......9.......... # 9........99999...9....9.9....9.......999........ Minha solução começou igual à do item anterior, mas desta vez criei também uma tabela com todos os pontos do mapa. Meu objetivo era fazer uma busca em largura e remover desta tabela os pontos já explorados.\n# Criar uma tabela de pontos a explorar points \u0026lt;- purrr::cross2(2:101, 2:101) |\u0026gt; purrr::map(purrr::flatten_int) |\u0026gt; purrr::transpose() |\u0026gt; purrr::set_names(\u0026quot;i\u0026quot;, \u0026quot;j\u0026quot;) |\u0026gt; tibble::as_tibble() |\u0026gt; tidyr::unnest(c(i, j)) A seguir eu criei uma função que explorava uma bacia a partir de um ponto “semente”. O primeiro passo era verificar se o ponto já tinha sido explorado e retornar 0 se sim (indicando que aquele ponto não contribuiria mais para o tamanho da bacia). Se o ponto não tivesse sido explorado, então o código o removia da tabela de pontos e verificava se ele tinha altura 9, mais uma vez retornando 0 se sim. O final da função aplicava uma recursão nos 4 vizinhos do ponto, somando os tamanhos das 4 sub-bacias encontradas mais 1 (indicando que o ponto “semente” contribuia em 1 para o tamanho total da bacia).\n# Explorar uma bacia explore \u0026lt;- function(a, b) { # Pular se o ponto já tiver sido explorado if (nrow(dplyr::filter(points, i == a, j == b)) == 0) return(0) # Marcar o ponto como explorado points \u0026lt;\u0026lt;- dplyr::filter(points, i != a | j != b) # Se a altura for 9, então ele não faz parte da bacia if (height[a, b] == 9) return(0) # Adicionar os pontos vizinhos à bacia return( explore(a - 1, b) + explore(a + 1, b) + explore(a, b - 1) + explore(a, b + 1) + 1 ) } A resposta para o item era o produto dos tamanhos das 3 maiores bacias do mapa, então o programa terminava iterando na matriz, calculando o tamanho de todas as bacias e seguindo para obter o resultado final.\n# Iterar por todos os pontos basins \u0026lt;- matrix(rep(0, 10404), 102, 102) for (i in 2:101) { for (j in 2:101) { basins[i, j] \u0026lt;- explore(i, j) } } # Multiplicar as 3 maiores bacias basins |\u0026gt; sort(decreasing = TRUE) |\u0026gt; magrittr::extract(1:3) |\u0026gt; prod() #\u0026gt; [1] 1048128 ","permalink":"https://blog.curso-r.com/posts/2021-12-09-advent-of-r-09/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 09"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nBusca em Sete Segmentos (A) O oitavo dia do AoC foi bastante difícil para mim. O problema começou pelo enunciado, que é longo e complexo, então realmente recomendo ler a versão original além do resumo que trago abaixo.\nDito isso, vamos lá. O problema dizia respeito a displays de sete segmentos, onde cada número é representado por um conjunto de segmentos acessos; de acordo com o diagrama abaixo, vemos que 0 é representado por abcefg, 1 é cf e assim por diante.\n# 0: 1: 2: 3: 4: # aaaa .... aaaa aaaa .... # b c . c . c . c b c # b c . c . c . c b c # .... .... dddd dddd dddd # e f . f e . . f . f # e f . f e . . f . f # gggg .... gggg gggg .... # 5: 6: 7: 8: 9: # aaaa aaaa aaaa aaaa aaaa # b . b . . c b c b c # b . b . . c b c b c # dddd dddd .... dddd dddd # . f e f . f e f . f # . f e f . f e f . f # gggg gggg .... gggg gggg O desafio é que, no nosso submarino, todo os displays estão com os fios trocados e, para piorar, cada display tem um arranjo diferente. A entrada do problema é uma série de linhas como a abaixo: como os 10 dígitos são representados em um display específico (em qualquer ordem) e, depois da barra, 4 dígitos que precisamos decodificar.\n# acedgfb cdfbe gcdfa fbcad dab cefabd cdfgeb eafb cagedb ab | # cdfeb fcadb cdfeb cdbaf Alguns dígitos são fáceis de identificar. Os números 1, 4, 7 e 8 usam números únicos de segmentos, então é possível perceber que, quando ab acenderam, o display estava tentando mostrar um 1. Seguindo a mesma lógica, dab é 7, eafb é 4 e acedgfb é 8.\nO objetivo do primeiro item do dia 08 era contar quantas vezes os dígitos 1, 4, 7 e 8 aparecem nas saídas que devemos decodificar (lado direito da barra). A solução foi bem simples, pois bastou pivotar a tabela e filtrar as linhas que tinham stringr::str_length() igual a 2, 3, 4, ou 7.\n\u0026quot;data-raw/08a_seven_segment_search.txt\u0026quot; |\u0026gt; readr::read_delim(\u0026quot; \u0026quot;, col_names = NULL) |\u0026gt; purrr::set_names( paste0(\u0026quot;P\u0026quot;, stringr::str_pad(1:10, 2, \u0026quot;left\u0026quot;, \u0026quot;0\u0026quot;)), \u0026quot;remove\u0026quot;, paste0(\u0026quot;V\u0026quot;, stringr::str_pad(1:4, 2, \u0026quot;left\u0026quot;, \u0026quot;0\u0026quot;)) ) |\u0026gt; dplyr::select(-remove) |\u0026gt; dplyr::select(V01:V04) |\u0026gt; tidyr::pivot_longer(V01:V04, names_to = \u0026quot;col\u0026quot;, values_to = \u0026quot;value\u0026quot;) |\u0026gt; dplyr::filter(stringr::str_length(value) %in% c(2, 4, 3, 7)) |\u0026gt; nrow() #\u0026gt; [1] 365 Busca em Sete Segmentos (B) O verdadeiro problema veio no item 2. Aqui o exercício abandona qualquer pretexto de bondade e pede de uma vez para decodificarmos os dígitos depois da barra baseados nos 10 padrões antes da barra. A saída deveria ser a soma de todos os números de 4 dígitos decodificados.\nMinha primeira tentativa de resolver o problema testava cada segmento em cada posição (essencialmente verificando todos os possíveis jeitos de embaralhar os fios) para ver em qual das configurações os padrões faziam sentido; depois seria só bater os padrões com os 4 dígitos da direita para ver quem é quem. Não preciso nem dizer que isso seria demorado demais para funcionar.\nDepois de um tempo olhando para o arquivo de entrada, entretanto, me veio uma luz: talvez eu pudesse analisar a frequência com a qual cada segmento aparece nos padrões. Perceba, por exemplo, que no diagrama acima o segmento e está ligado em 4 dígitos (0, 2, 6 e 8). O fato importante é que ele é o único segmento com essa propriedade!\nPartindo deste princípio, criei as seguinte regras para o código:\nO único segmento que aparecer 4 vezes nos padrões corresponderá ao e;\nO único segmento que aparecer 6 vezes nos padrões corresponderá ao b;\nO único segmento que aparecer 9 vezes nos padrões corresponderá ao f;\nNo padrão com 2 segmentos acessos, aquele que não representar o e corresponderá ao c (número 1).\nNo padrão com 3 segmentos acessos, aquele que não representar c ou f corresponderá ao a (número 7).\nNo padrão com 4 segmentos acessos, aquele que não representar b, c ou f corresponderá ao d (número 4).\nO segmento que ainda não tiver correspondente corresponderá ao g.\nO resto do código cuidava de organizar as letras de cada dígito de modo que fosse fácil transpor as correspondências dos 10 padrões para os 4 valores das saídas.\n# Decodificar uma linha da entrada decode \u0026lt;- function(entry) { # Encontra e quebra o padrão que tenha certa str_length() find_by_len \u0026lt;- function(patterns, len) { patterns |\u0026gt; magrittr::extract(stringr::str_length(patterns) == len) |\u0026gt; stringr::str_split(\u0026quot;\u0026quot;) |\u0026gt; purrr::pluck(1) } # Frequências de referência ref_freq \u0026lt;- list( \u0026quot;a\u0026quot; = 8, \u0026quot;b\u0026quot; = 6, \u0026quot;c\u0026quot; = 8, \u0026quot;d\u0026quot; = 7, \u0026quot;e\u0026quot; = 4, \u0026quot;f\u0026quot; = 9, \u0026quot;g\u0026quot; = 7 ) # Valores de referência ref_val \u0026lt;- list( \u0026quot;abdefg\u0026quot; = 6, \u0026quot;abcefg\u0026quot; = 0, \u0026quot;cf\u0026quot; = 1, \u0026quot;acdfg\u0026quot; = 3, \u0026quot;abcdfg\u0026quot; = 9, \u0026quot;abcdefg\u0026quot; = 8, \u0026quot;bcdf\u0026quot; = 4, \u0026quot;acf\u0026quot; = 7, \u0026quot;abdfg\u0026quot; = 5, \u0026quot;acdeg\u0026quot; = 2 ) # Calcular frequências desta entrada cur_freq \u0026lt;- entry |\u0026gt; dplyr::select(P01:P10) |\u0026gt; purrr::flatten_chr() |\u0026gt; stringr::str_split(\u0026quot;\u0026quot;) |\u0026gt; purrr::flatten_chr() |\u0026gt; table() # Criar um dicionário para traduzir os segmentos dict \u0026lt;- list() # Traduzir segmentos com frequências únicas dict[[\u0026quot;e\u0026quot;]] \u0026lt;- names(cur_freq[cur_freq == 4]) dict[[\u0026quot;b\u0026quot;]] \u0026lt;- names(cur_freq[cur_freq == 6]) dict[[\u0026quot;f\u0026quot;]] \u0026lt;- names(cur_freq[cur_freq == 9]) # Extrair padrões da entrada patterns \u0026lt;- entry |\u0026gt; dplyr::select(P01:P10) |\u0026gt; purrr::flatten_chr() # Determinar segmento que falta do 1 one \u0026lt;- find_by_len(patterns, 2) dict[[\u0026quot;c\u0026quot;]] \u0026lt;- one[!one %in% purrr::flatten_chr(dict)] # Determinar segmento que falta do 7 seven \u0026lt;- find_by_len(patterns, 3) dict[[\u0026quot;a\u0026quot;]] \u0026lt;- seven[!seven %in% purrr::flatten_chr(dict)] # Determinar segmento que falta do 4 four \u0026lt;- find_by_len(patterns, 4) dict[[\u0026quot;d\u0026quot;]] \u0026lt;- four[!four %in% purrr::flatten_chr(dict)] # Determinar último segmento que falta dict[[\u0026quot;g\u0026quot;]] \u0026lt;- names(cur_freq)[!names(cur_freq) %in% purrr::flatten_chr(dict)] # Traduzir segmentos dos valores de saída entry |\u0026gt; dplyr::select(V01:V04) |\u0026gt; purrr::flatten_chr() |\u0026gt; stringr::str_split(\u0026quot;\u0026quot;) |\u0026gt; purrr::map(~names(dict)[match(.x, dict)]) |\u0026gt; purrr::map(sort) |\u0026gt; purrr::map(stringr::str_c, collapse = \u0026quot;\u0026quot;) |\u0026gt; purrr::map(~purrr::flatten_chr(ref_val)[match(.x, names(ref_val))]) |\u0026gt; purrr::flatten_chr() |\u0026gt; as.integer() |\u0026gt; stringr::str_c(collapse = \u0026quot;\u0026quot;) |\u0026gt; as.numeric() } # Ler entrada, mapear decode() e somar todas os valores de saída \u0026quot;data-raw/08b_seven_segment_search.txt\u0026quot; |\u0026gt; readr::read_delim(\u0026quot; \u0026quot;, col_names = NULL) |\u0026gt; purrr::set_names( paste0(\u0026quot;P\u0026quot;, stringr::str_pad(1:10, 2, \u0026quot;left\u0026quot;, \u0026quot;0\u0026quot;)), \u0026quot;remove\u0026quot;, paste0(\u0026quot;V\u0026quot;, stringr::str_pad(1:4, 2, \u0026quot;left\u0026quot;, \u0026quot;0\u0026quot;)) ) |\u0026gt; dplyr::select(-remove) |\u0026gt; tibble::rowid_to_column(\u0026quot;id\u0026quot;) |\u0026gt; tidyr::nest(entry = c(P01:V04)) |\u0026gt; dplyr::mutate(output = purrr::map_dbl(entry, decode)) |\u0026gt; dplyr::summarise(output = sum(output)) |\u0026gt; dplyr::pull(output) #\u0026gt; [1] 975706 ","permalink":"https://blog.curso-r.com/posts/2021-12-08-advent-of-r-08/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 08"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nA Traição das Baleias (A) O dia 7 do AoC foi o mais rápido até agora. A nossa tarefa era determinar a posição horizontal na qual um exército de caranguejos deveria se alinhar, com a restrição de que deveríamos encontrar a posição que exigisse menos combustível.\nCada caranguejo estava equipado de um mini-submarino que gastava 1 unidade de combustível por unidade de deslocamento, logo o total de combustível gasto pela tropa para ir até a posição x seria simplesmente sum(abs(positions - x)). A saída era o combustível gasto para levar todos os caranguejos até a posição mais econômica.\n# Ler vetor de posições iniciais positions \u0026lt;- \u0026quot;data-raw/07a_the_treachery_of_whales.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; stringr::str_split(\u0026quot;,\u0026quot;) |\u0026gt; purrr::pluck(1) |\u0026gt; as.integer() # Iterar nas posições para encontrar a mais barata cheapest \u0026lt;- Inf for (pos in max(positions):min(positions)) { # Calcular o combustível gasto para a posição fuel \u0026lt;- sum(abs(positions - pos)) # Trocar a resposta se essa posição for mais econômica if (fuel \u0026lt; cheapest) cheapest \u0026lt;- fuel } # Imprimir cheapest #\u0026gt; [1] 328318 Note que não era necessário testar nenhuma posição fora do intervalo max(positions):min(positions)! Qualquer posição fora disso seria menos econômica do que a ponta mais próxima a ela dentro do intervalo.\nA Traição das Baleias (B) O segundo item mantinha o mesmo problema, mas mudava o cálculo do gasto de combustível dos mini-submarinos: o primeiro movimento consumiria 1 unidade de combustível, o segundo consumiria 2 unidades, o terceiro consumiria 3 e assim por diante.\nA única linha que muda dessa solução para a anterior é a que calcula o gasto de combustível para cada posição. Se um caranguejo estiver na posição a e quiser ir até a x, o seu consumo total será \\(\\sum_{k = 0}^{|a - x|} k\\). Abaixo a operação sum(purrr::map_int(positions, ~sum(0:abs(.x - pos)))) faz isso para todos os caranguejos.\n# Iterar nas posições para encontrar a mais barata cheapest \u0026lt;- Inf for (pos in max(positions):min(positions)) { # Calcular o combustível gasto para a posição fuel \u0026lt;- sum(purrr::map_int(positions, ~sum(0:abs(.x - pos)))) # Trocar a resposta se essa posição for mais econômica if (fuel \u0026lt; cheapest) cheapest \u0026lt;- fuel } # Imprimir cheapest #\u0026gt; [1] 328318 ","permalink":"https://blog.curso-r.com/posts/2021-12-07-advent-of-r-07/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 07"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nPeixes-lanterna (A) O dia 6 do AoC me pegou um pouco de surpresa. O primeiro item foi tranquilo de fazer: a entrada era uma lista de números que representavam os “contadores biológicos” de um cardume de peixes-lanterna e precisávamos retornar o número de peixes depois de 80 dias.\nOs peixes adultos demoram 7 dias (contador vai de 6 até 0) para gerar um novo peixe bebê e um peixe bebê demora 9 dias (contador vai de 8 até 0) para gerar seu primeiro filhote.\nEstado inicial : 3,4,3,1,2 Depois de 1 dia : 2,3,2,0,1 Depois de 2 dias: 1,2,1,6,0,8 Depois de 3 dias: 0,1,0,5,6,7,8 Depois de 4 dias: 6,0,6,4,5,6,7,8,8 Depois de 5 dias: 5,6,5,3,4,5,6,7,7,8 Depois de 6 dias: 4,5,4,2,3,4,5,6,6,7 Depois de 7 dias: 3,4,3,1,2,3,4,5,5,6 Depois de 8 dias: 2,3,2,0,1,2,3,4,4,5 Depois de 9 dias: 1,2,1,6,0,1,2,3,3,4,8 Depois de 10 dias: 0,1,0,5,6,0,1,2,2,3,7,8 Depois de 11 dias: 6,0,6,4,5,6,0,1,1,2,6,7,8,8,8 Depois de 12 dias: 5,6,5,3,4,5,6,0,0,1,5,6,7,7,7,8,8 Depois de 13 dias: 4,5,4,2,3,4,5,6,6,0,4,5,6,6,6,7,7,8,8 Depois de 14 dias: 3,4,3,1,2,3,4,5,5,6,3,4,5,5,5,6,6,7,7,8 Depois de 15 dias: 2,3,2,0,1,2,3,4,4,5,2,3,4,4,4,5,5,6,6,7 Depois de 16 dias: 1,2,1,6,0,1,2,3,3,4,1,2,3,3,3,4,4,5,5,6,8 Depois de 17 dias: 0,1,0,5,6,0,1,2,2,3,0,1,2,2,2,3,3,4,4,5,7,8 Depois de 18 dias: 6,0,6,4,5,6,0,1,1,2,6,0,1,1,1,2,2,3,3,4,6,7,8,8,8,8 O meu código até que ficou bem simples. Precisei apenas de uma função que, todo dia, subtraia 1 de todos os contadores, criava 1 peixe com contador 8 para cada peixe com contador -1 e, por fim, subia todos os peixes com contador -1 para 6.\n# Rodar n cíclos de reprodução do peixe-lanterna reproduce \u0026lt;- function(fish, n = 80) { # Condição de parada if (n == 0) return(length(fish)) # Reduzir contadores biológicos fish \u0026lt;- fish - 1L # Criar novos peixes e reiniciar contadores fish \u0026lt;- append(fish, rep_len(8L, length(fish[fish == -1L]))) fish[fish == -1L] \u0026lt;- 6L # Recursão reproduce(fish, n = n - 1) } # Ler uma lista de peixes e reproduzir por 80 dias \u0026quot;data-raw/06a_lanternfish.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; stringr::str_split(\u0026quot;,\u0026quot;) |\u0026gt; purrr::pluck(1) |\u0026gt; as.integer() |\u0026gt; reproduce() #\u0026gt; [1] 362666 Peixes-lanterna (B) O segundo item do exercício não mudava essencialmente nada em relação ao primeiro. Assumindo espaço e recursos infinitos, quantos peixes teríamos depois de 256 dias?\nPara resolver esse item, em teoria, seria necessário trocar apenas o valor do n por 256. Mas não foi o que aconteceu… Por causa da ineficiência do algoritmo, obter uma resposta demoraria horas e acabaria com a memória do meu computador. Foi necessário pensar em um novo método de resolver o problema.\nA solução abaixo foi inspirada pela função table(). Para reduzir a exigência de espaço e não precisar iterar ao longo de um vetor com todos os peixes, eu agrupei os peixes com o mesmo contador biológico em apenas uma linha de uma tabela! Assim o programa nunca precisava lidar com mais de 9 linhas por dia, resolvendo as complicações com espaço e tempo.\n# Rodar n cíclos de reprodução do peixe-lanterna reproduce \u0026lt;- function(fish, n = 80) { # Condição de parada if (n == 0) return(sum(fish$n)) # Reduzir contadores biológicos fish \u0026lt;- dplyr::mutate(fish, timer = timer - 1L) # Criar novos peixes babies \u0026lt;- fish |\u0026gt; dplyr::filter(timer == -1L) |\u0026gt; dplyr::mutate(timer = 8L) # Reiniciar contadores e recursão fish |\u0026gt; dplyr::bind_rows(babies) |\u0026gt; dplyr::mutate(timer = ifelse(timer == -1L, 6L, timer)) |\u0026gt; dplyr::group_by(timer) |\u0026gt; dplyr::summarise(n = sum(n)) |\u0026gt; reproduce(n = n - 1) } # Ler uma lista de peixes e reproduzir por 256 dias \u0026quot;data-raw/06b_lanternfish.txt\u0026quot; |\u0026gt; readr::read_lines() |\u0026gt; stringr::str_split(\u0026quot;,\u0026quot;) |\u0026gt; purrr::pluck(1) |\u0026gt; as.integer() |\u0026gt; tibble::as_tibble() |\u0026gt; purrr::set_names(\u0026quot;timer\u0026quot;) |\u0026gt; dplyr::count(timer) |\u0026gt; reproduce(n = 256) |\u0026gt; format(scientific = FALSE) #\u0026gt; [1] 1640526601595 ","permalink":"https://blog.curso-r.com/posts/2021-12-06-advent-of-r-06/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 06"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nAventura Hidrotermal (A) O quinto dia do AoC foi um pouco mais tranquilo do que o anterior porque eu tive ajuda da incrível Renata Hirota. Hoje tínhamos as coordenadas cartesianas do início e do fim de tubulações submarinas e o objetivo era descobrir quantos pontos do plano tinham mais de uma tubulação passando por eles. No primeiro item deveríamos considerar apenas as tubulações verticais e horizontais.\nUma entrada do tipo 1,1 -\u0026gt; 1,3 cobria os pontos 1,1, 1,2 e 1,3.\nUma entrada do tipo 9,7 -\u0026gt; 7,7 cobria os pontos 9,7, 8,7 e 7,7.\nA minha ideia começava filtrando os pontos em que x1 == x2 ou y1 == y2 e expandindo as coordenadas para criar uma lista que contivesse todos os pontos pelos quais as tubulações passavam. Eu resolvi isso com o paste(x1:x2, y1:y2), pois a paste() repetiria a coordenada que não muda ao longo da coordenada que muda: paste(9:7, 7:7) := \"9 7\" \"8 7\" \"7 7\".\nDepois disso bastava contar o números de ocorrências de cada ponto do plano, filtrar aqueles que ocorriam mais de 1 vez e contar quantos pontos restavam. Esta era a saída do problema.\n\u0026quot;data-raw/05a_hydrothermal_venture.txt\u0026quot; |\u0026gt; readr::read_csv(col_names = c(\u0026quot;x1\u0026quot;, \u0026quot;y1x2\u0026quot;, \u0026quot;y2\u0026quot;)) |\u0026gt; tidyr::separate(sep = \u0026quot; -\u0026gt; \u0026quot;, col = \u0026quot;y1x2\u0026quot;, into = c(\u0026quot;y1\u0026quot;, \u0026quot;x2\u0026quot;)) |\u0026gt; dplyr::filter(x1 == x2 | y1 == y2) |\u0026gt; dplyr::mutate( dif_x = purrr::map2(x1, x2, seq), dif_y = purrr::map2(y1, y2, seq), coord = purrr::map2(dif_x, dif_y, paste) ) |\u0026gt; tidyr::unnest(coord) |\u0026gt; dplyr::count(coord) |\u0026gt; dplyr::filter(n \u0026gt; 1) |\u0026gt; nrow() #\u0026gt; [1] 7142 Aventura Hidrotermal (B) O segundo item parecia bastante mais complexo, pois agora deveríamos considerar todas as tubulações da entrada, removendo o dplyr::filter() do item anterior. Mas uma especificação do enunciado facilitou tudo: todas as linhas diagonais tinham inclinação de 45 graus.\nUma entrada do tipo 1,1 -\u0026gt; 3,3 cobria os pontos 1,1, 2,2 e 3,3.\nUma entrada do tipo 9,7 -\u0026gt; 7,9 cobria os pontos 9,7, 8,8 e 7,9.\nIsso significa que a estratégia do paste() continuava funcionando! Note que paste(1:3, 1:3) := \"1 1\" \"2 2\" \"3 3\", então bastou tirar o dplyr::filter() que a solução estava pronta.\n\u0026quot;data-raw/05b_hydrothermal_venture.txt\u0026quot; |\u0026gt; readr::read_csv(col_names = c(\u0026quot;x1\u0026quot;, \u0026quot;y1x2\u0026quot;, \u0026quot;y2\u0026quot;)) |\u0026gt; tidyr::separate(sep = \u0026quot; -\u0026gt; \u0026quot;, col = \u0026quot;y1x2\u0026quot;, into = c(\u0026quot;y1\u0026quot;, \u0026quot;x2\u0026quot;)) |\u0026gt; dplyr::mutate( dif_x = purrr::map2(x1, x2, seq), dif_y = purrr::map2(y1, y2, seq), coord = purrr::map2(dif_x, dif_y, paste) ) |\u0026gt; tidyr::unnest(coord) |\u0026gt; dplyr::count(coord) |\u0026gt; dplyr::filter(n \u0026gt; 1) |\u0026gt; nrow() #\u0026gt; [1] 20012 ","permalink":"https://blog.curso-r.com/posts/2021-12-05-advent-of-r-05/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 05"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nLula Gigante (A) O quarto dia do AoC foi talvez o mais interessante até agora. Na primeira parte, precisávamos calcular a pontuação da cartela vencedora de um bingo americano: cada cartela é composta por 5 linhas e 5 colunas de números que devem ser riscados conforme eles são anunciados pelo sistema do submarino. A primeira cartela a riscar todos os números de uma linha ou coluna é a vencedora e sua pontuação é a soma de todos os números não riscados multiplicada pelo último número anunciado.\nA entrada era composta por uma linha com os números anunciados em sequência e, posteriormente, todas as cartelas da platéia:\n# 7,4,9,5,11,17,23,2,0,14,21,24,10,16,13,6,15,25,12,22,18,20,8,19,3,26,1 # # 22 13 17 11 0 # 8 2 23 4 24 # 21 9 14 16 7 # 6 10 3 18 5 # 1 12 20 15 19 # # 3 15 0 2 22 # 9 18 13 17 5 # 19 8 7 25 23 # 20 11 10 24 4 # 14 21 16 12 6 # # 14 21 17 24 4 # 10 16 15 9 19 # 18 8 23 26 20 # 22 11 13 6 5 # 2 0 12 3 7 Eu escolhi um caminho simples para resolver o problema, apesar de o código não ter ficado tão bom assim. Primeiro eu li a sequência de números e criei uma função que transpunha uma matrix numérica e a empilhava com a original.\n# Processar os números sorteados draws \u0026lt;- \u0026quot;data-raw/04a_giant_squid.txt\u0026quot; |\u0026gt; readr::read_lines(n_max = 1) |\u0026gt; stringr::str_split(\u0026quot;,\u0026quot;) |\u0026gt; purrr::pluck(1) |\u0026gt; as.numeric() # Converter as colunas de uma matrix para linhas e empilhar cols_to_rows \u0026lt;- function(df) { df |\u0026gt; dplyr::select(-board, -id) |\u0026gt; as.matrix() |\u0026gt; t() |\u0026gt; tibble::as_tibble(rownames = \u0026quot;id\u0026quot;) |\u0026gt; purrr::set_names(\u0026quot;id\u0026quot;, paste0(\u0026quot;C\u0026quot;, 1:5)) |\u0026gt; dplyr::mutate(board = df$board) |\u0026gt; dplyr::bind_rows(df) |\u0026gt; dplyr::relocate(board, id) |\u0026gt; purrr::set_names(\u0026quot;id\u0026quot;, \u0026quot;board\u0026quot;, paste0(\u0026quot;N\u0026quot;, 1:5)) } O objetivo de cols_to_rows() era criar uma tabela final com todas as linhas das cartelas e também todas as suas colunas; isso permitiu que eu riscasse os números sorteados aplicando dplyr::na_if() indiscriminadamente. Quando alguma linha da tabela fosse formada somente por NAs (indicando que uma linha ou coluna de alguma cartela estava completa), bastava extrair a cartela original, somar os seus valores não-NA e multiplicar o resultado pelo número sorteado mais recente. A função utilizada para isso se chamava winning_score() e operava recursivamente para poupar tempo.\n# Calcular a pontuação da cartela vencedora winning_score \u0026lt;- function(df, draws) { # Marcar o número sorteado com NA (nas linhas e colunas) df \u0026lt;- dplyr::mutate(df, dplyr::across(c(N1:N5), dplyr::na_if, draws[1])) # Filtrar possíveis linhas/colunas completas win \u0026lt;- dplyr::filter(df, dplyr::if_all(c(N1:N5), is.na)) # Se houver pelo menos uma linha/coluna completa... if (nrow(win) \u0026gt; 0) { # Extrair a cartela vencedora, somar os não-NA e multiplicar por draws[1] output \u0026lt;- df |\u0026gt; dplyr::filter(id == win$id, stringr::str_starts(board, \u0026quot;R\u0026quot;)) |\u0026gt; dplyr::select(-id, -board) |\u0026gt; purrr::flatten_dbl() |\u0026gt; sum(na.rm = TRUE) |\u0026gt; magrittr::multiply_by(draws[1]) # Retornar a pontuação return(output) } # Recursão para o próximo sorteio winning_score(df, draws[-1]) } # Ler cartelas, empilhas linhas com colunas e riscar usando NAs \u0026quot;data-raw/04a_giant_squid.txt\u0026quot; |\u0026gt; readr::read_table(skip = 1, col_names = paste0(\u0026quot;C\u0026quot;, 1:5)) |\u0026gt; dplyr::mutate(board = (dplyr::row_number() - 1) %/% 5) |\u0026gt; dplyr::group_by(board) |\u0026gt; dplyr::mutate(id = paste0(\u0026quot;R\u0026quot;, 1:5)) |\u0026gt; dplyr::group_split() |\u0026gt; purrr::map_dfr(cols_to_rows) |\u0026gt; winning_score(draws) #\u0026gt; [1] 33348 Lula Gigante (B) O segundo item do problema pedia o contrário: calcular a pontuação da última cartela a ter uma linha ou coluna completa, ou seja, da cartela perdedora. Na minha solução todo o código permaneceu igual, salvo pela função winning_score(), que virou loosing_score(). A grande novidade é que, quando o programa encontrava uma cartela vencedora, ele verificava se aquela era a última. Se não fosse, ele removia aquela cartela da tabela e, se fosse, ele retornava a pontuação.\n# Calcular a pontuação da cartela perdedora loosing_score \u0026lt;- function(df, draws) { # Marcar o número sorteado com NA (nas linhas e colunas) df \u0026lt;- dplyr::mutate(df, dplyr::across(c(N1:N5), dplyr::na_if, draws[1])) # Filter possible complete rows or cols win \u0026lt;- dplyr::filter(df, dplyr::if_all(c(N1:N5), is.na)) # Se houver pelo menos uma linha/coluna completa... if (nrow(win) \u0026gt; 0) { # Se restasse apenas uma cartela, calcular a sua pontuação if (length(unique(df$id)) == 1) { # Extrair a cartela perdedora, somar os não-NA e multiplicar por draws[1] output \u0026lt;- df |\u0026gt; dplyr::filter(stringr::str_starts(board, \u0026quot;R\u0026quot;)) |\u0026gt; dplyr::select(-id, -board) |\u0026gt; purrr::flatten_dbl() |\u0026gt; sum(na.rm = TRUE) |\u0026gt; magrittr::multiply_by(draws[1]) # Retornar a pontuação return(output) } # Jogar fora cartelas que já venceram df \u0026lt;- dplyr::filter(df, !id %in% win$id) } # Recursão para o próximo sorteio loosing_score(df, draws[-1]) } ","permalink":"https://blog.curso-r.com/posts/2021-12-04-advent-of-r-04/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 04"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nDiagnóstico Binário (A) Na primeira parte do terceiro dia do AoC somos apresentados aos diagnósticos do submarino. Cada linha é composta por um número binário e precisamos carclular, a partir deles, os índices gama e épsilon.\n# 00100 # 11110 # 10110 # 10111 # 10101 # 01111 # 00111 # 11100 # 10000 # 11001 # 00010 # 01010 Cada bit do fator gama é igual ao valor mais comum do bit correspondente na entrada, enquanto o épsilon funciona ao contrário. No exemplo acima, o primeiro bit mais comum é 1 e o segundo é 0, então o índice gama começará com 10… e o índice épsilon começará com 01…\nO meu código quebra os bits da entrada com tidyr::separate() e calcula o valor mais frequente com names(sort(-table(.x)))[1] (a moda estatística). É importante lembrar que épsilon é o oposto, então eu troquei todos os bits de gama com stringr::str_replace_all(). A resposta final é a multiplicação de gama por épsilon na base decimal.\n\u0026quot;data-raw/03a_binary_diagnostic.txt\u0026quot; |\u0026gt; readr::read_table(col_names = \u0026quot;reading\u0026quot;) |\u0026gt; tidyr::separate(reading, paste0(\u0026quot;B\u0026quot;, 0:12), \u0026quot;\u0026quot;) |\u0026gt; dplyr::select(-B0) |\u0026gt; dplyr::summarise_all(~names(sort(-table(.x)))[1]) |\u0026gt; tidyr::unite(\u0026quot;gamma\u0026quot;, dplyr::everything(), sep = \u0026quot;\u0026quot;) |\u0026gt; dplyr::mutate( epsilon = gamma |\u0026gt; stringr::str_replace_all(\u0026quot;0\u0026quot;, \u0026quot;!\u0026quot;) |\u0026gt; stringr::str_replace_all(\u0026quot;1\u0026quot;, \u0026quot;0\u0026quot;) |\u0026gt; stringr::str_replace_all(\u0026quot;!\u0026quot;, \u0026quot;1\u0026quot;) |\u0026gt; strtoi(base = 2), gamma = strtoi(gamma, base = 2), output = gamma * epsilon ) |\u0026gt; dplyr::pull(output) Diagnóstico Binário (B) O segundo item desse dia foi o mais difícil de todos, ainda mais considerando que eu tento resolver tudo em apenas uma pipeline. Usando os mesmos dados, precisamos obter a taxa de O\\(_2\\) e de CO\\(_2\\) do submarino, sendo que as regras são as seguintes:\nJogue fora os número que não atendem ao critério daquele gás.\nSe restar apenas 1 número, essa é a taxa daquele gás.\nCaso contrário, repita o processo com o próximo bit.\nE quais são os critérios?\nPara o oxigênio, determinamos o valor mais comum para o bit atual e jogamos fora todos os números que diferem, nessa posição, desse valor. Se 0 e 1 forem igualmente comuns, manter apenas os números com 1 no bit considerado.\nPara gás carbônico, determinamos o valor menos comum para o bit atual e jogamos fora todos os números que diferem, nessa posição, desse valor. Se 0 e 1 forem igualmente comuns, manter apenas os números com 0 no bit considerado.\nO primeiro passo da minha solução foi criar uma função que calcula a anti-moda de um vetor. Ela difere da função usada no item anterior somente pelo sinal de subtração, mas isso garante a ela uma propriedade importante: se 0 e 1 empatarem na contagem, ela retorna o valor que vem antes na ordem alfabética, ou seja, 0. Dessa forma a função antimode() realiza exatamente a operação que precisamos para determinar a taxa de gás carbônico.\nantimode \u0026lt;- function(x) names(sort(table(x)))[1] A função abaixo é uma versão recursiva do cálculo das taxas dos gases. A coluna current é só um atalho para deixar o filtro mais enxuto, pois ela não passa da do bit atual. O op(), porém, é a chave que nos permite usar a mesma função para calcular O\\(_2\\) e CO\\(_2\\); por padrão a função filtra os valores iguais à anti-moda, mas, com co2 = FALSE, ela filtra os valores diferentes da anti-moda, atendendo ao critério do oxigênio (incluindo o desempate)!\nA última linha chama a função de novo para o próximo bit, resolvendo o cálculo.\ngas \u0026lt;- function(df, co2 = TRUE, bit = 1) { # Condição de parada if (bit \u0026gt; 12 || nrow(df) == 1) return(df) # Escolher o operador apropriado if (co2) op \u0026lt;- `==` else op \u0026lt;- `!=` # Filtrar usando antimode() e fazer a recursão df |\u0026gt; dplyr::mutate(current = .data[[names(df)[bit]]]) |\u0026gt; dplyr::filter(op(current, antimode(current))) |\u0026gt; dplyr::select(-current) |\u0026gt; find_rating(co2 = co2, bit = bit + 1) } Só nos resta aplicar essa função na lista de números. Para tentar manter o fim do código em uma pipeline só (já que não foi possível com o resto), eu usei rep_len(list(df), 2) para duplicar a base e poder aplicar gas() e gas(co2 = FALSE) em uma linha só com purrr::map2_dfr(). O final do código deixa cada taxa em uma linha, junta os seu bits, as converte para decimal e multiplica seus valores. Essa é a saída.\n\u0026quot;data-raw/03b_binary_diagnostic.txt\u0026quot; |\u0026gt; readr::read_table(col_names = \u0026quot;reading\u0026quot;) |\u0026gt; tidyr::separate(reading, paste0(\u0026quot;B\u0026quot;, 0:12), \u0026quot;\u0026quot;) |\u0026gt; dplyr::select(-B0) |\u0026gt; list() |\u0026gt; rep_len(2) |\u0026gt; purrr::map2_dfr(list(gas, \\(df) gas(df, FALSE)), ~.y(.x)) |\u0026gt; tidyr::unite(\u0026quot;reading\u0026quot;, dplyr::everything(), sep = \u0026quot;\u0026quot;) |\u0026gt; dplyr::mutate(reading = strtoi(reading, base = 2)) |\u0026gt; dplyr::summarise(output = prod(reading)) |\u0026gt; dplyr::pull(output) ","permalink":"https://blog.curso-r.com/posts/2021-12-03-advent-of-r-03/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 03"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" Chamdos de “três pontinhos”, “reticências”, “dots” ou “ellipsis”, os ... são uma das funcionalidades mais comuns do R, mas ao mesmo tempo uma das menos conhecidas. Explicá-los em linguagem técnica é muito simples: eles são os argumentos variádicos do R! O difícil é entender de verdade o que eles são e como usá-los. Vamos abandonar o jargão e sigamos em frente, agora em bom português…\nObs.: O nome correto no R para os ... é dots, então vou usar esse termo a partir de agora. A prova disso é que, para consultar a sua documentação, executamos ?dots.\nOnde estão Como eu disse anteriormente, eles são bastante comuns, mas quão comuns exatamente? Talvez mais do que você imagine. Veja abaixo os protótipos de algumas poucas funções que talvez você conheça (ignore o NULL, ele é parte da saída da função args()):\nargs(sum) ## function (..., na.rm = FALSE) ## NULL args(c) ## function (...) ## NULL args(dplyr::mutate) ## function (.data, ...) ## NULL Te convenci? Entender os dots é, portanto, uma excelente arma no arsenal do programador de R, tanto que eles são usados pelas funções mais importantes da linguagem toda.\nO que são De forma bem geral, os dots são um argumento que, quando colocado na sua função, pode ser substituído por qualquer coisa pelo usuário. Na função sum(), por exemplo, os dots podem virar uma série de números (quantos o usuário quiser).\nsum(1, 2, 3, 4, 5) ## [1] 15 Quando falamos de argumentos normais, não precisamos declarar seus argumentos caso estejamos utilizando-os na ordem correta. Os dots, entretanto, podem ser substituídos por qualquer número de objetos, então eles quebram essa regra; qualquer argumento que vier depois dos dots precisa ser nomeado.\n# Não funciona do jeito esperado (TRUE mais um elemento dos dots) sum(1, 2, NA, 4, 5, TRUE) ## [1] NA # Agora sim sum(1, 2, NA, 4, 5, na.rm = TRUE) ## [1] 12 Sem os dots a função sum() estaria limitada a receber um vetor de números, mas com essa ferramenta ela passa a poder receber números separados como se fossem cada um um argumento. A função c(), entretanto, não poderia ser implementada sem os dots.\nO seu poder completo, porém, fica mais claro na função dplyr::select(). Aqui vemos que podemos até dar nomes arbitrários para os “argumentos” de dentro dos dots e a função pode usá-los sem o menor problema:\nmtcars |\u0026gt; dplyr::select(mpg, cil = cyl, marcha = gear) |\u0026gt; head() ## mpg cil marcha ## Mazda RX4 21.0 6 4 ## Mazda RX4 Wag 21.0 6 4 ## Datsun 710 22.8 4 4 ## Hornet 4 Drive 21.4 6 3 ## Hornet Sportabout 18.7 8 3 ## Valiant 18.1 6 3 Perceba que a função dplyr::select() não tem como saber quantas colunas nós vamos selecionar e quais nomes eu vou dar para cada uma, tornando impossível o uso de argumentos convencionais. O uso do dots é inevitável nesses casos.\nComo usá-los Agora que já vimos universalidade e importância dos dots, chegou a hora de entender como eles funcionam e como usá-los. Vamos começar com um exemplo simples: criar uma função que captura quaisquer argumentos que o usuário resolver passar e imprime seus valores.\n# Captura dots e os imprime como uma lista captura \u0026lt;- function(...) { list(...) } captura(arg1 = 1, arg2 = \u0026quot;b\u0026quot;, arg3 = FALSE) ## $arg1 ## [1] 1 ## ## $arg2 ## [1] \u0026quot;b\u0026quot; ## ## $arg3 ## [1] FALSE Simples, né? 90% das vezes podemos simplesmente transformar os dots em uma lista comum com list(...) e utilizá-la normalmente. Em breve ficará mais claro por que isso funciona.\nSe quisermos capturar argumentos específicos dentro dos dots, aí podemos usar uma função especial chamada ...elt() (sim, as reticências fazem parte de seu nome):\n# Captura dots e os imprime como uma lista captura_segundo \u0026lt;- function(...) { ...elt(2) } captura_segundo(arg1 = 1, arg2 = \u0026quot;b\u0026quot;, arg3 = FALSE) ## [1] \u0026quot;b\u0026quot; A terceira forma de usar os dots é os transportando para uma função que recebe dots. Como já deve ter ficado evidente, os dots podem ser substituídos por qualquer número de argumentos por parte do usuário, mas eles também podem ser passados como argumento no lugar dos dots de outra função!\nfiltra_seleciona \u0026lt;- function(marchas, ...) { mtcars |\u0026gt; dplyr::filter(gear == marchas) |\u0026gt; dplyr::select(...) |\u0026gt; head() } filtra_seleciona(4, mpg, cil = cyl) ## mpg cil ## Mazda RX4 21.0 6 ## Mazda RX4 Wag 21.0 6 ## Datsun 710 22.8 4 ## Merc 240D 24.4 4 ## Merc 230 22.8 4 ## Merc 280 19.2 6 No caso acima, os dots eram mpg, cil = cyl e isso foi transportado perfeitamente para dentro de dplyr::select().\nPara fechar este tutorial com chave de ouro, vamos criar uma função arbitrária que precisa de um argumento depois dos dots: nossa função deve receber qualquer quantidade de valores numéricos, ignorar o primeiro e somar o resto com n.\nignora_um_soma_n \u0026lt;- function(..., n = 0) { valores \u0026lt;- list(...) valores \u0026lt;- valores[-1] unlist(valores) + n } ignora_um_soma_n(1, 2, 3, 4, 5, n = 10) ## [1] 12 13 14 15 Espero que agora esteja pelo menos um pouco mais claro o funcionamento dos dots! Se não, pode entrar em contato comigo via Twitter ou postar uma dúvida no nosso fórum.\n","permalink":"https://blog.curso-r.com/posts/2021-12-03-tutorial-dots/","tags":["conceitos","dots"],"title":"Três Pontinhos: Como Funcionam os Dots"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nMergulhe (A) A parte 1 do segundo dia do AoC pede para lermos uma lista de comandos para um submarino e calcular a sua posição final. Os comandos possíveis são forward X (soma X à posição horizontal), up X (subtrai X da profundidade) e down X (soma X à profundidade), então precisamos fazer um dplyr::group_by(command == \"forward\") para que um grupo represente a posição horizontal e um represente a profundidade.\nPara concluir o código, como a resposta final é a posição horizontal multiplicada pela profundidade, temos que fazer um prod() ao final:\n\u0026quot;data-raw/02a_dive.txt\u0026quot; |\u0026gt; readr::read_delim(\u0026quot; \u0026quot;, col_names = c(\u0026quot;command\u0026quot;, \u0026quot;x\u0026quot;)) |\u0026gt; dplyr::mutate(x = ifelse(command == \u0026quot;up\u0026quot;, -x, x)) |\u0026gt; dplyr::group_by(command == \u0026quot;forward\u0026quot;) |\u0026gt; dplyr::summarise(x = sum(x)) |\u0026gt; dplyr::summarise(x = prod(x)) |\u0026gt; dplyr::pull(x) #\u0026gt; [1] 1727835 Mergulhe (B) A parte 2 complica um pouco a nossa vida. Os mesmos comandos agora possuem outro significado:\ndown X aumenta a mira em X unidades up X diminui a mira em X unidades. forward X faz duas coisas: Aumenta a posição horizontal em X unidades. Aumenta a profundidade em X vezes a mira atual. O meu código da primeira parte não permitiria resolver isso de forma eficiente. Minha solução foi fazer uma cumsum() da posição horizontal e uma da mira, que são as partes mais simples. Depois eu calculei a profundidade com cumsum(aim * x) (dado que a mira tinha sido calculada no passo anterior).\nA saída, mais uma vez é o produto entre a posição horizontal e a profundidade. Dessa vez a resposta vai estar na última linha da tabela, então o código precisa de um tail(1).\n\u0026quot;data-raw/02a_dive.txt\u0026quot; |\u0026gt; readr::read_delim(\u0026quot; \u0026quot;, col_names = c(\u0026quot;command\u0026quot;, \u0026quot;x\u0026quot;)) |\u0026gt; dplyr::mutate( horizontal = ifelse(command == \u0026quot;forward\u0026quot;, x, 0), horizontal = cumsum(horizontal), aim = ifelse(command == \u0026quot;down\u0026quot;, x, 0), aim = ifelse(command == \u0026quot;up\u0026quot;, -x, aim), aim = cumsum(aim), depth = ifelse(command == \u0026quot;forward\u0026quot;, aim * x, 0), depth = cumsum(depth), output = horizontal * depth ) |\u0026gt; utils::tail(1) |\u0026gt; dplyr::pull(output) #\u0026gt; [1] 1544000595 ","permalink":"https://blog.curso-r.com/posts/2021-12-02-advent-of-r-02/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 02"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O Advent of Code é um Calendário do Advento desenvolvido por Eric Wastl composto por 25 pequenos exercícios de programação que vão sendo disponibilizados, um a um, entre 1º de dezembro e o Natal de cada ano.\nMeu objetivo com o Advent of R é resolver todos os problemas do Advent of Code 2021 em R e documentar o processo através desta série de posts. Todo dia entre 01/12/2021 e 25/12/2021 eu vou tentar resolver o novo problema, documentar a minha solução aqui no blog e subir os meus scripts completos para um repositório público no GitHub.\nA minha esperança é que, com essa série, mais pessoas pratiquem seus conhecimentos de R resolvendo exercícios divertidos e desafiadores! Ao final da jornada vamos todos ter afiado nossas habilidades de R e, quem sabe, divulgado essa linguagem incrível para mais pessoas. Boas festas e bom código!\nVarredura de Sonar (A) A parte 1 do primeiro exercício do AoC envolve ler uma lista de números e ver quantas vezes os valores aumentam em relação ao anterior. Em linguagem matemática, precisamos avaliar quantas vezes \\(x_i \u0026gt; x_{i - 1}\\).\nPor exemplo, suponha a seguinte lista:\n# 199 # 200 # 208 # 210 # 200 # 207 # 240 # 269 # 260 # 263 Nesse caso, precisamos comparar cada número com o da linha anterior e verificar se ele representa que a série aumentou, diminuiu ou manteve-se constante.\n# 199 (NA) # 200 (aumentou) # 208 (aumentou) # 210 (aumentou) # 200 (diminuiu) # 207 (aumentou) # 240 (aumentou) # 269 (aumentou) # 260 (diminuiu) # 263 (aumentou) Tendo isso, podemos concluir que houveram 7 aumentos na série exemplo e essa seria a resposta do problema.\nO meu código para resolver o exercício ficou bem enxuto. Bastou ler a lista de número do arquivo disponibilizado como uma tabela e comparar seus valores com o seu dplyr::lag(); depois disso um dplyr::summarise() contou o número de TRUEs ignorando NAs.\n\u0026quot;data-raw/01a_sonar_sweep.txt\u0026quot; |\u0026gt; readr::read_table(col_names = \u0026quot;depth\u0026quot;) |\u0026gt; dplyr::mutate( prev_depth = dplyr::lag(depth), is_deeper = depth \u0026gt; prev_depth ) |\u0026gt; dplyr::summarise(n_deeper = sum(is_deeper, na.rm = TRUE)) |\u0026gt; dplyr::pull(n_deeper) #\u0026gt; [1] 1228 Varredura de Sonar (B) A segunda parte, entretanto, aumenta (com o perdão do trocadilho) a dificuldade. Dessa vez precisamos somar uma janela de 3 valores e comparar com a próxima janela, ou seja, verificar quantas vezes \\(\\sum_{k = i}^{i+2} x_k \u0026gt; \\sum_{k = i-1}^{i+1} x_k\\).\nObserve como as janelas funcionam:\n# 199 A # 200 A B # 208 A B C # 210 B C D # 200 E C D # 207 E F D # 240 E F G # 269 F G H # 260 G H # 263 H Nesse exemplo precisaríamos somar os números da janela A (199, 200, 208) e testar se isso é maior que a soma dos números da janela B (200, 208, 210). Então compararíamos B com C, C com D e assim por diante.\n# A: 607 (NA) # B: 618 (aumentou) # C: 618 (não mudou) # D: 617 (diminuiu) # E: 647 (aumentou) # F: 716 (aumentou) # G: 769 (aumentou) # H: 792 (aumentou) Alterando o código da primeira parte, eu criei as janelas usando dplyr::lead() e depois comparei as somas utilizando o mesmo dplyr::lag(). Mais uma vez o dplyr::summarise() contou o número de TRUEs ignorando NAs.\n\u0026quot;data-raw/01b_sonar_sweep.txt\u0026quot; |\u0026gt; readr::read_table(col_names = \u0026quot;depth\u0026quot;) |\u0026gt; dplyr::mutate( depth1 = dplyr::lead(depth, n = 1), depth2 = dplyr::lead(depth, n = 2), sum_depth = depth + depth1 + depth2, prev_sum_depth = dplyr::lag(sum_depth), is_deeper = sum_depth \u0026gt; prev_sum_depth ) |\u0026gt; dplyr::summarise(n_deeper = sum(is_deeper, na.rm = TRUE)) |\u0026gt; dplyr::pull(n_deeper) #\u0026gt; [1] 1257 ","permalink":"https://blog.curso-r.com/posts/2021-12-01-advent-of-r-01/","tags":["advent-of-r","tidyverse"],"title":"Advent of R: Dia 01"},{"author":["Beatriz","Julio"],"categories":["conceitos"],"contents":" Introdução Olá!\nRecentemente eu (Bea) andei explorando algumas APIs, com ajuda do Julio, que ficou respondendo as dúvidas! No processo aprendemos muitas coisas legais e resolvemos compartilhar com vocês através de alguns posts.\nO tema dá muito pano pra manga, pois usamos código em R para automatizar quase tudo! Nas últimas semanas, aprendemos a trabalhar com o GitHub (Ex: criar e configurar repositórios), Zoom (criar chamadas), Google Drive e Sheets (estruturar e editar arquivos), e Google Classroom (criar e configurar salas de aula).\nTudo isso usando muito tidyverse!\nMas calma!! Vamos começar do começo. Neste primeiro post, vamos falar sobre o que é uma API e mostrar um exemplo básico.\nO que é uma API? API é uma sigla para Application Programming Interface, ou seja, uma interface de computação. Sem usar palavras complicadas, uma explicação que está nos slides do curso de Deploy! é:\nUma API não deixa de ser um “link” que aceita parâmetros e retorna dados.\nAs APIs também são abordadas no curso de Web Scraping!\nPokéAPI Um exemplo de acesso à API é buscando na PokéAPI, uma API para buscar Pokémons! E com tantas possibilidades interessantes, porque buscar justamente Pokémons? Essa API tem fins educacionais, portanto é um ótimo lugar para começar a praticar o tema.\nO primeiro passo para acessar qualquer API é procurar uma documentação.\nNa documentação do PokéAPI, é mostrado que devemos usar o seguinte padrão:\nGET https://pokeapi.co/api/v2/{endpoint}/ Isso significa que devemos fazer uma requisição GET (apenas para consultar dados), começar com a url base (é a parte que não muda: https://pokeapi.co/api/v2/) e complementar com os parâmetros da busca (que chamamos de endpoint).\nO pacote httr é muito usado para acessar APIs através do R, e disponibiliza uma função chamada httr::GET() para fazer esse tipo de acesso!\nPor exemplo:\n# url_base - nunca muda na mesma API url_base \u0026lt;- \u0026quot;https://pokeapi.co/api/v2\u0026quot; # endpoint - é o que muda o resultado endpoint \u0026lt;- \u0026quot;/pokemon/ditto\u0026quot; # precisamos colar os textos para criar o link u_pokemon \u0026lt;- paste0(url_base, endpoint) # ver como o texto ficou colado # u_pokemon # \u0026gt; \u0026quot;https://pokeapi.co/api/v2/pokemon/ditto\u0026quot; # fazer a requisição do tipo GET r_pokemon \u0026lt;- httr::GET(u_pokemon) r_pokemon ## Response [https://pokeapi.co/api/v2/pokemon/ditto] ## Date: 2021-11-29 11:44 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 22.3 kB Esse resultado é um pouco diferente do que estamos acostumados! O que é mais importante de reparar é o status, pois indica se a requisição foi bem sucedida. Status: 200 significa que deu certo! :)\nAs APIs costumam retornar arquivos JSON, e para acessar o conteúdo que foi obtido podemos usar a função httr::content():\nc_pokemon \u0026lt;- httr::content(r_pokemon) Ao salvar o resultado da função em um objeto, podemos ver que agora a classe dele é uma lista:\nclass(c_pokemon) ## [1] \u0026quot;list\u0026quot; Podemos manipular essa lista com funções do tidyverse para obter uma tabela, assim fica mais fácil de trabalhar. Mas atenção, da seguinte forma ela ainda não está no formato tidy! Saiba mais o que são dados tidy no material do curso de Faxina de Dados.\nlibrary(magrittr) dados_ditto \u0026lt;- c_pokemon %\u0026gt;% purrr::map(unlist, recursive = TRUE) %\u0026gt;% purrr::map(tibble::enframe) %\u0026gt;% purrr::map_dfr( ~dplyr::mutate(.x, dplyr::across(.fns = as.character)), .id = \u0026quot;id\u0026quot; ) dplyr::glimpse(dados_ditto) ## Rows: 396 ## Columns: 3 ## $ id \u0026lt;chr\u0026gt; \u0026quot;abilities\u0026quot;, \u0026quot;abilities\u0026quot;, \u0026quot;abilities\u0026quot;, \u0026quot;abilities\u0026quot;, \u0026quot;abilities\u0026quot;,… ## $ name \u0026lt;chr\u0026gt; \u0026quot;ability.name\u0026quot;, \u0026quot;ability.url\u0026quot;, \u0026quot;is_hidden\u0026quot;, \u0026quot;slot\u0026quot;, \u0026quot;ability.nam… ## $ value \u0026lt;chr\u0026gt; \u0026quot;limber\u0026quot;, \u0026quot;https://pokeapi.co/api/v2/ability/7/\u0026quot;, \u0026quot;FALSE\u0026quot;, \u0026quot;1\u0026quot;, … Agora você pode usar o que aprendemos para pesquisar outros Pokémons! Quem sabe explorar a teoria de que o Ditto é um clone falho do Mew?\ndados_mew \u0026lt;- \u0026quot;https://pokeapi.co/api/v2/pokemon/mew\u0026quot; %\u0026gt;% httr::GET() %\u0026gt;% httr::content() %\u0026gt;% purrr::map(unlist, recursive = TRUE) %\u0026gt;% purrr::map(tibble::enframe) %\u0026gt;% purrr::map_dfr( ~dplyr::mutate(.x, dplyr::across(.fns = as.character)), .id = \u0026quot;id\u0026quot; ) dplyr::glimpse(dados_mew) ## Rows: 10,083 ## Columns: 3 ## $ id \u0026lt;chr\u0026gt; \u0026quot;abilities\u0026quot;, \u0026quot;abilities\u0026quot;, \u0026quot;abilities\u0026quot;, \u0026quot;abilities\u0026quot;, \u0026quot;base_experi… ## $ name \u0026lt;chr\u0026gt; \u0026quot;ability.name\u0026quot;, \u0026quot;ability.url\u0026quot;, \u0026quot;is_hidden\u0026quot;, \u0026quot;slot\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;name\u0026quot;,… ## $ value \u0026lt;chr\u0026gt; \u0026quot;synchronize\u0026quot;, \u0026quot;https://pokeapi.co/api/v2/ability/28/\u0026quot;, \u0026quot;FALSE\u0026quot;,… Saiba mais sobre a teoria aqui:\nÉ isso! Dúvidas, sugestões e críticas, mande aqui nos comentários. Comentem também quais exemplos, dentre os que foram listados, vocês gostariam de saber mais!!\nAté a próxima!\nVocê pode se interessar também por… Slides do curso de Deploy\nSlides do curso de Web Scraping sobre APIs\nSlides do curso de Faxina sobre tidy data\nPacote httr\n","permalink":"https://blog.curso-r.com/posts/2021-11-29-api-pokemon/","tags":["api","purrr","httr","web-scraping"],"title":"Acessando APIs com R: Exemplos com a PokéAPI!"},{"author":["William"],"categories":["pacotes"],"contents":" No último post, vimos como usar o pacote plotly para gerar gráficos dinâmicos do zero ou aproveitando um gráfico feito em ggplot. Neste post, falaremos de outra biblioteca gráfica em JavaScript que possui pacote em R: o echarts.\nO pacote em questão se chama echarts4r, portanto, se você ainda não o estiver instalado, rode o código abaixo:\ninstall.packages(\u0026quot;echarts4r\u0026quot;) Leia mais:\nGráficos no Shiny: ggplot2\nGráficos no Shiny: echarts\nO pacote echarts4r O pacote echarts4r não possui uma função ggecharts, equivalente à ggplotly do pacote plotly, que possibilitaria transformar gráficos feitos em ggplot em gráficos echarts. Assim, precisamos sempre construir nossos gráficos do zero, usando a sintaxe do echarts/echarts4r.\nO echarts4r possui semelhanças e diferenças com relação ao ggplot2. A semelhança mais importante é que construímos gráficos em camadas. A primeira diferença relevante é que essas camadas são unidas pelo %\u0026gt;%/|\u0026gt;, não pelo +. Outra diferença é que não temos uma função aes(), então o mapeamento das variáveis é feito diretamente nos argumentos das funções.\nVamos começar com um exemplo simples: um gráfico de dispersão.\nlibrary(echarts4r) mtcars |\u0026gt; e_charts(x = wt) |\u0026gt; e_scatter(serie = mpg) Veja que o gráfico não possui tooltip por padrão. Precisamos incluí-la na pipeline:\nmtcars |\u0026gt; e_charts(x = wt) |\u0026gt; e_scatter(serie = mpg) |\u0026gt; e_tooltip() Para fazermos um gráfico de linhas, usamos a função e_line(). Cada tipo de gráfico será produzido a partir de uma função do tipo e_*(), equivalente às funções geom_*() no ggplot2.\nggplot2::txhousing |\u0026gt; dplyr::mutate(year = as.character(year)) |\u0026gt; dplyr::group_by(year) |\u0026gt; dplyr::summarise(sales = mean(sales, na.rm = TRUE)) |\u0026gt; e_charts(x = year) |\u0026gt; e_line(serie = sales) |\u0026gt; e_tooltip() Ao contrário do ggplot2, dados agrupados com dplyr::group_by() influenciam a construção do gráfico. No código abaixo, a base sai do summarise agrupada por city, fazendo com que o echarts construa uma linha para cada cidade.\nggplot2::txhousing |\u0026gt; dplyr::filter(city %in% c(\u0026quot;Austin\u0026quot;, \u0026quot;Dallas\u0026quot;, \u0026quot;Houston\u0026quot;)) |\u0026gt; dplyr::mutate(year = as.character(year)) |\u0026gt; dplyr::group_by(city, year) |\u0026gt; dplyr::summarise(sales = mean(sales, na.rm = TRUE)) |\u0026gt; e_charts(x = year) |\u0026gt; e_line(serie = sales) |\u0026gt; e_tooltip() A biblioteca echarts possui uma extensa variedade de gráficos disponíveis. Você pode visitar a galeria de exemplos para ter uma boa ideia do que é possível fazer. Além disso, clicando nos exemplos, você tem acesso aos códigos JavaScript utilizados para construir os gráficos.\nCom as funções do pacote echarts4r, podemos fazer bastante do que a biblioteca echarts tem para oferecer. O que mostramos neste post foi tão pouco que nem poderíamos chamar de uma introdução. Para aprender mais sobre o echarts4r vale bastante a pena olhar os tutoriais na página do pacote.\nEm alguns casos, vamos encontrar gráficos ou elementos dentro de um gráfico que não podem ser construídos a partir dos parâmetros das funções do echarts4r. Nesses casos, vamos precisar nos socorrer da documentação do echarts e usar parâmetros que não estão definidos nas funções do echarts4r (o que é possível já que a maioria das funções possuem ...).\nA documentação do echarts pode assustar à primeira vista, mas segue um modelo padrão de documentação de bibliotecas JavaScript. Conforme vamos usando mais essas bibliotecas, seja para fazer gráficos, tabelas, mapas ou o que for, vamos nos acostumando a consumir essas documentações.\nNesse sentido, uma forma de seguir a maneira JavaScript de construir um echarts é usar a função e_list(). Com ela, definimos os parâmetros do gráfico a partir de listas e conseguimos reproduzir linha a linha um exemplo feito em JS. A seguir, reproduzimos exatamente este exemplo. Veja que a sintaxe dos dois códigos é muito parecida.\ne_chart() |\u0026gt; e_list(list( tooltip = list(trigger = \u0026quot;item\u0026quot;), legend = list(top = \u0026quot;5%\u0026quot;, left = \u0026quot;center\u0026quot;), series = list( list( name = \u0026quot;Access From\u0026quot;, type = \u0026quot;pie\u0026quot;, radius = c(\u0026quot;40%\u0026quot;, \u0026quot;70%\u0026quot;), avoidLabelOverlap = FALSE, itemStyle = list( borderRadius = 10, borderColor = \u0026quot;#fff\u0026quot;, borderWidth = 2 ), label = list(show = FALSE, position = \u0026quot;center\u0026quot;), emphasis = list( label = list( show = TRUE, fontSize = 40, fontWeight = \u0026quot;bold\u0026quot; ) ), labelLine = list(show = FALSE), data = list( list(value = 1048, name = \u0026quot;Search Engine\u0026quot;), list(value = 735, name = \u0026quot;Direct\u0026quot;), list(value = 580, name = \u0026quot;Email\u0026quot;), list(value = 484, name = \u0026quot;Union Ads\u0026quot;), list(value = 300, name = \u0026quot;Video Ads\u0026quot;) ) ) ) )) No Shiny Para adicionar um echarts no Shiny, utilizamos o par de funções echarts4r::echarts4rOutput() e echarts4r::renderEcharts4r(). Na função renderEcharts4r(), basta passarmos um código que retorne um gráfico echarts.\nRode o app abaixo para ver um exemplo.\nlibrary(shiny) library(echarts4r) vars \u0026lt;- ggplot2::txhousing |\u0026gt; dplyr::select(where(is.numeric), -year, -month, -date) |\u0026gt; names() cidades \u0026lt;- unique(ggplot2::txhousing$city) ui \u0026lt;- fluidPage( titlePanel(\u0026quot;echarts\u0026quot;), sidebarLayout( sidebarPanel( selectInput( \u0026quot;cidades\u0026quot;, \u0026quot;Selecione as cidades\u0026quot;, multiple = TRUE, choices = cidades, selected = cidades[1] ), selectInput( \u0026quot;serie\u0026quot;, \u0026quot;Selecione a série\u0026quot;, choices = vars, selected = vars[1] ) ), mainPanel( echarts4r::echarts4rOutput(\u0026quot;plot\u0026quot;) ) ) ) server \u0026lt;- function(input, output, session) { output$plot \u0026lt;- echarts4r::renderEcharts4r({ ggplot2::txhousing |\u0026gt; dplyr::filter(city %in% input$cidades) |\u0026gt; dplyr::mutate(year = as.character(year)) |\u0026gt; dplyr::group_by(city, year) |\u0026gt; dplyr::summarise(avg_serie = mean(.data[[input$serie]], na.rm = TRUE)) |\u0026gt; e_charts(x = year) |\u0026gt; e_line(serie = avg_serie) |\u0026gt; e_tooltip() }) } shinyApp(ui, server) É isso! Dúvidas, sugestões e críticas, mande aqui nos comentários.\nAté a próxima!\n","permalink":"https://blog.curso-r.com/posts/2021-11-11-shiny-echarts/","tags":["shiny","gráficos","echarts"],"title":"Gráficos no Shiny: echarts"},{"author":["Tereza"],"categories":["tutoriais"],"contents":" Datas no R são tratadas como um tipo especial de objeto, com classe Date.\nExistem algumas funções para converter diversos tipos de padrões de data para a classe Date, veja como funcionam algumas delas!\nLubridate Primeiramente, vamos carregar o pacote lubridate:\nlibrary(lubridate) Suponha que queremos criar um objeto da classe Date com a seguinte data: 20/11/2021\nA primeira coisa a ser feita, é identificar a ordem da entrada, nesse caso, é: dia, mês, ano (padrão brasileiro)\nEntão, basta usar a função dmy (day, month, year):\ndmy(\u0026quot;20/11/2021\u0026quot;) ## [1] \u0026quot;2021-11-20\u0026quot; Obs: note que o padrão do R para um objeto da classe Date é y/m/d (year, month, day)\nTrocando dia e mês de lugar na data anterior, obtemos: 11/20/2021.\nPara converter para a classe Date, identificamos que a ordem de entrada é: mês, dia, ano (padrão americano)\nE então, usamos a função mdy (month, day, year):\nmdy(\u0026quot;11/20/2021\u0026quot;) ## [1] \u0026quot;2021-11-20\u0026quot; Seguindo essa intuição:\nydm(\u0026quot;2021/20/11\u0026quot;) ## [1] \u0026quot;2021-11-20\u0026quot; Além disso, a função aceita diversos padrões de entrada, vejamos:\ndmy(20112021) ## [1] \u0026quot;2021-11-20\u0026quot; dmy(\u0026quot;20112021\u0026quot;) ## [1] \u0026quot;2021-11-20\u0026quot; dmy(\u0026quot;20-11-2021\u0026quot;) ## [1] \u0026quot;2021-11-20\u0026quot; dmy(\u0026quot;20.11.2021\u0026quot;) ## [1] \u0026quot;2021-11-20\u0026quot; Incluindo horas, minutos e segundos Suponha que queremos criar um objeto com a seguinte informação: 20/11/2021 16:34:29. Basta usar a função dmy_hms:\ndmy_hms(\u0026quot;20/11/2021 16:34:29\u0026quot;) ## [1] \u0026quot;2021-11-20 16:34:29 UTC\u0026quot; dmy_hms(\u0026quot;20112021163429\u0026quot;) ## [1] \u0026quot;2021-11-20 16:34:29 UTC\u0026quot; Os operadores %m+% e %m-% Podemos usar os operadores %m+% e %m-% para somar e subtrair datas.\nCriando um objeto date:\ndata \u0026lt;- dmy(\u0026quot;31-01-2022\u0026quot;) Por exemplo, para somar um mês em alguma data, fazemos:\ndata %m+% months(1) ## [1] \u0026quot;2022-02-28\u0026quot; Note que a função não excedeu o último dia do mês, ela retornou o último dia do mês seguinte.\nPodemos somar mais de um mês por vez, obtendo:\ndata %m+% months(1:12) ## [1] \u0026quot;2022-02-28\u0026quot; \u0026quot;2022-03-31\u0026quot; \u0026quot;2022-04-30\u0026quot; \u0026quot;2022-05-31\u0026quot; \u0026quot;2022-06-30\u0026quot; ## [6] \u0026quot;2022-07-31\u0026quot; \u0026quot;2022-08-31\u0026quot; \u0026quot;2022-09-30\u0026quot; \u0026quot;2022-10-31\u0026quot; \u0026quot;2022-11-30\u0026quot; ## [11] \u0026quot;2022-12-31\u0026quot; \u0026quot;2023-01-31\u0026quot; A expressão Date %m+% months(n) sempre retorna uma data do n-ésimo mês seguinte à data do objeto Date\nSubtraindo um dia:\ndata %m+% months(1:12) %m-% days(1) ## [1] \u0026quot;2022-02-27\u0026quot; \u0026quot;2022-03-30\u0026quot; \u0026quot;2022-04-29\u0026quot; \u0026quot;2022-05-30\u0026quot; \u0026quot;2022-06-29\u0026quot; ## [6] \u0026quot;2022-07-30\u0026quot; \u0026quot;2022-08-30\u0026quot; \u0026quot;2022-09-29\u0026quot; \u0026quot;2022-10-30\u0026quot; \u0026quot;2022-11-29\u0026quot; ## [11] \u0026quot;2022-12-30\u0026quot; \u0026quot;2023-01-30\u0026quot; Outro exemplo:\ndata_2 \u0026lt;- dmy(\u0026quot;29/02/2020\u0026quot;) data_2 %m+% years(1) ## [1] \u0026quot;2021-02-28\u0026quot; data_2 %m-% years(1) ## [1] \u0026quot;2019-02-28\u0026quot; ","permalink":"https://blog.curso-r.com/posts/2021-11-16.lubridate/","tags":["tutoriais"],"title":"Manipulando datas com o lubridate"},{"author":["William"],"categories":["pacotes"],"contents":" Muitas vezes, o destino dos nossos gráficos é um Word ou PDF. Nesses casos, independentemente se vamos ler o documento no computador ou na versão impressa, os gráficos precisam ser uma imagem estática. E então o pacote ggplot2 brilha.\n— Leia mais: Gráficos no Shiny: ggplot2\nQuando estamos construindo páginas ou aplicações Web (ou apresentações de slides em HTML), além de gráficos em formato de imagem, podemos construir visualizações utilizando bibliotecas JavaScript, que permitem animações e possuem diversas funcionalidades interativas, como tooltips, filtros, zoom e drilldrown.\nNeste post, vamos falar da biblioteca plotly, que permite transformarmos rapidamente nossos gráficos feitos com ggplot2 em gráficos animados e interativos.\nO pacote plotly Antes de mais nada, precisamos instalar o pacote plotly.\ninstall.packages(\u0026quot;plotly\u0026quot;) A função mais legal desse pacote é a plotly::ggplotly(). Com ela transformamos facilmente um ggplot em um gráfico plotly.\nlibrary(ggplot2) library(plotly) p \u0026lt;- mtcars |\u0026gt; ggplot(aes(x = wt, y = mpg)) + geom_point() ggplotly(p) Veja que esse gráfico possui um visual muito parecido com o do ggplot e, além disso,\nmostra uma tooltip quando passamos o cursor em cima de um ponto\npermite selecionar uma área do gráfico para dar zoom;\ne possui uma barra de ferramentas que nos permite aumentar e diminuir o zoom, focar em regiões do gráfico e baixar o gráfico como uma imagem estática.\nNo exemplo a seguir, além das funcionalidades acima, também podemos clicar na legenda para adicionar ou remover grupos de pontos do gráfico.\nlibrary(ggplot2) library(plotly) p \u0026lt;- mtcars |\u0026gt; ggplot(aes(x = wt, y = mpg, color = as.character(cyl))) + geom_point() ggplotly(p) Para controlar o que aparece na tooltip, podemos usar o parâmetro tooltip. Veja que adicionamos o modelo do carro e passamos por meio do aes text. O tema escolhido para o ggplot é respeitado pelo plotly.\nlibrary(ggplot2) library(plotly) p \u0026lt;- mtcars |\u0026gt; tibble::rownames_to_column() |\u0026gt; ggplot(aes(x = wt, y = mpg, color = as.character(cyl), text = rowname)) + geom_point() + theme_minimal() ggplotly(p, tooltip = c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;, \u0026quot;text\u0026quot;)) Também podemos construir um gráfico diretamente pelo plotly, mas isso exige aprendermos a sintaxe do pacote plotly e as opções disponíveis da biblioteca Plotly.\nplot_ly(mtcars, x = ~wt, y = ~mpg, type = \u0026quot;scatter\u0026quot;, mode = \u0026quot;markers\u0026quot;) Para aprender mais sobre como fazer gráficos diretamente no plotly, confira o tutorial oficial da biblioteca.\nNo Shiny Para adicionar um plotly no Shiny, criado a partir da função ggplotly() ou da função plot_ly(), utilizamos o par de funções plotly::plotlyOutput() e plotly::renderPlotly(). Na função renderPlotly(), basta passarmos um código que retorne um gráfico plotly, utilizando os inputs para especificar as variáveis.\nRode o app abaixo para ver um exemplo.\nlibrary(shiny) library(ggplot2) vars \u0026lt;- names(mtcars) ui \u0026lt;- fluidPage( titlePanel(\u0026quot;Plotly\u0026quot;), sidebarLayout( sidebarPanel( selectInput( \u0026quot;x\u0026quot;, \u0026quot;Eixo x\u0026quot;, choices = vars ), selectInput( \u0026quot;y\u0026quot;, \u0026quot;Eixo y\u0026quot;, choices = vars, selected = vars[2] ) ), mainPanel( plotly::plotlyOutput(\u0026quot;plot\u0026quot;) ) ) ) server \u0026lt;- function(input, output, session) { output$plot \u0026lt;- plotly::renderPlotly({ p \u0026lt;- mtcars |\u0026gt; tibble::rownames_to_column() |\u0026gt; ggplot(aes( x = .data[[input$x]], y = .data[[input$y]], text = rowname )) + geom_point() + theme_minimal() plotly::ggplotly(p) }) } shinyApp(ui, server) É isso! Dúvidas, sugestões e críticas, mande aqui nos comentários.\nAté a próxima!\n","permalink":"https://blog.curso-r.com/posts/2021-11-10-shiny-plotly/","tags":["shiny","gráficos","plotly"],"title":"Gráficos no Shiny: plotly"},{"author":["Caio"],"categories":["tutoriais"],"contents":" O RStudio é uma IDE (Integrated development environment) incrível. Em sua versão mais recente a ferramenta tem suporte para visualizações interativas, traz controle de versão embutido, consegue executar jobs em paralelo e permite programar não só em R, mas também em Python!\nEle não é, contudo, perfeito. Pessoalmente eu tenho algumas críticas a algumas de suas idiossincrasias, principalmente no tocante à camada interpretativa que embrulha o R… Depois de muitos anos programando R e usando somente o RStudio, a lista de coisas que começaram a me incomodar foi crescendo:\nÀs vezes, imprimir um objeto de texto muito grande trava a interface. Cancelar essa operação depois que ela foi iniciada também nem sempre funciona.\nO navegador de arquivos, com suas caixinhas clicáveis, deixa muito a desejar. Não deveria ser tão chato copiar e deletar arquivos.\nAté hoje não existe um jeito de substituir um termo (find-replace) em múltiplos arquivos. É possível procurar um termo na pasta toda (Ctrl + Shift + F), mas nada de substituí-lo.\nAté onde eu sei, é impossível abrir uma pasta com o RStudio sem que seja criado um arquivo .Rproj. Por que é necessário criar um arquivo para abrir uma mera pasta?\nPor padrão, as mensagens de diagnóstico continuam sujando as sessões locais. Novos usuários podem estranhar esse tipo de alerta inesperado.\n#\u0026gt; Registered S3 method overwritten by \u0026#39;quantmod\u0026#39;: #\u0026gt; method from #\u0026gt; as.zoo.data.frame zoo Honestamente, a interface está visualmente cada vez mais atrasada. Ícones coloridos e difíceis de diferenciar somados a um sistema de design limitado é mais que um incômodo em 2021.\nAinda é impossível executar funções do furrr no modo multicore dentro do console do RStudio.\nplan(\u0026quot;multicore\u0026quot;) #\u0026gt; Warning message: #\u0026gt; In supportsMulticoreAndRStudio(...) : #\u0026gt; [ONE-TIME WARNING] Forked processing (\u0026#39;multicore\u0026#39;) is not supported when #\u0026gt; running R from RStudio because it is considered unstable. For more details, #\u0026gt; how to control forked processing or not, and how to silence this warning in #\u0026gt; future R sessions, see ?parallelly::supportsMulticore A interrupção da execução continua tendo problemas com tarefas em C e paralelas. Quem nunca segurou Esc na esperança de interromper um comando só para ter que esperar ele terminar de rodar de qualquer jeito?\nA bombinha. Por que a IDE inteira cai se o problema está no código executado?\nUma Possível Alternativa Apesar de nenhum desses problemas ser fatal, eu ainda acho interessante procurar uma alternativa. Depender de apenas um software para programar também acaba virando um problema caso ele deixe de ser mantido; note que, no momento em que escrevo, aproximadamente 20% das issues já cadastradas no GitHub do RStudio ainda estão abertas.\nÉ assim que chegamos às alternativas. Eu já testei o Nvim-R (extensão para Vim programada por um brasileiro!) e o ESS (extensão para o Emacs), mas a curva de aprendizado das duas ferramentas acabou se tornando um obstáculo. No final eu cheguei ao vscode-R, a extensão de R para o VS Code.\nPara quem não sabe, o VS Code é atualmente o editor de código mais popular do mundo. De acordo com uma pesquisa feita pelo Stack Overflow em 2021, 71% dos programadores usam o VS Code para programar, então ele deve ter algo de bom.\nNa minha opinião, os benefícios do VS Code, em geral, giram em torno da flexibilidade da ferramenta. É muito fácil instalar plugins, modificar o seu visual e customizar o seu funcionamento, permitindo que a IDE se torne verdadeiramente sua. Já, no tocante ao R, o maior benefício é que a sua integração com a linguagem se dá através de um terminal, ou seja, não é embrulhada como no RStudio.\nGuia de Instalação Instalar o VS Code em si é fácil. Basta ir no site e baixar a versão para o seu sistema operacional. Depois de abrir o programa, você verá uma lista de extensões sugeridas e é aí que precisamos configurar o R.\nA extensão que procuramos é identificada pelo código ikuyadeu.r. Basta instalá-la e seguir as instruções para o seu sistema operacional: Windows, MacOS ou Linux.\nAssumindo que você esteja no Windows, os próximos passos são os seguintes:\nInstalar, no R, o pacote languageserver: install.packages(\u0026quot;languageserver\u0026quot;) Instalar a versão mais recente do Python. Se você tiver qualquer versão a partir da 3, já é o suficiente: Instalar o Radian, um console moderno para o R, e descobir onde ele foi instalado. Executar o seguinte no prompt de comando! pip install -U radian where.exe radian Abrir o arquivo de configurações avançadas do VS Code: aperte Ctrl + Shift + P, digite “open settings” e clique em “Open Settings (JSON)”. Colar o texto a seguir no arquivo e salvar (substitua o caminho do rterm pela localização do Radian): { \u0026quot;r.bracketedPaste\u0026quot;: true, \u0026quot;r.rterm.windows\u0026quot;: \u0026quot;C:\\\\Users\\\\user\\\\...\\\\radian.exe\u0026quot; } Caso você use o debugger, instalar a extensão R Debugger e o pacote vscDebugger: remotes::install_github(\u0026quot;ManuelHentschel/vscDebugger\u0026quot;) Habilitar a renderização de visualizações com o pacote httpgd: install.packages(\u0026quot;httpgd\u0026quot;) Finalmente, indicar o uso do httpgd para o VS Code: usethis::edit_r_profile() # Colar a linha abaixo no .Rprofile options(vsc.use_httpgd = TRUE) Usando o VS Code Se você tiver configurado tudo corretamente, o VS Code estará pronto para uso. Para programar em R, basta abrir a pasta de um projeto e começar! Assim que você executar o primeiro comando R com Ctrl + Enter, um console R irá aparecer e você pode seguir a vida normalmente.\nMinha principal sugestão para o seu futuro no VS Code é: explore! Se algo te incomoda, a change de isso ser modificável é de praticamente 100%. Veja vídeos no YouTube de outras pessoas trabalhando com R no VS Code e procure extensões legais na loja. Se algo parecer que está errado, se informe.\nA minha primeira semana com o VS Code foi de adaptação, configurando tudo do jeito que eu mais gosto. A partir daí, eu nunca mais precisei mexer em nada, então consigo atestar de que a plataforma é robusta.\nBônus: Configuração Extra Para que você não precise ter o mesmo trabalho que eu, aqui estão as minhas principais configurações do VS Code. Veja se elas funcionam para o seu estilo de trabalho. Note apenas que a minha configuração é do Mac! Troque a tecla cmd por ctrl de acordo.\nsettings.json:\n{ \u0026quot;r.rterm.mac\u0026quot;: \u0026quot;/opt/homebrew/bin/radian\u0026quot;, \u0026quot;r.bracketedPaste\u0026quot;: true, \u0026quot;editor.minimap.enabled\u0026quot;: false, \u0026quot;workbench.startupEditor\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;redhat.telemetry.enabled\u0026quot;: false, \u0026quot;editor.tabSize\u0026quot;: 2, \u0026quot;security.workspace.trust.untrustedFiles\u0026quot;: \u0026quot;open\u0026quot;, \u0026quot;explorer.confirmDelete\u0026quot;: false, \u0026quot;files.defaultLanguage\u0026quot;: \u0026quot;r\u0026quot;, \u0026quot;terminal.integrated.defaultLocation\u0026quot;: \u0026quot;editor\u0026quot;, \u0026quot;diffEditor.ignoreTrimWhitespace\u0026quot;: false, \u0026quot;files.trimTrailingWhitespace\u0026quot;: true, \u0026quot;editor.rulers\u0026quot;: [ 80, 120 ] } keybindings.json:\n// Place your key bindings in this file to override the defaults [ // RStudio key bindings { \u0026quot;key\u0026quot;: \u0026quot;cmd+shift+m\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;type\u0026quot;, \u0026quot;args\u0026quot;: { \u0026quot;text\u0026quot;: \u0026quot; %\u0026gt;%\u0026quot; }, \u0026quot;when\u0026quot;: \u0026quot;editorTextFocus \u0026amp;\u0026amp; editorLangId == \u0026#39;r\u0026#39;\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;alt+-\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;type\u0026quot;, \u0026quot;args\u0026quot;: { \u0026quot;text\u0026quot;: \u0026quot; \u0026lt;- \u0026quot; }, \u0026quot;when\u0026quot;: \u0026quot;editorTextFocus \u0026amp;\u0026amp; editorLangId == \u0026#39;r\u0026#39;\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;shift+cmd+l\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;r.loadAll\u0026quot; }, // Custom shortcuts { \u0026quot;key\u0026quot;: \u0026quot;shift+alt+cmd+left\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;workbench.action.moveEditorToPreviousGroup\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;shift+alt+cmd+right\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;workbench.action.moveEditorToNextGroup\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;shift+cmd+g\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;workbench.view.scm\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;workbench.scm.active\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;shift+cmd+g\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;workbench.view.scm\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;workbench.scm.active \u0026amp;\u0026amp; !gitlens:disabled \u0026amp;\u0026amp; config.gitlens.keymap == \u0026#39;chorded\u0026#39;\u0026quot; }, // Conflicts { \u0026quot;key\u0026quot;: \u0026quot;alt+cmd+left\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-workbench.action.terminal.focusPreviousPane\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;terminalFocus \u0026amp;\u0026amp; terminalProcessSupported\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;alt+cmd+right\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-workbench.action.terminal.focusNextPane\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;terminalFocus \u0026amp;\u0026amp; terminalProcessSupported\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;shift+cmd+m\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-workbench.actions.view.problems\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;workbench.panel.markers.view.active\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;shift+alt+cmd+left\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-cursorColumnSelectLeft\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;textInputFocus\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;ctrl+cmd+left\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-workbench.action.moveEditorToPreviousGroup\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;ctrl+cmd+right\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-workbench.action.moveEditorToNextGroup\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;shift+alt+cmd+right\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-cursorColumnSelectRight\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;textInputFocus\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;shift+cmd+g\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-editor.action.previousMatchFindAction\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;editorFocus\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;shift+cmd+g\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-workbench.action.terminal.findPrevious\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;terminalFindFocused \u0026amp;\u0026amp; terminalProcessSupported || terminalFocus \u0026amp;\u0026amp; terminalProcessSupported\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;cmd+g\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-editor.action.nextMatchFindAction\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;editorFocus\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;cmd+g\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-workbench.action.terminal.findNext\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;terminalFindFocused \u0026amp;\u0026amp; terminalProcessSupported || terminalFocus \u0026amp;\u0026amp; terminalProcessSupported\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;ctrl+shift+g\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-workbench.view.scm\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;workbench.scm.active\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;ctrl+shift+g\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-workbench.view.scm\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;workbench.scm.active \u0026amp;\u0026amp; !gitlens:disabled \u0026amp;\u0026amp; config.gitlens.keymap == \u0026#39;chorded\u0026#39;\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;shift+cmd+l\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-selectAllSearchEditorMatches\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;inSearchEditor\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;shift+cmd+l\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-editor.action.selectHighlights\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;editorFocus\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;shift+cmd+l\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-addCursorsAtSearchResults\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;fileMatchOrMatchFocus \u0026amp;\u0026amp; searchViewletVisible\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;alt+cmd+right\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-workbench.action.terminal.focusNextPane\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;terminalFocus \u0026amp;\u0026amp; terminalHasBeenCreated || terminalFocus \u0026amp;\u0026amp; terminalProcessSupported\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;alt+cmd+left\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;-workbench.action.terminal.focusPreviousPane\u0026quot;, \u0026quot;when\u0026quot;: \u0026quot;terminalFocus \u0026amp;\u0026amp; terminalHasBeenCreated || terminalFocus \u0026amp;\u0026amp; terminalProcessSupported\u0026quot; }, ] Caso você tenha problemas com o VS Code, mande uma mensagem no nosso fórum para que eu possa ajudar.\n","permalink":"https://blog.curso-r.com/posts/2021-11-06-r-no-vscode/","tags":["rstudio","vscode"],"title":"Programando R no VS Code"},{"author":["William"],"categories":["conceitos"],"contents":" No último post, falamos sobre as três peças fundamentais do fluxo de reatividade de um app: os valores reativos, as expressões reativas e os observers, o foco deste post.\nOs observers são o ponto final de um diagrama de reatividade de um aplicativo Shiny e, sem eles, o fluxo reativo não acontece. As funções render*(), que geram os nossos outputs, são o tipo mais comum de observer. Mas eles não são os únicos.\nMuitas vezes queremos usar a reatividade para disparar ações que não estão ligadas a geração de outputs, como o registro de informações em bases de dados ou a atualização de elementos da UI.\nNesses casos, podemos utilizar as funções observe() e oberveEvent(). Elas funcionam de maneira similar às funções reactive() e eventReactive(), mas em vez de criar expressões reativas, elas são observers.\nA função observe() A função observe({codigo}) monitora os valores e expressões reativas que estão dentro dela e roda seu código quando algum desses valores são modificados.\nA diferença do observe() para a função reactive() é que a primeira não gera expressões reativas, servindo apenas para códigos que têm efeitos colaterais.\nEssa função é muito utilizada com as funções da família update*(), que servem para atualizar valores de um input na UI.\nNa segunda caixa de seleção do exemplo a seguir, queremos selecionar apenas os filmes do(a) diretor(a) que selecionamos na primeira. Veja que usamos o texto Carregando... como um placeholder para o segundo selectInput().\nui \u0026lt;- fluidPage( selectInput( \u0026quot;dir\u0026quot;, \u0026quot;Selecione um(a) diretor(a)\u0026quot;, choices = sort(unique(imdb$diretor)) ), selectInput( \u0026quot;filme\u0026quot;, \u0026quot;Selecione um filme\u0026quot;, choices = \u0026quot;Carregando...\u0026quot; ) ) No server, atualizamos as escolhas da segunda caixa de seleção com a função updateSelectInput(). Veja que, como essa função está dentro de um observe, esse código será rodado novamente sempre que o valor de input$dir mudar.\nserver \u0026lt;- function(input, output, session) { observe({ opcoes \u0026lt;- imdb |\u0026gt; dplyr::filter(diretor == input$dir) |\u0026gt; dplyr::pull(titulo) updateSelectInput( session, inputId = \u0026quot;filme\u0026quot;, choices = opcoes ) }) } A função observeEvent() A função observeEvent() funciona assim como a observe(), mas ela escuta apenas um valor ou expressão reativa, que é definido em seu primeiro argumento, assim como na função eventReactive().\nEla é muito utiliza para disparar ações, como gravar informações em uma base de dados, a partir de botões.\nNo exemplo a seguir, queremos salvar o e-mail de uma pessoa quando ela clicar no botão “Enviar dados”. A função observeEvent() roda o código definido dentro dela quando o botão é clicado, salvando o e-mail em um arquivo de texto.\nui \u0026lt;- fluidPage( textInput(\u0026quot;email\u0026quot;, \u0026quot;Informe seu e-mail\u0026quot;), actionButton(\u0026quot;enviar\u0026quot;, \u0026quot;Enviar dados\u0026quot;) ) server \u0026lt;- function(input, output, session) { observeEvent(input$enviar, { write(input$email, \u0026quot;emails.txt\u0026quot;, append = TRUE) }) } As funções observe() e oberveEvent() aumentam bastante o leque de opções dos nossos aplicativos. Agora conseguimos criar fluxos reativos que não estão associados necessariamente a um output. Nessa linha, no nosso próximo post sobre reatividade, falaremos sobre como criar valores reativos que não estão associados a inputs.\nÉ isso! Dúvidas, sugestões e críticas, mande aqui nos comentários.\nAté a próxima!\n","permalink":"https://blog.curso-r.com/posts/2021-11-06-reatividade-pt2/","tags":["shiny","reatividade"],"title":"Entendendo a reatividade: observers"},{"author":["William"],"categories":["pacotes"],"contents":" Gráficos são a alegria da festa em um aplicativo Shiny (ou dashboards em geral). Embora possamos usar tabelas, textos ou caixinhas coloridas com valores para comunicar nossos resultados, são eles que geralmente chamam e prendem a atenção de quem está utilizando o app.\nPara dar a atenção que eles merecem, farei aqui uma série de posts para apresentar as principais alternativas para construção de gráficos dentro do Shiny. Falaremos do plotly, do echarts4r, do highcharter, começando neste post pelo nosso fiel companheiro: o ggplot2.\nO pacote ggplot2 é uma ferramenta maravilhosa para produzirmos gráficos no R. Quando entendemos a sua sintaxe, nos tornamos capazes de fazer uma variedade enorme de gráficos e dar a eles a cara que quisermos.\nNada do que o pacote ggplot2 tem para oferecer se perde quando estamos construindo gráficos dentro de um Shiny app. Pelo contrário, ainda ganhamos um novo recurso!\nPara inserir um ggplot em um aplicativo Shiny, utilizamos a dupla de funções plotOutput()/renderPlot(). Essas funções estão preparadas para receber um objeto gerado pelas funções do ggplot e renderizá-lo em uma imagem, que será inserida no HTML do app por meio da tag \u0026lt;img\u0026gt;. Veja um exemplo abaixo.\nlibrary(shiny) library(ggplot2) colunas \u0026lt;- names(mtcars) ui \u0026lt;- fluidPage( titlePanel(\u0026quot;Shiny app com um ggplot\u0026quot;), sidebarLayout( sidebarPanel( selectInput( \u0026quot;varX\u0026quot;, label = \u0026quot;Variável eixo X\u0026quot;, choices = colunas ), selectInput( \u0026quot;varY\u0026quot;, label = \u0026quot;Variável eixo Y\u0026quot;, choices = colunas, selected = colunas[6] ) ), mainPanel( plotOutput(\u0026quot;grafico\u0026quot;) ) ) ) server \u0026lt;- function(input, output, session) { output$grafico \u0026lt;- renderPlot({ ggplot(mtcars, aes(x = .data[[input$varX]], y = .data[[input$varY]])) + geom_point() }) } shinyApp(ui, server) Figura 1: App com dois inputs especificando as variáveis do eixo x e y de um ggplot . O objeto gerado pelo ggplot é uma lista com classe gg e ggplot, que contém todas as informações necessárias para o R desenhar o gráfico.\np \u0026lt;- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() class(p) ## [1] \u0026quot;gg\u0026quot; \u0026quot;ggplot\u0026quot; names(p) ## [1] \u0026quot;data\u0026quot; \u0026quot;layers\u0026quot; \u0026quot;scales\u0026quot; \u0026quot;mapping\u0026quot; \u0026quot;theme\u0026quot; ## [6] \u0026quot;coordinates\u0026quot; \u0026quot;facet\u0026quot; \u0026quot;plot_env\u0026quot; \u0026quot;labels\u0026quot; Repare que, se salvássemos o ggplot em um arquivo (.png por exemplo), poderíamos simplesmente usar a dupla imageOutput()/renderImage() para inserir um gráfico no nosso app, já que essas funções também criam uma tag \u0026lt;img\u0026gt;, mas a partir de um arquivo de imagem local.\nMas não precisar salvar o ggplot em um arquivo não é a única vantagem de utilizarmos as funções plotOutput()/renderPlot(). Essas funções inserem um objeto intermediário no diagrama de reatividade do app: um plotObj. Esse objeto é justamente a lista gerada pelas funções que utilizamos na construção do gráfico e que só é recalculado quando um dos inputs existentes no código da função renderPlot() muda.\nLeia mais: Diagramas de reatividade no shiny (reactlog)\nJá o gráfico renderizado depende não apenas desse plotObj, mas também do comprimento e altura da janela do navegador de quem estiver utilizando o app. Dessa maneira, o gráfico é renderizado não apenas quando o plotObj muda, mas também quando o espaço disponível para a tag \u0026lt;img\u0026gt; na tela também muda. Nesse segundo caso, o R gera o gráfico novamente, redesenhando seus elementos para a nova proporção de comprimento e altura. E o melhor é que ele faz isso sem precisar rodar o código da função renderPlot() novamente, pois tudo o que ele precisa já está salvo no plotObj.\nFigura 2: Gráfico sendo redimensionado conforme diminuímos o comprimento da tela. Sem esse recurso, nossos gráficos seriam apenas imagens sendo esticadas e achatadas, o que provavelmente os deixaria pixelados. Ao contrário do que acontece no R Markdown, em um relatório HTML ou em um flexdashboard por exemplo, no Shiny não precisamos nos preocupar muito com as dimensões de um ggplot. Ele será sempre automaticamente otimizado para o espaço disponível na tela.\nMas nem tudo são flores… Por melhor que consigamos mexer no visual do nosso ggplot utilizando a função theme(), no fim do dia ele continuará sendo apenas uma imagem no nosso app. Isso significa que não será possível atribuir a ele qualquer comportamento interativo, como tooltips, drildown ou ações geradas por cliques no gráfico. Para isso, precisaremos utilizar bibliotecas gráficas próprias para a Web, que geralmente utilizam JavaScript para construir os gráficos e gerar interatividade.\nNo próximo post, falaremos da biblioteca e pacote plotly, que permite transformamos nossos ggplots em gráficos interativos.\nÉ isso! Dúvidas, sugestões e críticas, mande aqui nos comentários. Até a próxima!\n","permalink":"https://blog.curso-r.com/posts/2021-10-21-shiny-ggplot2/","tags":["shiny","gráficos","echarts"],"title":"Gráficos no Shiny: ggplot2"},{"author":["William"],"categories":["conceitos"],"contents":" Quando escrevemos código R, dois paradigmas estão sempre presentes: - podemos avaliar uma linha de código assim que a escrevermos; e\nse decidirmos rodar todo o script de uma vez, as linhas de código serão avaliadas sequencialmente. Isso faz com que as nossas tarefas de análise de dados geralmente virem scripts sequenciais, cujo código não pode ser executado fora de ordem.\nO código abaixo, que executa a corriqueira tarefa de importar, manipular e visualizar uma base, mostra um exemplo disso. Construímos o código rodando linha a linha, para testar se estamos seguindo pelo caminho certo. Ao final, podemos rodar tudo de uma vez para obter o resultado desejado (o gráfico). Se o código for rodado fora de ordem, nada vai funcionar.\ntab_starwars \u0026lt;- dplyr::starwars tab_grafico \u0026lt;- tab_starwars |\u0026gt; tidyr::unnest(films) |\u0026gt; tidyr::drop_na(species) |\u0026gt; dplyr::group_by(films) |\u0026gt; dplyr::summarise(total_especies = dplyr::n_distinct(species)) |\u0026gt; dplyr::mutate( films = forcats::fct_reorder(films, total_especies) ) tab_grafico |\u0026gt; ggplot2::ggplot(ggplot2::aes(y = films, x = total_especies)) + ggplot2::geom_col() + ggplot2::theme_minimal() + ggplot2::labs(x = \u0026quot;Total de espécies\u0026quot;, y = \u0026quot;Filme\u0026quot;) A reatividade é um outro paradigma de programação. Com ela, não construímos códigos que serão rodados interativamente ou sequencialmente. A ideia da programação reativa é especificar um fluxo de reatividade, isto é, um diagrama de dependências que será utilizado para definir o que deve ser executado e quando. No contexto do Shiny, o fluxo de reatividade é quem decide quais outputs devem ser recalculados quando um input muda e pode ser composto por 3 tipos de estruturas: os valores reativos, as expressões reativas e os observers.\nOs valores reativos são a origem do fluxo reativo. Eles guardam as informações que vêm da UI (a partir dos inputs) e disparam o sinal de alerta sempre que essas informações mudam. Os valores reativos mais comuns são aqueles dentro da lista input.\nEsse sinal de alerta é um aviso dizendo que todos os outputs que dependem desse valor reativo precisam ser recalculados. Quem recebe esse sinal são os observers, isto é, as estruturas dentro do Shiny que guardam o código de cada output. Eles são o ponto final do fluxo de reatividade. Os observers mais comuns são as funções render*().\nMuitas vezes, um aplicativo shiny precisa de passos intermediários, entre o input de origem e o output final. Isto é, precisamos de uma estrutura que receba um valor reativo, faça alguma conta e devolva como resultado um valor também reativo, que será utilizado posteriormente em um observer. Essas estruturas são as expressões reativas.\nImagine um app que gere uma amostra de números aleatórios entre 1 e 10 e que o tamanho dessa amostra é definido por um input. Além disso, imagine que esse app também indique em texto qual foi o número mais sorteado. A figura a seguir mostra uma implementação desse app.\nRepare no código do app, apresentado a seguir, que a criação da amostra não poderia ter sido feita diretamente dentro das funções renderPlot() e renderText(), pois gerariam amostras diferentes1. Por outro lado, a geração da amostra precisa estar dentro de um contexto reativo, pois ela utiliza um valor reativo (input$num), o que tira da mesa a proposta de fazer isso diretamente dentro da função server.\nlibrary(shiny) ui \u0026lt;- fluidPage( \u0026quot;Histograma da distribuição normal\u0026quot;, sliderInput( inputId = \u0026quot;num\u0026quot;, label = \u0026quot;Selecione o tamanho da amostra\u0026quot;, min = 1, max = 1000, value = 100 ), plotOutput(outputId = \u0026quot;hist\u0026quot;), textOutput(outputId = \u0026quot;media\u0026quot;) ) server \u0026lt;- function(input, output, session) { amostra \u0026lt;- reactive({ sample(1:10, input$num, replace = TRUE) }) output$hist \u0026lt;- renderPlot({ barplot(table(amostra())) }) output$media \u0026lt;- renderText({ contagem \u0026lt;- sort(table(amostra()), decreasing = TRUE) mais_frequente \u0026lt;- names(contagem[1]) glue::glue(\u0026quot;O número mais sorteado foi {mais_frequente}.\u0026quot;) }) } shinyApp(ui, server) A solução nesse caso foi utilizar a função reactive(). Essa função cria a expressão reativa amostra, que é utilizada dentro das funções renderPlot() e renderText() para obtermos a amostra sorteada. Note que, para retornar o valor de uma expressão reativa, devemos chamá-la como se fosse uma função, abrindo e fechando parênteses após o nome: amostra().\nNesse exemplo, o input$num é um valor reativo, a amostra() é uma expressão reativa e as funções renderPlot() e renderText() são observers. O fluxo reativo se inicia com um mudança no valor do input$num e termina com a recriação do gráfico e do texto. O valor reativo, quando alterado, avisa à expressão reativa amostra que seu valor está desatualizado e, por sua vez, a amostra avisa aos observers renderPlot() e renderText() que seu valor está desatualizado. Assim, tanto a expressão reativa quanto os observers são recalculados e seus resultados enviado de volta para a UI.\nSe esses conceitos apresentados até agora estão muito abstratos, pense em uma fábrica de brinquedos. A fábrica utiliza algumas matérias-primas, como madeira, plástico e tecido, para fabricar os brinquetos. Nela, existem algumas máquinas que recebem a matéria-prima e a transformam em partes dos brinquedos, assim como máquinas que recebem tanto matérias-prima quanto essas partes pré-fabricadas e montam o brinquedo.\nNessa metáfora, a matéria-prima representa os valores reativos (cada material pode ser visto como um input), as máquinas que produzem as partes são as expressões reativas e a máquina que monta o brinquedo são os observers.\nNos próximos posts, falaremos com mais detalhes sobre essas estruturas. Daremos exemplos de valores reativos que não são inputs, observers que não são outputs e expressões reativas que não geram um fluxo de reatividade.\nÉ isso! Dúvidas, sugestões e críticas, mande aqui nos comentários.\nAté a próxima!\nClaro que poderíamos usar a função set.seed() para garantir que as amostras fossem as mesmas, mas imagine que não queremos escolher uma semente para a geração dos dados ou que, em algum outro contexto, o processo de amostragem fosse demorado e não queremos fazê-lo duas vezes.↩︎\n","permalink":"https://blog.curso-r.com/posts/2021-09-12-reatividade-pt1/","tags":["shiny","reatividade"],"title":"Entendendo a reatividade: fundamentos"},{"author":["Fernando"],"categories":["análises","conceitos"],"contents":" No nosso trabalho como Cientistas de Dados é muito comum procurar semelhanças e diferenças entre variáveis de bancos de dados. Existem muitos métodos para procurar por esse tipo de coisa e normalmente eles trabalham com a noção de dependência estatística e suas métricas ou com a noção de redução de dimensionalidade (a capacidade de representar muitas variáveis por um conjunto reduzido). Nessa primeira estratégia, as matrizes de correlação são as técnicas mais comuns e difundidas. No post de hoje, vamos explorar a função ggcorr do pacote GGally que nos ajuda a visualizar a matriz de correlação.\nQuando estamos trabalhando com dados contínuos, isso é: quando as nossas variáveis são todas numéricas (ou no mínimo ordenadas), é possível utilizar os métodos que estão na função ggcorr do pacote GGally. Essa função constrói um mapa de calor que nos mostra quais pares de variáveis da nossa tabela possuem maior correlação. Essa correlação por sua vez pode ser medida utilizando três métricas: o coeficiente de kendall e as correlações de pearson e spearman. Veja uma aplicação desse método à base de dados socioeconômicos do Gapminder. Aí é possível ver que a expectativa de vida e o pib são bastante correlacionados, mas o mesmo não acontece com o PIB e a população, por exemplo.\nlibrary(tidyverse) dados::dados_gapminder |\u0026gt; #select(-pais, -continente) |\u0026gt; GGally::ggcorr(method = c(\u0026#39;all.obs\u0026#39;, \u0026#39;spearman\u0026#39;)) Podemos também construir matrizes um pouco maiores e obter resultados parecidos:\nlibrary(tidyverse) dados::mtcarros |\u0026gt; #select(-pais, -continente) |\u0026gt; GGally::ggcorr(method = c(\u0026#39;all.obs\u0026#39;, \u0026#39;spearman\u0026#39;)) Gostou? Quer saber mais? Se você quiser aprender um pouco mais sobre esse assunto, temos alguns cursos que tocam os temas deste post. Dê uma olhada nos nossos cursos de Regressão Linear ou Machine Learning e aproveite!\nSe você quiser aprender um pouco mais sobre manipulação de dados com R, dê uma olhada no nosso curso R para Ciência de Dados I e aproveite!\nCaso você tenha dúvidas entre em contato com a gente pelos comentários aqui embaixo, pelo nosso Discourse ou pelo e-mail contato@curso-r.com.\n","permalink":"https://blog.curso-r.com/posts/2021-09-03-matrizes-correlacao/","tags":["base de dados"],"title":"Matrizes de correlação com o R usando o pacote GGally"},{"author":["William"],"categories":["divulgação"],"contents":" A useR é atualmente o maior evento de R do mundo. A edição 2021 aconteceu entre os dias 5 e 9 de julho, no modelo online, e contou com diversas apresentações, tutoriais, short talks e integração da comunidade.\nNeste post, vou listar o que rolou sobre Shiny na edição desse ano. Todas as palestras estão disponíveis no YouTube.\nEm resumo, o tema subjacente das palestras é “Olha como o Shiny é incrível!”, o que é muito legal para aumentar o nosso repertório de argumentos na hora de levar o Shiny para dentro das empresas e universidades.\nA minha preferida foi a “Unit Testing Shiny App Reactivity”, porque testes no Shiny são uma coisa que ainda não faço e queria bastante aprender. A palestra me motivou a continuar estudando o assunto.\nAll-in-one smartphone-based system for quantitative analysis of point-of-care diagnostics Tema: showcase\nNível: iniciante\nQuem: Weronika Schary e Filip Paskali\nDescrição: apresentação de um aplicativo Shiny que auxilia na detecção de doenças a partir de análises de imagens.\nLink: https://youtu.be/olmKEqBpWpE?t=130\nUnit Testing Shiny App Reactivity Tema: testes para aplicativos Shiny\nNível: intermediário\nQuem: Jonathan Sidi\nDescrição: apresentação do pacote {reactor}, que tem como objetivo diagnosticar problemas de reatividade em um Shiny app.\nLink: https://youtu.be/olmKEqBpWpE?t=1349\nShinyQuickStarter: Build Shiny apps interactively with Drag \u0026amp; Drop Tema: desenvolvimento de Shiny apps\nNível: iniciante\nQuem: Leon Binder\nDescrição: apresentação de um addin para o RStudio que permite a construção de aplicativos Shiny (e módulos) a partir de uma interface arraste e solte.\nLink: https://youtu.be/olmKEqBpWpE?t=2407\nThe ShinyProxy Good News Show Tema: deploy\nNível: intermediário\nQuem: Tobia De Koninck\nDescrição: apresentação do ShinyProxy, solução de código aberto para empresas gerenciarem o deploy de aplicativos Shiny.\nLink: https://youtu.be/olmKEqBpWpE?t=3676\nShiny PoC to Production Application in 8 steps Tema: desenvolvimento de Shiny apps\nNível: intermediário\nQuem: Marcin Dubel\nDescrição: apresentação de um workflow para desenvolvimento de aplicativos Shiny no contexto empresarial.\nLink: https://youtu.be/GMMGBlyl_ok?t=52\nStructure your app: introduction to Shiny modules Tema: desenvolvimento de Shiny apps\nNível: iniciante\nQuem: Jonas Hagenberg\nDescrição: motivação e introdução ao uso de módulos no Shiny.\nLink: https://youtu.be/oOYaHsPXLvs\nshiny.fluent and shiny.react: Build Beautiful Shiny Apps Using Microsoft’s Fluent UI Tema: desenvolvimento de Shiny apps\nNível: intermediário\nQuem: Marek Rogala\nDescrição: ensina a utilizar os pacotes {shiny.fluent} e {shiny.react} para construir UIs utilizando o framework Microsoft Fluent UI e a biblioteca JavaScript React.\nLink: https://youtu.be/LXpPNfHKyrU\nÉ isso! Dúvidas, sugestões e críticas, mande aqui nos comentários.\nAté a próxima!\n","permalink":"https://blog.curso-r.com/posts/2021-07-01-shiny-user2021/","tags":["shiny","useR"],"title":"Shiny na useR 2021"},{"author":["Caio"],"categories":["tutoriais"],"contents":" Uma das funcionalidades mais interessantes do R é a possibilidade de estender a linguagem para domínios específicos. A non-standard evaluation garante que até modificações à estrutura fundamental do R podem ser realizadas sem problema. Hoje vamos falar de um assunto que muita gente quer aprender, mas que pouca gente entende de verdade: funções anônimas.\nSe você não souber o que é uma função anônima, pode ser que você conheça esse conceito por outro nome. Também chamada de “função lambda”, “notação de fórmula” ou “notação de til”, ela aparece principalmente em programas que usam o {purrr} apesar de não serem exclusividade desse pacote. Resumindo, se você já viu algo do tipo ~.x e não entendeu do que se tratava, este post é para você.\nIntrodução Funções anônimas são, essencialmente, um jeito de simplificar a criação de funções pequenas. Em poucas palavras, o nosso objetivo é não ter que declarar uma função nova inteira com function() { ... } para poder usá-la dentro de um programa.\nO exemplo que será utilizado na explicação a seguir será a função conta_na() que (não surpreendentemente) conta o número de NAs em um vetor. Vamos usá-la dentro de um map() para que ela seja aplicada a todas as colunas de um data frame. Sendo assim, partiremos da forma mais verborrágica possível desse código e tentaremos chegar, intuitivamente, nas funções anônimas.\nUma ressalva importante é que a notação explicada aqui só funciona dentro do {tidyverse}! No final do texto será apresentada uma alternativa que funciona fora desse contexto, mas, por enquanto, a notação abaixo só pode aparecer nos argumentos .f, .fn e .fns utilizados dentro do {tidyverse}.\nConceito Vamos imaginar uma função conta_na() que conta o número de NAs em uma coluna de um data frame. Para aplicá-la a todas as colunas do data frame, podemos, por exemplo, utilizar a função map() do pacote {purrr} como no exemplo abaixo:\nconta_na \u0026lt;- function(vetor) { sum(is.na(vetor)) } map_dbl(starwars, conta_na) #\u0026gt; name height mass hair_color skin_color eye_color birth_year #\u0026gt; 0 6 28 5 0 0 44 #\u0026gt; sex gender homeworld species films vehicles starships #\u0026gt; 4 4 10 4 0 0 0 No R, quando temos uma função simples de uma linha, é perfeitamente possível não colocar o seu corpo em uma linha separada. Veja como o código já fica um pouco mais compacto (saída omitida daqui em diante):\nconta_na \u0026lt;- function(vetor) { sum(is.na(vetor)) } map_dbl(starwars, conta_na) Agora, se temos uma função que cabe inteira em uma linha, o R nos permite também omitir as chaves: por exemplo, m if-else pode ser escrito if (cond) resp1 else resp2 se as respostas não tiverem mais de uma linha. No nosso caso, vamos reduzir ainda mais a conta_na():\nconta_na \u0026lt;- function(vetor) sum(is.na(vetor)) map_dbl(starwars, conta_na) O próximo passo seria encurtar o nome do argumento da função, utilizando algo mais genérico. Poucas pessoas sabem, mas o R permite nomes começando com .! Por motivos que ficarão claros a seguir, vamos escolher .x para ser o nome do nosso argumento:\nconta_na \u0026lt;- function(.x) sum(is.na(.x)) map_dbl(starwars, conta_na) Agora vem a grande sacada. Tudo que fizemos até agora funciona no R como um todo, mas, se atendermos algumas condições extras, podemos reduzir ainda mais essa função.\nVamos lá: se a função i) tiver apenas uma linha, ii) tiver apenas 1 argumento, iii) tiver .x como seu único argumento .x e iv) estiver sendo utilizada como argumento de uma função do {tidyverse}, então podemos omitir o function(.x) (já que isso é informação redundante dadas as condições) e trocá-lo por um singelo ~:\nconta_na \u0026lt;- ~ sum(is.na(.x)) map_dbl(starwars, conta_na) O último passo é o motivo de chamarmos essa notação de “função anônima”. Dado que nossa função já é tão pequena e utilizamos ela em apenas um lugar, por que precisamos dar um nome para ela? É mais fácil declará-la diretamente dentro do map() sem um nome, ou seja, “anonimamente”:\nmap_dbl(starwars, ~ sum(is.na(.x))) #\u0026gt; name height mass hair_color skin_color eye_color birth_year #\u0026gt; 0 6 28 5 0 0 44 #\u0026gt; sex gender homeworld species films vehicles starships #\u0026gt; 4 4 10 4 0 0 0 Pronto, agora você sabe o que significa uma função do tipo ~.x. Para treinar, tente fazer o processo reverso como no caso abaixo:\n# Fração de valores distintos dentre todos map_dbl(starwars, ~ length(unique(.x)) / length(.x)) # Desanonimizar frac_distintos \u0026lt;- ~ length(unique(.x)) / length(.x) map_dbl(starwars, frac_distintos) # Remover a notação de til (não é mais necessário mexer no map()) frac_distintos \u0026lt;- function(.x) length(unique(.x)) / length(.x) # Utilizar um nome melhor para o argumento frac_distintos \u0026lt;- function(vec) length(unique(vec)) / length(vec) # Recolocar as chaves frac_distintos \u0026lt;- function(vec) { length(unique(vec)) / length(vec) } # Identar o corpo da função frac_distintos \u0026lt;- function(vec) { length(unique(vec)) / length(vec) } Agora fica bem mais fácil de entender o que faz o map() lá do começo.\nFuturo A pergunta óbvia agora é: existe um jeito de fazer algo assim fora do {tidyverse}? A resposta é sim e não.\nDesde o R 4.1, o R introduziu a sua própria notação anônima. Ela funciona de maneira muito similar ao ~, com a diferença de que você precisa dizer o nome do seu argumento. Abaixo deixo vocês com o processo de simplificação da função conta_na() para a sua versão anônima que pode ser utilizada em qualquer lugar e não só no {tidyverse}:\n# Conta o número de NAs em um vetor conta_na \u0026lt;- function(vetor) { sum(is.na(vetor)) } # Usar uma linha só conta_na \u0026lt;- function(vetor) { sum(is.na(vetor)) } # Sem necessidade de usar chaves conta_na \u0026lt;- function(vetor) sum(is.na(vetor)) # Se a função tem uma linha, podemos usar a nova notação conta_na \u0026lt;- \\(vetor) sum(is.na(vetor)) # O nome do argumento pode ser qualquer coisa, não importa conta_na \u0026lt;- \\(v) sum(is.na(v)) # Anonimizar map_dbl(starwars, \\(v) sum(is.na(v))) #\u0026gt; name height mass hair_color skin_color eye_color birth_year #\u0026gt; 0 6 28 5 0 0 44 #\u0026gt; sex gender homeworld species films vehicles starships #\u0026gt; 4 4 10 4 0 0 0 Quase tão bom quanto a notação de til!\n","permalink":"https://blog.curso-r.com/posts/2021-08-16-entendendo-anonimas/","tags":["tidyverse","purrr"],"title":"Entendendo Funções Anônimas de Uma Vez por Todas"},{"author":["Athos"],"categories":["tutoriais"],"contents":" Requerimento O seguinte passo a passo supõe conhecimento prévio de git e github com RStudio. A seção de Git/Github do livro Zen do R é uma ótima referência para aprender a usar o git e github no R.\nCriar o blog: passo-a-Passo Esse passo-a-passo é baseado no doc do distill da RStudio.\nRode install.packages(c(\"usethis\", \"distill\")) para instalar os pacotes {usethis} e {distill}. Rode usethis::create_package(\"blogDoAthos\") para criar novo projeto do RStudio. Rode usethis::use_git() para iniciar o git no seu projeto. Rode usethis::use_github(protocol = \"ssh\" para criar um repositório no seu github. Vá ao repositório do seu github, clique em settings e desça até achar a configuração do Github Pages. Escolha Figura 1: ghpages Rode file.create(\".nojekyll\") só pra evitar uns problemas com o ghpages. Rode distill::create_blog(\".\", \"Blog do Athos\") para criar seu blog. Commitar e pushar para o github. Ir para o link que o Github Pages mostrou na hora da configuração. Figura 2: ghpages Criar um post: passo-a-passo Rode distill::create_post(\"Título do Meu Primeiro Post\") para criar um Rmd novo. Com o Rmd do seu post aberto na sua frente, pressione CTRL+SHIF+K para renderizar (e gerar o HTML). Commitar e pushar para o github para publicar o seu post no ar. Editar o seu Rmd e depois repetir os itens 2 e 3 para subir a versão editada/atualizada do seu post. Exemplos de distill de R para se inspirar e explorar o github Trabalhos de Regressão Linear com R da Curso-R Blog da ABJ Site do próprio Distill para R Markdown RStudio AI Blog Ciência de Dados com R e PostgreSQL do JJ - José Jesus do R Blog da Beatriz Milz ","permalink":"https://blog.curso-r.com/posts/2021-08-16-distill_ghpages/","tags":["blog","github pages","distill"],"title":"Github Pages e Distill: Blog no Ar em 13 Passos"},{"author":["Caio"],"categories":["pacotes"],"contents":" droll [ˈdrōl] (adj.). 1. (Ing.) Divertido, especialmente de forma incomum.\ndroll é um pacote R para parsear a notação usada para descrever dados, analisar rolagens, calcular probabilidades de sucesso e gerar gráficos das distribuições dos resultados. Ele pode ajudar DMs detalhistas a preparar encontros (in)justos com antecedência ou decidir a DC apropriada para um teste na hora. Jogadores também podem usar o droll para determinar a melhor estratégia quando em uma situação difícil.\nEle foi feito para ser uma alternativa muito leve (só uma dependência obrigatória), muito rápida (menos de 0,4s para calcular a distribuição completa de 40d6) e muito precisa (representação interna simbólica cortesia do Ryacas) para o anydice no R.\nInstalação Instale a versão estável do CRAN com:\ninstall.packages(\u0026quot;droll\u0026quot;) Ou instale a versão em desenvolvimento do GitHub com:\n# install.packages(\u0026quot;remotes\u0026quot;) remotes::install_github(\u0026quot;curso-r/droll\u0026quot;) Uso O que você procura no droll? Você é um usuário nível 1, um programador experiente nível 10, ou um estatístico divino nível 20? Escolha sua classe:\n🖍️ Usuário O uso mais básico do droll envolve simplesmente rolar dados. Você pode criar qualquer dado que quiser com a função d() e escrever qualquer expressão que envolva aquele dado. Note que, se você quiser rolar NdX, você deveria escrever N * dX.\n# Criar alguns dados d20 \u0026lt;- d(20) d6 \u0026lt;- d(6) d4 \u0026lt;- d(4) # Rolar um teste enquanto abençoado (d20 + 8) + d4 #\u0026gt; [1] 19 # Rolar o dano!! 8 * d6 #\u0026gt; [1] 33 # Rolar um teste de resistência de destreza com desvantagem if (min(d20, d20) + 4 \u0026lt; 18) { print(\u0026quot;Dano completo!\u0026quot;) } else { print(\u0026quot;Metade do dano.\u0026quot;) } #\u0026gt; [1] \u0026quot;Dano completo!\u0026quot; Simples e fácil, certo? Se você é um DM, você também pode querer usar duas funções: check_prob() e check_dc(). Elas permitem que você, respectivamente, calcule a probabilidade de passar (ou falhar) em um teste e encontrar a DC necessária para que um teste tenha uma certa probabilidade de sucesso (ou erro). Você não precisa nem criar os dados que você vai usar dentro delas!\n# Qual é a probabilidade que esse jogador passe em um teste de DC 15? check_prob(d20 + 8, 15) #\u0026gt; [1] 0.7 # Qual deveria ser a DC para que esse jogador tenha 50% de chance de sucesso? check_dc(d20 + 8, 0.5) #\u0026gt; [1] 19 # Qual é a probabilidade desse jogador falhar em um teste de DC 10? check_prob(d20 + 8, 10, success = FALSE) #\u0026gt; [1] 0.05 # Qual deveria ser a DC para que esse jogador tenha 90% de chance de falha? check_dc(d20 + 8, 0.9, success = FALSE) #\u0026gt; [1] 27 Não há funções do tipo attack_*() porque as mecânicas de ataques e testes são as mesmas, ou seja, sucesso equivale a rolar um valor maior ou igual a um certo patamar. Essas funções podem, portanto, ser usadas para ataques também!\n🗡️ Programador Se você já está acostumado com a notação d/p/q/r do R, pode ser que você queira mais detalhes sobre a distribuição de uma rolagem. É para isso que as funções droll(), proll(), qroll(), e rroll() existem! Elas são, respectivamente, a densidade, a função de distribuição, a função de quantil, e a geração aleatória da distribuição definida por uma expressão de rolagem.\n# P[d20 + 8 = 12] droll(12, d20 + 8) #\u0026gt; [1] 0.05 # P[d20 + 8 \u0026lt;= 12] proll(12, d20 + 8) #\u0026gt; [1] 0.2 # inf{x: P[d20 + 8 \u0026lt;= x] \u0026gt;= 0.5} qroll(0.5, d20 + 8) #\u0026gt; [1] 18 # Amostrar 3 vezes de d20 + 8 rroll(3, d20 + 8) #\u0026gt; [1] 27 12 24 Quando você aprender a usar essas quatro funções, você pode olhar as suas variações plot_*() delas. Elas geram gráficos (usando o ggplot2 se ele estiver disponível) correspondendo às distribuições completas de d/p/q e um histograma simples no caso da plot_rroll().\n# Densidade de 8d6 droll_plot(8 * d6) # Função de distribuição de 8d6 proll_plot(8 * d6) # Função de quantil de 8d6 qroll_plot(8 * d6) # Histograma de 1000 rolagens de 8d6 rroll_plot(1000, 8 * d6) Toda função p/q também tem um conveniente argumento lower.tail que pode ser igual a FALSE para que os cálculos sejam feitos a partir da cauda superior da distribuição.\n🪄 Estatístico Já que você é um veterano do R, você precisa ser capaz de dobrar o droll à sua vontade. Se você gostaria de ver o tecido da realidade do droll, você pode usar a função r() para obter uma distribuição de rolagem completa. Se você quiser precisão máxima, você também pode impedir o droll de converter a sua representação interna (operada pelo Ryacas) para doubles com precise = TRUE.\n# Obter a distribuição completa de 8d6 r(8 * d6) #\u0026gt; # A tibble: 41 × 4 #\u0026gt; outcome n d p #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 8 1 0.000000595 0.000000595 #\u0026gt; 2 9 8 0.00000476 0.00000536 #\u0026gt; 3 10 36 0.0000214 0.0000268 #\u0026gt; 4 11 120 0.0000714 0.0000982 #\u0026gt; 5 12 330 0.000196 0.000295 #\u0026gt; 6 13 792 0.000472 0.000766 #\u0026gt; 7 14 1708 0.00102 0.00178 #\u0026gt; 8 15 3368 0.00201 0.00379 #\u0026gt; 9 16 6147 0.00366 0.00745 #\u0026gt; 10 17 10480 0.00624 0.0137 #\u0026gt; # … with 31 more rows # Poder ilimitado r(8 * d6, precise = TRUE) #\u0026gt; # A tibble: 41 × 4 #\u0026gt; outcome n d p #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 8 1 1/1679616 1/1679616 #\u0026gt; 2 9 8 1/209952 1/186624 #\u0026gt; 3 10 36 1/46656 5/186624 #\u0026gt; 4 11 120 5/69984 55/559872 #\u0026gt; 5 12 330 55/279936 55/186624 #\u0026gt; 6 13 792 11/23328 143/186624 #\u0026gt; 7 14 1708 427/419904 2995/1679616 #\u0026gt; 8 15 3368 421/209952 707/186624 #\u0026gt; 9 16 6147 683/186624 695/93312 #\u0026gt; 10 17 10480 655/104976 11495/839808 #\u0026gt; # … with 31 more rows A tabela retornada por r() pode ser usada no lugar do argumento roll de todas as funções discutidas acima. Isso pula os cálculos internos, um atalho valioso se você quiser rodar múltiplos diagnósticos para a mesma expressão de rolagem.\nComo um estatístico nível 20, você também não está limitado pelos dados internos do droll. Você pode criar dados personalizados usando a mesma função d() descrita anteriormente.\n# Criar um dado \u0026quot;fudge\u0026quot; dF \u0026lt;- d(-1:1) rroll(5, dF) #\u0026gt; [1] 0 1 0 0 -1 # Criar um 2d20kh, um \u0026quot;dado de vantagem\u0026quot; df \u0026lt;- r(max(d20, d20)) kh \u0026lt;- d(rep(df$outcome, df$n)) rroll(5, kh) #\u0026gt; [1] 17 12 13 14 16 ","permalink":"https://blog.curso-r.com/posts/2021-08-12-droll/","tags":["dnd","probabilidade","dados"],"title":"Rolando Dados com {droll}"},{"author":["Fernando"],"categories":["discussões"],"contents":" Programação é uma das habilidades mais importantes da atualidade, principalmente para quem mexe com análise de dados. Hoje nós vamos explorar 5 motivos para te lembrar ou relembrar de porque R é uma das maiores linguagens para Ciência de Dados. Alguns dos motivos se aplicam a todas as linguagens de programação, mas sempre vamos destacar porque o R é especial, e alguns outros são específicos da nossa querida linguagem. Vamos a eles!\nMotivo 1: R é completamente gratuito! dplyr, ggplot2, Rmarkdown e tidymodels são apenas alguns exemplos de bibliotecas para R que, assim como a própria linguagem, são 100% gratuitas e mantidas pela comunidade. Quem usa R pode aproveitar uma ampla gama de funcionalidades sem gastar um centavo sequer com licenças, softwares, planos etc.\nMotivo 2: Grandes pessoas da comunidade disponibilizam seu trabalho em R Várias pesssoas importantes da comunidade de estatística e machine learning participam ativamente da comunidade R e impulsionam seus trabalhos teóricos através de bibliotecas feitas especificamente para R. Para citar um exemplo, Trevor Hastie, Jerome Friedman, Rob Tibshirani e quem colabora com eles lançaram uma primeira versão do pacote (glmnet)[https://glmnet.stanford.edu/articles/glmnet.html] em 2010 antes de fazê-lo em qualquer outra linguagem. Esse pacote foi fruto de uma série de pesquisas desses autores e quem usa R pode aproveitar as primeiras implementações logo após a publicação dos resultados.\nMotivo 3: R possui um ecossistema que incentiva a reprodutibilidade Uma das maiores virtudes de se usar uma linguagem de programação para análise de dados é garantir que qualquer resultado, da conta mais simples ao gráfico mais complexo, pode ser reproduzida de maneira completamente fiel por um programa de computador. Isso é importante por vários motivos, mas principalmente porque (1) te ajuda a economizar tempo, já que você pode simplesmente usar o mesmo programa para vários problemas parecidos, e (2) em muitos contextos, como na pesquisa científica por exemplo, é importante que se saiba com muita precisão como cada resultado foi produzido, inclusive para que ele possa ser verificado por pares posteriormente.\nEntão, se já nos convencemos de que reprodutibilidade é importante, podemos ver claramente porque R é tão especial: existe todo um ecossistema que suporta e incentiva a reprodutibilidade. Desde a existência do pacote reprex (que te ajuda a compartilhar pequenos pedaços de código em texto simples) até ferramentas como Rmarkdown e as boas práticas incentivadas pelo RStudio, por exemplo. Além disso, como grande parte da comunidade R é formada por pessoas que trabalham na academia, muito do que se produz e discute nos blogs e fóruns é como garantir que nossas análises sejam reprodutíveis.\nMotivo 4: R tem uma comunidade vibrante e crescente Ter alguém para conversar e tirar dúvidas é uma parte importante do aprendizado de praticamente qualquer tópico. Em R no Brasil e no mundo temos uma grande e crescente comunidade disposta a ajudar e discutir, inclusive divulgando materiais gratuitos introdutórios e avançados para que muitas pessoas possam aproveitar. Nesse sentido, um esforço particularmente relevante da comunidade foca-se também em promover a inclusão de grupos sub representados no mundo tech, como é o caso de iniciativas como a R-Ladies.\nMotivo 5: Os focos da linguagem são modelagem, visualização e análise de dados Desde os seus primórdios em meados de 1976, R e suas encarnações anteriores foram focadas em fornecer ferramentas para quem faz análise de dados, seja isso modelagem, visualização ou elaboração de relatórios. Por isso, nesses 45 anos de desenvolvimento da linguagem, a comunidade se focou principalmente em simplificar a vida de quem manipula bases de dados e, consequentemente, hoje temos ferramentas que produzem resultados sofisticados a partir de uma sintaxe simples e consistente. Não que outras linguagens não tenham grandes contribuições, mas a herança das gerações anteriores faz com que R seja ainda hoje uma das mais ricas no quesito “dados”.\nGostou? Quer saber mais? Se você quiser dar os seus primeiros passos para aprender a manipular dados com R, dê uma olhada no nosso curso R para Ciência de Dados I e aproveite!\nCaso você tenha dúvidas, entre em contato com a gente pelos comentários aqui embaixo, pelo nosso Discourse ou pelo e-mail contato@curso-r.com.\n","permalink":"https://blog.curso-r.com/posts/2021-07-23-por-que-usar-r/","tags":["tutoriais"],"title":"5 motivos para usar R"},{"author":["Caio"],"categories":["tutoriais"],"contents":" Este é um tutorial sobre o pacote {rlang}, um dos mais poderosos e menos conhecidos do R. Ele é vital para a notação compacta do {tidyverse}, conhecida como tidy eval, mas mesmo assim poucas pessoas sabem como ele funciona e como utilizá-lo para criar funções no estilo tidy.\nO tutorial não é curto, mas fizemos o nosso melhor para começar com calma e terminar com colinhas para facilitar o uso deste material no dia a dia. Se for necessário, leia e releia uma mesma seção para ter certeza de que o conceito apresentado foi completamente absorvido. No caso de dúvidas, entre em contato conosco e com o resto da comunidade R no nosso Discourse.\nSe estiver com preguiça, deixei uma colinha no final do post. Mas, para os corajosos, preparem-se para alguns novos conceitos de programação, vários filhotes de cachorro e muito {rlang}!\nO melhor amigo do R A analogia que vamos usar para explicar o {rlang} gira em torno de um parque repleto de filhotes fofinhos. Nós temos uma única missão nesse parque: fazer carinho nos cachorros. Para isso, temos uma função carinho() que recebe o nome de um filhote e imprime uma frase explicando em quem estamos fazendo carinho. O objeto que descreve o filhote se resume a uma string com a sua cor.\nrex \u0026lt;- \u0026quot;laranja\u0026quot; carinho(rex) #\u0026gt; Fazendo carinho no filhote laranja Para facilitar a compreensão do código, vamos também ilustrar esse cenário. Na figura abaixo, note que é criado um objeto rex que recebe a figura de um filhote laranja. A função carinho() é retratada como uma mão parada que, quando executada, retorna uma mão fazendo carinho no filhote de cor apropriada (essencialmente a string retornada pelo código acima).\nExpressões Agora que temos uma base sólida para a analogia, podemos introduzir o primeiro conceito importante do {rlang}: expressões. Uma expressão no R não passa do código antes antes que ele seja avaliado, ou seja, aquilo que você escreve e que, depois de executado no console do RStudio, se torna um resultado. Em quase 100% dos casos, o R não faz nenhuma distinção entre a expressão e o valor que ela retorna, de modo que executar carinho(rex) fica equivalente a executar carinho(\"laranja\"). Esse comportamento é chamado de avaliação ansiosa, justamente porque o R avalia cada parte da expressão tão cedo quanto for possível.\nEssa, entretanto, não é única forma de avaliação. Também é possível capturar uma expressão, “impedindo” o R de avaliá-la, em um processo denominado avaliação preguiçosa. A função do {rlang} que faz isso se chama expr() e ela retorna a expressão passada, vulgo o código escrito.\ne \u0026lt;- expr(carinho(max)) e #\u0026gt; carinho(max) Veja que não importa que não existe ainda um filhote chamado max! Como estamos lidando apenas com uma expressão sem contexto, isso é perfeitamente possível.\nVoltando para o nosso parque de cachorros, a avaliação preguiçosa se torna quase uma promessa de fazer carinho em um filhote. Temos toda a informação necessária (no caso, o nome do filhote), mas não transformamos isso na ação de fazer carinho: não “chamamos o filhote para perto” para acariciá-lo. Perceba que na figura abaixo não há as marcas de movimento da mão, pois estamos congelando a cena antes de o filhote vir até nós.\nAmbientes Na nossa analogia, o próximo conceito representa o parque em si, um lugar onde há uma correspondência entre nomes de cachorros. No R, um ambiente funciona como um dicionário que contém definições de objetos acompanhados pelos valores que eles carregam.\nAbaixo, vamos “trazer” dois novos cachorros para o parque, ou seja, criar dois novos objetos. A função env_print() mostra todas as correspondências presentes no ambiente (incluindo a da função carinho()), além de algumas informações extras que não nos interessam agora.\nmax \u0026lt;- \u0026quot;marrom\u0026quot; dex \u0026lt;- \u0026quot;bege\u0026quot; env_print() #\u0026gt; \u0026lt;environment: global\u0026gt; #\u0026gt; parent: \u0026lt;environment: package:rlang\u0026gt; #\u0026gt; bindings: #\u0026gt; * rex: \u0026lt;chr\u0026gt; #\u0026gt; * e: \u0026lt;language\u0026gt; #\u0026gt; * mtcars: \u0026lt;tibble[,11]\u0026gt; #\u0026gt; * carinho: \u0026lt;fn\u0026gt; #\u0026gt; * dex: \u0026lt;chr\u0026gt; #\u0026gt; * max: \u0026lt;chr\u0026gt; Na analogia, estamos colocando os cachorros max e dex no parque, permitindo que possamos eventualmente fazer carinho neles. Vamos apenas ignorar a definição da função carinho() para que isso não atrapalhe o resto da explicação.\nPerceba que o resultado da avaliação de uma expressão depende completamente do ambiente. Na hora de executar um código, o R procura as definições de todos os objetos no ambiente e os substitui dentro da expressão. Agora vamos ver o que aconteceria se tentássemos fazer carinho no filhote chamado max em outro parque…\nAvaliando expressões Avaliação nua (bare evaluation no original) é o processo pelo qual o {rlang} permite que forneçamos explicitamente um ambiente no qual avaliar uma expressão. É como se pudéssemos escolher o parque no qual vamos chamar um filhote para acariciá-lo.\nNo código abaixo vamos visitar um outro parque, isto é, criar uma função. O ambiente dentro de uma função herda as definições do ambiente global, mas podemos fazer alterações lá dentro que não são propagadas para fora. Vide a função abaixo: p() define um objeto max com a cor verde e avalia (ou seja, executa) uma expressão lá dentro.\np \u0026lt;- function(x) { max \u0026lt;- \u0026quot;verde\u0026quot; eval_tidy(x) } Seguindo a analogia dos filhotes, é como se visitássemos um outro parque onde há um cachorro chamado max cuja cor é verde (além dos outros dois cachorros que já havíamos visto no parque antigo).\nComo a função eval_tidy(), por padrão, utiliza o ambiente corrente para avaliar expressões, então p(e) deve indicar carinho em um filhote verde e não mais em um filhote marrom. Note que, apesar de não estarmos passando um ambiente explicitamente para a eval_tidy(), ela está obtendo esse ambiente através de caller_env(), o valor padrão para seu argumento env.\np(e) #\u0026gt; Fazendo carinho no filhote verde Na ilustração a seguir vemos o que aconteceria no nosso parque. Apesar de estarmos chamando o nome do cachorro marrom, como estamos em outro parque (uma nova função), o cachorro que responderá ao chamado aqui é verde!\nQuosures Agora que você entende o que é uma expressão, o que é um ambiente e como podemos avaliar uma expressão dentro de um ambiente, chegou a hora de entender a estrutura mais importante do {rlang}: as quosures. Esse nome estranho vem de quote e closure, dois conceitos extremamente importantes da Ciência da Computação, mas explicar o que eles significam foge do escopo deste tutorial.\nUma quosure, apesar de parecer um conceito complexo, não passa de uma expressão que carrega um apontador para seu ambiente consigo. Isso não parece ser muito útil, mas é a quosure que permite que as funções do {tidyverse} sejam capazes de acessar as colunas de uma tabela e variáveis declaradas no ambiente global.\nq \u0026lt;- quo(carinho(max)) q #\u0026gt; \u0026lt;quosure\u0026gt; #\u0026gt; expr: ^carinho(max) #\u0026gt; env: global Pensando nos filhotes, uma quosure é a promessa de fazer carinho em um cachorro sabendo exatamente o endereço do parque em que ele estava Note que, na saída acima, o env se chama “global”, justamente porque estamos trabalhando diretamente na sessão base do R.\nNa figura abaixo, juntamente da cena retratada na ilustração sobre expressões, vemos um qualificador de max, especificando onde devemos encontrar ele. Isso é significativamente diferente de simplesmente gritar pelo max mais próximo.\nAvaliando quosures Assim como utilizamos a avaliação nua para obter o resultado de uma expressão em um certo ambiente, podemos usar a avaliação tidy (de tidy evaluation) para obter o resultado de uma quosure no ambiente que ela carrega.\nAqui, depois de capturar a quosure, podemos fazer o que quisermos com o ambiente na qual avaliaremos ela, pois o único ambiente que importará na avaliação tidy é o de seu ambiente de origem. Sendo assim, perceba que o argumento env de eval_tidy() não foi levado em conta!\np(q) #\u0026gt; Fazendo carinho no filhote marrom É difícil traduzir esse processo para a analogia dos filhotes, mas seria algo como voltar para o endereço do parque original antes de fazer carinho no filhote cujo nome é max.\nBang-bang A peça final do quebra-cabeça do {rlang} é o bang-bang, também conhecido como quasiquotation e expresso na forma de duas exclamações: !!. Essa funcionalidade, exclusiva ao {rlang}, permite que façamos uma “avaliação ansiosa seletiva” em uma expressão ou quosure. Em breve ficará mais claro onde isso pode ser útil, mas antes é necessário ver como usar o bang-bang na prática.\nO bang-bang “força” a avaliação de uma parte da expressão, liberando o R para fazer parte do seu trabalho de avaliação ansiosa. Veja, no código abaixo, como funciona a captura de uma expressão que usa o bang-bang. Atente-se para o fato de que, na seção anterior, alteramos o valor de max.\nexpr(carinho(!!max)) #\u0026gt; carinho(\u0026quot;marrom\u0026quot;) Na analogia dos filhotes, o bang-bang está essencialmente chamando um cachorro pelo nome antes que façamos carinho nele. Ao invés do contorno branco que vimos nas ilustrações sobre expressões e quosures, agora vemos a mão parada ao lado de um filhote específico.\nDe volta para casa Apesar de termos visto um pouco de código R na sessão anterior, agora é necessário aprofundar um pouco os exemplos. Prometo que não será nada muito difícil, mas é impossível entender como aplicar o {rlang} no mundo real sem ver alguns casos de uso.\nNa maior parte das ocasiões, não usaremos nenhuma das funções vistas até agora, salvo pelo bang-bang (que na verdade não é uma função, mas sim uma sintaxe). O principal uso do {rlang}, na verdade, é capturar código que o usuário escreve, então é necessário conhecer novas versões de expr() e quo() que são capazes de capturar expressões e quosures vindas de fora de uma função.\nEnriquecimento O conceito de enriquecimento vem de uma analogia meio ruim criada pelo autor do {rlang}; para simplificar, pense que as versões enriquecidas de expr() e quo() são mais “fortes” que as versões normais, sendo capazes de sair de dentro de uma função para capturar expressões do lado de fora.\nAbaixo é possível ver uma função que tenta capturar o nome de um filhote, mas é incapaz de fazê-lo por causa da avaliação ansiosa do R. O correto seria capturar a expressão escrita pelo usuário e imprimí-la como uma string.\nnome1 \u0026lt;- function(filhote) { cat(\u0026quot;O nome do filhote é\u0026quot;, filhote) } nome1(dex) #\u0026gt; O nome do filhote é bege Podemos ver a versão correta da função desejada em nome2(). Ela captura a expressão do usuário com enexpr() (a versão enriquecida de expr()) e converte esse objeto em uma string com expr_text(), permitindo que a função imprima o nome do filhote.\nnome2 \u0026lt;- function(filhote) { nome \u0026lt;- enexpr(filhote) cat(\u0026quot;O nome do filhote é\u0026quot;, expr_text(nome)) } nome2(dex) #\u0026gt; O nome do filhote é dex Como não havia necessidade nenhuma de capturar o ambiente do usuário nesse exemplo, usamos apenas enexpr(). Na maioria das situações, entretanto, é preciso usar enquo() para obter o comportamento correto. Já que quosures incluem expressões, expr() e enexpr() quase nunca são estritamente necessárias, então vamos simplificar tudo e seguir apenas com as quosures.\nNo código abaixo a função explica() precisa tanto da expressão quanto do ambiente da mesma, ou seja, da quosure escrita pelo usuário.\nexplica \u0026lt;- function(acao) { quosure \u0026lt;- enquo(acao) cat(\u0026quot;`\u0026quot;, quo_text(quosure), \u0026quot;` retorna:\\n\u0026quot;, sep = \u0026quot;\u0026quot;) eval_tidy(quosure) } explica(carinho(dex)) #\u0026gt; `carinho(dex)` retorna: #\u0026gt; Fazendo carinho no filhote bege Preste bastante atenção em explica(), pois pode ser que não seja fácil entender como ela funciona. A primeira função utilizada é a enquo() (quo() enriquecida), que captura a expressão do usuário juntamente com o seu ambiente. A seguir, temos apenas que converter a quosure em string com quo_text() para poder imprimí-la. O último passo é avaliar a quosure para obter um resultado exatamente igual ao que o usuário obteria se decidisse executar a expressão passada como argumento.\nCurly-curly A combinação da enquo() com o bang-bang é justamente a forma correta de implementar funções que trabalham com o {tidyverse}. A função summarise(), por exemplo, não passa de um enquo() disfarçado, o que quer dizer que podemos usar o bang-bang para “injetar” o nome de uma variável dentro de um cálculo.\nmedia1 \u0026lt;- function(df, var) { summarise(df, resultado = mean(var)) } media1(mtcars, cyl) #\u0026gt; Error: Problem with `summarise()` column `resultado`. #\u0026gt; ℹ `resultado = mean(var)`. #\u0026gt; x object \u0026#39;cyl\u0026#39; not found O código acima, que não usa bang-bang, retorna um erro. O problema é que a summarise() está tentando tirar a média de um objeto chamado var, que carrega um objeto chamado cyl, que simplesmente não existe no ambiente global. Abaixo, usando bang-bang e enquo(), o código funciona como esperado porque mean(!!var) se torna mean(cyl) dentro da summarise().\nmedia2 \u0026lt;- function(df, var) { var \u0026lt;- enquo(var) summarise(df, resultado = mean(!!var)) } media2(mtcars, cyl) #\u0026gt; # A tibble: 1 × 1 #\u0026gt; resultado #\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 6.19 O {tidyverse} nos fornece um atalho para essa combinação poderosa de enquo() com !!: o {{ }}, mais conhecido como curly-curly. Agora que você já entende exatamente o que está acontecendo por trás dos panos, saber onde usar o curly-curly é mais fácil.\nmedia3 \u0026lt;- function(df, var) { summarise(df, resultado = mean({{var}})) } media3(mtcars, cyl) #\u0026gt; # A tibble: 1 × 1 #\u0026gt; resultado #\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 6.19 Splice A penúltima funcionalidade do {rlang} a compreender é o conceito de splice (“emendar” em português), que se manifesta nas versões pluralizadas das funções apresentadas até agora. Essencialmente, expr()/enexpr e quo()/enquo() só conseguem lidar com uma única expressão ou quosure, então temos outras versões para trabalhar com múltiplas expressões ou quosures.\nNa prática, a principal função que utilizaremos é enquos(). Ela captura todo o conteúdo de uma elipse e o transforma em uma lista de quosures como no exemplo abaixo. As versões plurais também acompanham o bang-bang-bang, o irmão com splice do bang-bang.\nmedia4 \u0026lt;- function(df, ...) { vars \u0026lt;- enquos(...) summarise(df, across(c(!!!vars), mean)) } media4(mtcars, cyl, disp, hp) #\u0026gt; # A tibble: 1 × 3 #\u0026gt; cyl disp hp #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 6.19 231. 147. Se você não estiver familiarizado com a across(), basta saber apenas que o primeiro argumento é um vetor de colunas (similar ao que passaríamos para select()) e o segundo é uma função para utilizar no resumo das colunas especificadas. Aqui o !!! está apenas transformando a chamada em across(c(cyl, disp, hp), mean).\nSímbolos Existe ainda um conceito que não abordamos até agora: símbolos. Um símbolo não passa do nome de um objeto, ou seja, rex, max e dex na analogia dos filhotes; mais especificamente, um símbolo é uma expressão com algumas restrições sobre o seu conteúdo. A função sym(), especificamente, transforma uma string em um símbolo, permitindo que ela seja usada junto com outras expressões.\nmedia6 \u0026lt;- function(df, var) { var \u0026lt;- ensym(var) summarise(df, resultado = mean(!!var)) } media6(mtcars, \u0026quot;cyl\u0026quot;) #\u0026gt; # A tibble: 1 × 1 #\u0026gt; resultado #\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 6.19 Miscelânea Fora os vários conceitos já apresentados, restam apenas duas breves considerações para praticamente zerar o {rlang}:\nO curly-curly funciona com strings, mas não com splice, então a família sym() torna-se quase desnecessária juntamente com o !! ao mesmo tempo em que o !!! permanece essencial.\nHá um operador específico (:=, chamado de morsa) para quando precisamos forçar a execução de algo no lado esquerdo de um cálculo, mesmo quando usando o curly-curly.\nmedia7 \u0026lt;- function(df, col, ...) { args \u0026lt;- enquos(...) summarise(df, {{col}} := mean(!!!args)) } media7(mtcars, \u0026quot;nova_coluna\u0026quot;, drat, na.rm = TRUE) #\u0026gt; # A tibble: 1 × 1 #\u0026gt; nova_coluna #\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 3.60 De volta para o trabalho Depois de tanto conteúdo, agora você consegue entender as colinhas que apresentamos abaixo para facilitar o seu uso do {rlang} no dia-a-dia. Ao final também deixamos as referências deste tutorial para que você possa aprofundar ainda mais os seus conhecimentos de tidy eval.\nVocabulário Vocábulo Tradução Significado Código Ambiente Environment Dicionário de nomes e valores env() Avaliação ansiosa Eager evaluation Avaliação de todo objeto o mais rápido possível Avaliação nua Bare evaluation Avaliação que necessita de um ambiente passado explicitamente eval_tidy(e) Avaliação preguiçosa Lazy evaluation Avaliação de cada objeto conforme a necessidade quo(), etc. Avaliação tidy Tidy evaluation Avaliação que utiliza o ambiente da quosure eval_tidy(q) Bang-bang Bang-bang Operador utilizado para forçar a avaliação de um objeto !! Bang-bang-bang Bang-bang-bang Operador utilizado para forçar a avaliação de vários objetos !!! Curly-curly Curly-curly Atalho para enquo() + !! {{ }} Elipse Ellipsis Argumento de uma funcão que pode receber múltiplas entradas ... Enriquecimento Enriching Processo que permite a captura de código do usuário enquo(), etc. Expressão Expression Código R antes de avaliado expr() Morsa Walrus Operador utilizado para permitir expressões do lado esquerdo de uma igualdade := Quosure Quosure Expressão que carrega seu ambiente consigo quo() Símbolo Symbol Expressão que pode representar apenas o nome de um objeto sym() Splice Splice Processo que permite a captura de múltiplas expressões, etc. quos(), etc. Principais funções Objeto Simples Enriquecido Símbolo sym()/syms() ensym()/ensyms() Expressão expr()/exprs() enexpr()/enexprs() Quosure quo()/quos() enquo()/enquos() Principais padrões Os padrões incluem dois exemplos que não foram explicados durante o tutorial. Para saber mais, consulte as referências no final do texto.\nDescrição Usuário Programador Expressão do lado esquerdo media(df, col) {{col}} := mean(var) Expressão do lado direito media(df, var) col = mean({{var}}) Expressões do lado direito media(df, var, arg1 = 0) col = mean(!!!var) Expressões no meio agrupar(df, var1, var2) group_by(df, ...) Símbolo do lado esquerdo media(df, \u0026amp;quot;col\u0026amp;quot;) {{col}} := mean(var) Símbolo do lado direito media(df, \u0026amp;quot;var\u0026amp;quot;) col = mean(.data$var) Referências Advanced R: Metaprogramming\nDocumentação do {rlang}\nProgramando com o tidyverse: introdução ao pacote rlang\nProgramming with dplyr\nTidy evaluation with rlang: Cheat Sheet\nUsing ggplot2 in packages\n","permalink":"https://blog.curso-r.com/posts/2021-07-27-rlang-para-filhotes/","tags":["rlang","tidy","tidyverse"],"title":"Tutorial: {rlang} para Filhotes"},{"author":["William"],"categories":["tutorial"],"contents":" É comum precisarmos colocar em nossos aplicativos páginas com orientações, informações adicionais ou referências.\nEssas páginas geralmente são compostas por textos, links e imagens, facilmente produzidos em um arquivo Markdown. Em contrapartida, construir esses elementos dentro da UI dá bem mais trabalho, pois tudo precisa ser encapsulado por uma função da lista shiny::tags$.\nPor exemplo, vamos supor que eu queira colocar a minha mini bio (a seguir) em um app que estou fazendo.\nWilliam Amorim\nDoutor em Estatística pelo IME-USP. Trabalhando diariamente com análise de dados, programação em R e criação de dashboards. Falo sobre Shiny no Twitter.\nO arquivo Markdown para construir essa mini bio seria bem simples:\n\u0026lt;center\u0026gt; --- \u0026lt;img src=\u0026quot;img/foto_william.jpg\u0026quot; width=\u0026quot;130px\u0026quot; style=\u0026quot;border-radius: 65px;\u0026quot;/\u0026gt; **William Amorim** Doutor em Estatística pelo IME-USP. Trabalhando diaramente com análise de dados, programação em R e criação de dashboards. Falo sobre Shiny no [Twitter](https://twitter.com/wamorim_). --- \u0026lt;/center\u0026gt; Veja que só precisamos usar a tag img para deixar a imagem redonda e usamos a tag center pois realmente compensa o crime. Caso contrário, o código seria totalmente Markdown. A mesma mini bio já é bem mais chato de construir e manter na UI de um shiny app.\nui \u0026lt;- fluidPage( fluidRow( column( width = 12, shiny::tags$hr(), shiny::tags$img( src = \u0026quot;img/foto_william.jpg\u0026quot;, width = \u0026quot;130px\u0026quot;, style = \u0026quot;border-radius: 65px; display: block; margin: auto;\u0026quot; ), shiny::tags$p( shiny::tags$strong(\u0026quot;William Amorim\u0026quot;), style = \u0026quot;text-align: center;\u0026quot; ), shiny::tags$p( style = \u0026quot;text-align: center;\u0026quot;, \u0026quot;Doutor em Estatística pelo IME-USP. Trabalhando diaramente com análise de dados, programação em R e criação de dashboards. Falo sobre Shiny no\u0026quot;, shiny::tags$a( href = \u0026quot;https://twitter.com/wamorim_\u0026quot;, \u0026quot;Twitter.\u0026quot; ) ), shiny::tags$hr() ) ) ) Mesmo um exemplo simples já começa a deixar claro o problema: produzir muitos elementos HTML na UI rapidamente transforma seu código em um emaranhado de funções aninhadas e cheias de texto. O mesmo vale para textos muito grandes. Embora nesse caso nem sempre tenhamos muitas tags HTML para criar, scripts R não foram feitos para atender aos cuidados que textos carecem.\nA melhor prática nessas situações é justamente transferir esses elementos para um arquivo Markdown e pedir que o Shiny transforme em HTML e inclua no lugar adequado apenas na hora do runApp(). Para isso usamos a função shiny::includeMarkdown().\nSupondo que salvamos o markdown da mini bio em um arquivo minibio_william.md, a nossa UI então ficaria:\nui \u0026lt;- fluidPage( fluidRow( column( width = 12, includeMarkdown(\u0026quot;minibio_william.md\u0026quot;) ) ) ) Vale ressaltar que essa função compila arquivos Markdown (.md) e não R Markdown (.Rmd). Se você gostaria de rodar códigos R para gerar saídas HTML, você deve fazer isso dentro do próprio Shiny.\nÉ isso! Dúvidas, sugestões e críticas, mande aqui nos comentários.\nAté a próxima!\n","permalink":"https://blog.curso-r.com/posts/2021-07-22-shiny-markdown/","tags":["shiny","markdown"],"title":"Fuja das tags! Incluindo Markdown no Shiny"},{"author":["Fernando"],"categories":["pacotes"],"contents":" Em muitas situações, principalmente quando o assunto é finanças, precisamos contar os dias úteis entre duas datas. Como essa é uma questão que depende de calendários regionais, isso pode ser um pouco chato de fazer diretamente no R, pois você precisará importar uma lista de feriados. Felizmente o pacote bizdays que está disponível no CRAN ajuda nessa tarefa e hoje vamos explorar algumas de suas funcionalidades.\nUsando o pacote bizdays A função principal do pacote é a função bizdays, que recebe duas datas e um calendário e retorna a quantidade de dias úteis entre essas datas.\nAtualmente o pacote bizdays suporta a lista de feriados do calendário da ANBIMA, que considera os feriados bancários e é a referência mais comum quando o precisamos contar dias úteis. Isso pode acabar atrapalhando algumas aplicações, pois o calendário da ANBIMA considera, por exemplo, que a quinta-feira da Semana Santa é um dia útil. Ele não considera também feriados municipais ou estaduais. Por outro lado, o calendário da ANBIMA é usado como referência em muitos cálculos de instituições financeiras e é o padrão brasileiro em muitos contextos.\nPara contar os dias úteis tomando como referência esse calendário, basta utilizarmos os comandos abaixo\nlibrary(bizdays) primeiro_de_janeiro_2021 \u0026lt;- as.Date(\u0026quot;2020-01-01\u0026quot;) primeiro_de_julho_2021 \u0026lt;- as.Date(\u0026quot;2021-07-01\u0026quot;) # A ordem dos parâmetros será # data de origem # data final # nome do calendário que será utilizados. # podemos acessar a lista usando o comando calendars() bizdays(primeiro_de_janeiro_2021, primeiro_de_julho_2021, \u0026quot;Brazil/ANBIMA\u0026quot;) ## [1] 374 Criando seu próprio calendário Caso você queira usar um calendário próprio ou incluir novos feriados, você pode criar um novo calendário que ficará disponível para ser usado como referência no terceiro parâmetro da função bizdays. Para criar um calendário, precisamos chamar a função create.calendar como vemos abaixo:\nlibrary(bizdays) data(holidaysANBIMA, package = \u0026#39;bizdays\u0026#39;) # carrega a lista de feriados até 2079 create.calendar( name = \u0026quot;novo_calendario\u0026quot;, holidays = c(holidaysANBIMA, # holidays é um vetor de calendarios. o vetor holidaysANBIMA # contém os feriados do calendário da ANBIMA. as.Date(paste0(2003:2079, \u0026quot;-11-20\u0026quot;))), # aqui incluímos os feriados do # Dia da Conciência Negra no dia 20 de Novembro de todos os anos # de 2003 a 2079 weekdays = c(\u0026quot;saturday\u0026quot;, \u0026quot;sunday\u0026quot;) # aqui definimos quais dias da semana NÃO são considerados dias úteis ) bizdays(primeiro_de_janeiro_2021, primeiro_de_julho_2021, \u0026quot;novo_calendario\u0026quot;) ## [1] 373 Gostou? Quer saber mais? Se você quiser aprender um pouco mais sobre manipulação de dados com R, dê uma olhada no nosso curso R para Ciência de Dados I e aproveite!\nCaso você tenha dúvidas, entre em contato com a gente pelos comentários aqui embaixo, pelo nosso Discourse ou pelo e-mail contato@curso-r.com.\n","permalink":"https://blog.curso-r.com/posts/2021-07-16-bizdays/","tags":["tutoriais"],"title":"Como contar dias úteis no R"},{"author":["William"],"categories":["tutoriais"],"contents":" Neste post, vamos mostrar como usar o widget radioGroupButtons do pacote shinyWidgets para construir um app com seletor de visualizações, isto é, botões que mudam o tipo de visualização apresentada na tela.\nComo exemplo, vamos construir um shiny app que mostra um gráfico de barras, um gráfico de linhas ou uma tabela (a depender da escolha da pessoa que estiver usando) do número de gols do Brasileirão1 por temporada.\nO primeiro passo é configurar o widget corretamente. Veja abaixo que o segredo é utilizar os argumentos choiceValues e choiceNames. Para o primeiro, passamos os valores que serão acessados dentro do server a depender da escolha na UI (equivalente ao argumento choices). Ao segundo, podemos passar tanto textos (que seriam escritos dentro do botão) quanto ícones. Nesse caso, utilizamos ícones da biblioteca Font Awesome, com ajuda da função shiny::icon(). Repare também que deixamos o tamanho dos botões um pouco maiores setando o argumento size = \"lg.\nshinyWidgets::radioGroupButtons( inputId = \u0026quot;vis_escolhida\u0026quot;, label = \u0026quot;\u0026quot;, choiceValues = c(\u0026quot;barras\u0026quot;, \u0026quot;linhas\u0026quot;, \u0026quot;tabela\u0026quot;), choiceNames = list( icon(\u0026quot;bar-chart\u0026quot;), icon(\u0026quot;line-chart\u0026quot;), icon(\u0026quot;table\u0026quot;) ), size = \u0026quot;lg\u0026quot;, selected = \u0026quot;barras\u0026quot; ) Agora, precisamos construir a lógica do Output, tanto na UI como no server. Como a nossa visualização pode gerar gráficos ou uma tabela, precisaremos de funções *Output() e render*() diferentes. Dessa forma, vamos utilizar na nossa UI um OutputUI().\n# A nossa UI ficará assim ui \u0026lt;- fluidPage( fluidRow( column( width = 12, h1(\u0026quot;App com seletor de visualizações\u0026quot;) ) ), br(), fluidRow( column( offset = 1, width = 11, shinyWidgets::radioGroupButtons( inputId = \u0026quot;vis_escolhida\u0026quot;, label = \u0026quot;\u0026quot;, choiceValues = c(\u0026quot;barras\u0026quot;, \u0026quot;linhas\u0026quot;, \u0026quot;tabela\u0026quot;), choiceNames = list( icon(\u0026quot;bar-chart\u0026quot;), icon(\u0026quot;line-chart\u0026quot;), icon(\u0026quot;table\u0026quot;) ), size = \u0026quot;lg\u0026quot;, selected = \u0026quot;barras\u0026quot; ) ) ), br(), fluidRow( column( width = 12, uiOutput(\u0026quot;vis\u0026quot;) ) ) ) Agora, no server, podemos criar funções *Output() diferentes a depender da visualização escolhida. Usamos plotOutput() para os gráficos e reactable::reactableOutput() para a tabela.\noutput$vis \u0026lt;- renderUI({ if (input$vis_escolhida %in% c(\u0026quot;barras\u0026quot;, \u0026quot;linhas\u0026quot;)) { plotOutput(\u0026quot;grafico\u0026quot;) } else if (input$vis_escolhida == \u0026quot;tabela\u0026quot;) { reactable::reactableOutput(\u0026quot;tabela\u0026quot;) } }) Agora vamos construir nossas visualizações. Primeiro, vamos montar a base que precisamos para gerar tanto os gráficos quanto a tabela.\nOs dados vêm do pacote brasileirao. Se você não possui esse pacote instalado, basta rodar o código abaixo:\nremotes::install_github(\u0026quot;williamorim/brasileirao\u0026quot;) Como a tabela que gera os gráficos não depende de nenhum valor ou expressão reativa, podemos colocar o código que a gera diretamente no server.\ntab \u0026lt;- brasileirao::matches |\u0026gt; dplyr::filter( score != \u0026quot;x\u0026quot;, season %in% 2006:2020 ) |\u0026gt; tidyr::separate( score, c(\u0026quot;gols_casa\u0026quot;, \u0026quot;gols_visitante\u0026quot;), sep = \u0026quot;x\u0026quot;, convert = TRUE ) |\u0026gt; dplyr::mutate( gols = gols_casa + gols_visitante ) |\u0026gt; dplyr::group_by(season) |\u0026gt; dplyr::summarise(gols = sum(gols)) Para gerar os gráficos, só precisamos de um renderPlot() e um if/else para devolver o gráfico certo.\noutput$grafico \u0026lt;- renderPlot({ grafico_base \u0026lt;- tab |\u0026gt; ggplot(aes(x = season, y = gols)) + labs(x = \u0026quot;Temporada\u0026quot;, y = \u0026quot;Número de gols\u0026quot;) + theme_minimal() + ggtitle(\u0026quot;Número de gols do Brasileirão por temporada\u0026quot;) if (input$vis_escolhida == \u0026quot;linhas\u0026quot;) { grafico_base + geom_line(color = \u0026quot;dark green\u0026quot;) } else if (input$vis_escolhida == \u0026quot;barras\u0026quot;) { grafico_base + geom_col(width = 0.5, fill = \u0026quot;dark green\u0026quot;) } }) Para gerar a tabela, precisamos apenas de um renderReactable().\noutput$tabela \u0026lt;- reactable::renderReactable({ tab |\u0026gt; reactable::reactable( fullWidth = FALSE, columns = list( season = reactable::colDef( name = \u0026quot;Temporada\u0026quot; ), gols = reactable::colDef( name = \u0026quot;Número de gols\u0026quot; ) ) ) }) Juntando tudo, temos o app a seguir:\nlibrary(shiny) library(ggplot2) ui \u0026lt;- fluidPage( fluidRow( column( width = 12, h1(\u0026quot;App com seletor de visualizações\u0026quot;) ) ), br(), fluidRow( column( offset = 1, width = 11, shinyWidgets::radioGroupButtons( inputId = \u0026quot;vis_escolhida\u0026quot;, label = \u0026quot;\u0026quot;, choiceValues = c(\u0026quot;barras\u0026quot;, \u0026quot;linhas\u0026quot;, \u0026quot;tabela\u0026quot;), choiceNames = list( icon(\u0026quot;bar-chart\u0026quot;), icon(\u0026quot;line-chart\u0026quot;), icon(\u0026quot;table\u0026quot;) ), size = \u0026quot;lg\u0026quot;, selected = \u0026quot;barras\u0026quot; ) ) ), br(), fluidRow( column( width = 12, uiOutput(\u0026quot;vis\u0026quot;) ) ) ) server \u0026lt;- function(input, output, session) { output$vis \u0026lt;- renderUI({ if (input$vis_escolhida %in% c(\u0026quot;barras\u0026quot;, \u0026quot;linhas\u0026quot;)) { plotOutput(\u0026quot;grafico\u0026quot;) } else if (input$vis_escolhida == \u0026quot;tabela\u0026quot;) { reactable::reactableOutput(\u0026quot;tabela\u0026quot;) } }) tab \u0026lt;- brasileirao::matches |\u0026gt; dplyr::filter( score != \u0026quot;x\u0026quot;, season %in% 2006:2020 ) |\u0026gt; tidyr::separate( score, c(\u0026quot;gols_casa\u0026quot;, \u0026quot;gols_visitante\u0026quot;), sep = \u0026quot;x\u0026quot;, convert = TRUE ) |\u0026gt; dplyr::mutate( gols = gols_casa + gols_visitante ) |\u0026gt; dplyr::group_by(season) |\u0026gt; dplyr::summarise(gols = sum(gols)) output$grafico \u0026lt;- renderPlot({ grafico_base \u0026lt;- tab |\u0026gt; ggplot(aes(x = season, y = gols)) + labs(x = \u0026quot;Temporada\u0026quot;, y = \u0026quot;Número de gols\u0026quot;) + theme_minimal() + ggtitle(\u0026quot;Número de gols do Brasileirão por temporada\u0026quot;) if (input$vis_escolhida == \u0026quot;linhas\u0026quot;) { grafico_base + geom_line(color = \u0026quot;dark green\u0026quot;) } else if (input$vis_escolhida == \u0026quot;barras\u0026quot;) { grafico_base + geom_col(width = 0.5, fill = \u0026quot;dark green\u0026quot;) } }) output$tabela \u0026lt;- reactable::renderReactable({ tab |\u0026gt; reactable::reactable( fullWidth = FALSE, columns = list( season = reactable::colDef( name = \u0026quot;Temporada\u0026quot; ), gols = reactable::colDef( name = \u0026quot;Número de gols\u0026quot; ) ) ) }) } shinyApp(ui, server) É isso! Dúvidas, sugestões e críticas, mande aqui nos comentários.\nAté a próxima!\nCampeonato Brasileiro de futebol da Série A.↩︎\n","permalink":"https://blog.curso-r.com/posts/2021-07-01-shiny-seletor-vis/","tags":["shiny","widgets"],"title":"Crie seletores de visualização no Shiny"},{"author":["William","Fernando"],"categories":["pacotes"],"contents":" Se você não sabe o que é DnD, já vamos tirar isso do caminho: DnD não é um novo modelo de Deep Learning. DnD é a abreviação de Dungeons \u0026amp; Dragons, um dos jogos de RPG1 mais famosos da história, considerado a origem dos RPGs modernos.\nPor falta de oportunidade, eu (William) nunca joguei RPG, mas sempre fui muito fã da ideia e dos conceitos. Uma das paradas que acho mais divertidas é a dinâmica de alinhamentos do DnD. Mestre de muitas aventuras, o Fernando explica a seguir em bom português como esse alinhamento funciona.\nO alinhamento DnD Em resumo, essa dinâmica diz que qualquer personagem pode ser posicionado em uma matriz de 9 posições, formadas pela combinação de dois eixos: o ético e o moral.\nO lado ético possui 3 categorias: lawful, neutral e chaotic. Uma personagem lawful ou ordeira é aquela que segue as leis da sociedade em que vive mesmo que isso possa contrariar suas convicções ou valores pessoais, enquanto um personagem chaotic ou caótico se sente confortável em quebrar qualquer tipo de lei, pois faz as suas próprias regras, com pouco ou nenhum apreço pelas normas estabelecidas socialmente. Uma personagem neutral ou neutra seria aquela que se insere dentro do conjunto de normas, mas não tem nenhuma convicção forte sobre segui-las ou não. Diferente de uma personagem ordeira, que segue à risca o que está estabelecido, ou caótica, que nega por princípio parte das regras estabelecidas, uma personagem neutra simplesmente não pensa sobre o assunto.\nO lado moral também possui 3 categorias: good, neutral e evil. Essa escala é mais fácil de explicar. São as personagens boas (good), más (evil) e aquelas que vão na onda (neutral). Ao contrário do nosso mundo real, a fantasia simulada em um jogo de DnD não tem espaço para relativismo moral e nuances. Uma personagem altruísta, generosa, piedosa e/ou qualquer outra característica que não causa sofrimento nenhum é enquadrada no espectro positivo da escala: ela é boa ou good. Já uma personagem egoísta, cruel por esporte, insensível etc é má ou evil. A dificuldade aqui está nos personagens neutros, mas a ideia é parecida com o que falamos no lado ético: esse posicionamento se define pela distância das outras possibilidades. A neutralidade se define pela ambiguidade: todas as atitudes podem ser legítimas independentemente de leis, sofrimento de outras pessoas etc, tudo depende do contexto, não de convicções fortes ou desejo pessoal.\nVamos a alguns exemplos: o herói mítico inglês Robin Hood é um bom exemplo de um personagem chaotic good, pois ele efetivamente quebra as regras vigentes porque acredita que isso é justo (roubar dos ricos), mas o faz para ajudar as pessoas que mais precisam (dar aos pobres), um motivo altruísta. Indo para o mundo de Star Wars Han Solo pode ser considerado um personagem chaotic neutral, pelo menos no começo da história. Ele é um contrabandista, vive sua vida quebrando sistematicamente as regras e não vê problema nisso, de onde concluímos que ele pelo menos pensou no assunto. Além disso, ele não ajuda o Luke e a Léia, personagens neutral good, por nenhum motivo específico. Sua única preocupação inicial é o dinheiro. Para mais exemplos, confira este vídeo em inglês disponível no Youtube.\nComo esse sistema de alinhamentos é flexível o suficiente para enquadrar várias obras diferentes, uma moda que já surgiu e resurgiu na internet algumas vezes é fazer o alinhamento DnD de personagens de alguma série ou filme qualquer.\nVeja alguns exemplos de que você pode achar por aí:\nO pacote DnDalignment Mesmo não sabendo o que é DnD e nunca tendo jogado RPG, o exercício de alinhar personagens nesse grid geralmemnte é bem divertido, principalmente quando você está discutindo o seu posicionamento com outras pessoas. É uma maneira bem divertida de revisitar e discutir obras que você gosta. :)\nE fica ainda mais divertido se você puder fazer isso no R! :D\nFoi por isso que o Fernando e eu criamos o pacote DnDalignment. Com ele, você pode facilmente gerar uma imagem de alinhamento DnD com arquivos que estão na sua máquina ou links para imagens na interet.\nAntes de mais nada, instale o pacote. Ele só está disponível no GitHub.\nremotes::install_github(\u0026quot;williamorim/DnDalignment\u0026quot;) Para criar o alinhamento, basta usar a função DnDalignment::create_alignment(). Cada argumento da função se refere a um alinhamento e você pode passar o caminho para uma imagem local ou o link para uma imagem na internet.\nPara exemplificar, a gente criou o alinhamento dos personagens da série The Office:\nlibrary(DnDalignment) create_alignment( chaotic_good = \u0026quot;https://www.cheatsheet.com/wp-content/uploads/2021/04/The-Office.jpg\u0026quot;, neutral_good = \u0026quot;https://uploads.jovemnerd.com.br/wp-content/uploads/2021/02/the-office-jim-canta-musica-abertura.jpg\u0026quot;, chaotic_neutral = \u0026quot;https://img.ibxk.com.br/2021/01/20/20133848690155.png\u0026quot;, chaotic_evil = \u0026quot;https://www.looper.com/img/gallery/the-todd-packer-scene-in-the-office-that-went-too-far/l-intro-1620750071.jpg\u0026quot;, lawful_good = \u0026quot;https://static3.srcdn.com/wordpress/wp-content/uploads/2019/11/Phyllis-The-Office.jpg?q=50\u0026amp;fit=crop\u0026amp;w=740\u0026amp;h=370\u0026amp;dpr=1.5\u0026quot;, lawful_evil = \u0026quot;https://i.ytimg.com/vi/BWNhTOd9uXI/maxresdefault.jpg\u0026quot;, neutral_evil = \u0026quot;https://cdn.jwplayer.com/v2/media/46qsexlL/poster.jpg?width=720\u0026quot;, lawful_neutral = \u0026quot;https://uploads.jovemnerd.com.br/wp-content/uploads/2021/01/the-office-dwight-gerenta.jpg\u0026quot;, true_neutral = \u0026quot;https://pbs.twimg.com/media/EXlUzHzXQAEgIZt.png\u0026quot; ) E aí? O que achou das nossas escolhas? Se você discorda, tweet o seu alinhamento e marque a gente no Twitter: Curso-R, William e Fernando.\nAté a próxima!\nSigla em inglês para Role Playing Game que em português significa Jogo de intepretação de papéis. RPGs são jogos parecidos com a encenação de uma peça de teatro, mas com regras como as de Banco Imobiliário para estruturar a interpretação de papéis.↩︎\n","permalink":"https://blog.curso-r.com/posts/2021-06-23-dnd/","tags":["shiny","Dnd","theoffice"],"title":"Faça alinhamentos DnD no R"},{"author":["William"],"categories":["tutoriais"],"contents":" Tooltips são uma ótima maneira de se comunicar com a pessoa utilizando o seu app, pois permitem passar todo tipo de informação extra e não ocupam espaço da UI.\nEssencialmente, elas são textos que só aparecem quando passamos o mouse em cima de algum elemento da tela. Por exemplo, passe o mouse em cima da frase a seguir:\nExistem várias soluções disponíveis para incluirmos tooltips em um aplicativo Shiny, mas a minha preferida atualmente é o pacote tippy.\nAntes de mais nada, instale o pacote:\ninstall.packages(\u0026quot;tippy\u0026quot;) Para colocar uma tooltip em um elemento do seu app, basta usar a função with_tippy().\ntippy::with_tippy( numericInput( \u0026quot;tamanho\u0026quot;, label = \u0026quot;Selecione o tamanho da amostra\u0026quot;, value = 1000, step = 1000 ), tooltip = \u0026quot;Amostra de uma distribuição Normal com média 0 e variância 100.\u0026quot; ) Você também pode aplicar a mesma tooltip a vários elementos utilizando a função tippy_class().\nfluidRow( column( width = 4, div( class = \u0026quot;valuebox-tip\u0026quot;, shinydashboard::valueBoxOutput(\u0026quot;valor_1\u0026quot;, width = 12) ) ), column( width = 4, div( class = \u0026quot;valuebox-tip\u0026quot;, shinydashboard::valueBoxOutput(\u0026quot;valor_2\u0026quot;, width = 12) ) ), column( width = 4, div( class = \u0026quot;valuebox-tip\u0026quot;, shinydashboard::valueBoxOutput(\u0026quot;valor_3\u0026quot;, width = 12) ) ), column( width = 12, plotOutput(\u0026quot;grafico\u0026quot;) ), tippy::tippy_class( \u0026quot;valuebox-tip\u0026quot;, content = \u0026quot;Você precisa importar o css do pacote shinydashboard se quiser usar valueBoxes fora do shinydashboard.\u0026quot; ) ) Você pode customizar a tooltip seguindo os parâmetros da documentação oficial da biblioteca tippy.js.\ntippy::tippy_class( \u0026quot;valuebox-tip\u0026quot;, content = \u0026quot;Você precisa importar o css do pacote shinydashboard se quiser usar valueBoxes fora do shinydashboard.\u0026quot;, arrow = TRUE, placement = \u0026quot;left\u0026quot; ) Os fragmentos de código acima pertencem ao app a seguir. Rode o app para ver as tooltips em funcionamento.\nlibrary(shiny) ui \u0026lt;- fluidPage( shinyWidgets::useShinydashboard(), titlePanel(\u0026quot;Usando tooltips\u0026quot;), sidebarLayout( sidebarPanel( tippy::with_tippy( numericInput( \u0026quot;tamanho\u0026quot;, label = \u0026quot;Selecione o tamanho da amostra\u0026quot;, value = 1000, step = 1000 ), tooltip = \u0026quot;Amostra de uma distribuição Normal com média 0 e variância 100.\u0026quot; ) ), mainPanel( fluidRow( column( width = 4, div( class = \u0026quot;valuebox-tip\u0026quot;, shinydashboard::valueBoxOutput(\u0026quot;valor_1\u0026quot;, width = 12) ) ), column( width = 4, div( class = \u0026quot;valuebox-tip\u0026quot;, shinydashboard::valueBoxOutput(\u0026quot;valor_2\u0026quot;, width = 12) ) ), column( width = 4, div( class = \u0026quot;valuebox-tip\u0026quot;, shinydashboard::valueBoxOutput(\u0026quot;valor_3\u0026quot;, width = 12) ) ), column( width = 12, plotOutput(\u0026quot;grafico\u0026quot;) ), tippy::tippy_class( \u0026quot;valuebox-tip\u0026quot;, content = \u0026quot;Você precisa importar o css do pacote shinydashboard se quiser usar valueBoxes fora do shinydashboard.\u0026quot;, arrow = TRUE, placement = \u0026quot;left\u0026quot; ) ) ) ) ) server \u0026lt;- function(input, output, session) { amostra \u0026lt;- reactive(rnorm(input$tamanho, sd = 10)) output$valor_1 \u0026lt;- shinydashboard::renderValueBox({ shinydashboard::valueBox( value = round(mean(amostra()), 1), subtitle = \u0026quot;Média dos valores\u0026quot;, icon = icon(\u0026quot;info-circle\u0026quot;) ) \\ }) output$valor_2 \u0026lt;- shinydashboard::renderValueBox({ shinydashboard::valueBox( value = round(var(amostra()), 1), subtitle = \u0026quot;Variância dos valores\u0026quot;, icon = icon(\u0026quot;info-circle\u0026quot;) ) }) output$valor_3 \u0026lt;- shinydashboard::renderValueBox({ shinydashboard::valueBox( value = round(sd(amostra()), 1), subtitle = \u0026quot;Desvio-padrão dos valores\u0026quot;, icon = icon(\u0026quot;info-circle\u0026quot;) ) }) output$grafico \u0026lt;- renderPlot(hist(amostra())) } shinyApp(ui, server) Como mostramos no exemplo no começo do post, o você também pode colocar tooltips em documentos HTML gerados a partir do R Markdown. Para isso, utilize a função tippy.\ntippy::tippy(\u0026quot;Passe o mouse aqui!\u0026quot;, tooltip = \u0026quot;Olá! Eu sou uma tooltip :)\u0026quot;) É isso! Dúvidas, sugestões e críticas, mande aqui nos comentários.\nAté a próxima!\n","permalink":"https://blog.curso-r.com/posts/2021-06-22-tippy/","tags":["shiny","tooltips"],"title":"Colocando tooltips no Shiny (e no Rmarkdown)"},{"author":["William"],"categories":["tutoriais"],"contents":" É muito comum termos visualizações no nosso app que demoram para serem geradas. Quando estamos carregando o app, isso pode fazer com que parte da UI fique em branco, parecendo que a página está quebrada ou fazendo com que alguém passe em branco pelo output que tivemos tanto trabalho para fazer. Quando a visualização está sendo recalculada, o padrão do Shiny é deixar a versão anterior acizentada até que a nova apareça, o que pode gerar estranheza e também passar a ideia de que o aplicativo quebrou.\nÉ uma boa prática sempre avisarmos a quem estiver usando o app que alguma coisa está acontecendo por trás das cortinas. Quando a espera é muito longa, devemos sempre que possível explicar o porquê a pessoa está esperando e dar uma estimativa do tempo. Nesses casos, barras de carregamento são a melhor alternativa. Falaremos delas em um outro post.\nQuando a espera não é tão grande (entre 2 e 10 segundos, por exemplo), animações giratórias ou de looping infinito podem ser utilizadas para indicar que algo vai aparecer ali e reduzir um pouco a percepção do tempo de espera.\nNeste post, falaremos de dois pacotes que deixam muito simples a tarefa de incluir essas animações em nossos outputs: o shinycssloaders e o shinyWidgets.\nSe você ainda não tem esses pacotes instalados, ambos estão no CRAN:\ninstall.packages(\u0026quot;shinycssloaders\u0026quot;) install.packages(\u0026quot;shinyWidgets\u0026quot;) O shinycssloaders é um pacote mantido pelo Dean Attali que possui uma única função: withSpinner(). Para colocar a animação de carregamento em uma visualização, basta colocar a função *Output() dentro da função withSpinner()! Sempre que ela estiver sendo calculada, um spinner será mostrado no lugar.\nRode o Shiny app abaixo para ver um exemplo:\nlibrary(shiny) ui \u0026lt;- fluidPage( titlePanel(\u0026quot;Exemplo shinyWidgets::addSpinner\u0026quot;), sidebarLayout( sidebarPanel( selectInput( inputId = \u0026quot;variavel\u0026quot;, label = \u0026quot;Escolha uma variável\u0026quot;, choices = names(mtcars) ) ), mainPanel( shinycssloaders::withSpinner( plotOutput(outputId = \u0026quot;histograma\u0026quot;), type = 4, color = \u0026quot;orange\u0026quot;, size = 2 ) ) ) ) server \u0026lt;- function(input, output, session) { output$histograma \u0026lt;- renderPlot({ Sys.sleep(5) hist(mtcars[[input$variavel]]) }) } shinyApp(ui, server) Além de 8 opções de animações diferentes, que você pode trocar com o argumento type, também é possível ajustar o tamanho, a cor, a cor de fundo e até usar uma imagem própria como animação1.\nVeja aqui um Shiny app que apresenta todas as opções de customização do shinycssloaders.\nO pacote shinyWidgets é mantido pelo pessoal da dreamRs e possui diversos widgets muito úteis (falaremos bastante desse pacote em próximos posts). Adicionamos animações de carregamento utilizando a função addSpinner() e, assim como a função shinycssloards::withSpinner(), basta embrulhar suas funções *Output() com a função addSpinner().\nSão 9 opções de animação, escolhidas por meio do argumento spin. Aqui podemos customizar apenas a cor delas. Rode o app a seguir para ver um exemplo.\nlibrary(shiny) ui \u0026lt;- fluidPage( titlePanel(\u0026quot;Exemplo shinyWidgets::addSpinner\u0026quot;), sidebarLayout( sidebarPanel( selectInput( inputId = \u0026quot;variavel\u0026quot;, label = \u0026quot;Escolha uma variável\u0026quot;, choices = names(mtcars) ) ), mainPanel( shinyWidgets::addSpinner( plotOutput(outputId = \u0026quot;histograma\u0026quot;), spin = \u0026quot;cube\u0026quot;, color = \u0026quot;purple\u0026quot; ) ) ) ) server \u0026lt;- function(input, output, session) { output$histograma \u0026lt;- renderPlot({ Sys.sleep(5) hist(mtcars[[input$variavel]]) }) } shinyApp(ui, server) É isso! Dúvidas, sugestões e críticas, mande aqui nos comentários.\nAté a próxima!\nPode ser uma imagem estática ou GIF.↩︎\n","permalink":"https://blog.curso-r.com/posts/2021-06-14-loaders/","tags":["shiny","widgets"],"title":"Adicionando loaders ao seu Shiny app"},{"author":["William"],"categories":["tutoriais"],"contents":" O meu maior desafio durante o desenvolvimento de um Shiny app complexo é a construção do fluxo de reatividade. Não foram raras as vezes que gastei horas pensando em uma lógica de programação que gerasse o comportamento que eu gostaria ou que simplesmente fizesse o app funcionar.\nAlém de a reatividade ser um conceito abstrato e invisível no código, precisamos não apenas garantir que o app funcione, mas também que funcione adequadamente. Se ao mudar o valor de um slider, por exemplo, a gente espera que um gráfico seja atualizado, dois problemas podem surgir: (1) o gráfico não ser recalculado e (2) o gráfico ser recalculado mais de uma vez.\nOs dois problemas indicam um fluxo de reatividade mal construído. A diferença é que no primeiro caso o app não funciona e no segundo ele funciona de maneira inadequada.\nPara evitar esses problemas, podemos imaginar ou desenhar o fluxo de reatividade para investigar onde está a falha. Essa é uma tarefa simples em apps com poucos inputs e outputs, mas extremamente difícil ou inviável em apps complexos.\nNesses casos, ou mesmo nos casos simples, podemos utilizar o pacote reactlog. Com ele, além de criarmos facilmente o diagrama do fluxo de reatividade de qualquer Shiny app, podemos olhar o que acontece por trás das curtinas da reatividade quando executamos o aplicativo.\nAntes de mais nada, instale o pacote reactlog.\ninstall.packages(\u0026quot;reactlog\u0026quot;) Em seguida, no Console, rode o código abaixo. Isso vai habilitar o reactlog para qualquer app que você rodar na sessão de R atual.\noptions(shiny.reactlog = TRUE) Por fim, rode o seu app e utilize o comando CTRL + F3 (no Mac, command + F3). O seu navegador abrirá uma nova aba com o diagrama de reatividade. Veja o exemplo a seguir. Primeiro temos a UI de um aplicativo que gera o histograma de uma amostra com distribuição normal. O tamanho da amostra é determinado pelo sliderInput. Sempre que o tamanho da amostra muda, o gráfico é recalculado.\nFigura 1: Shiny app que mostra o histograma de uma amostra com distribuição normal. O tamanho da amostra é determinado por um sliderInput. Veja agora o diagrama de reatividade associado a esse app. A forma dos 4 elementos mais a esquerda representa inputs ou valores reativos, a forma do elemento plotObj representa expressões reativas e a forma do elemento output$hist representa os outputs (ou observadores).\nFigura 2: Diagrama de reatividade do app anterior. Parece muito mais complicado do que deveria, né? Acontece que além do input e output, o diagrama também apresenta elementos referentes ao tamanho da janela da pessoa que está utilizando o app. Esses elementos influenciam na imagem produzida para o gráfico dentro do HTML, que é redimensionada a depender do tamanho da tela. Por isso a existência do elemento intermediário plotObj, que guarda as instruções para gerar o gráfico criadas pelo código R. Assim, o código R no servidor não precisa ser rodado novamente para que a imagem do gráfico seja redimensionada.\nPodemos filtrar o diagrama para mostrar apenas o fluxo relacionado aos inputs do aplicativo escrevendo input no campo de busca. Repare também que os comandos no canto superior esquerdo permitem visualizar o fluxo de reatividade das ações realizadas entre a inicialização do app e o momento em que criamos o diagrama (quando pressionamos CTRL + F3).\nFigura 3: Diagrama de reatividade do app anterior. Em resumo, com um diagrama de reatividade em mãos, podemos:\nver os inputs dos quais cada output depende e não depende;\ninvestigar porque o código de um output não é rodado ou roda duas vezes quando acionamos um input do qual ele deveria depender;\nter uma visão menos abstrada do fluxo de reatividade e entender melhor o que acontece quando executamos cada parte do nosso app.\nUm ponto importante: por razões de segurança e performance, nunca habilite o reaclog em ambientes de produção. Quando ele está habilitado, qualquer pessoal utilizando o seu app pode ver pelo menos parte do seu código fonte (que eventualmente pode conter informações sensíveis).\nVocê pode aprender mais sobre o funcionamento do reaclog clicando aqui.\nÉ isso! Dúvidas, sugestões e críticas, mande aqui nos comentários. Até a próxima!\n","permalink":"https://blog.curso-r.com/posts/2021-06-10-reactlog/","tags":["shiny","reatividade","reactlog"],"title":"Diagramas de reatividade no shiny (reactlog)"},{"author":["Fernando"],"categories":["pacotes"],"contents":" Você provavelmente conhece o pacote ggplot2, mas você sabia que também é possível criar gráficos animados sem nenhuma dificuldade? Essa é exatamente a função do pacote gganimate e hoje nós vamos conversar um pouco sobre as animações.\nIdeia geral O pacote gganimate parte de um objeto ggplot2 que pode ser decomposto em vários plots repetidos. A animação que vemos na tela é uma composição a partir de todas essas imagens. As possibilidades de composição são definidas pelas chamadas à funções transition_*, que definem de que modo a sua animação deve ser se comportar entre um gráfico e outro. Você pode conferir a nossa live ou o cheatsheet para ver alguns exemplos.\nO exemplo abaixo usa transition_reveal(data_aplicacao) do pacote gganimate para transformar um ggplot2 comum em várias imagens que representam a curva de pessoas vacinadas ao longo do tempo:\nlibrary(tidyverse) library(bigrquery) conexao_covid \u0026lt;- dbConnect( bigrquery::bigquery(), project = \u0026quot;basedosdados\u0026quot;, dataset = \u0026quot;br_ms_vacinacao_covid19\u0026quot;, billing = \u0026quot;live-curso-r-bd-2\u0026quot; ) conexao_populacao \u0026lt;- dbConnect( bigrquery::bigquery(), project = \u0026quot;basedosdados\u0026quot;, dataset = \u0026quot;br_ibge_populacao\u0026quot;, billing = \u0026quot;live-curso-r-bd-2\u0026quot; ) vacinacao_base \u0026lt;- tbl(conexao_covid, \u0026quot;microdados_vacinacao\u0026quot;) %\u0026gt;% count(sigla_uf, data_aplicacao, dose) %\u0026gt;% collect() %\u0026gt;% mutate( data_aplicacao = as.Date(data_aplicacao) ) populacao_estados \u0026lt;- tbl(conexao_populacao, \u0026quot;municipio\u0026quot;) %\u0026gt;% filter(ano == 2020) %\u0026gt;% collect() %\u0026gt;% mutate( id_estado = str_sub(id_municipio, 1, 2), sigla_uf = case_when( id_estado == \u0026quot;12\u0026quot; ~ \u0026quot;AC\u0026quot;, id_estado == \u0026quot;27\u0026quot; ~ \u0026quot;AL\u0026quot;, id_estado == \u0026quot;16\u0026quot; ~ \u0026quot;AP\u0026quot;, id_estado == \u0026quot;13\u0026quot; ~ \u0026quot;AM\u0026quot;, id_estado == \u0026quot;29\u0026quot; ~ \u0026quot;BA\u0026quot;, id_estado == \u0026quot;53\u0026quot; ~ \u0026quot;DF\u0026quot;, id_estado == \u0026quot;23\u0026quot; ~ \u0026quot;CE\u0026quot;, id_estado == \u0026quot;32\u0026quot; ~ \u0026quot;ES\u0026quot;, id_estado == \u0026quot;52\u0026quot; ~ \u0026quot;GO\u0026quot;, id_estado == \u0026quot;21\u0026quot; ~ \u0026quot;MA\u0026quot;, id_estado == \u0026quot;51\u0026quot; ~ \u0026quot;MT\u0026quot;, id_estado == \u0026quot;50\u0026quot; ~ \u0026quot;MS\u0026quot;, id_estado == \u0026quot;31\u0026quot; ~ \u0026quot;MG\u0026quot;, id_estado == \u0026quot;15\u0026quot; ~ \u0026quot;PA\u0026quot;, id_estado == \u0026quot;25\u0026quot; ~ \u0026quot;PB\u0026quot;, id_estado == \u0026quot;41\u0026quot; ~ \u0026quot;PR\u0026quot;, id_estado == \u0026quot;26\u0026quot; ~ \u0026quot;PE\u0026quot;, id_estado == \u0026quot;22\u0026quot; ~ \u0026quot;PI\u0026quot;, id_estado == \u0026quot;33\u0026quot; ~ \u0026quot;RJ\u0026quot;, id_estado == \u0026quot;24\u0026quot; ~ \u0026quot;RN\u0026quot;, id_estado == \u0026quot;43\u0026quot; ~ \u0026quot;RS\u0026quot;, id_estado == \u0026quot;11\u0026quot; ~ \u0026quot;RO\u0026quot;, id_estado == \u0026quot;14\u0026quot; ~ \u0026quot;RR\u0026quot;, id_estado == \u0026quot;42\u0026quot; ~ \u0026quot;SC\u0026quot;, id_estado == \u0026quot;35\u0026quot; ~ \u0026quot;SP\u0026quot;, id_estado == \u0026quot;28\u0026quot; ~ \u0026quot;SE\u0026quot;, id_estado == \u0026quot;17\u0026quot; ~ \u0026quot;TO\u0026quot; ) ) %\u0026gt;% group_by(sigla_uf) %\u0026gt;% summarise( populacao = sum(populacao) ) vacinacao_base_por_estado \u0026lt;- vacinacao_base %\u0026gt;% ungroup() %\u0026gt;% mutate( regiao = case_when( sigla_uf %in% c(\u0026quot;SP\u0026quot;, \u0026quot;RJ\u0026quot;, \u0026quot;MG\u0026quot;, \u0026quot;ES\u0026quot;) ~ \u0026quot;Sudeste\u0026quot;, sigla_uf %in% c(\u0026quot;SC\u0026quot;, \u0026quot;PR\u0026quot;, \u0026quot;RS\u0026quot;) ~ \u0026quot;Sul\u0026quot;, sigla_uf %in% c(\u0026quot;MT\u0026quot;, \u0026quot;DF\u0026quot;, \u0026quot;GO\u0026quot;, \u0026quot;MS\u0026quot;) ~ \u0026quot;Centro-Oeste\u0026quot;, sigla_uf %in% c(\u0026quot;AC\u0026quot;, \u0026quot;AM\u0026quot;, \u0026quot;RO\u0026quot;, \u0026quot;RR\u0026quot;, \u0026quot;PA\u0026quot;, \u0026quot;AP\u0026quot;, \u0026quot;TO\u0026quot;) ~ \u0026quot;Norte\u0026quot;, TRUE ~ \u0026quot;Nordeste\u0026quot; ) ) %\u0026gt;% filter(dose == \u0026quot;2\u0026quot;) %\u0026gt;% group_by(regiao, sigla_uf) %\u0026gt;% arrange(regiao, sigla_uf, data_aplicacao) %\u0026gt;% mutate( n_acum = cumsum(n), ) %\u0026gt;% filter(data_aplicacao \u0026gt;= as.Date(\u0026quot;2021-01-01\u0026quot;)) %\u0026gt;% left_join( populacao_estados ) %\u0026gt;% mutate( percentual_vacinado = n_acum/populacao ) populacao_regiao \u0026lt;- vacinacao_base_por_estado %\u0026gt;% group_by(regiao) %\u0026gt;% summarise( populacao = sum(unique(populacao)) ) vacinacao_base_por_regiao \u0026lt;- vacinacao_base %\u0026gt;% ungroup() %\u0026gt;% mutate( regiao = case_when( sigla_uf %in% c(\u0026quot;SP\u0026quot;, \u0026quot;RJ\u0026quot;, \u0026quot;MG\u0026quot;, \u0026quot;ES\u0026quot;) ~ \u0026quot;Sudeste\u0026quot;, sigla_uf %in% c(\u0026quot;SC\u0026quot;, \u0026quot;PR\u0026quot;, \u0026quot;RS\u0026quot;) ~ \u0026quot;Sul\u0026quot;, sigla_uf %in% c(\u0026quot;MT\u0026quot;, \u0026quot;DF\u0026quot;, \u0026quot;GO\u0026quot;, \u0026quot;MS\u0026quot;) ~ \u0026quot;Centro-Oeste\u0026quot;, sigla_uf %in% c(\u0026quot;AC\u0026quot;, \u0026quot;AM\u0026quot;, \u0026quot;RO\u0026quot;, \u0026quot;RR\u0026quot;, \u0026quot;PA\u0026quot;, \u0026quot;AP\u0026quot;, \u0026quot;TO\u0026quot;) ~ \u0026quot;Norte\u0026quot;, TRUE ~ \u0026quot;Nordeste\u0026quot; ) ) %\u0026gt;% filter(dose == \u0026quot;2\u0026quot;) %\u0026gt;% group_by(regiao, data_aplicacao) %\u0026gt;% summarise( n = sum(n) ) %\u0026gt;% group_by(regiao) %\u0026gt;% arrange(regiao, data_aplicacao) %\u0026gt;% mutate( n_acum = cumsum(n), ) %\u0026gt;% filter(data_aplicacao \u0026gt;= as.Date(\u0026quot;2021-02-01\u0026quot;)) %\u0026gt;% left_join( populacao_regiao ) %\u0026gt;% mutate( percentual_vacinado = n_acum/populacao ) library(gganimate) grafico_por_regiao \u0026lt;- vacinacao_base_por_regiao %\u0026gt;% #filter(regiao == \u0026quot;Sudeste\u0026quot;) %\u0026gt;% #filter(data_aplicacao \u0026lt;= as.Date(\u0026quot;2021-04-15\u0026quot;)) %\u0026gt;% ungroup() %\u0026gt;% arrange(data_aplicacao) %\u0026gt;% #mutate(data_aplicacao = as.numeric(data_aplicacao)) %\u0026gt;% ggplot(aes(x = data_aplicacao, y = percentual_vacinado, color = regiao)) + #geom_col() + #geom_line(size = 1.2) + geom_point(size = 6) + transition_reveal(data_aplicacao) + scale_color_viridis_d() + theme_bw(20) + shadow_wake(wake_length = 0.5, size = 3) + ease_aes(\u0026quot;cubic-in\u0026quot;) + labs(x = \u0026quot;Data de referência\u0026quot;, y= \u0026quot;% da população vacinado com a 2a dose\u0026quot;, titulo = \u0026quot;Evolução da vacinação contra COVID-19 no Brasil por região\u0026quot;, caption = \u0026quot;Fonte: openDataSus/Base dos dados\u0026quot;) animate(grafico_por_regiao, height = 600, width =800) Gostou? Quer saber mais? Confira a nossa live!\nSe você quiser aprender um pouco mais sobre RMarkdown e Visualização de Dados em geral, dê uma olhada no nosso curso Relatórios e visualização de dados e aproveite!\n","permalink":"https://blog.curso-r.com/posts/2021-06-09-gganimate/","tags":["ggplot2"],"title":"Animando gráficos feitos em ggplot2: o pacote gganimate"},{"author":["Fernando"],"categories":["tutoriais"],"contents":" A manipulação de dados, ou data wrangling, consome a maior parte do tempo de quem trabalha com Ciência de Dados. Provavelmente por isso existem vários frameworks diferentes para executar comandos dessa categoria. Em R, as duas principais bibliotecas para isso são o dplyr e o data.table. Existem muitas diferenças entre esses dois pacotes e hoje vamos explorar o pacote data.table e também a ponte entre os dois, chamado dtplyr.\nPor que existem dois pacotes? A diferença entre os dois pacotes pode ser resumida em dois tópicos:\nVelocidade e uso de memória do código: o data.table em geral é mais veloz. De tempos em tempos são feitas comparações de performance entre dplyr e data.table e o pacote data.table roda em menos tempo em vários cenários, como pode ser visto aqui e aqui.\nSintaxe: o dplyr é um dos pacotes principais do tidyverse, data.table é mais fiel à sintaxe do R puro. dplyr foi feito para ser usado com pipes, imita a sintaxe dos verbos de SQL, já o data.table traz novas features à sintaxe comum do R.\nAntes de explorar o data.table e o dtplyr, nenhum dos dois pacotes é absolutamente melhor do que o outro. Embora o data.table seja mais veloz e consuma menos memória que o dplyr, os códigos escritos nesse framework exigem uma sintaxe específica. Pelo tempo, o data.table tem uma vantagem, mas uma sintaxe dplyr pode ser compreendida por interpretadores de SQL ou até mesmo spark. Além disso, códigos dplyr são compatíveis com outros frameworks tidy, tal como o pacote tidyr, o pacote purrr etc. Essas diferenças podem diminuir o valor da eficiência do data.table: se o computador gasta menos tempo executando um comando, mas você gasta mais o seu tempo (ou o de outras pessoas), a eficiência valeu a pena? É importante considerar o contexto.\nO pacote data.table e o pacote dtplyr A ideia básica do data.table é incluir muitas funcionalidades ao operador [. Um data.frame transformado em um data.table passa a funcionar da seguinte maneira:\nlibrary(data.table) DTvoos \u0026lt;- data.table(dados::voos) DTvoos[ mes == \u0026quot;9\u0026quot;, # antes da primeira vírgula podemos fazer filtros .(valor = mean(atraso_saida, na.rm = TRUE)), # depois da segunda vírgula podemos manipular colunas: # aqui podemos fazer seleções e também podemos criar # novas colunas. a seleção funciona que nem o \u0026quot;[\u0026quot; # normal. para criar colunas nomeadas, precisamos # usar a notação \u0026quot;.()\u0026quot; # # na verdade, no segundo elemento podemos colocar também uma função que retorne uma tibble # inclusive existem vários helpers para usar aqui, como o .N, que conta o numero de linhas # da tibble agrupada, .SD, que permite fazermos manipulações mais complexas # nos pedacos da base etc by = .(origem) # na terceira vírgula, podemos fazer contas agrupadas usando o parâmetro \u0026quot;by\u0026quot; # que fica disponível uma vez que fazemos \u0026quot;library(data.table)\u0026quot;. Para escolher # as funções que serão usadas no agrupamento sem aspas, precisamos usar a # notação ] ## origem valor ## 1: JFK 6.635776 ## 2: EWR 7.290954 ## 3: LGA 6.207439 Embora a notação do .() seja bem diferente do que normalmente fazemos em dplyr, a lógica do pacote é similar: as operacoes são separadas em filtros, manipulações e agrupamentos. As manipulações nas colunas podem ser bastante variadas e isso é um ponto de divergência importante entre o dplyr e o data.table, mas também é verdade que existem várias similaridades. Por isso existe o pacote dtplyr:\nTraduzindo dplyr para data.table: o pacote dtplyr A lógica aqui é simples: como o data.table e o dplyr são minimamente parecidos, muitos códigos dplyr podem ser traduzidor para data.table e as computações podem ser feitas só apos a tradução. Dê uma olhada no código abaixo, que os dados que vieram de base e a chamada que foi feita usando o data.table:\nlibrary(dtplyr) library(magrittr) library(dplyr) library(tidyr) voos2 \u0026lt;- lazy_dt(dados::voos) # esse aqui é o pulo do gato. um `lazy_dt` vai sempre traduzir # os verbos do dplyr para comandos `DT` voos2 %\u0026gt;% group_by(origem) %\u0026gt;% summarise( valor = mean(atraso_saida, na.rm = TRUE) ) ## Source: local data table [3 x 2] ## Call: `_DT1`[, .(valor = mean(atraso_saida, na.rm = TRUE)), keyby = .(origem)] ## ## origem valor ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 EWR 15.1 ## 2 JFK 12.1 ## 3 LGA 10.3 ## ## # Use as.data.table()/as.data.frame()/as_tibble() to access results Claro que existem situações em que a tradução não será possível, principalmente quando tentarmos usar funções do pacote tidyr que não tem suporte no dtplyr. Por outro lado, para grande parte das manipulações simples que fazemos com dplyr, o pacote dtplyr vai dar conta.\nComparações e considerações finais Então, o que a gente deve usar corriqueiramente? Essa pergunta evidentemente não tem uma resposta única, pois como já observamos aqui a ferramenta melhor para resolver um problema depende do contexto em que você está. Entretanto, temos algumas vantagens em usar o dtplyr quando é possível, pois ele é agnóstico. Além disso, ele tem vantagens como deixar explícito na sintaxe a ordem em que os comandos de um data.table são executados, o que não acontece no próprio data.table, que depende de conhecimento sobre o funcionamento interno do código.\nDe toda forma, existe espaço pra todo mundo e certamente quem programa em R se beneficia de conhecer os dois pacotes!\nGostou? Quer saber mais? Se você quiser aprender um pouco mais sobre manipulação de dados com R, dê uma olhada no nosso curso R para Ciência de Dados I e aproveite!\nCaso você tenha dúvidas, entre em contato com a gente pelos comentários aqui embaixo, pelo nosso Discourse ou pelo e-mail contato@curso-r.com.\n","permalink":"https://blog.curso-r.com/posts/2021-06-01-data-table/","tags":["faxina","manipulação","pacotes"],"title":"Manipulação de dados usando o pacote data.table"},{"author":["Fernando"],"categories":["tutoriais"],"contents":" Todo mundo que trabalha com Ciência de Dados uma hora ou outra vai se ver mexendo com informações públicas, direta ou indiretamente. O governo é um grande repositório de dados, por isso é muito provável que uma hora ou outra algum dado disponibilizado publicamente seja relevante para você.\nMesmo sendo tão importante, também é muito comum que as bases de dados públicas demandem grandes trabalhos de limpeza e organização, assim como todas as bases de dados, públicas ou não. As dificuldades quando se mexe com bases de dados públicas muitas vezes acontecem pois elas agregam informações de toda a população e, por isso, ficam muito grandes. Soma-se a isso o fato de existirem bases para os mais variados fins, o que não necessariamente otimiza o uso para quem analisará esses dados.\nPara simplificar a nossa vida, o pessoal do Base dos Dados ou BD+, para quem tem intimidade, faz o hercúleo trabalho de organizar, limpar e disponibilizar mais de 900 (!!!!) bases de dados gratuitamente para toda a comunidade! Como se não bastasse, a galera do BD+ ainda construiu um pacote de R para facilitar o acesso da nossa comunidade ao seu repositório de dados.\nNo post de hoje, vamos explorar o pacote basedosdados e também vamos ver como aproveitar tudo que está disponível no BigQuery do BD+ usando a sintaxe usual do dplyr.\nO pacote basedosdados Os dados do BD+ são dispobinibilizados em uma instância do BigQuery, um serviço da Google Cloud Platform que possibilita a construção de um data warehouse sem servidor, escalável e de alto desempenho. As consultas à plataforma podem ser realizados em ANSI SQL (uma linguagem de consultas segundo o padrão americano). O pacote basedosdados oferece algumas funções auxiliares para simplificar o processo de submissão de uma query.\nAntes de explorar as funções do pacote, é necessário que você crie um projeto na GCP, pois o BigQuery do BD+ é apenas o repositório dos dados: a execução das queries propriamente ditas é realizada e cobrada na ponta que submeteu as queries. Ou seja, tipicamente você criaria uma query para ser executada tomando como referência o BigQuery do BD+, mas os custos de execução ficam atrelados à sua própria conta GCP. Entretanto, isso não é motivo para preocupação: a GCP atualmente disponibiliza gratuitamente a manipulação de 1 TB de dados/mês e o armazenamento de 100 GB para qualquer pessoa.\nPara seguir os próximos passos e aprender mais sobre a GCP acesse https://console.cloud.google.com/.\nAcessando o BD+ com o pacote basedosdados O pacote basedosdados tem quatro funções: read_sql, download, set_billing_id e get_billing_id. Os ingredientes principais para usar essas funções são o billing_id, que é o nome da sua conta criada na GCP e query, uma consulta escrita na linguagem SQL própria do BigQuery.\nNo exemplo abaixo vamos baixar os PIBs de alguns anos lá da BD+:\nlibrary(basedosdados) # aqui você define o seu projeto billing_id basedosdados::set_billing_id(\u0026quot;seu_billing_id\u0026quot;) # aqui definimos a nossa query query \u0026lt;- \u0026quot;SELECT pib.id_municipio, pop.ano, pib.PIB / pop.populacao * 1000 as pib_per_capita FROM `basedosdados.br_ibge_pib.municipios` as pib JOIN `basedosdados.br_ibge_populacao.municipios` as pop ON pib.id_municipio = pop.id_municipio LIMIT 20 \u0026quot; # aqui carregamos o arquivo para o R pib_per_capita \u0026lt;- read_sql(query) # também seria possível baixar o resultado e salvá-lo em um arquivo .csv # download(query, \u0026quot;pib_per_capita.csv\u0026quot;) Note que, uma vez que o código foi executado, aparece uma mensagem informando uma quantidade de dados que foi consumida na sua conta. Até que o montante de dados consumido pela sua conta ultrapasse os 100TB, você não vai precisar fazer nenhum pagamento.\nAcessando o BD+ com o pacote bigrquery e dplyr Com o pacote bigrquery, que faz parte da iniciativa R-DBI de integração de R e Bancos de Dados, é possível acessar uma instância BigQuery como se acessa qualquer outra base dados e inclusive aproveitar a conveniência do pacote dbplyr para escrever código dplyr que será traduzido diretamente para BigQuery.\nDê uma olhada no código abaixo, que calcula o IDEB médio por município:\nlibrary(bigrquery) library(tidyverse) conexao_ideb \u0026lt;- dbConnect( bigrquery::bigquery(), project = \u0026quot;basedosdados\u0026quot;, dataset = \u0026quot;br_inep_ideb\u0026quot;, billing = \u0026quot;seu_billing_id\u0026quot; ) escola \u0026lt;- tbl(conexao_ideb, \u0026quot;escola\u0026quot;) %\u0026gt;% group_by(ano, estado_abrev, municipio) %\u0026gt;% summarise(ideb = mean(ideb, na.rm = TRUE)) %\u0026gt;% # até aqui foi só dplyr básico ungroup() %\u0026gt;% # a mágica acontece nesse collect! collect() Show de bola, né? Então vai agora apoiar a galera do BD+ pra garantir que a gente continue contando com uma iniciativa tão legal na nossa comunidade!\nGostou? Quer saber mais? Acesse o site do BD+ e todas as suas redes sociais. Veja também a nossa live se quiser ver outros códigos parecidos com esses que montamos aqui.\nSe você quiser aprender um pouco mais sobre manipulação de dados com R, dê uma olhada no nosso curso R para Ciência de Dados I e aproveite!\nCaso você tenha dúvidas, entre em contato com a gente pelos comentários aqui embaixo, pelo nosso Discourse ou pelo e-mail contato@curso-r.com.\n","permalink":"https://blog.curso-r.com/posts/2021-05-28-basedosdados/","tags":["web scraping","faxina","purrr","rvest"],"title":"Acessando bases públicas em R: o pacote basedosdados"},{"author":["Beatriz Milz"],"categories":["tutoriais"],"contents":" Alguns animes tem muitos fillers, que são episódios que não se baseiam na história original, e não agregam na história principal. Muitas vezes parece uma encheção de linguiça mesmo, contendo diversos episódios em sequência que não contribuem com a história que estamos acompanhando.\nNaruto é um caso onde existem muuuitos fillers! Então eu pulo esses episódios sem dó, usando a Lista de episódios de Naruto Shippuden disponível no Wikipédia.\nO objetivo deste post é apresentar uma forma de importar estes dados para o R, para que a gente não precise acessar toda hora a página do wikipedia consiga fazer uma visualização da distribuição de episódios fillers ao longo dos 500 episódios do anime! Vamos lá, dattebayo!\nColeta de dados com Web Scraping Vamos usar uma técnica chamada web scraping, que é baseada em raspar informações de páginas de internet. Primeiro, precisamos buscar o código HTML referente à página do Wikipédia que queremos raspar as informações sobre os episódios de Naruto Shippuden:\n# Criando um objeto chamado \u0026quot;url_wiki\u0026quot;, que contém a url da página do wikipedia url_wiki \u0026lt;- \u0026quot;https://pt.wikipedia.org/wiki/Lista_de_epis%C3%B3dios_de_Naruto_Shippuden\u0026quot; # Lendo o código html referente à página do wikipedia # E salvando em um objeto chamado wikipedia_html wikipedia_html \u0026lt;- rvest::read_html(url_wiki) O que o objeto wikipedia_html contém?\nwikipedia_html ## {html_document} ## \u0026lt;html class=\u0026quot;client-nojs\u0026quot; lang=\u0026quot;pt\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt; ## [1] \u0026lt;head\u0026gt;\\n\u0026lt;meta http-equiv=\u0026quot;Content-Type\u0026quot; content=\u0026quot;text/html; charset=UTF-8 ... ## [2] \u0026lt;body class=\u0026quot;skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr ... Esse objeto apresenta o código HTML referente à página que queremos raspar!\nBuscando informações sobre a primeira temporada Agora precisamos descobrir como acessar as informações que queremos. Olhando a página, podemos ver que os dados de cada temporada estão apresentados em uma tabela diferente. Vamos tentar primeiramente buscar as informações para a tabela referente à primeira temporada, e quando este código estiver funcionando bem, podemos adaptar o código para buscar os dados das outras temporadas!\nEstou utilizando o navegador Google Chrome, e esse navegador tem uma opção que ajuda muito a explorar o código: o inspetor de elementos! Para acessar essa ferramenta, você pode usar o atalho Ctrl + Shift + C, ou clicando no menu superior: “Visualizar” \u0026gt; “Desenvolvedor” \u0026gt; “Inspecionar elementos”. Caso você utilize outro navegador, recomendo que procure o equivalente ao inspetor de elementos deste navegador.\nAo acessar o inspetor de elementos, podemos navegar com o cursor e conseguir visualizar o código equivalente a essa parte da página. Uma forma de localizar conteúdos dentro do HTMl é utilizando o XPath. Na imagem a seguir, é possível ver que selecionei a tabela referente à primeira temporada, e ao apertar o código na área Elements com o botão direito do mouse, é possível copiar o XPath navegando em “Copy” \u0026gt; “Copy XPath”:\nO XPath copiado é: //*[@id=\"mw-content-text\"]/div[1]/table[4]. Com isso, podemos acessar os conteúdos da tabela:\n# Carregando o pipe do pacote magrittr library(magrittr, include.only = \u0026quot;%\u0026gt;%\u0026quot;) # Podemos buscar o conteúdo com o xpath, # e as tabelas que estão presentescom a função html_table() tabela \u0026lt;- \u0026quot;https://pt.wikipedia.org/wiki/Lista_de_epis%C3%B3dios_de_Naruto_Shippuden\u0026quot; %\u0026gt;% # ler o html rvest::read_html() %\u0026gt;% # buscar o xpath rvest::html_node(xpath = \u0026#39;//*[@id=\u0026quot;mw-content-text\u0026quot;]/div[1]/table[4]\u0026#39;) %\u0026gt;% # ler a tabela rvest::html_table() Vamos observar essa tabela referente à primeira temporada: ela contém algumas colunas repetidas, e existe mais de uma linha com informações para o mesmo episódio. Será necessário fazer uma leve faxina de dados antes de analisar os dados!\ndplyr::glimpse(tabela) ## Rows: 90 ## Columns: 5 ## $ Nº \u0026lt;int\u0026gt; 1, 1, NA, 2, 2, NA, 3, 3, NA, 4, 4, NA, 5, 5,… ## $ `Título original` \u0026lt;chr\u0026gt; \u0026quot;Volta para Casa\u0026quot;, \u0026quot;帰郷\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;A Akatsuki se… ## $ `Título original` \u0026lt;chr\u0026gt; \u0026quot;Volta para Casa\u0026quot;, \u0026quot;Kikyō\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;A Akatsuki s… ## $ `Título(s) em português` \u0026lt;chr\u0026gt; \u0026quot;Voltando Para Casa\u0026quot;, \u0026quot;Voltando Para Casa\u0026quot;, \u0026quot;… ## $ `Data de estreia` \u0026lt;chr\u0026gt; \u0026quot;15 de Fevereiro de 2007\u0026quot;, \u0026quot;15 de Fevereiro d… Antes de avançarmos, vamos fazer uma pré-faxina leve: adicionar o número da temporada na base, e transformar o número do episódio em texto (já que a informação se é filler ou não aparece nessa coluna, mas nem todas as temporadas tem fillers - assim padronizamos e evitamos erros posteriormente).\ntabela %\u0026gt;% # limpar os nomes das variáveis janitor::clean_names() %\u0026gt;% # adicionando o número da temporada dplyr::mutate(numero_temporada = 1, # transforma a variável `no` em texto no = as.character(no)) %\u0026gt;% dplyr::glimpse() ## Rows: 90 ## Columns: 6 ## $ no \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, NA, \u0026quot;2\u0026quot;, \u0026quot;2\u0026quot;, NA, \u0026quot;3\u0026quot;, \u0026quot;3\u0026quot;, NA, \u0026quot;4\u0026quot;, \u0026quot;… ## $ titulo_original \u0026lt;chr\u0026gt; \u0026quot;Volta para Casa\u0026quot;, \u0026quot;帰郷\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;A Akatsuki se Mo… ## $ titulo_original_2 \u0026lt;chr\u0026gt; \u0026quot;Volta para Casa\u0026quot;, \u0026quot;Kikyō\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;A Akatsuki se M… ## $ titulo_s_em_portugues \u0026lt;chr\u0026gt; \u0026quot;Voltando Para Casa\u0026quot;, \u0026quot;Voltando Para Casa\u0026quot;, \u0026quot;\u0026quot;, … ## $ data_de_estreia \u0026lt;chr\u0026gt; \u0026quot;15 de Fevereiro de 2007\u0026quot;, \u0026quot;15 de Fevereiro de 2… ## $ numero_temporada \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … Já está um pouco mais claro o que a base contém, né?\nBuscando informações de todas as temporadas! Na etapa anterior, conseguimos obter a tabela da primeira temporada. Podemos criar uma função para buscar as informações de outras temporadas também!\nOlhando as tabelas das outras temporadas, é possível observar que o XPath segue o mesmo padrão, alterando apenas o número ao final. Isso será um argumento na função, chamado n_tabela. O outro argumento que é interessante adicionar na função é o número da temporada: numero_temporada.\nbuscar_tabela \u0026lt;- function(n_tabela, numero_temporada) { # link da página \u0026quot;https://pt.wikipedia.org/wiki/Lista_de_epis%C3%B3dios_de_Naruto_Shippuden\u0026quot; %\u0026gt;% # lê o htmo rvest::read_html() %\u0026gt;% # procura pelo xpath rvest::html_node(xpath = glue::glue(\u0026#39;//*[@id=\u0026quot;mw-content-text\u0026quot;]/div[1]/table[{n_tabela}]\u0026#39;)) %\u0026gt;% # lê a tabela rvest::html_table() %\u0026gt;% # limpar os nomes das variáveis janitor::clean_names() %\u0026gt;% # adicionando o numero da temporada como uma variavel # e padronizando o numero do episodio para texto dplyr::mutate(numero_temporada, no = as.character(no)) } Os números ao final do XPath se iniciam no número 4, e são incrementados de 2 em 2. Portanto, neste caso, os XPath estão organizados dessa forma:\nTemporada 1: //*[@id=\"mw-content-text\"]/div[1]/table[4] Temporada 2: //*[@id=\"mw-content-text\"]/div[1]/table[6] Temporada 3: //*[@id=\"mw-content-text\"]/div[1]/table[8] …… Temporada 20: //*[@id=\"mw-content-text\"]/div[1]/table[42] Podemos criar uma sequência com a função seq(), para para buscar todas as tabelas de uma vez, sendo que a sequência deve começar no número 4, e incrementar de 2 em 2, até o número 42, formando um vetor de 20 números (referente às 20 temporadas).\nPodemos usar a função purrr::map2_dfr() para aplicar a função buscar_tabela() para buscar as informações das tabelas de todas as temporadas, e unir em uma única base.\ntabela_bruta \u0026lt;- purrr::map2_dfr(.x = seq(from = 4, to = 42, by = 2), # A sequencia começa em 4 e vai até 42, contando de 2 em 2. # assim teremos um vetor com 20 números, referente às # 20 temporadas! Isso será usado no primeiro argumento # da função buscar_tabela, ou seja, # no argumento `n_tabela`. .y = 1:20, # isso será usado no segundo argumento # da funcao buscar_tabela, ou seja, # no argumento `numero_temporada`. .f = buscar_tabela # função que queremos utilizar ) Faxina Conseguimos baixar tudo e salvar em uma única base: tabela_bruta, porém essa base precisa ser limpa. Vamos tentar descobrir os problemas? A função dplyr::glimpse() é útil para dar uma olhada na base:\ndplyr::glimpse(tabela_bruta) ## Rows: 1,500 ## Columns: 6 ## $ no \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, NA, \u0026quot;2\u0026quot;, \u0026quot;2\u0026quot;, NA, \u0026quot;3\u0026quot;, \u0026quot;3\u0026quot;, NA, \u0026quot;4\u0026quot;, \u0026quot;… ## $ titulo_original \u0026lt;chr\u0026gt; \u0026quot;Volta para Casa\u0026quot;, \u0026quot;帰郷\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;A Akatsuki se Mo… ## $ titulo_original_2 \u0026lt;chr\u0026gt; \u0026quot;Volta para Casa\u0026quot;, \u0026quot;Kikyō\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;A Akatsuki se M… ## $ titulo_s_em_portugues \u0026lt;chr\u0026gt; \u0026quot;Voltando Para Casa\u0026quot;, \u0026quot;Voltando Para Casa\u0026quot;, \u0026quot;\u0026quot;, … ## $ data_de_estreia \u0026lt;chr\u0026gt; \u0026quot;15 de Fevereiro de 2007\u0026quot;, \u0026quot;15 de Fevereiro de 2… ## $ numero_temporada \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … Vamos então listar os problemas para arrumar e deixar a base tidy:\nTem variáveis que estão duplicadas, como o título do episódio;\nA data de estreia do episódio está como tipo texto;\nA informação se o episódio é filler está contido na variável do número do episódio;\nQueremos apenas uma linha por episódio;\nVamos então fazer uma faxina nessa base, e deixar ela tidy! Vamos começar tentando resolver os itens 1 à 3:\ntabela_faxinando \u0026lt;- tabela_bruta %\u0026gt;% # separar a coluna `no` em: `numero_episodio` e `tipo_episodio` tidyr::separate(no, into = c(\u0026quot;numero_episodio\u0026quot;, \u0026quot;tipo_episodio\u0026quot;), \u0026quot;\\\\(\u0026quot;, extra = \u0026quot;merge\u0026quot;) %\u0026gt;% dplyr::mutate( # transformar a variavel `numero_episodio` em tipo numérico numero_episodio = readr::parse_number(numero_episodio), # remover o parenteses desnecessário na variável tipo_episodio tipo_episodio = stringr::str_replace_all(tipo_episodio, \u0026quot;\\\\)\u0026quot;, \u0026quot;\u0026quot;), # Na variavel `tipo_episodio`, quando está vazio é porque faz # parte da história principal (é chamado de Canônico). # Então vamos usar dplyr::case_when() para arrumar essa variável. tipo_episodio = dplyr::case_when( is.na(tipo_episodio) ~ \u0026quot;Canon\u0026quot;, tipo_episodio == \u0026quot;½filler\u0026quot; ~ \u0026quot;Semi-filler\u0026quot;, TRUE ~ tipo_episodio ), # transformar a variavel `data_de_estreia` em tipo data data_de_estreia = readr::parse_date( data_de_estreia, format = \u0026quot;%d de %B de %Y\u0026quot;, locale = readr::locale(\u0026quot;pt\u0026quot;) ) ) %\u0026gt;% # remover colunas repetidas do titulo do episodio dplyr::select(-titulo_original, -titulo_original_2) %\u0026gt;% # renomeia para deixar o nome da variável mais claro dplyr::rename(\u0026quot;titulo_episodio\u0026quot; = titulo_s_em_portugues) %\u0026gt;% # move a coluna numero_temporada para o inicio dplyr::relocate(numero_temporada, .before = numero_episodio) Arrumamos algumas coisas já, e a tabela está bem melhor: não tem mais variáveis duplicadas, os nomes das variáveis estão padronizados, as classes das variáveis também estão adequadas:\ndplyr::glimpse(tabela_faxinando) ## Rows: 1,500 ## Columns: 5 ## $ numero_temporada \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ numero_episodio \u0026lt;dbl\u0026gt; 1, 1, NA, 2, 2, NA, 3, 3, NA, 4, 4, NA, 5, 5, NA, 6, … ## $ tipo_episodio \u0026lt;chr\u0026gt; \u0026quot;Canon\u0026quot;, \u0026quot;Canon\u0026quot;, \u0026quot;Canon\u0026quot;, \u0026quot;Canon\u0026quot;, \u0026quot;Canon\u0026quot;, \u0026quot;Canon\u0026quot;,… ## $ titulo_episodio \u0026lt;chr\u0026gt; \u0026quot;Voltando Para Casa\u0026quot;, \u0026quot;Voltando Para Casa\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;Os A… ## $ data_de_estreia \u0026lt;date\u0026gt; 2007-02-15, 2007-02-15, NA, 2007-02-15, 2007-02-15, … Agora vamos arrumar algo muito importante: queremos que cada linha represente um episódio único.\nepisodios_naruto_shippuden \u0026lt;- tabela_faxinando %\u0026gt;% # remover os NA`s presentes na coluna `numero_episodio` tidyr::drop_na(numero_episodio) %\u0026gt;% # aqui teremos duas linhas por episódio, sendo que elas estão repetidas. # a função dplyr::distinct() permitirá descartar as linhas repetidas dplyr::distinct() Pronto! Agora temos na tabela apenas uma linha por episódio. Podemos verificar isso com o dplyr::glimpse() novamente: a série tem 500 episódios, e a base gerada contém 500 linhas.\ndplyr::glimpse(episodios_naruto_shippuden) ## Rows: 500 ## Columns: 5 ## $ numero_temporada \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ numero_episodio \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16… ## $ tipo_episodio \u0026lt;chr\u0026gt; \u0026quot;Canon\u0026quot;, \u0026quot;Canon\u0026quot;, \u0026quot;Canon\u0026quot;, \u0026quot;Canon\u0026quot;, \u0026quot;Canon\u0026quot;, \u0026quot;Canon\u0026quot;,… ## $ titulo_episodio \u0026lt;chr\u0026gt; \u0026quot;Voltando Para Casa\u0026quot;, \u0026quot;Os Akatsuki Entram em Ação\u0026quot;, \u0026quot;… ## $ data_de_estreia \u0026lt;date\u0026gt; 2007-02-15, 2007-02-15, 2007-02-22, 2007-03-01, 2007… Além disso, conseguimos conferir a quantidade de episódios filler:\nepisodios_naruto_shippuden %\u0026gt;% dplyr::count(tipo_episodio, sort = TRUE) %\u0026gt;% knitr::kable(col.names = c(\u0026quot;Tipo de episódio\u0026quot;, \u0026quot;Número de episódios\u0026quot;)) Tipo de episódio Número de episódios Canon 259 Filler 188 Novel 25 Semi-filler 23 Gaiden 4 Omake 1 Visualização Agora que já temos a base arrumada, podemos fazer uma visualização para descobrir a distribuição de episódios fillers no anime. A visualização abaixo está separada por temporada, sendo que a ordem dos episódios no gráfico é: inicia no canto inferior esquerdo, e sobe em direção ao canto superior esquerdo.\n# Criando a paleta de cores para usar paleta \u0026lt;- c(\u0026quot;#fe7934\u0026quot;, \u0026quot;#5b8cb9\u0026quot;, \u0026quot;#083b7c\u0026quot;, \u0026quot;#e1e020\u0026quot;, \u0026quot;#ba1820\u0026quot;, \u0026quot;#403e4b\u0026quot;) # O pacote que disponibiliza a função ggwaffle é: # devtools::install_github(\u0026quot;hrbrmstr/waffle\u0026quot;) # Criando a visualização: grafico \u0026lt;- episodios_naruto_shippuden %\u0026gt;% dplyr::count(numero_temporada, numero_episodio, tipo_episodio) %\u0026gt;% dplyr::mutate(tipo_episodio = forcats::fct_relevel( tipo_episodio, c(\u0026quot;Canon\u0026quot;, \u0026quot;Omake\u0026quot;, \u0026quot;Novel\u0026quot;, \u0026quot;Gaiden\u0026quot;, \u0026quot;Semi-filler\u0026quot;, \u0026quot;Filler\u0026quot;) )) %\u0026gt;% ggplot2::ggplot(ggplot2::aes(fill = tipo_episodio, values = n)) + waffle::geom_waffle( n_rows = 5, color = \u0026quot;white\u0026quot;, show.legend = TRUE, alpha = 0.9 ) + ggplot2::facet_wrap( ~ numero_temporada) + ggplot2::theme_void() + ggplot2::scale_fill_manual(values = paleta) + ggplot2::labs(fill = \u0026quot;Tipo de episódio\u0026quot;, title = \u0026quot;Temporadas de Naruto Shippuden \\n\u0026quot;) + ggplot2::theme( text = ggplot2::element_text(family = \u0026quot;Ninja Naruto\u0026quot;, color = \u0026quot;white\u0026quot;), plot.margin = ggplot2::margin(.5, .5, .5, .5, \u0026quot;cm\u0026quot;), plot.title = ggplot2::element_text( hjust = 0.5, size = 20, color = paleta[3] ), legend.position = \u0026quot;bottom\u0026quot; ) # Imagem que usaremos de fundo imagem \u0026lt;- \u0026quot;https://uploaddeimagens.com.br/images/003/256/849/full/Monumento_dos_Hokage.jpg?1621801239\u0026quot; # Colocando uma imagem de fundo ggimage::ggbackground(gg = grafico, background = imagem) Edit: Comentaram que a paleta de cores não facilitou a visualização para pessoas daltônicas, então fiz uma versão com a paleta viridis (obrigada Luiz Paulo Carvalho!):\ngrafico_viridis \u0026lt;- grafico + ggplot2::scale_fill_viridis_d() ggimage::ggbackground(gg = grafico_viridis, background = imagem) As únicas temporadas livres de fillers são a primeira e a segunda! Depois de concluir essas temporadas é importante ficar de olho na lista para não acabar assistindo os episódios de enrolação.\nConclusão Espero que esse post tenha sido interessante, e seja útil para quem não quer perder tempo com fillers quem deseja obter algum conteúdo do Wikipédia de forma mais fácil!\nTchau! Até a próxima!\n","permalink":"https://blog.curso-r.com/posts/2021-05-24-naruto/","tags":["web scraping","faxina","purrr","rvest"],"title":"Buscando informações na Wikipédia: Lista de episódios de Naruto Shippuden"},{"author":["Fernando"],"categories":["pacotes"],"contents":" Você provavelmente já sabe que o pacote ggplot2 é uma super ferramenta para construção de gráficos, mas você sabia que também é possível construir composições com vários gráficos sem nenhuma dificuldade? Essa é exatamente a função do pacote patchwork e hoje nós vamos conversar um pouco sobre essa que é uma das expressões mais importantes do pacote ggplot2.\nIdeia geral O pacote patchwork parte de vários objetos ggplot2, nomeados por exemplo como g1, g2, g3, e nos fornece ferramentas para declarar de que forma esses gráficos serão posicionados em uma figura maior. Veja o exemplo abaixo em que colocamos três figuras lado a lado:\nlibrary(patchwork) library(ggplot2) library(magrittr) # primeiro grafico g1 \u0026lt;- dados::pinguins %\u0026gt;% ggplot(aes(x = comprimento_bico, y = profundidade_bico)) + geom_point() # segundo grafico g2 \u0026lt;- dados::pinguins %\u0026gt;% ggplot(aes(x = profundidade_bico)) + geom_density() # terceiro grafico g3 \u0026lt;- dados::pinguins %\u0026gt;% ggplot(aes(x = comprimento_bico)) + geom_density() # codigo usando o pacote patchwork # aqui usamos esse operador \u0026#39;+\u0026#39; (que também poderia ser `|`), que fica disponível # quando carregamos o pacote g1 + g2 + g3 Nesse nosso exemplo, utilizamos o operador + para colocar os gráficos lado a lado, ou seja, para criar colunas novas. Se o nosso objetivo fosse criar linhas, precisaríamos usar o operador /, que normalmente representa divisões no contexto aritmético. Essa escolha por parte de quem desenvolveu o pacote se dá pelo fato dos divisores (o que vem à esquerda do operador de divisão) serem posicionados na parte inferior das frações. Veja o exemplo abaixo:\n# codigo usando o pacote patchwork # aqui usamos esse operador \u0026#39;/\u0026#39;, que fica disponível # quando carregamos o pacote g1 / g2 / g3 O verdadeiro poder do pacote patchwork se revela quando misturamos essas duas operações e (s )s para organizar nosso código:\n# codigo usando o pacote patchwork # aqui usamos esse operador \u0026#39;/\u0026#39;, que fica disponível # quando carregamos o pacote (g1 + g2) / g3 # exemplo mais maluco, podemos misturar quantos gráficos a gente quiser e do jeito que a gente quiser: (g1+((g1 + g2) / g3))/g3 Incluindo elementos que não são gráficos: o pacote gridExtra A lógica geral do pacote patchwork também funciona com outros tipos de objetos, como textos e tabelas. Entretanto, as operações que vimos até agora só funcionam se estivermos misturando gráficos do ggplot2 ou grobs oriundos dos pacotes grid e gridExtra. No exemplo abaixo, podemos ver como incluir tabelas e textos usando as funções gridExtra::tableGrob e gridExtra::textGrob\n# Exemplo com gráfico # primeira linha (g1 + grid::textGrob(\u0026quot;Todos os gráficos indicam que está\\nrolando uma parad bi-modal.\u0026quot;))/ # segunda linha (g2+g3) tabela \u0026lt;- dados::pinguins %\u0026gt;% dplyr::summarise( comprimento_bico = mean(comprimento_bico, na.rm = TRUE), profundidade_bico = mean(profundidade_bico, na.rm = TRUE) ) %\u0026gt;% tidyr::pivot_longer(dplyr::everything(), names_to = \u0026quot;Variável\u0026quot;, values_to = \u0026quot;Média\u0026quot;) # Exemplo com tabela # primeira linha (g1 + gridExtra::tableGrob(tabela, rows = c(\u0026quot;\u0026quot;, \u0026quot;\u0026quot;)))/ # segunda linha (g2+g3) Modificando a composição Até agora nós vimos como montar o que aparecerá na tela, partindo de objetos pré-prontos, mas o patchwork também permite que façamos algumas mudanças na composição final, tanto usando os estilos comuns do ggplot2 quanto usando as funções plot_annotation, para incluir títulos e subtítulos, e plot_layouts, para manipular larguras e posicionamentos. Veja o exemplo abaixo, que utiliza todas essas funcionalidades.\n(( # gráfico (# primeira coluna (g2 / g3) | # segunda coluna g1) + # alterando a largura plot_layout(widths = c(1, 2) ) ) + # plot_annotation( title = \u0026quot;Comparação entre os a profundidade e o comprimento dos bicos\u0026quot;, subtitle = \u0026quot;Os graficos indicam a presença de uma distribuição bimodal, possivelmente porque existem várias espécies na base.\u0026quot;, caption = \u0026quot;Fonte: github.com/cienciadedatos/dados\u0026quot; )) \u0026amp; # trocando os temas para branco. o operador `\u0026amp;` garante que o tema será aplicado # a todos os sub-plots theme_bw() Considerações finais Essas são as principais funcionalidades do pacote patchwork, a riqueza de possibilidades surge das suas combinações. Para mais detalhes, visite o site com a documentação oficial do pacote.\nGostou? Quer saber mais? Se você quiser aprender um pouco mais sobre RMarkdown e Visualização de Dados em geral, dê uma olhada no nosso curso Relatórios e visualização de dados e aproveite!\n","permalink":"https://blog.curso-r.com/posts/2021-05-19-patchwork/","tags":["ggplot2"],"title":"Como unir vários gráficos feitos em ggplot2: o pacote patchwork"},{"author":["Caio"],"categories":["discussões"],"contents":" A profecia estava correta: o R finalmente vai ganhar um pipe próprio, embutido à linguagem em si. Mais de 7 anos depois do primeiro commit do {magrittr}, o %\u0026gt;%, operador com o qual compartilhamos tantas boas memórias, símbolo do R moderno e inspiração para o logo da Curso-R, vai se tornar obsoleto. De acordo com a agenda do R Project, no badalar da meia-noite do dia 18/05/2021 será lançada oficialmente a versão 4.1.0 (“Camp Pontanezen”) do R, contendo, dentre outras novidades, suporte para o operador |\u0026gt;.\nDevo admitir que estou um pouco triste com a despedida, como se eu estivesse prestes a mudar de casa: pode ser que a casa nova seja melhor e maior, mas, de uma forma ou de outra, estou abandonando um lugar que foi palco de várias boas memórias por um lugar estéril e inerte…\nSó que mesmo assim nós seguimos em frente e mudamos de casa! As lembranças não vão embora e temos a oportunidade de criar mais boas memórias com esse novo plano de fundo. Sendo assim, vamos entender as principais mudanças do R 4.1 e, principalmente, como usar o novo pipe.\nO novo pipe: |\u0026gt; À primeira vista, o novo pipe é igual ao antigo: usado no final de uma linha para passar o resultado do que está à sua esquerda como primeiro argumento da linha debaixo. Aqui temos um exemplo simples do uso mais comum dos pipes:\nlibrary(dplyr) # Uma pipeline com o pipe novo starwars |\u0026gt; group_by(species, sex) |\u0026gt; select(species, sex, height) |\u0026gt; summarise(height = mean(height, na.rm = TRUE)) |\u0026gt; pull(height) |\u0026gt; summary() #\u0026gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #\u0026gt; 66.0 163.0 181.3 173.5 196.0 264.0 As diferenças aparecem quanto tentamos usar o ., a saber, o dot placeholder. Bem comum no antigo pipe, o uso do . para escolher onde a substituição deve ocorrer é simplesmente proibido no novo pipe. Essa decisão foi tomada porque o . gera situações ambíguas como a descrita a seguir:\n# Vamos gerar essa string a partir de \u0026quot;-\u0026quot; %\u0026gt;% paste0() \u0026quot;-\u0026gt;\u0026lt;-\u0026quot; #\u0026gt; [1] \u0026quot;-\u0026gt;\u0026lt;-\u0026quot; # Podemos começar colando o hífen na esquerda \u0026quot;-\u0026quot; %\u0026gt;% paste0(\u0026quot;\u0026gt;\u0026quot;) #\u0026gt; [1] \u0026quot;-\u0026gt;\u0026quot; # Agora adicionamos o . para colar na direita também... # Mas isso dá errado! \u0026quot;-\u0026quot; %\u0026gt;% paste0(\u0026quot;\u0026gt;\u0026lt;\u0026quot;, .) #\u0026gt; [1] \u0026quot;\u0026gt;\u0026lt;-\u0026quot; # Precisamos colocar o . em dois lugares diferentes, # incluindo, contraintuitivamente, no primeiro argumento \u0026quot;-\u0026quot; %\u0026gt;% paste0(., \u0026quot;\u0026gt;\u0026lt;\u0026quot;, .) #\u0026gt; [1] \u0026quot;-\u0026gt;\u0026lt;-\u0026quot; Assim, em troca de nunca mais precisar carregar o {magrittr}, usar usethis::use_pipe() ou se preocupar com o ambiente do {future}, vamos ser obrigados a criar funções parciais que sempre esperam a substituição no seu primeiro argumento. Mas, falando em funções, chegamos à segunda maior novidade do R 4.1…\nFunções anônimas: \\(x) Se você se acostumou com a notação de fórmulas do {purrr}, então funções anônimas não são nada de novo. Também conhecidas como lambdas, funções anônimas não passam de funções que não precisam receber um nome, ou seja, podem ser utilizadas pelo programa na hora de sua criação. No caso do {purrr} isso se dava através de uma gambiarra feita em cima das fórmulas (~.x), mas, ainda assim, isso era melhor do que a sintaxe bastante prolixa que o R básico exigia (function(x) x).\nA nova sintaxe é, na minha opinião, bastante melhor que a estratégia do {purrr}. Apesar de ela ser um pouco mais extensa, ela funciona em todos os lugares (não só no tidyverse), permite nomes arbitrários para os argumentos e não bagunça a função do ~. A nova função anônima é declarada com uma barra oblíqua para a esquerda seguida por todos os argumentos entre parênteses e o código; essencialmente, é igual à notação antiga, mas function vira \\. A escolha do símbolo \\ também é ótima, pois é igual à de outras linguagens e porque lembra um λ (lambda) com a perna cortada. Para comparar todas as diferentes notações, segue um breve exemplo (saídas suprimidas):\nlibrary(purrr) # Sem função anônima soma_um \u0026lt;- function(x) { x + 1 } map(1:10, soma_um) # A notação antiga do {base} é extensa map(1:10, function(x) x + 1) # A notação do {purrr} força o .x map(1:10, ~ .x + 1) # A notação nova permite qualquer nome map(1:10, \\(n) n + 1) # A notação nova funciona fora do tidyverse lapply(1:10, \\(x) x + 1) # Para mais de 2 argumentos, o {purrr} muda pmap(list(1:10, 11:20, 21:30), ~ ..1 + ..2 + ..3) # A notação nova continua consistente pmap(list(1:10, 11:20, 21:30), \\(x, y, z) x + y + z) De quebra, na minha opinião, também será muito mais fácil de ensinar o \\(x) do que o ~.x. A notação é intuitiva, consistente e funciona fora do tidyverse, então, para mim, o novo lambda é até mais interessante do que o novo pipe!\nMiscelânea Dependendo de quando você estiver lendo este post, a versão 4.1 já vai ter passado pelo code freeze que está agendado para 11/05/2021, que é quando não são mais permitidas alterações no código da nova versão e ela se torna uma release candidate. Conforme escrevo ainda estão permitidas alterações no código, então pode ser que alguma coisa mude até o lançamento final do R 4.1, mas o grosso não deve mudar muito.\nAssim, para te poupar desse trabalho, eu li o NEWS da nova versão a fim de saber quais são as novidades mais interessantes do 4.1. Fora o novo pipe e o novo lambda, achei algumas coisas dignas de nota:\nUsar c() para combinar fatores agora retorna um fator que também combina todos os níveis (de forma similar ao forcats::fct_c());\napply() agora tem um argumento simplify para desabilitar a simplificação de resultados;\nO novo utilitário ...names() retorna os nomes dos argumentos passados via ... (possivelmente facilitando o trabalho do {tidyselect});\nAs funções URLencode() e URLdecode() agora funcionam com vetores de URIs;\ngrep(), sub(), regexp(), etc. ficaram mais rápidas para fatores longos com poucos níveis e\nduplicated() e anyDuplicated() agora são otimizados para vetores numéricos já ordenados via ALTREP.\nConclusão Se você não consegue esperar para testar as novas funcionalidades, já é possível baixar a versão beta do R 4.1 diretamente do site do R Project. O time do R Core apenas pede que você reporte quaisquer bugs detectados!\nPara quem é mais paciente e prefere esperar o lançamento oficial daqui alguns dias, abra o seu RStudio, aperte CTRL + SHIFT + M com carinho e agradeça por todas as aventuras patrocinadas pelo pipe. O pacote {magrittr} continuará lá, funcionando, mas cada vez menos relevante… Apesar de estarmos entrando em uma nova fase do R, as lembranças e aprendizados do passado nunca irão embora.\n%\u0026gt;%\n","permalink":"https://blog.curso-r.com/posts/2021-05-06-o-novo-pipe-esta-chegando/","tags":["programação","pipe"],"title":"R 4.1: o novo pipe está chegando!"},{"author":["Fernando"],"categories":["conceitos"],"contents":" Quando você está criando uma apresentação de slides super elegante com o pacote xaringan ou um relatório minimalista com alguma encarnação do pacote rmarkdown, é normal sentir falta de visualizar o resultado final ao vivo. Essa é uma funcionalidade muito comum em vários outros softwares de edição de documentos e a falta dela pode ser frustrante no nosso contexto, mesmo que rmarkdown tenha vantagens como flexibilidade e reprodutibilidade.\nNeste post vamos te mostrar dois jeitos de suprir essa necessidade:\nUse a função xaringan::infinite_moon_reader Atualmente o pacote xaringan disponibiliza para nós uma função chamada xaringan::infinite_moon_reader, que é uma mão na roda principalmente para quem está editando slides. O funcionamento é bastante simples: você pode chamar essa função sem nenhum argumento e a janela Viewer do RStudio vai se transformar em uma versão ao vivo do documento que você estiver editando no momento em que rodar a função. Você também pode utilizar um argumento moon para ativar o live preview em algum arquivo específico que te interesse.\nO gif abaixo, criado pelo próprio mantenedor principal do xaringan, ilustra muito bem essa funcionalidade.\nknitr::include_graphics(\u0026#39;https://user-images.githubusercontent.com/163582/53144527-35f7a500-3562-11e9-862e-892d3fd7036d.gif\u0026#39;) Figura 1: Ilustração sobre o pacote rmarkdown, criada por Yihui Xie. Use o Visual Markdown Editing do Rstudio Desde setembro do ano passado a janela de edição de texto do RStudio ganhou um novo ícone. Quando você estiver editando um rmd, pode clicar no compasso que fica no canto superior direito da janela e entrar no modo visual editing. Nesse modo, os chunks já são convertidos automaticamente no seu output e isso também vale para html widgets, fórmulas matemáticas em MathJax (aquelas que a gente cria com o símbolo cifrão $) entre outras. Não é tão direto quando o infinite_moon_reader, mas para documentos mais complexos pode ser interessante.\nknitr::include_graphics(\u0026#39;https://blog.rstudio.com/2020/09/30/rstudio-v1-4-preview-visual-markdown-editing/images/visual-editing.png\u0026#39;) Gostou? Quer saber mais? Se você quiser aprender um pouco mais sobre RMarkdown e Visualização de Dados em geral, dê uma olhada no nosso curso Relatórios e visualização de dados e aproveite!\nFigura 2: Ilustração sobre o pacote rmarkdown, criada por Allison Horst. ","permalink":"https://blog.curso-r.com/posts/2021-05-05-visualizar-rmd-ao-vivo/","tags":["r-markdown","rstudio"],"title":"Como pré-visualizar um relatório em  RMarkdown"},{"author":["Fernando"],"categories":["análises","conceitos","tutoriais"],"contents":" Lá no nosso canal do Youtube nós conversamos um pouco sobre alguns conceitos básicos de causalidade. Falamos sobre a importância de ter um grafo descrevendo a relação entre as nossas variáveis para que a gente não seja enganado pelas correlações espúrias, sobre viéses de seleção e, no final, mostramos a aplicação de um método para construir um grafo de relacionamento entre as nossas variáveis usando o algoritmo PC. Neste post, vamos te mostrar como implementar esse algoritmo nas suas bases de dados.\nEntretanto, antes de implementar o algoritmo, vamos analisar como funciona esse algoritmo.\nFuncionamento geral do algoritmo PC A principal ideia por trás do algoritmo PC é que, dadas três variáveis \\(X\\), \\(Y\\), \\(Z\\), o único jeito de verificarmos \\(X \\not\\perp Y | Z\\) (isso é, não X é independente de Y quando condicionamos no valor da variável Z) é se a relação causal entre elas for \\(X \\rightarrow Z \\leftarrow Y\\) (\\(Z\\) é um colisor). Se \\(X \\perp Y \\ | \\ Z\\) fosse verdade, qualquer uma das setas (ou todas) poderiam estar ao contrário do que estão em \\(X \\rightarrow Z \\leftarrow Y\\).\nSendo assim, o algoritmo PC consegue decidir onde fica a ponta de algumas das setinhas usando esse critério e repetir esses testes sucessivamente. Em linhas gerais, o algoritmo faz o seguinte:\nComece com um grafo em que todas as variáveis estão conectadas, mas sem a ponta das setas. Para todo par de variáveis \\(X\\) e \\(Y\\), verifique se \\(X \\perp Y\\). Se a resposta for “Sim”, remova essa conexão. Para todo par de variáveis \\(X\\) e \\(Y\\) e para todo conjunto de variáveis \\(S = \\{Z_1, Z_2, ...\\}\\) que não contem \\(X\\) e \\(Y\\), verifique se \\(X \\perp Y \\ | \\ S\\). Se a resposta for “Sim”, remova essa conexão. Para todas as triplas de variáveis que sobraram \\(X - Z - Y\\), verifique se \\(X \\not \\perp Y \\ | \\ Z\\). Se a resposta for “Sim”, oriente essas setinhas na direção de \\(Z\\). Caso haja alguma setinha que está sem orientação, utilize o resultado dos testes que obteve no passo 3. para eventualmente orientar algumas outras setar por exclusão. O algortimo PC no pacote bnlearn library(bnlearn) library(tidyverse) Similar ao que fizemos na live, aqui vamos procurar um grafo de relacionamentos entre as variáveis da base Auto, disponível no pacote ISLR. Vamos excluir algumas variáveis dessa base e aplicar algumas transformações:\ndados_para_ajuste \u0026lt;- ISLR::Auto %\u0026gt;% dplyr::select(-name, -origin) # Vamos excluir algumas variáveis estritamente qualitativas Para buscar as relações entre as variáveis da tabela dados_para_ajuste, basta utilizarmos a função bnlearn::pc.stable, cujo resultado pode ser plotado usando a função plot.\ndados_para_ajuste %\u0026gt;% bnlearn::pc.stable(alpha = .01) %\u0026gt;% plot() Bacana, né? O melhor é que é simples. Esse pacote, inclusive, permite que a gente misture variáveis contínuas e categórias, basta trocar a opção de teste de independência no parâmetro test.\nGostou? Quer saber mais? Se você quiser aprender um pouco mais sobre esse assunto, temos alguns cursos que tocam os temas deste post. Dê uma olhada nos nossos cursos de Regressão Linear ou Machine Learning e aproveite!\nSe você quiser aprender um pouco mais sobre manipulação de dados com R, dê uma olhada no nosso curso R para Ciência de Dados I e aproveite!\nCaso você tenha dúvidas entre em contato com a gente pelos comentários aqui embaixo, pelo nosso Discourse ou pelo e-mail contato@curso-r.com.\n","permalink":"https://blog.curso-r.com/posts/2021-04-27-causal-discovery-como-estimar-redes-causais-no-r/","tags":["faxina","modelagem"],"title":"Causal Discovery: como estimar redes causais no R"},{"author":["Fernando"],"categories":["análises","conceitos"],"contents":" Você já passou pela situação de fazer um gráfico e se deparar com uma imagem triste como essa? Quando você nem sequer consegue ver o que está acontecendo porque um ponto está deixando a sua escala completamente bagunçada?\nEsses dados que dificultam a nossa vida normalmente são chamados de outliers ou também de dados discrepantes e pontos fora da curva. Existem muitos jeitos de tratar esse tipo de dados, pois se por um lado ele pode ser um erro da base e deve ser descartado, também existem situações em que os outliers são características importantes dos seus dados e merecem ser compreendidos em profundidade. Hoje, nós vamos falar sobre uma estratégia rápida e fácil que tem o potencial de resolver os seus problemas sem te dar dor de cabeça.\nO método mais comum para encontrar outliers usa o famoso z-score, que nada mais é que uma transformação de uma variável \\(X\\) que padroniza a sua posição (pois a média do z-score é sempre 0, enquanto a média de \\(X\\) pode ser qualquer coisa) e a sua escala (pois \\(X\\) pode variar muito ou pouco, com qualquer desvio padrão, mas o z-score não. o z-score terá sempre desvio padrão igual a 1). A fórmula do z-score é:\n\\[Z_{SCORE} (X) = \\frac{X-Média(X)}{Desvio Padrão (X)}\\]\nUma estratégia para encontrar outliers, então, é marcar aqueles pontos que, nessa escala, estão muito discrepantes. Normalmente se adota um ponto de corte pequeno, como -2 e 2 ou no máximo -3 e 3. Serão considerados outliers todos os pontos que estiverem fora desse intervalo.\nVamos ver o que aconteceria se usássemos essa regra para arrumar o primeiro gráfico que mostramos? Primeiro, vamos criar uma base de dados onde poderemos trabalhar. Ela se chamará mtcars_com_outliers, como se vê abaixo.\nmtcars_com_outliers \u0026lt;- mtcars %\u0026gt;% # aqui vamos incluir os pontos bizarros... dplyr::bind_rows( tibble::tibble( mpg = c(0.1, 1039, 481, 1402), wt = c(3.21, 1.8230, 2.6740, 3.6720) ) ) Agora, vamos criar uma coluna e_outlier que vai nos dizer se estamos falando de um outlier ou não. Para não perder a viagem, também vamos fazer um gráfico que ajude a identificar visualmente os pontos que podem ser outliers.\nmtcars_com_outliers %\u0026gt;% dplyr::mutate( z_score_mpg = (mpg-mean(mpg))/sd(mpg), e_outlier_mpg = dplyr::if_else(abs(z_score_mpg) \u0026gt; 2, \u0026quot;É outlier\u0026quot;, \u0026quot;Não é outlier\u0026quot;) ) %\u0026gt;% ggplot2::ggplot(ggplot2::aes(x = wt, y = mpg, color = e_outlier_mpg)) + ggplot2::geom_point(size = 3) + ggplot2::theme_minimal(15) + ggplot2::labs(x = \u0026quot;Peso do carro\u0026quot;, y = \u0026quot;Consumo de combustível (milhas/galão)\u0026quot;, color = \u0026quot;\u0026quot;) Usamos o ponto de corte 2 e parece que um outlier ficou de fora, né? Vamos diminuir um pouco o ponto de corte e ver se a gente consegue pegar esse que ficou fora.\nmtcars_com_outliers %\u0026gt;% dplyr::mutate( z_score_mpg = (mpg-mean(mpg))/sd(mpg), e_outlier_mpg = dplyr::if_else(abs(z_score_mpg) \u0026gt; 1, \u0026quot;É outlier\u0026quot;, \u0026quot;Não é outlier\u0026quot;) ) %\u0026gt;% ggplot2::ggplot(ggplot2::aes(x = wt, y = mpg, color = e_outlier_mpg)) + ggplot2::geom_point(size = 3) + ggplot2::theme_minimal(15) + ggplot2::labs(x = \u0026quot;Peso do carro\u0026quot;, y = \u0026quot;Consumo de combustível (milhas/galão)\u0026quot;, color = \u0026quot;\u0026quot;) Conseguimos! Vamos fazer um novo plot para analisar os dados do jeito que a gente gostaria de ter feito desde o começo, se não fossem esses outliers chatos.\nmtcars_com_outliers %\u0026gt;% dplyr::mutate( z_score_mpg = (mpg-mean(mpg))/sd(mpg), e_outlier_mpg = dplyr::if_else(abs(z_score_mpg) \u0026gt; 1, \u0026quot;É outlier\u0026quot;, \u0026quot;Não é outlier\u0026quot;) ) %\u0026gt;% dplyr::filter(e_outlier_mpg == \u0026quot;Não é outlier\u0026quot;) %\u0026gt;% ggplot2::ggplot(ggplot2::aes(x = wt, y = mpg)) + ggplot2::geom_point(size = 3) + ggplot2::theme_minimal(15) + ggplot2::labs(x = \u0026quot;Peso do carro\u0026quot;, y = \u0026quot;Consumo de combustível (milhas/galão)\u0026quot;, color = \u0026quot;\u0026quot;) Agora parece que tem mais um, o que é que a gente faz? A gente até poderia repetir o processo: rodar o z-score nessa base filtrada, experimentar alguns cortes, refazer os gráfico e seguir até não conseguirmos identificar nenhum outro outlier. Mas isso não parece escalar muito bem, né?\nE se eu te dissesse que tem uma pequena modificação que a gente pode fazer no z-score que simplifica o nosso trabalho? Pois tem, e é o que normalmente se chama de z-score robusto. A fórmula é só um pouquinho diferente:\n\\[Z_{SCORE}\\ ROBUSTO(X) = \\frac{X-Mediana(X)}{MAD(X)}\\] Se você comparar essa fórmula com a outra que postamos lá em cima, duas coisas mudaram: a média virou mediana e o desvio padrão virou MAD. O MAD é um pouco menos conhecido que a mediana, mas ele está para o desvio padrão assim como a média está para a mediana. Essas duas medidas, o MAD e a mediana, são mais robustas do que a média e o desvio padrão, respectivamente, porque consideram a estatística de ordem central, ao invés de tomar uma média de todos os dados ou, no caso do desvio padrão, uma média de todos os desvio quadráticos. Por conta desse comportamento baseado em estatísticas de ordem, o z-score robusto se comporta muito melhor do que o z-score comum, principalmente quando o assunto é encontrar outliers. De resto, vamos fazer mais ou menos a mesma coisa que fizemos antes: escolher um corte e seguir. Podemos inclusive começar escolhendo 2 ou 3 para ver o que acontece:\nmtcars_com_outliers %\u0026gt;% dplyr::mutate( z_score_mpg = (mpg-median(mpg))/mad(mpg), e_outlier_mpg = dplyr::if_else(abs(z_score_mpg) \u0026gt; 3, \u0026quot;É outlier\u0026quot;, \u0026quot;Não é outlier\u0026quot;) ) %\u0026gt;% ggplot2::ggplot(ggplot2::aes(x = wt, y = mpg, color = e_outlier_mpg)) + ggplot2::geom_point(size = 3) + ggplot2::theme_minimal(15) + ggplot2::labs(x = \u0026quot;Peso do carro\u0026quot;, y = \u0026quot;Consumo de combustível (milhas/galão)\u0026quot;, color = \u0026quot;\u0026quot;) E olha só que beleza, encontramos todos os outliers, sem precisar fazer o próximo gráfico! Legal né?\nConta pra gente aí nos comentários se você já conhecia o z-score robusto e o MAD.\nGostou? Quer saber mais? Se você quiser aprender um pouco mais sobre manipulação de dados com R, dê uma olhada no nosso curso R para Ciência de Dados I e aproveite!\nCaso você tenha dúvidas entre em contato com a gente pelos comentários aqui embaixo, pelo nosso Discourse ou pelo e-mail contato@curso-r.com.\n","permalink":"https://blog.curso-r.com/posts/2021-04-15-como-encontrar-outliers/","tags":["base de dados"],"title":"Uma maneira prática de encontrar outliers nos seus dados"},{"author":["Fernando"],"categories":["análises"],"contents":" Uma das grandes conquistas da comunidade brasileira de dados abertos nos últimos anos foi a disponibilização periódica dos dados cadastrais de todas as pessoas jurídicas brasileiras. Nós já exploramos essa base em detalhes em outros posts do blog e hoje vamos voltar ao assunto te ajudando a acessar essa base de um jeito muito, muito rápido!\nÉ um tutorial super simples, de 2 passinhos, que surgiu no contexto de uma pergunta feita lá no nosso Discourse. Inclusive, recomendamos que vocês frequentem o nosso fórum, pois muitas dicar de ouro surgem nas conversar que rolam por lá.\nVamos então ao nosso tutorial:\nPasso 1: Baixe o banco de dados em formato SQLite. SQLite é uma tecnologia muito conveniente para aproveitar as qualidades de um sistema de gerenciamento de banco de dados sem precisar instalar muitos programas ou configurar passos muito complexos. Ao contrário de outras tecnologias, como MySQL por exemplo, SQLite permite que, sem muito trabalho, você consiga fazer queries em uma base grande como a da receita federal e recuperar a resposta para dentro do R. Por isso, nesse nosso tutorial enxuto vamos utilizar essa tecnologia.\nPara simplificar a nossa vida, o querido George Santiago disponibiliza para a comunidade um pacote feito de R para processar todos os dados brutos da RFB e uma versão em SQLite dessa mesma base. Sendo assim, como a comunidade já fez uma grande parte do trabalho, o que resta para nós é baixar a base em SQLite por este link.\nPasso 2: Acesse a base usando o combo de pacotes DBI+RSQLite+dplyr O passo final é usar a infraestrutura do pacote DBI e dplyr para acessar a base SQLite que baixamos no passo anterior. Entretanto, antes de fazer isso, vamos fazer um pequeno tratamento nos nossos CNPJs para que eles saiam do formato padrão 13.347.016/0001-17 para o formato que de armazenamento na base da RFB “13347016000117”. Você pode consultar o código que faz esse procedimento nas linhas abaixo:\n#install.packages(\u0026quot;stringr\u0026quot;) lista_de_cnpjs \u0026lt;- c( \u0026#39;13.347.016/0001-17\u0026#39;, ) %\u0026gt;% stringr::str_remove_all(\u0026quot;[:punct:]\u0026quot;) %\u0026gt;% # remove pontuações, tais como \u0026quot;,\u0026quot;, \u0026quot;.\u0026quot; e \u0026quot;-\u0026quot; stringr::str_pad(pad = \u0026quot;0\u0026quot;, width = 14, side = \u0026quot;left\u0026quot;) # caso o CNPJ tenha menos do que 14 digitos, preenche o CNPJ com 0s à esquerda Depois dessa limpeza, temos só mais um passo a executar. Precisamos nos conectar com a base de dados em SQLite e utilizar esse vetor de CNPJs como filtro:\n#install.packages(\u0026quot;RSQLite\u0026quot;) #install.packages(\u0026quot;dplyr\u0026quot;) con \u0026lt;- DBI::dbConnect(RSQLite::SQLite(), \u0026quot;bd_dados_qsa_cnpj.db\u0026quot;) # conecta o R ao banco de dados dados_cadastrais \u0026lt;- dplyr::tbl(con, \u0026quot;cnpj_dados_cadastrais_pj\u0026quot;) %\u0026gt;% # carrega a tabela de dados cadastrais dplyr::filter(cnpj %in% local(lista_de_cnpjs)) %\u0026gt;% # filtra as linhas da tabela \u0026quot;cnpj_dados_cadastrais_pj\u0026quot; que estejam no vetor \u0026quot;lista_de_cnpjs\u0026quot; dplyr::collect() %\u0026gt;% # carrega o resultado da consulta para o R ACABOU! Caso você queira consultar mais CNPJs, basta aumentar a lista de documentos lá no vetor lista_de_cnpjs. Bacana, né? Esperamos esses passos tenham tomado só um minuto mesmo. Deixe o seu comentário aí embaixo se demorou mais.\nCódigo completo Para que você consiga sair usando esse código por aí, vamos colocar aqui embaixo o código completo, com todos as partes juntas. Boa sorte!\n#install.packages(\u0026quot;DBI\u0026quot;) #install.packages(\u0026quot;RSQLite\u0026quot;) #install.packages(\u0026quot;dplyr\u0026quot;) con \u0026lt;- DBI::dbConnect(RSQLite::SQLite(), \u0026quot;bd_dados_qsa_cnpj.db\u0026quot;) # conecta o R com o SQLite que baixamos lista_de_cnpjs \u0026lt;- c( \u0026#39;13.347.016/0001-17\u0026#39;, # aqui você insere os CNPJs que quer consultar ) %\u0026gt;% stringr::str_remove_all(\u0026quot;[:punct:]\u0026quot;) %\u0026gt;% # remove pontuações stringr::str_pad(pad = \u0026quot;0\u0026quot;, width = 14, side = \u0026quot;left\u0026quot;) # deixa todos os documento com o mesmo comprimento cnpj \u0026lt;- dplyr::tbl(con, \u0026quot;cnpj_dados_cadastrais_pj\u0026quot;) # acessa a tabela \u0026quot;cnpj_dados_cadastrais_pj\u0026quot; dados_cadastrais \u0026lt;- dplyr::cnpj %\u0026gt;% # carrega a tabela de dados cadastrais dplyr::filter(cnpj %in% local(lista_de_cnpjs)) %\u0026gt;% # filtra as linhas da tabela \u0026quot;cnpj_dados_cadastrais_pj\u0026quot; que estejam no vetor \u0026quot;lista_de_cnpjs\u0026quot; dplyr::collect() # carrega o resultado da consulta para o R Gostou? Quer saber mais? Se você quiser aprender um pouco mais sobre manipulação de dados com R, dê uma olhada no nosso curso R para Ciência de Dados I e aproveite!\nCaso você tenha dúvidas entre em contato com a gente pelos comentários aqui embaixo, pelo nosso Discourse ou pelo e-mail contato@curso-r.com.\n","permalink":"https://blog.curso-r.com/posts/2021-04-01-acessar-cnpj/","tags":["rfb","banco de dados"],"title":"Como acessar a base de dados de CNPJ da receita federal em literalmente 1 minuto"},{"author":["Beatriz Milz","Fernando Corrêa"],"categories":["conceitos","erros","debugging","boas práticas"],"contents":" Quando estamos programando, é muito comum executar algum código e ele gerar um erro! Algumas vezes é difícil entender as mensagens de erro para começar a buscar ajuda, e isso pode atrasar nossas tarefas (e também nos desanimar).\nFigura 1: Ilustração por Allison Horst Principalmente quando estamos iniciando a nossa jornada de aprendizagem de R, alguns erros podem ser mais frequentes. Esse post tem como objetivo listar alguns desses erros comuns, baseado na minha experiência aprendendo e ensinando R, e também respondendo dúvidas :)\nVamos então ler um pouco sobre alguns erros frequentes, e como resolvê-los? Prepara um café ou chá, e vamos lá! ☕\nObjeto não encontrado Imagine que queremos calcular a média do orçamento dos filmes que fazem parte da base de dados sobre filmes do IMDB, e recebemos o seguinte erro:\nmean(imdb$orcamento, na.rm = TRUE) #\u0026gt; Error in mean(imdb$orcamento, na.rm = TRUE): objeto \u0026#39;imdb\u0026#39; não encontrado Created on 2021-03-26 by the reprex package (v1.0.0)\nO R está dizendo para nós que não está encontrando o objeto imdb! Como podemos resolver esse erro? Precisamos carregar esse objeto no nosso ambiente, para que ele esteja disponível para uso!\nPara isso, precisamos usar o sinal de atribuição: \u0026lt;-.\nNo exemplo abaixo, importamos o arquivo imdb.csv e salvamos essa base de dados no objeto imdb. Dessa forma, não obtemos um erro quando tentamos calcular a média da mesma forma que fizemos anteriormente, pois o objeto será encontrado e o código conseguirá ser executado!\nimdb \u0026lt;- readr::read_csv2(\u0026quot;https://raw.githubusercontent.com/curso-r/202010-r4ds-1/master/dados/imdb2.csv\u0026quot;) mean(imdb$orcamento, na.rm = TRUE) ## [1] 35237114 Não foi possível encontrar a função “…” Já tentou executar um código e o retorno foi essa mensagem de erro?\nglimpse(imdb) #\u0026gt; Error in glimpse(imdb): não foi possível encontrar a função \u0026quot;glimpse\u0026quot; Created on 2021-03-26 by the reprex package (v1.0.0)\nIsso significa que o R não encontrou essa função. No R, podemos instalar pacotes que contém funções para que a gente utilize nas nossas rotinas. Porém, é preciso carregar o pacote para que as funções do mesmo fiquem disponíveis para uso. Podemos carregar um pacote utilizando a função library().\nNo exemplo abaixo, a função glimpse() faz parte do pacote {dplyr}. Carregando o pacote no início do código deixará todas as funções do pacote {dplyr} disponíveis para uso (incluindo a função que queremos usar):\nlibrary(dplyr) glimpse(imdb) ## Rows: 3,713 ## Columns: 15 ## $ titulo \u0026lt;chr\u0026gt; \u0026quot;Avatar \u0026quot;, \u0026quot;Pirates of the Caribbean: At World\u0026#39;s End... ## $ ano \u0026lt;dbl\u0026gt; 2009, 2007, 2012, 2012, 2007, 2010, 2015, 2016, 2006... ## $ diretor \u0026lt;chr\u0026gt; \u0026quot;James Cameron\u0026quot;, \u0026quot;Gore Verbinski\u0026quot;, \u0026quot;Christopher Nola... ## $ duracao \u0026lt;dbl\u0026gt; 178, 169, 164, 132, 156, 100, 141, 183, 169, 151, 15... ## $ cor \u0026lt;chr\u0026gt; \u0026quot;Color\u0026quot;, \u0026quot;Color\u0026quot;, \u0026quot;Color\u0026quot;, \u0026quot;Color\u0026quot;, \u0026quot;Color\u0026quot;, \u0026quot;Color\u0026quot;... ## $ generos \u0026lt;chr\u0026gt; \u0026quot;Action|Adventure|Fantasy|Sci-Fi\u0026quot;, \u0026quot;Action|Adventure... ## $ pais \u0026lt;chr\u0026gt; \u0026quot;USA\u0026quot;, \u0026quot;USA\u0026quot;, \u0026quot;USA\u0026quot;, \u0026quot;USA\u0026quot;, \u0026quot;USA\u0026quot;, \u0026quot;USA\u0026quot;, \u0026quot;USA\u0026quot;, \u0026quot;US... ## $ classificacao \u0026lt;chr\u0026gt; \u0026quot;A partir de 13 anos\u0026quot;, \u0026quot;A partir de 13 anos\u0026quot;, \u0026quot;A par... ## $ orcamento \u0026lt;dbl\u0026gt; 237000000, 300000000, 250000000, 263700000, 25800000... ## $ receita \u0026lt;dbl\u0026gt; 760505847, 309404152, 448130642, 73058679, 336530303... ## $ nota_imdb \u0026lt;dbl\u0026gt; 7.9, 7.1, 8.5, 6.6, 6.2, 7.8, 7.5, 6.9, 6.1, 7.3, 6.... ## $ likes_facebook \u0026lt;dbl\u0026gt; 33000, 0, 164000, 24000, 0, 29000, 118000, 197000, 0... ## $ ator_1 \u0026lt;chr\u0026gt; \u0026quot;CCH Pounder\u0026quot;, \u0026quot;Johnny Depp\u0026quot;, \u0026quot;Tom Hardy\u0026quot;, \u0026quot;Daryl Sa... ## $ ator_2 \u0026lt;chr\u0026gt; \u0026quot;Joel David Moore\u0026quot;, \u0026quot;Orlando Bloom\u0026quot;, \u0026quot;Christian Bale... ## $ ator_3 \u0026lt;chr\u0026gt; \u0026quot;Wes Studi\u0026quot;, \u0026quot;Jack Davenport\u0026quot;, \u0026quot;Joseph Gordon-Levitt... Outra forma de utilizar a função sem que seja necessário carregar o pacote, é utilizando o operador :: (você pode ler mais sobre ele neste capítulo do livro Zen do R).\nLeia o código abaixo como: quero usar a função glimpse() do pacote dplyr\ndplyr::glimpse(imdb) ## Rows: 3,713 ## Columns: 15 ## $ titulo \u0026lt;chr\u0026gt; \u0026quot;Avatar \u0026quot;, \u0026quot;Pirates of the Caribbean: At World\u0026#39;s End... ## $ ano \u0026lt;dbl\u0026gt; 2009, 2007, 2012, 2012, 2007, 2010, 2015, 2016, 2006... ## $ diretor \u0026lt;chr\u0026gt; \u0026quot;James Cameron\u0026quot;, \u0026quot;Gore Verbinski\u0026quot;, \u0026quot;Christopher Nola... ## $ duracao \u0026lt;dbl\u0026gt; 178, 169, 164, 132, 156, 100, 141, 183, 169, 151, 15... ## $ cor \u0026lt;chr\u0026gt; \u0026quot;Color\u0026quot;, \u0026quot;Color\u0026quot;, \u0026quot;Color\u0026quot;, \u0026quot;Color\u0026quot;, \u0026quot;Color\u0026quot;, \u0026quot;Color\u0026quot;... ## $ generos \u0026lt;chr\u0026gt; \u0026quot;Action|Adventure|Fantasy|Sci-Fi\u0026quot;, \u0026quot;Action|Adventure... ## $ pais \u0026lt;chr\u0026gt; \u0026quot;USA\u0026quot;, \u0026quot;USA\u0026quot;, \u0026quot;USA\u0026quot;, \u0026quot;USA\u0026quot;, \u0026quot;USA\u0026quot;, \u0026quot;USA\u0026quot;, \u0026quot;USA\u0026quot;, \u0026quot;US... ## $ classificacao \u0026lt;chr\u0026gt; \u0026quot;A partir de 13 anos\u0026quot;, \u0026quot;A partir de 13 anos\u0026quot;, \u0026quot;A par... ## $ orcamento \u0026lt;dbl\u0026gt; 237000000, 300000000, 250000000, 263700000, 25800000... ## $ receita \u0026lt;dbl\u0026gt; 760505847, 309404152, 448130642, 73058679, 336530303... ## $ nota_imdb \u0026lt;dbl\u0026gt; 7.9, 7.1, 8.5, 6.6, 6.2, 7.8, 7.5, 6.9, 6.1, 7.3, 6.... ## $ likes_facebook \u0026lt;dbl\u0026gt; 33000, 0, 164000, 24000, 0, 29000, 118000, 197000, 0... ## $ ator_1 \u0026lt;chr\u0026gt; \u0026quot;CCH Pounder\u0026quot;, \u0026quot;Johnny Depp\u0026quot;, \u0026quot;Tom Hardy\u0026quot;, \u0026quot;Daryl Sa... ## $ ator_2 \u0026lt;chr\u0026gt; \u0026quot;Joel David Moore\u0026quot;, \u0026quot;Orlando Bloom\u0026quot;, \u0026quot;Christian Bale... ## $ ator_3 \u0026lt;chr\u0026gt; \u0026quot;Wes Studi\u0026quot;, \u0026quot;Jack Davenport\u0026quot;, \u0026quot;Joseph Gordon-Levitt... Argumento não numérico para operador binário / non-numeric argument to binary operator De toda a nossa lista de problemas, esse é aquele com a mensagem de erro mais complicada. Você já tentou fazer algum cálculo como esse abaixo e o retorno foi essa mensagem de erro?\nx \u0026lt;- \u0026quot;a\u0026quot; x+1 #\u0026gt; Error in x + 1: argumento não-numérico para operador binário Created on 2021-03-29 by the reprex package (v0.3.0)\nIsso é muito comum, principalmente quando estamos trabalhando em scripts mais longos, e essa mensagem de erro é o R tentando nos avisar que nós estamos passando um texto, no caso a letra “a”, para uma operação que espera receber um número. Na verdade, o R é até um pouco mais preciso: a operação esperava um número e nós frustramos essa expectativa, por isso o erro menciona um “argumento não-numérico”. No caso nós tentamos fazer uma conta (a adição representada pelo símbolo +) usando um texto.\nO que podemos fazer quando esse erro aparece? Minha recomendação é examinar o seu código para verificar que tipo de argumento as suas operações matemáticas (por exemplo +, -, * e ^) estão recebendo. Muito provavelmente algum desses argumentos não é um número…\nConflito de funções Um problema silencioso que pode acontecer é o conflito de funções. Esse tipo de erro é um pouco mais complicado de identificar pois a mensagem de erro causada por esse problema não é padronizada.\nMas o que é um conflito de funções?\nO R permite que a gente instale e carregue diversos pacotes. E esses pacotes podem ter funções com nomes iguais.\nPor exemplo, o pacote {dplyr} possui uma função chamada filter(), assim como o pacote {base} também tem uma função com o mesmo nome (o {base} sempre está carregado quando iniciamos o R).\nQuando usamos uma função sem usar o operador ::, o R vai buscar no ambiente a função com esse nome que foi carregada por último. Então podemos usar uma função A quando queremos realmente usar a função B.\nExemplo: queremos filtrar os filmes do diretor Zack Snyder. Obtemos esse erro dizendo que o objeto imdb não foi encontrado, porém o objeto está carregado!\nfilter(imdb, diretor == \u0026quot;Zack Snyder\u0026quot;) #\u0026gt; Error in as.ts(x): objeto \u0026#39;imdb\u0026#39; não encontrado Created on 2021-03-26 by the reprex package (v1.0.0)\nIsso acontece pois o R utilizou a função filter() do pacote {base}, e não a função filter() do pacote {dplyr}!\nUma forma de evitar esses erros de conflitos é utilizar o operador ::, que foi citado anteriormente. Assim estamos falando explicitamente em qual pacote essa função está:\ndplyr::filter(imdb, diretor == \u0026quot;Zack Snyder\u0026quot;) ## # A tibble: 7 x 15 ## titulo ano diretor duracao cor generos pais classificacao orcamento ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Batma~ 2016 Zack S~ 183 Color Action~ USA A partir de ~ 250000000 ## 2 Man o~ 2013 Zack S~ 143 Color Action~ USA A partir de ~ 225000000 ## 3 Watch~ 2009 Zack S~ 215 Color Action~ USA A partir de ~ 130000000 ## 4 Legen~ 2010 Zack S~ 101 Color Action~ USA Livre 80000000 ## 5 Sucke~ 2011 Zack S~ 128 Color Action~ USA A partir de ~ 82000000 ## 6 300 2006 Zack S~ 117 Color Action~ USA A partir de ~ 65000000 ## 7 Dawn ~ 2004 Zack S~ 110 Color Action~ USA A partir de ~ 26000000 ## # ... with 6 more variables: receita \u0026lt;dbl\u0026gt;, nota_imdb \u0026lt;dbl\u0026gt;, ## # likes_facebook \u0026lt;dbl\u0026gt;, ator_1 \u0026lt;chr\u0026gt;, ator_2 \u0026lt;chr\u0026gt;, ator_3 \u0026lt;chr\u0026gt; Não existe um pacote chamado … / There is no package called .. Quando queremos carregar um pacote, e ele não está instalado, o erro gerado é esse:\nlibrary(tidyverse) #\u0026gt; Error in library(tidyverse) : there is no package called ‘tidyverse’ Created on 2021-03-26 by the reprex package (v1.0.0)\nComo resolver? Precisamos instalar o pacote!\nA forma mais comum de instalação de um pacote é através do CRAN, utilizando a função install.packages():\ninstall.packages(\u0026quot;tidyverse\u0026quot;) Após instalar o pacote, conseguiremos carregá-lo normalmente!\nlibrary(tidyverse) #\u0026gt; ── Attaching packages ───── tidyverse 1.3.0 ── #\u0026gt; ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 #\u0026gt; ✓ tibble 3.1.0 ✓ dplyr 1.0.5 #\u0026gt; ✓ tidyr 1.1.3 ✓ stringr 1.4.0 #\u0026gt; ✓ readr 1.4.0 ✓ forcats 0.5.1 #\u0026gt; ── Conflicts ──────── tidyverse_conflicts() ── #\u0026gt; x dplyr::filter() masks stats::filter() #\u0026gt; x dplyr::lag() masks stats::lag() O pacote … não está disponível para essa versão do R / Package ‘…’ is not available for this version of R Nesse exemplo, mostraremos um caso onde não é gerado um erro, e sim uma mensagem de aviso (warning). Entretanto, o código não executa como esperamos, e pode impossibilitar a execução das etapas seguintes dos scripts.\nImagine que queremos calcular a média da massa corporal dos Pinguins que fazem parte da base de dados sobre Pinguins, disponível no pacote {dados}.\ninstall.packages(\u0026quot;dados\u0026quot;) #\u0026gt; Warning: package \u0026#39;dados\u0026#39; is not available for this version of R #\u0026gt; #\u0026gt; A version of this package for your version of R might be available elsewhere, #\u0026gt; see the ideas at #\u0026gt; https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages Created on 2021-03-26 by the reprex package (v1.0.0)\nEssa mensagem acontece quando queremos instalar um pacote que não está disponível no CRAN. O que eu recomendo, neste caso, é buscar o repositório no GitHub do pacote desejado.\nNo caso do pacote que estamos mostrando neste exemplo, este é o link do repositório.\nCaso queira instalar o pacote através do GitHub, devemos utilizar a função install_github() do pacote {remotes}. Como argumento, devemos informar a organização ou pessoa a quem este repositório “pertence”, e o nome do repositório: \"nome_da_organizacao/nome_do_repositorio\". No caso do pacote {dados}, a organização é cienciadedatos, e o repositório é chamado de dados. Portanto, para instalar esse pacote, devemos executar o código a seguir:\nremotes::install_github(\u0026quot;cienciadedatos/dados\u0026quot;) Agora conseguimos utilizar funções e bases de dados do pacote dados:\nlibrary(dados) mean(pinguins$massa_corporal, na.rm = TRUE) ## [1] 4201.754 O arquivo … não existe no diretório de trabalho atual / … does not exist in current working directory E quando queremos importar uma base de dados (ou outro arquivo) para trabalhar no R, e recebemos essa mensagem?\nimdb \u0026lt;- readr::read_csv2(\u0026quot;imdb2.csv\u0026quot;) #\u0026gt; i Using \u0026#39;,\u0026#39; as decimal and \u0026#39;.\u0026#39; as grouping mark. Use `read_delim()` for more control. #\u0026gt; Error: \u0026#39;imdb2.csv\u0026#39; does not exist in current working directory (\u0026#39;C:/Users/seu_usuario/Documents\u0026#39;). Created on 2021-03-29 by the reprex package (v0.3.0)\nO R está informando que não encontrou o arquivo que a gente deseja importar. A maior parte das vezes quando esse erro acontece é por que 1) esse arquivo não existe no nosso computador, ou 2) estamos indicando o caminho incorreto até o arquivo.\nFigura 2: Ilustração sobre projetos no RStudio, por Allison Horst Vamos considerar que já sabemos que o arquivo existe no computador, e trabalhar na resolução da segunda opção: precisamos corrigir o caminho até o arquivo. É muito mais fácil lidar com isso quando trabalhamos com projetos no RStudio (.Rproj). Recomendo muito ler desse capítulo do livro Zen do R, pois fala bastante sobre as vantagens dessa abordagem.\nQuando trabalhamos com projetos no RStudio (.Rproj), os caminhos são relativos à pasta raiz do projeto. Recomendo adicionar dentro do seu projeto o arquivo que deseja abrir. E para descobrir o caminho, existe uma super dica: o RStudio nos ajuda a navegar nos arquivos presentes no nosso projeto. É necessário escrever duas aspas \"\", posicionar o mouse entre elas (isso é, clicando entre as suas aspas como se a gente fosse escrever algo dentro delas), e apertar a tecla TAB do teclado. O RStudio abrirá janelinha que possibilitará que a gente navegue nos arquivos do nosso projeto dessa forma.\nDescobrindo o caminho correto, podemos importar a base sem que aquele erro seja gerado!\nimdb \u0026lt;- readr::read_csv2(\u0026quot;dados/imdb2.csv\u0026quot;) #\u0026gt; ℹ Using \u0026#39;,\u0026#39; as decimal and \u0026#39;.\u0026#39; as grouping mark. Use `read_delim()` for more control. #\u0026gt; #\u0026gt; ── Column specification ──────────────────────────────────────────────────────── #\u0026gt; cols( #\u0026gt; titulo = col_character(), #\u0026gt; ano = col_double(), #\u0026gt; diretor = col_character(), #\u0026gt; duracao = col_double(), #\u0026gt; cor = col_character(), #\u0026gt; generos = col_character(), #\u0026gt; pais = col_character(), #\u0026gt; classificacao = col_character(), #\u0026gt; orcamento = col_double(), #\u0026gt; receita = col_double(), #\u0026gt; nota_imdb = col_double(), #\u0026gt; likes_facebook = col_double(), #\u0026gt; ator_1 = col_character(), #\u0026gt; ator_2 = col_character(), #\u0026gt; ator_3 = col_character() #\u0026gt; ) Created on 2021-03-26 by the reprex package (v1.0.0)\nConclusão Espero que essas dicas sejam úteis para você resolver os erros no futuro, e você seja mais feliz enquanto escreve seus códigos em R! E caso ainda tenha dúvidas, recomendo perguntar lá no Fórum Discourse da Curso-R!\nFigura 3: Ilustração por Allison Horst Gostou? Quer saber mais? Se você quiser aprender um pouco mais sobre R em geral, dê uma olhada nos Cursos da Curso-R e aproveite! O próximo curso a iniciar é o R para Ciência de Dados II, e a primeira parte do curso é sobre organização de projetos, um tema muito presente neste post. 🎉\n","permalink":"https://blog.curso-r.com/posts/2021-03-29-desvendando-erros/","tags":["conceitos"],"title":"Desvendando erros: Entendendo mensagens de erro comuns em R."},{"author":["Fernando"],"categories":["conceitos"],"contents":" RMarkdown é uma das ferramentas mais importantes e versáteis para quem pratica Ciência de Dados. Como não podia deixar de ser, as vantagens de usar o querido .Rmd se devem às inúmeras soluções que deixam seus documentos elegantes e cheios de detalhes. Nesse post vamos te dar dicas para turbinar o seu RMarkdown com o melhor que o R tem a oferecer. Nossas dicas são curtinhas e objetivas, mas vêm sempre acompanhadas de um documento de exemplo para te ajudar a colocar em prática quando precisar.\nVamos a elas:\n1. Use o pacote prettydoc Muitas vezes a gente quer impressionar com um documento colorido, com uma fonte elegante e agradável pra quem lê, mas o html_doc padrão não ajuda muito nessa empreitada. Nessa situação você tem duas opções: (a) deixar o seu relatório bonito criando um CSS customizado para a página, como você pode ver aqui ou (b) usar um template pronto (e bonito) como aqueles do pacote prettydoc. Embora as opções de customização sejam um pouco limitadas, praticamente sem esforço você consegue fazer um documento impressionante e que vai pegar muito bom com a sua audiência. Dá uma olhada:\nExemplo de documento.\nSite do pacote.\n2. Construa tabelas usando o pacote flextable Esse é um daqueles casos em que uma imagem vale mais do que mil palavras… Dá uma olhada nesse exemplo de visualização da tabela mtcars que podemos construir usando o pacote flextable:\nMuito bonito, né? O pacote flextable é uma biblioteca em R que foi construída pensando em (a) construir tabelas muito bonitas e versáteis sem quase nenhum sofrimento e (b) usar o mesmo pacote para fazer tabelas para Word, PDF, PPT e HTML. A sintaxe é um pouco carregada, mas o resultado compensa muito.\nExemplo de documento.\nSite do pacote.\n3. Impressione nas suas apresentações usando xaringan Muito do que a gente faz só é divulgado para o mundo se colocamos isso em formato de apresentação de slides. O pacote xaringan permite que você faça isso com muito estilo e flexibilidade. Dá uma olhada nesse exemplo e saia usando:\nExemplo de apresentação + tutorial.\nCapítulo de livro sobre o pacote.\nGostou? Quer saber mais? Se você quiser aprender um pouco mais sobre RMarkdown e Visualização de Dados em geral, dê uma olhada no nosso curso Relatórios e visualização de dados e aproveite!\nFigura 1: Ilustração sobre o pacote rmarkdown, criada por Allison Horst. ","permalink":"https://blog.curso-r.com/posts/2021-03-18-dicas-de-rmarkdown-que-voce-precisa-conhecer/","tags":["faxina","gráficos"],"title":"3 Dicas de Rmarkdown que você precisa saber"},{"author":["Beatriz Milz","William"],"categories":["divulgação"],"contents":" Olá!\nEste post é uma continuação dos posts onde apresentamos os trabalhos premiados no curso R para Ciência de Dados I (este e este).\nNeste post, apresentaremos algumas dicas para elaborar relatórios com o pacote rmarkdown!\nFigura 1: Ilustração sobre o pacote rmarkdown, criada por Allison Horst. Dicas para elaborar relatórios Contextualize! A primeira dica é escrever uma introdução para ajudar a contextualizar a sua análise. Por exemplo:\nQual é a pergunta que deseja responder a partir dos dados?\nQual é a base de dados utilizadas? É legal também contextualizar a base.\nQual é a fonte destes dados? De qual(ou quais) ano(s) essa base se refere? Quais são as variáveis presentes na base de dados? Da mesma forma, é legal também escrever uma conclusão ao final da análise. A pergunta inicial foi respondida? Se sim, qual é a resposta?\nQuem é o seu público-alvo? É interessante pensar tambem quem são as pessoas do seu público alvo, e adaptar o relatório para este público. Para quem você está escrevendo?\nPor exemplo, em um relatório de negócios, dificilmente você deixará os códigos visíveis no seu relatório. Por outro lado, em um relatório feito para as pessoas do seu time de cientistas de dados, pode ser interessante deixar os códigos para que saibam o que foi feito na análise.\nControle como o conteúdo é apresentado! Os Chunks nos relatórios feitos com RMarkdown são campos de código onde podemos executar códigos em R. As chunk options (ou opções de chunk), definem como esse código e os resultados gerados aparecerão no relatório final. A seguir estão alguns úteis:\nNão é interessante aparecer mensagens de aviso (warnings) e mensagens geradas pelo código no relatório. Para ocultar essas mensagens e avisos, é possível utilizar as seguintes opções de chunk: message = FALSE, warning = FALSE (mas cuidado, pois essas mensagens podem não ser importantes para as pessoas leitoras do relatório, mas podem ser úteis para você!).\nUtilize as chunk options para personalizar como as imagens devem aparecer! Por exemplo:\nfig.align='center' - para centralizar horizontalmente as imagens;\necho=FALSE - para não mostrar o código que gera as imagens;\nout.width=\"90%\" - para especificar a largura da página que deve ser ocupada pela imagem (neste exemplo, deixamos 90% para que a imagem ocupe 90% da largura disponível na página).\nUtilize as chunk options para personalizar como os códigos devem aparecer! Por exemplo:\nApresentar o código e o resultado: echo=TRUE\nApresentar apenas o código, e não executá-lo: echo=TRUE, eval=FALSE\nApresentar apenas os resultados: echo=FALSE\nTabelas Quando quiser representar dados em tabelas, não esqueça de formatá-las com alguma função com essa finalidade. Por exemplo, primeiro vamos criar uma tabela pequena com os 5 personagens com maior massa presentes na base starwars.\n# Carregar o pacote tidyverse library(tidyverse) # Criar uma tabela de exemplo sw_top5_maiormassa \u0026lt;- dplyr::starwars %\u0026gt;% arrange(desc(mass)) %\u0026gt;% select(name, mass) %\u0026gt;% head(5) Podemos apresentar esses dados usando apenas o nome da base criada, porém não ficará com uma formatação legal:\nsw_top5_maiormassa ## # A tibble: 5 x 2 ## name mass ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Jabba Desilijic Tiure 1358 ## 2 Grievous 159 ## 3 IG-88 140 ## 4 Darth Vader 136 ## 5 Tarfful 136 Podemos usar diversas funções para formatar esses dados como uma tabela organizada. Um exemplo de função é a knitr::kable():\nsw_top5_maiormassa %\u0026gt;% knitr::kable(col.names = c(\u0026quot;Nome\u0026quot;, \u0026quot;Massa\u0026quot;)) Nome Massa Jabba Desilijic Tiure 1358 Grievous 159 IG-88 140 Darth Vader 136 Tarfful 136 Não esqueça de conferir este post que apresenta dicas para formatar as tabelas!\nConfira também outras funções úteis para gerar tabelas, como por exemplo: gt::gt(), DT::datatable(), kableExtra::kbl() e muitas outras!\nCódigo em linha Podemos adicionar resultados de códigos junto aos textos, utilizando a seguinte sintaxe:\n`r codigo_em_r` Por exemplo, podemos primeiro criar alguns objetos para usarmos no texto:\npersonagem_mais_baixo \u0026lt;- dplyr::starwars %\u0026gt;% arrange(height) %\u0026gt;% slice(1) media_altura \u0026lt;- dplyr::starwars %\u0026gt;% summarise(media_altura = mean(height, na.rm = TRUE)) %\u0026gt;% pull() Agora podemos usar estes objetos no texto:\nA base starwars, disponível no pacote {dplyr}, apresenta 87 personagens presentes nos filmes. O personagem com menor altura da franquia é o Yoda, com a altura de 66 centímetros. A média da altura dos personagens presentes na base é de 1.74 metros.\nO código a seguir gera o parágrafo que apresentamos acima:\nA base starwars, disponível no pacote `{dplyr}`, apresenta `r nrow(dplyr::starwars)` personagens presentes nos filmes. O personagem com menor altura da franquia é o `r personagem_mais_baixo[[1]]`, com a altura de `r personagem_mais_baixo[[2]]` centímetros. A média da altura dos personagens presentes na base é de `r round(media_altura/100, 2)` metros. Sabemos então que os personagens da série The Mandalorian não estão na base, pois o personagem Baby Yoda teria uma estatura mais baixa que o Yoda! Mas um exemplo legal sobre a utilidade do código em linha é pensar que, se algum dia atualizarem a base, este relatório ao ser gerado novamente (através do botão knit) teria seus resultados atualizados também!\nUm extra: escreva sobre assuntos que te interessa! Uma última dica (que nem sempre é possível): escreva sobre temas que te interessa! As análises sempre ficam mais interessantes quando entendemos melhor os dados.\nConclusões Essas dicas surgiram a partir da avaliação dos trabalhos do curso R para Ciência de Dados I. Porém existem muitas outras possibilidades para elaborar relatórios! Recomendamos a leitura do capítulo sobre relatórios do livro Ciência de Dados em R. Além disso, dia 18 de março começam as aulas do curso de Relatórios e visualização de dados, então se você tem interesse nesse tema, confira a página do curso!\nE caso você tenha interesse em saber mais sobre os outros cursos, confira a página de cursos!\n","permalink":"https://blog.curso-r.com/posts/2021-03-15-dicas-relatorios-r4ds1_relatorios/","tags":["rmarkdown"],"title":"Feedback dos trabalhos do R para Ciência de Dados (Parte III): Dicas para Relatórios"},{"author":["Fernando"],"categories":["conceitos"],"contents":" Ciência de Dados é uma área nova e, embora relacionada a muitas outras áreas mais tradicionais como estatística e computação, pode parecer um mar vasto demais. Por isso, nesse post vamos recomendar 3 livros muito legais para começar os seus estudos.\nMuitos e muitos livros poderiam aparecer nessa lista, mas para garantir que mais pessoas possam ter acesso às nossas recomendações selecionamos apenas obras já publicadas em português. Nosso recorte também foi feito considerando materiais introdutórios que não exijam muito conhecimento prévio de quem lê. De toda forma, muitos outros livros ficaram de fora da lista, mas esses já representam um bom ponto de partida.\n1. “Estatística Básica”, por Wilton Bussab e Pedro Morettin Estatística é uma das principais ferramentas no cinto de utilidades de Cientistas de Dados. Embora também seja importante cultivar outros conhecimentos, as técnicas e estratégias de análise tradicionais do campo da Estatística resolvem muitos problemas comuns da vida de quem mexe com dados.\n“Estatística Básica” é usado em diversos cursos introdutórios de estatística e explica com clareza conceitos básicos como tipos de variáveis, testes de hipótese e princípios básicos de amostragem. Algumas passagens do livro soam antiquadas para quem o lê hoje em dia, mas sendo um grande e completo clássico, esse livro não pode faltar na sua estante.\n2. “Storytelling com Dados: um Guia Sobre Visualização de Dados Para Profissionais de Negócios”, por Cole Nussbaumer Knaflic O verdadeiro trabalho de quem trabalha com dados é, antes de tudo, conseguir traduzir o que os dados têm a dizer para uma linguagem que seja útil para um negócio, pesquisa ou qualquer outra forma de investigação. As pessoas que vão consumir nossos resultados, em geral, precisam acessar o amplo contexto no qual os dados estão inseridos e muitas vezes a forma mais produtiva de transmitir as nossas informações é contando uma história com começo, meio e fim. Às vezes podemos nos esquecer disso, já que gastamos uma grande parte do nosso tempo em tarefas complementares, como limpeza e modelagem de dados, mas nunca se pode perder de vista que no final vamos precisar contar uma história pra alguém, e possivelmente só aí nosso trabalho terá sido útil.\n“Storytelling com Dados: um Guia Sobre Visualização de Dados Para Profissionais de Negócios” abraça essa filosofia com muito afinco e a autora americana Cole Nussbaumer Knaflic nos guia pelas técnicas que podem nos ajudar a apresentar nossos dados da maneira mais efetiva o possível: quais gráficos devemos usar, para quais aspectos devemos chamar atenção, como fornecer contexto para a audiência e muitos outros temas que você não pode viver sem.\n3. “R Para Data Science”, por Hadley Wickham e Garrett Grolemund Muitas pessoas definem como Ciência de Dados como uma união entre métodos quantitativos em geral (estatística, matemática, machine learning etc), programação, e uma atuação focada a negócios. Desses três temas, apenas um ainda não estava contemplado na nossa lista: a progamação.\n“R para Data Science” é uma obra prima moderna que ensina a extrair o melhor da linguagem R, contextualizando o uso de uma linguagem de programação para o uso mais frequente por quem pratica Ciência de Dados: geração de análises reprodutíveis, escaláveis e muito poderosas. O seu trabalho diário como Cientista de Dados certamente vai melhorar muito depois de ler esse livro.\nGostou? Quer saber mais? Se você quiser aprender um pouco mais sobre manipulação de dados como abordada no livro “R para Data Science”, dê uma olhada nos nossos cursos de R para Ciência de Dados I e R para Ciência de Dados II. Se você quiser aprender um pouco mais sobre visualização de dados, dê uma olhada no nosso curso Relatórios e visualização de dados e aproveite!\n","permalink":"https://blog.curso-r.com/posts/2021-03-08-recomendacao-livros/","tags":["faxina","modelagem"],"title":"3 livros essenciais para Cientistas de Dados "},{"author":["William","Julio"],"categories":["análises"],"contents":" Motivação Será que a ausência de torcida nos jogos de futebol causada pela pandemia tem algum efeito no desempenho dos times mandantes?\nPara responder essa pergunta, o Julio e eu investigamos os resultados da era de pontos corridos da Série A do Brasileirão.\nAcerte o seu aí que eu arredondo o meu aqui. Está valendo! - Silvio Luiz, narrador esportivo brasileiro\nOs dados Os dados utilizados na análise abaixo estão no pacote {brasileirao}. O pacote está disponível neste repositório e se você quiser saber mais sobre como usar, confira este post.\nhead(brasileirao::matches) ## season date home score away ## 1 2003 2003-03-29 Atlético PR 2x0 Grêmio ## 2 2003 2003-03-29 Guarani 4x2 Vasco ## 3 2003 2003-03-30 Corinthians 0x3 Atlético MG ## 4 2003 2003-03-30 Goiás 2x2 Paysandu ## 5 2003 2003-03-30 Criciúma 2x0 Fluminense ## 6 2003 2003-03-30 Cruzeiro 2x2 São Caetano Arrumando Antes de iniciar a análise, precisamos dar aquele famoso tapa na base. A partir da coluna score, que tem seus valores no estilo gols_mandante x gols_visitante, construímos as seguintes variáveis de interesse:\nresult: que indica se o resultado foi vitória do mandante, empate ou derrota do mandante;\npontos: que indica o número de pontos feitos pelo mandante na partida.\nTambém criamos um variável torcida, que indica como sem_torcida a temporada de 2020 e como com_torcida as demais.\nlibrary(tidyverse) library(lubridate) tab_resul_mandante \u0026lt;- brasileirao::matches %\u0026gt;% dplyr::filter(season \u0026lt;= 2020) %\u0026gt;% separate( score, c(\u0026quot;home_score\u0026quot;, \u0026quot;away_score\u0026quot;), sep = \u0026quot;x\u0026quot;, convert = TRUE ) %\u0026gt;% mutate( home_win = home_score \u0026gt; away_score, result = case_when( home_score \u0026gt; away_score ~ \u0026quot;Vitória do mandante\u0026quot;, home_score == away_score ~ \u0026quot;Empate\u0026quot;, home_score \u0026lt; away_score ~ \u0026quot;Derrota do mandante\u0026quot; ), pontos = case_when( home_score \u0026gt; away_score ~ 3, home_score == away_score ~ 1, home_score \u0026lt; away_score ~ 0 ), torcida = ifelse(season == 2020, \u0026quot;sem_torcida\u0026quot;, \u0026quot;com_torcida\u0026quot;) ) Algumas visualizações A nossa primeira investigação foi sobre a proporção de vitórias dos mandantes, comparando a temporada de 2020 com as demais.\nO gráfico abaixo mostra a proporção de vitórias, empates e derrotas dos mandantes em cada temporada. Vemos que:\nA proporção de vitórias dos mandantes no Brasileirão de pontos corridos varia de 44% a 55%.\n2020, com 45%, foi a temporada com a segunda menor proporção de vitórias dos mandantes, perdendo para 2017 (44%).\ntab_grafico \u0026lt;- tab_resul_mandante %\u0026gt;% count(season, torcida, result) %\u0026gt;% group_by(season, torcida) %\u0026gt;% mutate(prop = n/sum(n)) %\u0026gt;% ungroup() tab_grafico %\u0026gt;% ggplot(aes(x = season, y = prop, fill = result)) + geom_col(aes(colour = torcida), position = \u0026quot;stack\u0026quot;) + geom_hline(yintercept = .5, linetype = 2, colour = 2, size = 1) + geom_label( aes(label = scales::percent(prop, accuracy = 1), y = prop / 2), size = 3, color = \u0026quot;white\u0026quot;, data = filter(tab_grafico, result == \u0026quot;Vitória do mandante\u0026quot;) ) + theme_minimal(14) + scale_colour_manual(values = c(\u0026quot;transparent\u0026quot;, \u0026quot;black\u0026quot;)) + guides(colour = FALSE) + theme( legend.position = \u0026quot;bottom\u0026quot;, plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.text = element_text(size = 10) ) + scale_fill_viridis_d(begin = .8, end = .2) + scale_y_continuous(labels = scales::percent) + labs( title = \u0026quot;Desempenho dos mandantes\u0026quot;, subtitle = \u0026quot;por temporada\u0026quot;, x = \u0026quot;Temporada\u0026quot;, y = \u0026quot;Proporção\u0026quot;, fill = \u0026quot;\u0026quot; ) Em vez de olhar a proporção dos resultados, poderíamos olhar diretamente a quantidade de pontos somados pelos mandantes em cada temporada.\nO gráfico a seguir mostra que o número de pontos por temporada tende a permanecer entre 650 e 700, com algumas exceções para mais e para menos. Novamente, 2020 foi a temporada com o segundo menor número de pontos feitos pelos mandantes (621, contra 604 da temporada 2017).\ntab_resul_mandante %\u0026gt;% filter(season \u0026gt;= 2006) %\u0026gt;% group_by(season) %\u0026gt;% summarise(total_pontos = sum(pontos)) %\u0026gt;% ggplot(aes(x = season, y = total_pontos)) + geom_line(color = viridis::viridis(1, begin = 0.2)) + geom_label(aes(label = total_pontos), color = viridis::viridis(1, begin = 0.2)) + theme_minimal(14) + labs( x = \u0026quot;Temporada\u0026quot;, y = \u0026quot;Número de pontos do mandante\u0026quot; ) Por fim, podemos investigar o efeito para cada time. A tabela a seguir mostra o número de pontos como mandante conquistados por cada time na temporada de 2020 contra a média de pontos como mandante nas últimas 3 temporadas (2017, 2018 e 2019).\nlibrary(reactable) tab_resul_mandante %\u0026gt;% filter(season %in% c(2017:2020)) %\u0026gt;% mutate(home = brasileirao::fix_names(home)) %\u0026gt;% group_by(season, torcida, home) %\u0026gt;% summarise(total_pontos = sum(pontos)) %\u0026gt;% group_by(torcida, home) %\u0026gt;% summarise(media_pontos = mean(total_pontos)) %\u0026gt;% ungroup() %\u0026gt;% pivot_wider( id_cols = home, names_from = torcida, values_from = media_pontos, names_prefix = \u0026quot;pts_\u0026quot; ) %\u0026gt;% filter(!is.na(pts_sem_torcida)) %\u0026gt;% mutate( diferenca_rel = (pts_sem_torcida - pts_com_torcida) / pts_com_torcida ) %\u0026gt;% arrange(desc(diferenca_rel)) %\u0026gt;% mutate( pts_com_torcida = round(pts_com_torcida, 1), diferenca_rel = scales::percent(diferenca_rel, accuracy = 0.1) ) %\u0026gt;% reactable( pagination = FALSE, compact = TRUE, style = \u0026quot;margin-bottom: 1px;\u0026quot;, columns = list( home = colDef( name = \u0026quot;Time\u0026quot; ), pts_com_torcida = colDef( name = \u0026quot;Média últimas 3 temporadas\u0026quot;, na = \u0026quot;—\u0026quot;, align = \u0026quot;center\u0026quot; ), pts_sem_torcida = colDef( name = \u0026quot;Pontos como mandante em 2020\u0026quot;, align = \u0026quot;center\u0026quot; ), diferenca_rel = colDef( name = \u0026quot;Diferença percentual\u0026quot;, na = \u0026quot;—\u0026quot;, align = \u0026quot;center\u0026quot; ) ) ) Conclusões Existe efeito da ausência de torcida no desempenho dos mandantes? Como bons cientistas, não sabemos.\nDescritivamente, os dados indicam que, se o efeito existe, parece ser pequeno. Pelo menos menor do que o senso comum esperaria. Embora a proporção de vitórias e número de pontos dos mandantes tenho sido abaixo da média, não foi o pior ano e não foi tão diferente das outras temporadas abaixo da média.\nOlhando por time, vimos queda no desempenho de alguns times que mantiveram ou reforçaram o elenco nos últimos anos, como Palmeiras e Flamengo. Outros times que não apresentaram muita variação de elenco, como São Paulo e Internacional, mantiveram a média dos últimos anos.\nPessoalmente, juntando o que vi nos dados e nos jogos, eu acredito na existência de um efeito de torcida, mas acho muito difícil estudá-lo isoladamente. As interações com time, estádio, adversário, posição na tabela, entre outros fatores, podem ser muito importantes e, olhando só a média simples, veríamos um efeito muito menor. Além disso, existe o efeito pandemia, que mudou alguns aspectos do jogo (como as 5 substuições e o calendário ainda mais apertado) e trouxe mais ruídos (como os surtos de COVID-19 que aconteceram em vários times).\nAssim, deixo as seguintes perguntas:\nO que aconteceu em 2017?\nCom os dados disponíveis, é possível testar (estatisticamente) o efeito da torcida? Quais seriam as suposições e limitações?\nE você? Acredita no efeito da torcida?\nÉ isso! Dúvidas, sugestões e críticas, mande aqui nos comentários.\nAté a próxima!\n","permalink":"https://blog.curso-r.com/posts/2021-03-05-brasileirao-sem-torcida/","tags":["futebol","covid19"],"title":"Efeito da ausência de torcida nos jogos do Brasileirão"},{"author":["William"],"categories":["pacotes"],"contents":" Recentemente tive vontade de começar a brincar com dados de futebol, mais especificamente o que sobrou do futebol brasileiro.\nDepois de uma busca bem rápida na internet, resolvi que montaria a minha própria base, até como uma forma de praticar web scraping.\nFoi assim que surgiu o pacote {brasileirao}, que reune os resultados de todas as partidas da Série A do Campeonato Brasileiro das temporadas de 2003 a 2020.\nAbrem-se as cortinas e começa o espetáculo - Fiori Gigliotti, narrador esportivo brasileiro\nO pacote O pacote está disponível neste repositório. Para instalar direto do R, basta rodar o código abaixo:\nremotes::install_github(\u0026quot;williamorim/brasileirao\u0026quot;) Para acessar os dados das partidas, chame o objeto matches.\nlibrary(dplyr) library(brasileirao) matches ## # A tibble: 7,646 × 5 ## season date home score away ## \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2003 2003-03-29 Athletico PR 2x0 Grêmio ## 2 2003 2003-03-29 Guarani 4x2 Vasco ## 3 2003 2003-03-30 Corinthians 0x3 Atlético MG ## 4 2003 2003-03-30 Goiás 2x2 Paysandu ## 5 2003 2003-03-30 Criciúma 2x0 Fluminense ## 6 2003 2003-03-30 Cruzeiro 2x2 São Caetano ## 7 2003 2003-03-30 Flamengo 1x1 Coritiba ## 8 2003 2003-03-30 Fortaleza 0x0 Bahia ## 9 2003 2003-03-30 Internacional 1x1 Ponte Preta ## 10 2003 2003-03-30 Juventude 2x2 São Paulo ## # … with 7,636 more rows Para ver os dados de uma temporada, você pode usar a função filter(), do pacote dplyr.\nmatches %\u0026gt;% filter(season == 2020) ## # A tibble: 380 × 5 ## season date home score away ## \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2020 2020-08-08 Fortaleza 0x2 Athletico PR ## 2 2020 2020-08-08 Coritiba 0x1 Internacional ## 3 2020 2020-08-08 Sport 3x2 Ceará ## 4 2020 2020-08-09 Flamengo 0x1 Atlético MG ## 5 2020 2020-08-09 Santos 1x1 Bragantino ## 6 2020 2020-08-09 Grêmio 1x0 Fluminense ## 7 2020 2020-09-30 Botafogo 1x2 Bahia ## 8 2020 2020-09-30 Corinthians 0x0 Atlético GO ## 9 2020 2020-12-03 Goiás 0x3 São Paulo ## 10 2020 2021-01-26 Palmeiras 1x1 Vasco ## # … with 370 more rows O pacote também possui uma função para padronizar o nome dos times: fix_names(). Alguns times, como o Atlético mineiro aparecem como Atlético MG e Atlético-MG na base. Também há casos de times que mudaram de nome nesse período, como o Athetico e o Grêmio Barueri. Pode isso, Arnaldo?\nVeja a seguir um exemplo da função em ação:\natleticos \u0026lt;- matches %\u0026gt;% filter(stringr::str_detect(home, \u0026quot;Atlético\u0026quot;)) %\u0026gt;% pull(home) %\u0026gt;% unique() atleticos ## [1] \u0026quot;Atlético MG\u0026quot; \u0026quot;Atlético GO\u0026quot; fix_names(atleticos) ## [1] \u0026quot;Atlético MG\u0026quot; \u0026quot;Atlético GO\u0026quot; Ela também pode ser utilizada diretamente em uma coluna com nome de times usando o mutate().\nmatches %\u0026gt;% mutate(home = fix_names(home), away = fix_names(away)) ## # A tibble: 7,646 × 5 ## season date home score away ## \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2003 2003-03-29 Athletico PR 2x0 Grêmio ## 2 2003 2003-03-29 Guarani 4x2 Vasco ## 3 2003 2003-03-30 Corinthians 0x3 Atlético MG ## 4 2003 2003-03-30 Goiás 2x2 Paysandu ## 5 2003 2003-03-30 Criciúma 2x0 Fluminense ## 6 2003 2003-03-30 Cruzeiro 2x2 São Caetano ## 7 2003 2003-03-30 Flamengo 1x1 Coritiba ## 8 2003 2003-03-30 Fortaleza 0x0 Bahia ## 9 2003 2003-03-30 Internacional 1x1 Ponte Preta ## 10 2003 2003-03-30 Juventude 2x2 São Paulo ## # … with 7,636 more rows O pacote é atualizado diriamente, a partir de um script executado via Github Actions. Esse fluxo é pausado nos meses entre temporadas.\nDe onde os dados vieram? Os dados das temporadas de 2003 a 2019 vieram do site Chance de gol. Particularmente, eu não gosto muito de previsões de resultados esportivos, mas o site é bem legal para quem gosta do tema! E, claro, ele é bem fácil de scrapear, o que é importante se você é nível 1 em Web Scraping igual eu.\nComo no Chance de Gol não tinha os dados da temporada de 2020, eu fui buscar na página do ge. A tarefa também bem tranquila, pois os dados da página são atualizados a partir de uma API, que consegui acessar diretamente.\nO código que utilizei nos dois casos pode ser encontrado aqui.\nPróximos passos Meu objetivo é continuar implementando o pacote, trazendo mais dados sobre as partidas da Série A (local, público, cartões, posse de bola, chutes a gol etc), além de completar com os dados das demais séries (B, C e D).\nSe você encontrar qualquer problema no pacote ou inconsistência nos dados, não deixe de me avisar abrindo uma issue no repositório.\nÉ isso! Dúvidas, sugestões e críticas, mande aqui nos comentários.\nAté a próxima!\n","permalink":"https://blog.curso-r.com/posts/2021-03-02-brasileirao/","tags":["futebol","web scraping"],"title":"O pacote {brasileirao}"},{"author":["Fernando"],"categories":["análises","conceitos","tutoriais","r"],"contents":" A distribuição normal (também conhecida como distribuição gaussiana) é um dos conceitos mais importantes da estatística. Ela nos ajuda a interpretar resultados experimentais em todas as áreas da ciência e também é o pressuposto de diversos modelos estatísticos, como por exemplo os modelos de regressão linear.\nNessa série de posts, nós te ajudamos a analisar estatisticamente se uma variável segue ou não a distribuição normal de maneira simples e rápida. Na parte 1 falamos sobre gráficos que podem te ajudar a fazer isso e na parte 2 falamos do pacote PoweR, que concentra uma série de procedimentos estatísticos.\nNo último post da nossa série vamos falar sobre um ponto muito importante: por que precisamos fazer testes de normalidade? Felizmente, a resposta é simples, mas nem tanto:\nVocê deve testar a normalidade dos seus dados quando isso for um pressuposto da sua análise Isso pode parecer óbvio, mas é a coisa mais importante que você precisa saber. Alguns métodos estatísticos funcionam direitinho quando existe uma distribuição normal por trás, como por exemplo a regressão linear simples: os intervalos de confiança têm as propriedades que dizem ter, as estimativas dos betas não terão viés etc. Isso quer dizer que na ausência de normalidade você não deveria usar uma regressão? NÃO! Regressões lineares funcionam bem em muitos contextos, mas se você quiser realizar testes de hipóteses sobre os parâmetros da sua regressão, por exemplo, você pode ter problemas.\nAlém da regressão linear simples, muitos modelos pressupõem direta ou indiretamente a normalidade dos seus dados, como por exemplo:\nANOVAs paramétricas (de uma via, duas vias, MANOVA)… Regressões lineares simples e múltiplas Clusterização por K-médias Também existem outros procedimentos estatísticos que PODEM precisar que os dados brutos sigam a distribuição normal, mas não vamos falar muito deles pois se o tamanho da amostra for suficientemente grande (\\(N\\) \u0026gt; 100, por simplicidade) você pode seguir com segurança. Estamos falando de todos os sabores de teste t, por exemplo.\nPor que devo testar normalidade depois de fazer uma regressão? Regressões, bem como ANOVAs, MANOVAs e variações sobre o tema normalmente seguem o seguinte padrão:\n\\[Y_i = \\alpha + \\beta X_i+ \\epsilon_i \\\\ \\epsilon_i\\text{ segue a distribuição normal com média } 0\\text{ e variância }\\sigma\\]\nA partir dessa segunda observação sobre a normalidade dos erros, é possível afirmar várias coisas sobre \\(\\alpha\\), \\(\\beta\\) e \\(Y_i\\): quais são os valores mais prováveis dessas variáveis e quão provável é que eles não sejam 0, por exemplo. Se essas considerações sobre a variabilidade das estimativas te interessam, você deveria testar a normalidade dos resíduos da sua regressão, isso é, a normalidade dos erros \\(\\hat{Y}_i - Y\\) cometidos pelo seu ajuste \\(\\hat{Y}_i\\).\nPor que devo testar normalidade depois de ajustar uma clusterização usando o método das k-médias? Aqui a coisa é um pouco mais feia do que no caso da regressão, mas também é interessante. Uma das maneiras de construir esse algoritmo é assumindo que dentro dos seus dados, sem que você saiba, existe um certo número de distribuições gaussianas. O método das k-médias é perfeito quando essa hipótese é verdadeira: ele é capaz de encontrar perfeitamente esses grupos com um tamanho de amostra suficientemente grande. Mas o que acontece se essa hipótese for falsa?\nUm exemplo interessante analisado pelo David Robinson é o caso em que os dados estão distribuídos ao longo de um círculo. A imagem abaixo mostra o que acontece quando rodamos um algoritmo de k-médias nesse conjunto de dados: nós quebramos as esferas/círculos no meio.\nEsquisito né? Os dados de fato se dividem em dois grupos (o círculo de fora e a bola de dentro), o k-médias rodou, mas encontrou grupos diferentes dos “reais”. Essa é uma situação em que usar um teste de normalidade poderia nos dar uma dica de que estamos nesse cenário de desalinho entre o algoritmo utilizado e os dados disponíveis: é muito claro que os grupos encontrados pelo k-médias não seguem uma distribuição normal multivariada. A partir desse teste, poderíamos partir para um outro algoritmo que fosse capaz de encontrar esse padrão, como uma clusterização hierárquica.\nConsiderações finais Lendo esse post você pode estar se perguntando “o algoritmo sempre roda, para qualquer conjunto de dados. para que a normalidade importa?”. De fato, sempre podemos usar todos os algoritmos em todos os contextos, mesmo nos inadequados, e isso não necessariamente é ruim. Com cuidado, você sempre conseguirá interpretar os resultados corretamente e extrair das análises o que for mais útil para você. O que é realmente importante é ter consciência: saber as regras para poder quebra-las, quando for adequado, ou trocar de estratégia quando precisar.\nGostou? Quer saber mais? Se você quiser aprender um pouco mais sobre esse assunto, temos alguns cursos que tocam os temas deste post. Dê uma olhada nos nossos cursos de Regressão Linear ou Machine Learning e aproveite!\n","permalink":"https://blog.curso-r.com/posts/2021-02-26-testes-de-normalidade/","tags":["faxina","modelagem"],"title":"Testes de normalidade em R - Parte 3: Quando eu devo fazer um teste de normalidade?"},{"author":["Athos"],"categories":["análises","conceitos","tutoriais","r"],"contents":" library(torch) library(torchvision) library(zeallot) device \u0026lt;- torch_device(if(cuda_is_available()) \u0026quot;cuda\u0026quot; else \u0026quot;cpu\u0026quot;) cpu \u0026lt;- torch_device(\u0026quot;cpu\u0026quot;) Neural Style Transfer é uma das técnicas mais divertidas e “artísticas” do deep learning. A imagem abaixo resume o que a gente vai fazer.\ncontent\nVocê fornece duas imagens à rede neural: style e content e o resultado será a imagem content com o estilo de style. É como se fosse um filtro do instagram, mas com o estilo do seu artista predileto =P.\nEste post é uma adaptação para R + torch do exercício do curso ‘Convolutional Neural Networks’ do deeplearning.ai que eu fiz, originalmente em Python + tensorflow. Quando comecei a escrever esse post, tinha a intenção de ser o mais didático possível, mas acho que eu não conseguiria superar o curso do Coursera, então vou me ater aos principais pontos e comentar o código para quem quiser criar suas próprias imagens! Além disso, este vídeo em português mostra um sumário da estratégia do NST.\nPS: durante a escrita desse post eu descobri que o Daniel já tinha feito um codigozinho de NST para o torchvision. Parte do código roubei de lá kkk.\nConceitos e ideias importantes A ideia é gerar uma imagem generated G que tenha conteúdo similar a content C e estilo similar a style S. O conteúdo e o estilo são, geralmente, extraídos de uma convnet pré-treinada. O artigo original (Gatys, Ecker, and Bethge 2015) utiliza o VGG19, que já vem dentro do {torchvision}.\n# VGG19 model vgg \u0026lt;- model_vgg19(pretrained = TRUE)$features$to(device = device) # congelando os pesos vgg$parameters %\u0026gt;% purrr::walk(function(param) param$requires_grad_(FALSE)) Então o VGG19 vai nos fornecer as features e agora precisamos definir funções de custo para achar a imagem que tem o melhor compromisso entre o conteúdo de uma imagem e o estilo de outra. A construção do algoritmo pode ser divida em três partes:\nFunção de custo do content: \\(L_{content}(C, G)\\) Função de custo do style: \\(L_{style}(S, G)\\) Função de custo global: \\(L(G) = \\alpha L_{content}(C, G) + \\beta L_{style}(S, G)\\) Função de custo do content Sobre a escolha entre camadas rasas versus camadas profundas:\nAs primeiras camadas de uma rede convolucional tendem a extrair padrões mais básicos como bordas e texturas simples. Camadas mais profundas costumam captar características mais sofisticadas como texturas detalhadas e objetos. E sobre a escolha de uma camada do meio, queremos a imagem generated com conteúdo similar ao content. A estratégia é:\nescolher uma camada da rede para representar este tal “conteúdo.” pegar essa camada para cada uma das imagens content e generated. fazer a rede forçar com que essas duas camadas sejam o mais parecidas possível. Então a função de custo para refletir o ponto (3) pode ser simplesmente o erro quadrático médio entre os pixels dessa camada:\ncontent_loss \u0026lt;- function(content_layer, generated_layer) { nnf_mse_loss(content_layer, generated_layer) } OBS: Na prática, você irá obter o resultado “mais legal” se escolher camadas da meiúca: nem tão rasa, nem tão profunda. Já que a VGG19 possui 37 camadas, a escolhida pode ser, por exemplo, a camada 14.\nFunção de custo do style A finalidade da função de custo do style é minimizar a distância entre as tais Gram matrix das imagens style e generated.\nGram matrix A matriz de estilo é chamada de Gram matrix na literatura. Na matemática, dado um conjunto de vetores, a Gram matrix é matriz dos produtos internos dos pares destes vetores. É como se fosse uma matriz de correlação, mas sem centralizar pela média. Na prática, pega-se uma camada da rede, transforma em um tensor 2D (matriz) e calcula \\(A * A^T\\).\ngram_matrix \u0026lt;- function(tensor) { c(b,c,h,w) %\u0026lt;-% tensor$size() tensor \u0026lt;- tensor$view(c(c, h*w)) torch_matmul(tensor, tensor$t())/(h*w) } A função de custo \\(L_{style}(S, G)\\) é o bom e velho erro quadrático médio entre as Gram matrices.\nstyle_loss \u0026lt;- function(style_layer, generated_layer) { style_gram \u0026lt;- gram_matrix(style_layer) generated_gram \u0026lt;- gram_matrix(generated_layer) nnf_mse_loss(style_gram, generated_gram) } Camadas dos estilos Em vez de uma, pega-se cinco camadas intermediárias da rede para calcular as distâncias entre as respectivas Gram matrices. A função de custo vai passar a ser uma ponderação dessas cinco distâncias: \\(L_{style}(S, G) = \\sum_{l=1}^{5}\\lambda^{[l]}L_{style}^{[l]}(S,G)\\)\nstyle_layers \u0026lt;- c(2, 7, 12, 21, 29) # escolha das layers da VGG lambdas \u0026lt;- 1e5/(c(1,10,10,10,10)^2) # pesos para cada uma das layers no estilo final Agora vale criar uma nn_module() que retorne as camadas intermediárias da rede (no caso VGG19). O argumento layers_out permite escolher quais camadas queremos trazer.\nfeatures \u0026lt;- nn_module( initialize = function(convnet) { # poderia ser qualquer convnet pré-treinada. Iremos usar a VGG19 self$convnet \u0026lt;- convnet }, forward = function(img, layers_out = NULL) { layers \u0026lt;- seq_along(self$convnet) # 1 a 37 if(is.null(layers_out)) layers_out \u0026lt;- layers conv_outs \u0026lt;- purrr::accumulate(layers, ~self$convnet[[.y]](.x), .init = img) # lista de 37 tensores conv_outs[layers_out] # lista apenas com as layers selecionadas } ) Otimização Abaixo segue código comentado para rodar a otimização.\n#funções auxiliares to_r \u0026lt;- function(x) as.numeric(x$to(device = cpu)) plot_image \u0026lt;- function(tensor) { im \u0026lt;- tensor$to(device = \u0026quot;cpu\u0026quot;)[1,..]$ permute(c(2, 3, 1))$ to(device = \u0026quot;cpu\u0026quot;)$ clamp(0,1) %\u0026gt;% # make it [0,1] as.array() par(mar = c(0,0,0,0)) plot(as.raster(im)) } load_image \u0026lt;- function(path, geometry = \u0026quot;250x200\u0026quot;) { img \u0026lt;- path %\u0026gt;% magick_loader() %\u0026gt;% magick::image_resize(geometry) %\u0026gt;% transform_to_tensor() %\u0026gt;% torch_unsqueeze(1) img$to(device = device) } Os parâmetros e inputs que valem a pena experimentar são:\ncontent e style: Imagens de input: a de conteúdo e a de estilo. content_layer: a layer da VGG19 para extrair o conteúdo imagem de conteúdo. style_layers: as layers da VGG19 para extrair o estilo imagem de estilo. lambdas: os pesos de cada feature de estilo na otimização. content_weight: o peso das features de conteúdo na otimização. style_weight: o peso das features de estilo (global) na otimização. Outros dois parâmetros que afetam drasticamente o resultado são as dimensões das duas imagens de input. No código abaixo onde tem \"400x400\" e \"350x500\" pode-se trocar por outras dimensões a fim de se obter resultados diferentes.\nNo processo de criação você irá mexer nesses parâmetros o tempo todo!\n# INPUT: content and style images content \u0026lt;- load_image(\u0026quot;content/posts/2021-02-22-neural-style-transfer/cristoredentor3.jpg\u0026quot;, \u0026quot;400x400\u0026quot;) style \u0026lt;- load_image(\u0026quot;content/posts/2021-02-22-neural-style-transfer/vangogh_starry_night.jpg\u0026quot;, \u0026quot;350x500\u0026quot;) # style and content feature setup content_layer \u0026lt;- 14 style_layers \u0026lt;- c(2, 7, 12, 21, 29) lambdas \u0026lt;- 1e5/(c(1,10,10,10,10)^2) content_weight \u0026lt;- 2 style_weight \u0026lt;- 4e-1 content\nstyle\n# FEATURES: extraídas da VGG19 vgg_features \u0026lt;- features(vgg) content_features \u0026lt;- vgg_features(content, content_layer) style_features \u0026lt;- vgg_features(style, style_layers) # OUTPUT: generated image generated \u0026lt;- torch_clone(content)$requires_grad_(TRUE) optim \u0026lt;- optim_adam(generated, lr = 0.02) lr_scheduler \u0026lt;- lr_step(optim, 100, 0.9) # loop de otimização for(step in seq_len(8000)) { optim$zero_grad() # atualiza as features da imagem que está sendo gerada generated_features \u0026lt;- vgg_features(generated, c(content_layer, style_layers)) # losses LC \u0026lt;- content_loss(content_features[[1]], generated_features[[1]]) LS \u0026lt;- 0 for(i in seq_along(lambdas)) LS \u0026lt;- LS + lambdas[i]*style_loss(style_features[[i]], generated_features[-1][[i]]) loss \u0026lt;- content_weight * LC + style_weight * LS loss$backward() optim$step() lr_scheduler$step() # feedback if(step %% 100 == 0) { cat(glue::glue(\u0026quot;LC = {to_r(LC)} - LS = {to_r(LS)} - Loss = {to_r(loss)}\\n\\n\u0026quot;)) plot_image(generated) } } # imagem final plot_image(generated) LC = 2.70741701126099 - LS = 36.7998428344727 - Loss = 20.1347713470459 LC = 2.61282753944397 - LS = 24.3190288543701 - Loss = 14.9532661437988 LC = 2.54735898971558 - LS = 19.7909469604492 - Loss = 13.0110969543457 LC = 2.49572896957397 - LS = 16.9353790283203 - Loss = 11.7656097412109 LC = 2.44669985771179 - LS = 16.6177845001221 - Loss = 11.5405139923096 E aí? Ficou com cara de pintura? Comente o que achou! Tente com suas imagens e compartilhe com a gente =). Aprender como NST funciona é um grande exercício para aprimorar o entendimento sobre modelos de deep learning em geral, principalmente sobre como podemos criar funções de custo mais especializadas em um determinado contexto.\nAprenda Deep learning com a Curso-R Se você quiser entrar no incrível mundo das redes profundas, nosso curso de Deep Learning é a melhor porta de entrada, conheça!\nGatys, Leon A., Alexander S. Ecker, and Matthias Bethge. 2015. “A Neural Algorithm of Artistic Style.” http://arxiv.org/abs/1508.06576. ","permalink":"https://blog.curso-r.com/posts/2021-02-22-neural-style-transfer/","tags":["deep learning","torch","neural style transfer","modelagem"],"title":"Neural Style Transfer com Torch"},{"author":["William"],"categories":["tutoriais"],"contents":" Abaixo segue um passo-a-passo para criar seu blogzinho com o R + blogdown e escrever posts com todo o poder do RMarkdown.\nCaminho das pedras do {blogdown} Abrir o RStudio limpo (sem estar em nenhum projeto)\nCriar o projeto com git/github\nusethis::create_project() usethis::use_git() usethis::use_github() Criar o site com o pacote {blogdown}\ninstall.packages(\"blogdown\") blogdown::new_site(\"caminho\", theme = \"usuario/repositorio\") https://themes.gohugo.io/ para escolher temas Configurar o site no arquivo config.toml\nMexer nos posts dentro da pasta content/post\nUsar blogdown::new_post() para criar novos pots. Rodar blogdown::serve_site() para testar local.\nRodar blogdown::build_site() para montar o site final.\nSubir para o Github para colocar em produção (se estiver usando o GH pages)\nClique aqui para assistir ao pedaço de live gravada da gente criando um blogdown do zero.\nE aqui para ver o repo do github com os códigos resultantes!\nE se quiser ajuda profissional nessa caminhada, conheça nossos cursos de deploy, visualização e relatórios!\n","permalink":"https://blog.curso-r.com/posts/2021-02-12-receitinha-para-criar-seu-blog-com-blogdown/","tags":["blog","blogdown","web"],"title":"Receitinha para criar seu blog com {blogdown}"},{"author":["Fernando"],"categories":["análises","conceitos","tutoriais","r"],"contents":" A distribuição normal (também conhecida como distribuição gaussiana) é um dos conceitos mais importantes da estatística. Ela nos ajuda a interpretar resultados experimentais em todas as áreas da ciência e também é o pressuposto de diversos modelos estatísticos, como por exemplo os modelos de regressão linear.\nNessa série de posts, vamos te ajudar a analisar estatisticamente se uma variável segue ou não a distribuição normal de maneira simples e rápida. Hoje vamos falar especificamente de testes de hipóteses.\nAproveite e dê uma olhada na parte 1 se você também quiser sobre gráficos que ajudam a detectar desvios da distribuição normal.\nR básico Os pacotes do R básico, que já são instalados automaticamente, contam com dois únicos testes de normalidade pré-implementados: ks.test para o teste de Kolmogorov-Smirnov e shapiro.test para o teste de Shapiro-Wilk. Claro que é possível implementar outros testes manualmente, mas essas funções já calculam as estatísticas de teste e fornecem valores p, inclusive com algumas parametrizações disponíveis. Vamos exemplificar o funcionamento dessas funções com a coluna hp (ou horse-power) da base mtcars.\nA função shapiro.test tem uma interface bem amigável. Você insere um vetor, que pode muito bem ser uma coluna de um banco de dados, e recebe de volta o valor calculado da estatística de teste e o valor p calculado. A implementação dessa função segue as especificações de um paper famoso de 1995. No caso da coluna hp, o resultado do teste de Shapiro-Wilk é um valor p de 0.0488082. Ou seja, observamos um desvio improvável com relação ao que se esperaria caso os dados realmente viessem de uma distribuição normal. Não parece que seria isso que aconteceria, né? O gráfico dava outra ideia.\nshapiro.test(mtcars$hp) ## ## Shapiro-Wilk normality test ## ## data: mtcars$hp ## W = 0.93342, p-value = 0.04881 Para tirar a dúvida, vamos ver qual é o resultado do teste de Kolmogorov-Smirnov com a função ks.test. Nesse caso, a usabilidade é um pouco pior do que no caso da função shapiro.test, já que você também precisa imputar manualmente a média e o desvio padrão do seu vetor. Nesse caso, obtemos um valor p maior (aproximadamente 30%), mais condizente com o gráfico que construímos antes.\nks.test(mtcars$hp, \u0026quot;pnorm\u0026quot;, mean(mtcars$hp), sd(mtcars$hp), exact = TRUE) ## ## One-sample Kolmogorov-Smirnov test ## ## data: mtcars$hp ## D = 0.16639, p-value = 0.3037 ## alternative hypothesis: two-sided O pacote PowerR Mesmo que o R básico disponha de apenas dois testes, dois pesquisadores da universidade de Montreal compilaram uma série de procedimentos estatísticos no pacote PoweR e o resultado está disponível no CRAN. A ideia central do pacote é viabilizar a comparação massiva de vários testes de hipóteses, inclusive testes de normalidade, por isso todos os testes podem ser executados rapidamente e de maneira robusta. A título de comparação, o teste de Shapiro-Wilk no R básico só funciona até 5000 observações, enquanto no PoweR essa limitação não existe.\nAo invés de ter uma função para cada teste, como fizemos nos exemplos anteriores, neste pacote é necessário fazer referência a códigos internos que estão descritos na documentação do pacote. A vantagem dessa interface é que a saída é padronizada de em qualquer função que a gente escolha usar. Vamos dar uma olhada em como ficam o resultado dos testes de Shapiro-Wilk.\n#install.packages(\u0026#39;PoweR\u0026#39;) library(PoweR) # o código identificador da distribuição de Shapiro-Wilk é o número 21 statcompute(21, mtcars$hp) ## $statistic ## [1] 0.9334193 ## ## $pvalue ## [1] 0.04880824 ## ## $decision ## [1] 1 1 ## ## $alter ## [1] 4 ## ## $stat.pars ## [1] NA ## ## $symbol ## [1] \u0026quot;W\u0026quot; Infelizmente o PoweR não tem a exata implementação do teste de Kolmogorov-Smirnov que temos no R. Entretanto, existem diversos testes, como o de Anderson-Darling e o teste Lilliefors, outros procedimentos comuns para testar a normalidade dos dados. No exemplo abaixo, podemos ver que esses dois outros testes concordam com o teste de Shapiro-Wilk: o resultado observado é improvável sob a hipótese de que os dados realmente foram obtidos de uma distribuição normal.\nlibrary(PoweR) print(\u0026quot;O código identificador da distribuição de Lilliefors é o número 1\u0026quot;) ## [1] \u0026quot;O código identificador da distribuição de Lilliefors é o número 1\u0026quot; statcompute(1, mtcars$hp) ## $statistic ## [1] 0.1663854 ## ## $pvalue ## [1] 0.02448416 ## ## $decision ## [1] 1 1 ## ## $alter ## [1] 3 ## ## $stat.pars ## [1] NA ## ## $symbol ## [1] \u0026quot;K-S\u0026quot; print(\u0026quot;O código identificador da distribuição de Anderson-Darling é o número 2\u0026quot;) ## [1] \u0026quot;O código identificador da distribuição de Anderson-Darling é o número 2\u0026quot; statcompute(2, mtcars$hp) ## $statistic ## [1] 0.7077449 ## ## $pvalue ## [1] 0.05839104 ## ## $decision ## [1] 0 1 ## ## $alter ## [1] 3 ## ## $stat.pars ## [1] NA ## ## $symbol ## [1] \u0026quot;AD^*\u0026quot; Você pode dar uma olhada em todos os procedimentos que estão disponíveis no PoweR e escolher os que você achar mais interessantes para você. Existem várias referências disponíveis na documentação do pacote e você pode se informar sobre eles através dos artigos que fundamentam os testes. É interessante usar vários testes diferentes, já que cada um deles funciona melhor/pior em determinadas situações, mas isso é um assunto para o próximo texto da série… Até lá!\nGostou? Quer saber mais? Se você quiser aprender um pouco mais sobre esse assunto, temos alguns cursos que tocam os temas deste post. Dê uma olhada nos nossos cursos de Regressão Linear ou Machine Learning e aproveite!\n","permalink":"https://blog.curso-r.com/posts/2021-02-04-testes-de-normalidade/","tags":["faxina","modelagem"],"title":"Testes de normalidade em R - Parte 2: Testes"},{"author":["Fernando"],"categories":["análises","conceitos","tutoriais","r"],"contents":" A distribuição normal (também conhecida como distribuição gaussiana) é um dos conceitos mais importantes da estatística. Ela nos ajuda a interpretar resultados experimentais em todas as áreas da ciência e também é o pressuposto de diversos modelos estatísticos, como por exemplo os modelos de regressão linear.\nNessa série de posts, vamos te ajudar a analisar estatisticamente se uma variável segue ou não a distribuição normal de maneira simples e rápida. Hoje vamos falar especificamente de gráficos!\nMétodo visual (Q-Q plot) Nada como construir um gráfico para nos informar rapidamente o que está acontecendo com uma variável! Para isso, vamos usar o pacote ggpubr, que além do Q-Q plot também ajuda a produzir ggplot2 prontos para publicação.\nVamos começar analisando a distribuição de duas variáveis de bases distintas: a potência dos carros (variável hp) na clássica mtcars e o comprimento da nadadeira dos pinguins de palmer (disponível em português no pacote dados: https://github.com/cienciadedatos/dados).\nComeçando pelo peso dos carros, a função que vamos usar para fazer os nosso gráfico é a função ggqqplot. O que essa função faz é comparar os quantis empíricos da nossa variável padronizada com os quantis teóricos da distribuição normal com média 0 e variância 1. Parece complicado, mas o gráfico é bem mais fácil de entender:\nlibrary(ggpubr) ggqqplot(mtcars$hp) A linha preta representa os resultados esperados se os dados seguissem a distribuição normal. Já a faixa cinza por sua vez representa a variabilidade tolerável ao redor dessa linha. Quanto mais o nossos dados (pontinhos pretos) escaparem dessa faixa mais fora do padrão da distribuição normal os dados estão. No caso acima nenhum ponto saiu da faixa, o que indica que os nossos dados seguem uma distribuição normal.\nVamos dar uma olhara em um caso em que não há distribuição normal para ver como fica?\nlibrary(ggpubr) #devtools::install_github(\u0026quot;https://github.com/cienciadedatos/dados\u0026quot;) library(dados) ggqqplot(pinguins$comprimento_nadadeira) Dessa vez dá para notar que tem muitos pontos fora da faixa né? Pois essa é a evidência de que estamos fora da distribuição normal!\nSe você quisesse parar de ler aqui e seguir com as suas análises de normalidade, você até poderia, mas perderia as malandragens que vamos te ensinar daqui para frente…\nMuitos dados podem prejudicar sua visualização! O método gráfico é fantástico quando temos poucos dados, mas conforme o tamanho da nossa base aumenta o tempo de plotagem e a visibilidade geral do gráfico é prejudicada.\nA base diamantes já nos dá uma ideia de como a visualização pode ficar ruim. Vamos analisar a variável profundidade para ter uma ideia:\nlibrary(ggpubr) #devtools::install_github(\u0026quot;https://github.com/cienciadedatos/dados\u0026quot;) library(dados) ggqqplot(diamante$profundidade) Fica claro que a variável não segue a distribuição normal, mas a faixa cinza fica completamente esmagada, nem conseguimos ver quantos pontos estão dentro ou fora dela. O gráfico não é completamente inútil, pois é possível ver que existem inúmeros pontos que depõem contra a normalidade da variável, mas é um fato que o resultado não é tão elegante quanto o anterior.\nO que fazemos nesse cenário então? É aqui que entra outro jeito legal de verificar se uma variável tem distribuição normal: um teste estatístico.\nMas isso é um assunto para o nosso próximo post! Fique ligada ou ligado!\nGostou? Quer saber mais? Se você quiser aprender um pouco mais sobre esse assunto, temos alguns cursos que tocam os temas deste post. Dê uma olhada nos nossos cursos de Regressão Linear ou Machine Learning e aproveite!\n","permalink":"https://blog.curso-r.com/posts/2021-01-28-testes-de-normalidade/","tags":["faxina","modelagem"],"title":"Testes de normalidade em R - Parte 1: Gráficos"},{"author":["Athos"],"categories":["Tutoriais"],"contents":" Resumo tabulizer::extract_tables(\"caminho/do/arquivo.pdf\", pages = 153)\nIsso é tudo!! Eu resolvi escrever esse post porque fiquei contente por ter tido grande facilidade em conseguir trazer uma tabela do PDF para o R depois do CTLR+C/CTRL+V falhar miseravelmente.\nUm potencial obstáculo pode ser o famigerado {rJava} do qual o {tabulizer} depende. Mas, no linux, eu rodei sudo apt install default-jdk default-jre e funcionou pra mim! No windows não testei, mas tem esse tutorial aqui.\nTarefa Extrair uma tabela de uma página de um arquivo pdf. No url_pdf está o endereço de um PDF da internet.\nurl_pdf \u0026lt;- \u0026quot;https://curso-r.github.io/main-regressao-linear/referencias/Ci%C3%AAncia%20de%20Dados.%20Fundamentos%20e%20Aplica%C3%A7%C3%B5es.%20Vers%C3%A3o%20parcial%20preliminar.%20maio%20Pedro%20A.%20Morettin%20Julio%20M.%20Singer.pdf\u0026quot; Olhada na página de PDF # tira print da pagina 153 e salva como imagem PNG. pdftools::pdf_convert(url_pdf, pages = 153, filenames = \u0026quot;pag153.png\u0026quot;) ## Converting page 153 to pag153.png... done! ## [1] \u0026quot;pag153.png\u0026quot; # mostra a imagem PNG. knitr::include_graphics(\u0026quot;/images/posts/conteudo/tabulizer/pag153.webp\u0026quot;) Extração da tabela PDF -\u0026gt; R # extrai a tabela do PDF (e não do PNG!) tabela_extrida_do_pdf \u0026lt;- tabulizer::extract_tables(url_pdf, pages = 153) tabela_extrida_do_pdf[[1]] ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] \u0026quot;\u0026quot; \u0026quot;Idade\u0026quot; \u0026quot;Distância\u0026quot; \u0026quot;\u0026quot; \u0026quot;Idade\u0026quot; \u0026quot;Distância\u0026quot; ## [2,] \u0026quot;Ident\u0026quot; \u0026quot;(anos)\u0026quot; \u0026quot;(m)\u0026quot; \u0026quot;Ident\u0026quot; \u0026quot;(anos)\u0026quot; \u0026quot;(m)\u0026quot; ## [3,] \u0026quot;1\u0026quot; \u0026quot;18\u0026quot; \u0026quot;170\u0026quot; \u0026quot;16\u0026quot; \u0026quot;55\u0026quot; \u0026quot;140\u0026quot; ## [4,] \u0026quot;2\u0026quot; \u0026quot;20\u0026quot; \u0026quot;197\u0026quot; \u0026quot;17\u0026quot; \u0026quot;63\u0026quot; \u0026quot;117\u0026quot; ## [5,] \u0026quot;3\u0026quot; \u0026quot;22\u0026quot; \u0026quot;187\u0026quot; \u0026quot;18\u0026quot; \u0026quot;65\u0026quot; \u0026quot;140\u0026quot; ## [6,] \u0026quot;4\u0026quot; \u0026quot;23\u0026quot; \u0026quot;170\u0026quot; \u0026quot;19\u0026quot; \u0026quot;66\u0026quot; \u0026quot;100\u0026quot; ## [7,] \u0026quot;5\u0026quot; \u0026quot;23\u0026quot; \u0026quot;153\u0026quot; \u0026quot;20\u0026quot; \u0026quot;67\u0026quot; \u0026quot;137\u0026quot; ## [8,] \u0026quot;6\u0026quot; \u0026quot;25\u0026quot; \u0026quot;163\u0026quot; \u0026quot;21\u0026quot; \u0026quot;68\u0026quot; \u0026quot;100\u0026quot; ## [9,] \u0026quot;7\u0026quot; \u0026quot;27\u0026quot; \u0026quot;187\u0026quot; \u0026quot;22\u0026quot; \u0026quot;70\u0026quot; \u0026quot;130\u0026quot; ## [10,] \u0026quot;8\u0026quot; \u0026quot;28\u0026quot; \u0026quot;170\u0026quot; \u0026quot;23\u0026quot; \u0026quot;71\u0026quot; \u0026quot;107\u0026quot; ## [11,] \u0026quot;9\u0026quot; \u0026quot;29\u0026quot; \u0026quot;153\u0026quot; \u0026quot;24\u0026quot; \u0026quot;72\u0026quot; \u0026quot;123\u0026quot; ## [12,] \u0026quot;10\u0026quot; \u0026quot;32\u0026quot; \u0026quot;137\u0026quot; \u0026quot;25\u0026quot; \u0026quot;73\u0026quot; \u0026quot;93\u0026quot; ## [13,] \u0026quot;11\u0026quot; \u0026quot;37\u0026quot; \u0026quot;140\u0026quot; \u0026quot;26\u0026quot; \u0026quot;74\u0026quot; \u0026quot;140\u0026quot; ## [14,] \u0026quot;12\u0026quot; \u0026quot;41\u0026quot; \u0026quot;153\u0026quot; \u0026quot;27\u0026quot; \u0026quot;75\u0026quot; \u0026quot;153\u0026quot; ## [15,] \u0026quot;13\u0026quot; \u0026quot;46\u0026quot; \u0026quot;150\u0026quot; \u0026quot;28\u0026quot; \u0026quot;77\u0026quot; \u0026quot;120\u0026quot; ## [16,] \u0026quot;14\u0026quot; \u0026quot;49\u0026quot; \u0026quot;127\u0026quot; \u0026quot;29\u0026quot; \u0026quot;79\u0026quot; \u0026quot;103\u0026quot; ## [17,] \u0026quot;15\u0026quot; \u0026quot;53\u0026quot; \u0026quot;153\u0026quot; \u0026quot;30\u0026quot; \u0026quot;82\u0026quot; \u0026quot;120\u0026quot; CQD! ⬛\nBônus: Faxina na tabela Uma vez dentro do R, agora é jogo. Bora faxinar que é o que a gente sabe fazer de melhor. O desafio é empilhar de 3 em 3 colunas, Ident, (anos), (m). Temos dois indivíduos por linha, precisamos empilhá-los para no fim termos uma linha para cada indivíduo como manda o manual. O herói aqui é o pivot_longer().\nlibrary(tidyverse) library(janitor) tabela_extrida_do_pdf[[1]] %\u0026gt;% as_tibble(.name_repair = \u0026quot;unique\u0026quot;) %\u0026gt;% row_to_names(2) %\u0026gt;% clean_names() %\u0026gt;% pivot_longer(everything(), names_to = c(\u0026quot;.value\u0026quot;, \u0026quot;conjunto\u0026quot;), names_sep = \u0026quot;_\u0026quot;) %\u0026gt;% select(-conjunto) %\u0026gt;% head() %\u0026gt;% kableExtra::kable() ident anos m 1 18 170 16 55 140 2 20 197 17 63 117 3 22 187 18 65 140 Leituras relacionadas Sugiro a leitura do post PDF e OCR do Julio Trecenti para funcionalidades mais avançadas do {pdftools}, do {tesseract} e do {tabulizer}.\nComentário aleatório Aproveitem para conhecer o livro Ciência de dados: Fundamentos e Prática de 2020 dos Professores Julio Singer e Renato Morettin.\nValeu!\n","permalink":"https://blog.curso-r.com/posts/2021-01-08-tabulizer/","tags":["faxina","pdf","tabulizer","pdftools"],"title":"{tabulizer}: Tabela do PDF para data.frame em segundos"},{"author":["Caio"],"categories":["pacotes"],"contents":" {stockfish} é um pacote R que implementa o protocolo aberto de comunicação UCI e acompanha uma instalação do Stockfish 11, um motor de xadrez muito popular, open source e poderoso escrito em C++.\nInstalação install.packages(\u0026quot;stockfish\u0026quot;) Ou instale a versão em desenvolvimento do GitHub com:\n# install.packages(\u0026quot;remotes\u0026quot;) remotes::install_github(\u0026quot;curso-r/stockfish\u0026quot;) Você também pode encontrar mais (e mais recentes) versões do motor Stockfish para usar com o {stockfish} na página de download do projeto.\nExemplo É tão fácil usar o {stockfish} quanto qualquer outro pacote UCI. Você deve inicializar o motor com fish$new() e enviar comandos com os seus métodos internos, lembrando apenas de rodar quit() quando você terminar.\nlibrary(stockfish) # Inicializar o motor engine \u0026lt;- fish$new() # Examinar o processo em plano de fundo engine$process #\u0026gt; PROCESS \u0026#39;stockfish\u0026#39;, running, pid 173670. # Procurar a melhor jogada engine$go() #\u0026gt; [1] \u0026quot;bestmove e2e4 ponder d7d5\u0026quot; # Começar um jogo a partir de um FEN engine$ucinewgame() engine$position(\u0026quot;6rk/2Q3p1/5qBp/5P2/8/7P/6PK/8 w - - 15 51\u0026quot;) engine$go() #\u0026gt; [1] \u0026quot;bestmove g6f7 ponder g8d8\u0026quot; # Finalizar o motor engine$quit() Uso fish, a principal classe do pacote, representa um motor Stockfish, permitindo que o usuário envie comandos e receba saídas de acordo com o protocolo UCI. Resumindo, um objeto fish, quando criado, cria um processo Stockfish desacoplado e faz um pipe com as suas entrada e saída padrões.\nPara mais informações, veja a sua documentação completa executando ?fish.\nStockfish Incluso O pacote acompanha uma instalação do Stockfish, um motor de xadrez muito popular, open source e poderoso escrito em C++. Ele consegue chegar em um ELO de 3516, roda no Windows, macOS, Linux, iOS e Android, e pode ser compilado em menos de um minuto.\nQuando o {stockfish} (inicial minúscula) estiver sendo instalado, o código-fonte do Stockfish (inicial maiúscula) é compilado e o executável resultante é armazenado junto com os seus pacote R. Isso não é uma instalação para todos os usuários! Você não precisa dar privilégios administrativos para ele rodar ou se preocupar com qualquer software adicional.\nO único ponto negativo é que a versão inclusa é o Stockfish 11, não a versão mais recente (Stockfish 12). Isso ocorre porque a versão 12 precisa de downloads adicionais, o que aumentaria dramaticamente o tempo de instalação. Se quiser, você pode baixar a versão de sua escolha e passar o caminho para o executável como argumento para fish$new().\nProtocolo UCI UCI (Universal Chess Interface) é um protocolo aberto de comunicação que permite que motores de xadrez se comuniquem com interfaces de usuário. Na verdade, a classe fish implementa o protocolo UCI como publicado por Stefan-Meyer Kahlen, apenas com um foco no Stockfish. Isso significa que alguns métodos não são implementados (veja Pegadinhas) e que todos os testes são feitos com o Stockfish, mas todo o resto deveria funcionar perfeitamente com outros motores.\nO texto entre aspas no fim da documentação de cada método foi extraído diretamente do protocolo UCI oficial para que você possa ver exatamente o que aquele comando pode fazer. No geral, os comandos são bem autoexplicativos, exceto pela LAN (long algebraic notation), a notação de jogadas usada pelo UCI. Nessa notação, jogadas são registradas usando as posições inicial e final da peça, por exemplo, e2e4, e7e5, e1g1 (roque pequeno das brancas), e7e8q (promoção), 0000 (jogada vazia).\nImplementação Todo o trabalho duro da classe fish é feito pelo pacote {processx}. O processo do Stockfish é criado com processx::process$new e a entrada/saída é feita com write_input() e read_output(). Um aspecto importante do protocolo de comunicação de qualquer motor UCI é esperar por respostas, e aqui isso é feito com um loop que verifica o processo com poll_io() e para assim que a saída volta vazia.\nAntes de implementar o protocolo UCI manualmente, o pacote usava o {bigchess}. Ele é um pacote incrível criado por @rosawojciech, mas ele tem algumas dependências que estão além do escopo do {stockfish} e, no final, eu queria mais controle da API (por exemplo usando {R6}).\nPegadinhas A classe fish tem algumas idiossincrasias que o usuário deve ter em mente quando tentar se comunicar com o Stockfish. Algumas são devidas a escolhas de implementação, mas outras estão relacionadas ao protocolo UCI em si. Esta não é uma lista completa (e você provavelmente deveria ler a documentação do UCI), mas aqui vão algumas coisas para ficar de olho.\nNem todos os métodos UCI foram implementados: como o {stockfish} foi feito com o Stockfish (e especialmente o Stockfish 11) em mente, dois métodos do UCI que não funcionam com o motor não foram implementados. Eles são debug() e register().\nA maioria dos métodos retorna silenciosamente: como a maioria dos comandos UCI não retornam nada ou retornam texto padrão, a maioria dos métodos retorna silenciosamente. As exceções são run(), isready(), go() e stop(); você pode ver exatamente o que eles retornam lendo as suas documentações.\nNem toda opção do Stockfish vai funcionar: pelo menos quando usando a versão inclusa do Stockfish, nem todas as opções documentadas vão funcionar com setoption(). Isso ocorre porque, como descrito acima, o pacote vem com o Stockfish 11, que não é a versão mais recente. Opções que não vão funcionar estão marcadas com um asterisco.\nTempos são em milissegundos. diferentemente da maioria das funções do R, todo método que recebe um intervalo de tempo espera que o mesmo seja em milissegundos, não segundos.\n","permalink":"https://blog.curso-r.com/posts/2020-12-18-stockfish/","tags":["xadrez"],"title":"Stockfish no R"},{"author":["Beatriz Milz","William"],"categories":["divulgação"],"contents":" Olá!\nEste post é uma continuação do post onde apresentamos os trabalhos premiados no curso R para Ciência de Dados I, e baseado nos feedbacks que enviamos para as pessoas alunas, escrevemos algumas dicas de formatação de tabelas. Neste post, apresentaremos algumas dicas para elaborar gráficos com o pacote ggplot2! Gráficos são representações visuais dos dados. Se bem construído, a informação nele é absorvida mais rapidamente. Além disso, funcionam como pausas, deixando a leitura de um relatório menos cansativa.\nFigura 1: Ilustração sobre o pacote ggplot2, criada por Allison Horst. Dicas para formatar melhor seus gráficos Primeiro vamos carregar o pacote tidyverse:\nlibrary(tidyverse) O próximo passo é carregar a base que utilizaremos como exemplo: a base de filmes do IMDB.\nimdb \u0026lt;- read_delim( \u0026quot;https://raw.githubusercontent.com/curso-r/202010-r4ds-1/master/dados/imdb2.csv\u0026quot;, \u0026quot;;\u0026quot;, escape_double = FALSE, trim_ws = TRUE ) Exemplo: Diretores de sucesso! Preparar a base de dados Neste exemplo, vamos criar um gráfico que apresente o lucro médio dos filmes segundo o diretor, para identificar pessoas diretoras de sucesso! Primeiro precisaremos obter as informações necessárias para criar o gráfico.\nA base imdb apresenta 3713 linhas, sendo que cada linha possui informação de um filme. Dentre as variáveis presentes na base são o orçamento e a receita dos filmes. Com essas variáveis, é possível calcular o lucro de cada filme. Porém, antes vamos remover os filmes das quais não temos as informações de orçamento e/ou receita (ou seja, NA). Existem diversas formas de remover os valores faltantes - NA - de uma base, então utilizaremos a função drop_na(), do pacote tidyr, indicando como argumento as colunas em que desejamos remover os NA. Depois, utilizando a função mutate(), criaremos uma nova coluna que apresenta o valor do lucro de cada filme:\nimdb_lucro \u0026lt;- imdb %\u0026gt;% drop_na(orcamento, receita) %\u0026gt;% mutate(lucro = receita - orcamento) Para obter a média do lucro por diretor, podemos utilizar as funções group_by() para agrupar por diretor, e summarise() para fazer as sumarizações necessárias: neste caso calcularemos a média (com a função mean()), e também o número de filmes por diretor (com a função n() - que conta o número de linhas, sendo que cada linha representa um filme).\nimdb_lucro_diretores \u0026lt;- imdb_lucro %\u0026gt;% group_by(diretor) %\u0026gt;% summarise(media_lucro = mean(lucro), numero_filmes = n()) head(imdb_lucro_diretores) ## # A tibble: 6 x 3 ## diretor media_lucro numero_filmes ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 Aaron Schneider 1676553 1 ## 2 Aaron Seltzer 28546578 1 ## 3 Abel Ferrara -11272676 1 ## 4 Adam Carolla -1394057 1 ## 5 Adam Goldberg -1647420 1 ## 6 Adam Marcus 13435068 1 Porém não podemos apresentar todas as 1419 pessoas diretoras no mesmo gráfico, pois não seria legível! Precisamos definir alguns critérios para selecionar as pessoas diretoras que aparecerão no gráfico.\nO primeiro critério escolhido é ter dirigido mais que um filme! Com a função filter(condicao), esse critério é possível de ser satisfeito. Depois, ordenaremos a base de forma decrescente segundo o lucro médio, assim teremos nas primeiras linhas as pessoas diretoras com os maiores lucros médios. Isso é possível de ser feito com as funções arrange(desc(variavel)). Com a função slice(), podemos ‘cortar’ a base: utilizando como argumento 1:10, estamos solicitando apenas as linhas 1 até 10.\nNeste caso, o resultado destes critérios será uma base com as 10 pessoas diretoras com maior lucro médio e que tenham dirigido mais de um filme.\nimdb_top_diretores \u0026lt;- imdb_lucro_diretores %\u0026gt;% filter(numero_filmes \u0026gt; 1) %\u0026gt;% arrange(desc(media_lucro)) %\u0026gt;% slice(1:10) imdb_top_diretores ## # A tibble: 10 x 3 ## diretor media_lucro numero_filmes ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 George Lucas 277328296 5 ## 2 Colin Trevorrow 252717532. 2 ## 3 Pierre Coffin 237275640 2 ## 4 Joss Whedon 199202360. 3 ## 5 James Cameron 194620985 6 ## 6 Roger Allers 188543668. 2 ## 7 Pete Docter 158113780. 3 ## 8 Francis Lawrence 151100394. 5 ## 9 Peter Jackson 132967515 5 ## 10 Andrew Adamson 130611730 4 Criar o gráfico Agora podemos começar a criar o gráfico (e apresentar algumas dicas!).\nO código abaixo apresenta um gráfico de barras (geom_col()) simples, criado com o pacote ggplot2, e vamos melhorá-lo por etapas!\nimdb_top_diretores %\u0026gt;% ggplot() + geom_col(aes(x = diretor, y = media_lucro)) A primeira dica é: quando os nomes no eixo x estiverem se sobrepondo (no caso deste gráfico, o nome dos diretores), podemos inverter os eixos. Uma forma de fazer isso é utilizando a função coord_flip():\nimdb_top_diretores %\u0026gt;% ggplot() + geom_col(aes(x = diretor, y = media_lucro)) + coord_flip() Agora está muito mais fácil de ler os nomes dos diretores!\nQuando trabalhamos com variáveis que representam dinheiro (como no caso da variável lucro), podemos formatar os valores para que fiquem formatados como dinheiro, utilizando a função scales::dollar() :\nimdb_top_diretores %\u0026gt;% ggplot() + geom_col(aes(x = diretor, y = media_lucro)) + scale_y_continuous(labels = scales::dollar) + coord_flip() Agora que os eixos estão bem formatados, podemos formatar a legenda para ficar claro o que cada eixo significa! A função labs() permite escrever o que desejamos que apareça como legenda em cada elemento (por exemplo: labs(x = \"Legenda do eixo X\", y = \"Legenda do eixo Y\") .\nimdb_top_diretores %\u0026gt;% ggplot() + geom_col(aes(x = diretor, y = media_lucro)) + scale_y_continuous(labels = scales::dollar) + labs(x = \u0026quot;Pessoas diretoras\u0026quot;, y = \u0026quot;Lucro médio\u0026quot;) + coord_flip() É interessante também ordenar as barras do gráfico de barras, pois facilita a comparação das categorias (neste caso, as pessoas diretoras).\nPara isso, podemos tratar a variável diretor como um fator (factor) 1, e ordenar os níveis (levels) de diretores segundo a variável media_lucro. A função fct_reorder() do pacote forcats pode ser utilizada para isso, pois ela ordena os níveis de um fator utilizando a ordem de outra variável.\nimdb_top_diretores %\u0026gt;% mutate(diretor = forcats::fct_reorder(diretor, media_lucro)) %\u0026gt;% ggplot() + geom_col(aes(x = diretor, y = media_lucro)) + scale_y_continuous(labels = scales::dollar) + coord_flip() + labs(x = \u0026quot;Pessoas diretoras\u0026quot;, y = \u0026quot;Lucro médio\u0026quot;) Por último (mas também importante) é uma dica estética, porém tem o poder de deixar o gráfico muito mais atrativo! Existem várias funções no pacote ggplot2 com temas para os gráficos, e elas usam o prefixo theme_ (por exemplo, theme_light()). Além disso, existem pacotes que oferecem mais temas, como o ggthemr e o ggthemes!\nO exemplo abaixo utiliza o tema theme_light():\nimdb_top_diretores %\u0026gt;% mutate(diretor = forcats::fct_reorder(diretor, media_lucro)) %\u0026gt;% ggplot() + geom_col(aes(x = diretor, y = media_lucro)) + scale_y_continuous(labels = scales::dollar) + labs(x = \u0026quot;Pessoas diretoras\u0026quot;, y = \u0026quot;Lucro médio\u0026quot;) + coord_flip() + theme_bw() O exemplo abaixo utiliza o tema flat do pacote ggthemr:\nimdb_top_diretores %\u0026gt;% mutate(diretor = forcats::fct_reorder(diretor, media_lucro)) %\u0026gt;% ggplot() + geom_col(aes(x = diretor, y = media_lucro)) + scale_y_continuous(labels = scales::dollar) + coord_flip() + labs(x = \u0026quot;Pessoas diretoras\u0026quot;, y = \u0026quot;Lucro médio\u0026quot;) + ggthemr::ggthemr(\u0026quot;flat\u0026quot;)$theme Conclusões Depois dessas dicas, o resultado é um gráfico mais atraente e informativo do que a versão básica que fizemos com o ggplot2 no começo do post!\nExistem infinitas possibilidades para criar visualizações de dados com R. Neste post apresentamos algumas dicas simples para formatar melhor nossos gráficos, e no próximo post apresentaremos mais algumas dicas sobre relatórios com o pacote R Markdown.\nE caso você tenha interesse em saber mais sobre os cursos, a página de cursos foi atualizada, e as incrições para os cursos de 2021 já estão abertas!\nFalamos um pouco sobre fatores no post anterior.↩︎\n","permalink":"https://blog.curso-r.com/posts/2020-17-02-dicas-relatorios-r4ds1_graficos/","tags":["gráficos"],"title":"Feedback dos trabalhos do R para Ciência de Dados (Parte II): Dicas para Gráficos"},{"author":["Beatriz Milz","William Amorim"],"categories":["divulgação"],"contents":" Na última edição do curso de R para Ciência de dados I, realizada em outubro de 2020, apresentamos para as pessoas alunas como trabalho final a tarefa de realizar uma análise estatística (descritiva) e comunicar os resultados utilizando um relatório produzido em R Markdown. Como sugestão, as pessoas poderiam utilizar a base de filmes do IMDB para realizar a análise, porém eram livres para escolher outros temas de interesse também.\nAssim como nas últimas edições do curso, para motivar as pessoas alunas, resolvemos premiar os melhores trabalhos com vagas em qualquer um dos nossos cursos. Neste post apresentamos o objetivo deste tipo de avaliação, além dos trabalhos premiados da turma de maio de 2020.\nEste post será um pouco diferente deste: primeiro apresentaremos os trabalhos premiados, e então baseado nos feedbacks que escrevemos sobre os trabalhos, apresentaremos algumas dicas para visualizações de dados e relatórios com RMarkdown. O conteúdo foi dividido para que o post não ficasse muito extenso, então neste post as dicas serão sobre formatação de tabelas!\nTrabalhos premiados Luísa Böck: O trabalho da Luisa é uma análise sobre a série The Office, e tanto a análise dos dados, a parte estética e o texto ficaram muito legais! Para quem gosta da série, é uma oportunidade para aprender mais sobre ela! Confira o trabalho neste link.\nAndré Oliveira: O trabalho do André é uma análise utilizando a base de filmes do IMDB. O trabalho também ficou muito legal: a análise dos dados, a contextualização e a parte estética foram bem trabalhadas! Confira o trabalho neste link.\nParabéns, Luísa e André!\n5 Dicas para formatar melhor suas tabelas Agora seguimos para as dicas!\nPrimeiro vamos carregar o pacote tidyverse:\nlibrary(tidyverse) O próximo passo é carregar a base que utilizaremos como exemplo: a base de filmes do IMDB.\nimdb \u0026lt;- read_delim( \u0026quot;https://raw.githubusercontent.com/curso-r/202010-r4ds-1/master/dados/imdb2.csv\u0026quot;, \u0026quot;;\u0026quot;, escape_double = FALSE, trim_ws = TRUE ) Exemplo 1: Ordenar, formatar dinheiro e nomear as colunas No exemplo a seguir, primeiramente vamos selecionar as variáveis que representam o nome e o orçamento dos filmes (com a função select()). É interessante ordenar a tabela segundo alguma variável, para facilitar a leitura. No exemplo a seguir, vamos ordenar pelo orçamento de forma decrescente utilizando a função arrange(desc(variavel)).\nVamos selecionar apenas as primeiras linhas da base, para facilitar a apresentação neste post (utilizando a função head()), e salvar em um objeto chamado filmes_orcamento (que usaremos nos códigos a seguir).\nfilmes_orcamento \u0026lt;- imdb %\u0026gt;% select(titulo, orcamento) %\u0026gt;% arrange(desc(orcamento)) %\u0026gt;% head() filmes_orcamento ## # A tibble: 6 x 2 ## titulo orcamento ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Pirates of the Caribbean: At World\u0026#39;s End 300000000 ## 2 John Carter 263700000 ## 3 Tangled 260000000 ## 4 Spider-Man 3 258000000 ## 5 The Dark Knight Rises 250000000 ## 6 Avengers: Age of Ultron 250000000 Para variáveis que representam dinheiro, é possível usar a função scales::dollar() para que os valores fiquem formatados como dinheiro (e fiquem mais legíveis!).\nfilmes_orcamento %\u0026gt;% mutate(orcamento = scales::dollar(orcamento)) ## # A tibble: 6 x 2 ## titulo orcamento ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Pirates of the Caribbean: At World\u0026#39;s End $300,000,000 ## 2 John Carter $263,700,000 ## 3 Tangled $260,000,000 ## 4 Spider-Man 3 $258,000,000 ## 5 The Dark Knight Rises $250,000,000 ## 6 Avengers: Age of Ultron $250,000,000 A tabela anteiror foi apresentada igual a um código que aparece como retorno no console do R, e isso não fica legal em um relatório. Existem diversas formas de apresentar tabelas, mas neste exemplo utilizaremos a função knitr::kable() 1. Além disso, é comum que o nome das variáveis estejam escritas com letras minúsculas, sem acentuação, etc (por exemplo: orcamento). Para que a tabela fique melhor formatada, podemos renomear o nome das colunas usando o argumento col.names da função knitr::kable(), informando um vetor com os novos nomes para as colunas da tabela.\nimdb %\u0026gt;% select(titulo, orcamento) %\u0026gt;% arrange(desc(orcamento)) %\u0026gt;% mutate(orcamento = scales::dollar(orcamento)) %\u0026gt;% head() %\u0026gt;% knitr::kable(col.names = c(\u0026quot;Título do filme\u0026quot;, \u0026quot;Orçamento\u0026quot;)) Título do filme Orçamento Pirates of the Caribbean: At World’s End $300,000,000 John Carter $263,700,000 Tangled $260,000,000 Spider-Man 3 $258,000,000 The Dark Knight Rises $250,000,000 Avengers: Age of Ultron $250,000,000 A tabela ficou mais fácil de ser lida. O que achou do resultado?\nExemplo 2: Arredondar valores e ordenar categorias No exemplo a seguir, vamos calcular a nota média dos filmes na base do IMDB para cada classificação etária. Fazemos isso utilizando as funções group_by() e summarise().\nSalvaremos o resultado em um objeto chamado nota_classificacao (que usaremos nos códigos a seguir).\nnota_classificacao \u0026lt;- imdb %\u0026gt;% group_by(classificacao) %\u0026gt;% summarise(nota_media_imdb = mean(nota_imdb, na.rm = TRUE)) nota_classificacao ## # A tibble: 4 x 2 ## classificacao nota_media_imdb ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 A partir de 13 anos 61.8 ## 2 A partir de 18 anos 64.6 ## 3 Livre 62.6 ## 4 Outros 66.3 O primeiro ponto interessante para arrumar são os valores da variável nota_media_imdb, arredondando os valores para que não apareça as casas decimais. Para melhorar a apresentação, podemos usar a função round() (utilizada para arredondar números) junto com a função mutate() (que altera ou cria novas variáveis). Com o argumento digits da função round(), é possível definir quantas casas decimais você quer que o número tenha.\nnota_classificacao %\u0026gt;% mutate(nota_media_imdb = round(nota_media_imdb, digits = 0)) ## # A tibble: 4 x 2 ## classificacao nota_media_imdb ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 A partir de 13 anos 62 ## 2 A partir de 18 anos 65 ## 3 Livre 63 ## 4 Outros 66 Na base que estamos usando de exemplo, a variável classificacao é uma variável categórica do tipo chr (character), ou seja, um texto. Por padrão, a ordem utilizada é a alfabética (repare no resultado anterior, primeiro aparecem as classificações que começam com A, depois com L e por último a que começa com O).\nPara ordenar essa variável, precisamos transformar ela em um fator. Portanto, usaremos o pacote forcats, que faz parte do tidyverse e possui funções para trabalhar com fatores 2.\nUsando a função mutate() com fct_relevel() (do pacote forcats), podemos alterar a ordem dos níveis desta variável, passando como argumento:\na variável que queremos transformar em fator e alterar os seus níveis (os levels),\num vetor com os níveis (levels), na ordem que desejamos.\nPosteriormente podemos ordenar a variável utilizando a função arrange(), e teremos como resultado a tabela ordenada de uma forma que faz mais sentido:\nnota_classificacao %\u0026gt;% mutate(nota_media_imdb = round(nota_media_imdb, digits = 0)) %\u0026gt;% mutate(classificacao = forcats::fct_relevel( classificacao, c(\u0026quot;Livre\u0026quot;, \u0026quot;A partir de 13 anos\u0026quot;, \u0026quot;A partir de 18 anos\u0026quot;, \u0026quot;Outros\u0026quot;) )) %\u0026gt;% arrange(classificacao) %\u0026gt;% knitr::kable(col.names = c(\u0026quot;Classificação etária\u0026quot;, \u0026quot;Nota média no IMDB\u0026quot;)) Classificação etária Nota média no IMDB Livre 63 A partir de 13 anos 62 A partir de 18 anos 65 Outros 66 Conclusão Neste post conhecemos os trabalhos premiados no curso de R para Ciência de dados I oferecido em outubro/2020, assim como algumas dicas que surgiram a partir do feedback dos trabalhos.\nNos próximos posts apresentaremos mais algumas dicas sobre gráficos com o pacote ggplot2 e relatórios com o pacote R Markdown.\nE caso você tenha interesse em saber mais sobre os cursos, a página de cursos foi atualizada, e as incrições para os cursos de 2021 já estão abertas!\nOutros pacotes que permitem criar tabelas são: DT, reactable, flextable e kableExtra. Essa página também apresenta materiais muito ricos sobre elaboração de tabelas!↩︎\nCaso queira saber mais sobre fatores e o pacote forcats, recomendo consutar este material da Curso-R.↩︎\n","permalink":"https://blog.curso-r.com/posts/2020-12-03-dicas-relatorios-r4ds1_tabelas/","tags":["relatórios"],"title":"Trabalhos premiados do curso R para Ciência de Dados I + Dicas !"},{"author":["Caio"],"categories":["tutoriais"],"contents":" {chess} é uma interface opinionada em R para o python-chess, uma biblioteca incrível criada por Niklas Fiekas. Ela permite que usuários leiam e escrevam arquivos PGN, além de possibilitar a criação e exploração de árvores de jogos como aquelas presentes em livros de xadrez.\nEste pacote ainda está amadurecendo! Então eu encorajo todos a enviar suas sugestões e bugs em issues no repositório do pacote.\nInstalação Você pode instalar a versão mais recente do {chess} do CRAN com:\ninstall.packages(\u0026quot;chess\u0026quot;) Isso deveria automaticamente instalar o python-chess no seu ambiente do {reticulate}, mas você também pode fazer isso explicitamente com o comando:\nchess::install_chess() Exemplo Para ler um jogo existente, basta usar read_game(). Para explorar ele, você pode usar forward()/back(), assim como variations()/variation() para ver todas as variações disponíveis para o próximo movimento e escolher uma delas.\nlibrary(chess) # Ler o primeiro jogo de My 60 Memorable Games file \u0026lt;- system.file(\u0026quot;m60mg.pgn\u0026quot;, package = \u0026quot;chess\u0026quot;) fischer_sherwin \u0026lt;- read_game(file, n_max = 1) # Posição inicial fischer_sherwin #\u0026gt; \u0026lt;Start\u0026gt; #\u0026gt; r n b q k b n r #\u0026gt; p p p p p p p p #\u0026gt; . . . . . . . . #\u0026gt; . . . . . . . . #\u0026gt; . . . . . . . . #\u0026gt; . . . . . . . . #\u0026gt; P P P P P P P P #\u0026gt; R N B Q K B N R # Navegar para 4. g3 fischer_sherwin %\u0026gt;% forward(7) #\u0026gt; \u0026lt;4. g3\u0026gt; #\u0026gt; r . b q k b n r #\u0026gt; p p . p . p p p #\u0026gt; . . n . p . . . #\u0026gt; . . p . . . . . #\u0026gt; . . . . P . . . #\u0026gt; . . . P . N P . #\u0026gt; P P P . . P . P #\u0026gt; R N B Q K B . R # Ver todas as variações de 4... fischer_sherwin %\u0026gt;% forward(7) %\u0026gt;% variations() #\u0026gt; \u0026lt;4... Nf6\u0026gt; \u0026lt;4... d5\u0026gt; #\u0026gt; r . b q k b . r r . b q k b n r #\u0026gt; p p . p . p p p p p . . . p p p #\u0026gt; . . n . p n . . . . n . p . . . #\u0026gt; . . p . . . . . . . p p . . . . #\u0026gt; . . . . P . . . . . . . P . . . #\u0026gt; . . . P . N P . . . . P . N P . #\u0026gt; P P P . . P . P P P P . . P . P #\u0026gt; R N B Q K B . R R N B Q K B . R # Seguir a linha auxiliar fischer_sherwin %\u0026gt;% forward(7) %\u0026gt;% variation(2) #\u0026gt; \u0026lt;4... d5\u0026gt; #\u0026gt; r . b q k b n r #\u0026gt; p p . . . p p p #\u0026gt; . . n . p . . . #\u0026gt; . . p p . . . . #\u0026gt; . . . . P . . . #\u0026gt; . . . P . N P . #\u0026gt; P P P . . P . P #\u0026gt; R N B Q K B . R Você também pode criar o seu próprio jogo com game() e adicionar variações ao mesmo: a função move() adiciona jogadas assim como ramos na árvore do jogo. Strings são convertidas para jogadas simples, enquanto list() funciona exatamente como os parênteses de um PGN, criando uma variação para a última jogada. Aqui podemos ver como recriar o Mate do Pastor e algumas formas de evitá-lo:\n# Mate do Pastor e algumas defesas scholars_mate \u0026lt;- game() %\u0026gt;% move(\u0026quot;e4\u0026quot;) %\u0026gt;% move(\u0026quot;e5\u0026quot;, list(\u0026quot;e6\u0026quot;), list(\u0026quot;d5\u0026quot;)) %\u0026gt;% move(\u0026quot;Bc4\u0026quot;) %\u0026gt;% move(\u0026quot;Nc6\u0026quot;, list(\u0026quot;Nf6\u0026quot;)) %\u0026gt;% move(\u0026quot;Qh5\u0026quot;) %\u0026gt;% move(\u0026quot;Nf6\u0026quot;, list(\u0026quot;g6\u0026quot;, \u0026quot;Qf3\u0026quot;, \u0026quot;Nf6\u0026quot;)) %\u0026gt;% move(\u0026quot;Qxf7\u0026quot;) # Última jogada da linha principal scholars_mate #\u0026gt; \u0026lt;4. Qxf7#\u0026gt; #\u0026gt; r . b q k b . r #\u0026gt; p p p p . Q p p #\u0026gt; . . n . . n . . #\u0026gt; . . . . p . . . #\u0026gt; . . B . P . . . #\u0026gt; . . . . . . . . #\u0026gt; P P P P . P P P #\u0026gt; R N B . K . N R Note que há muitas formas de estruturar a entrada de move(). Veja vignette(\"chess\") para mais informações.\n{chess} também traz muitas formas de ver tanto o jogo como um todo, quanto o tabuleiro em um momento específico.\n# Tabuleiro com unicode (não fica bonito no navegador) print(scholars_mate, unicode = TRUE) #\u0026gt; \u0026lt;4. Qxf7#\u0026gt; #\u0026gt; ♜ . ♝ ♛ ♚ ♝ . ♜ #\u0026gt; ♟ ♟ ♟ ♟ . ♕ ♟ ♟ #\u0026gt; . . ♞ . . ♞ . . #\u0026gt; . . . . ♟ . . . #\u0026gt; . . ♗ . ♙ . . . #\u0026gt; . . . . . . . . #\u0026gt; ♙ ♙ ♙ ♙ . ♙ ♙ ♙ #\u0026gt; ♖ ♘ ♗ . ♔ . ♘ ♖ # Exportar o FEN do tabuleiro fen(scholars_mate) #\u0026gt; [1] \u0026quot;r1bqkb1r/pppp1Qpp/2n2n2/4p3/2B1P3/8/PPPP1PPP/RNB1K1NR b KQkq - 0 4\u0026quot; # Ver o PGN depois de um movimento str(back(scholars_mate, 3)) #\u0026gt; 2... Nc6 3. Qh5 Nf6 ( 3... g6 4. Qf3 Nf6 ) 4. Qxf7# # Exportar o PGN depois de um movimento pgn(back(scholars_mate, 3)) #\u0026gt; [1] \u0026quot;2... Nc6 3. Qh5 Nf6 ( 3... g6 4. Qf3 Nf6 ) 4. Qxf7#\u0026quot; # Imagem do tabuleiro atual plot(scholars_mate) Motivação O python-chess serviu como inspiração (e base) para o {chess}. Enquanto a versão original (e o {rchess}, por sinal) trabalha genericamente com “geração de jogadas, validação de jogadas” (com classes poderosas e sintaxe orientada a objeto), o {chess} é focado em facilitar a criação e exploração de árvores PGN.\nAo limitar o escopo da API, eu acredito que o pacote fica mais intuitivo para pessoas que só querem um jeito rápido de criar análises de jogos compartilháveis ou facilmente explorar os jogos dos outros sem precisar depender de um software visual.\nO primeiro uso do {chess} foi me ajudando a estudar o My 60 Memorable Games do Bobby Fischer. Depois de um parsing muito difícil, eu consegui converter o livro todo para PGN e disponibilizá-lo no lichess, mas eu ainda achava que a interface não era boa o suficiente…\n","permalink":"https://blog.curso-r.com/posts/2020-11-25-chess/","tags":["xadrez"],"title":"Xadrez no R com {chess}"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Hoje acordei curioso para saber qual era a capital mais distante de Brasília. Essa não é uma questão tão trivial quanto parece, pois, como a Terra é esférica (apesar de todas as controvérsias), podemos chegar em um ponto por mais de um caminho. Por isso, resolvi calcular todas as distâncias e montar um mapinha!\nEncontrando as coordenadas geográficas O primeiro passo para essa aventura foi encontrar as coordenadas geográficas das capitais de todo o mundo. Encontrei nesse post uma forma de fazer isso em R. O post estava desatualizado, então resolvi reescrevê-lo da maneira tidy.\nComeçamos usando o maravilhoso pacote {httr} para obter o HTML da página que contém os dados. Precisei modificar o User-Agent pois, sem ele, a requisição retornava com código 406.\n## Download a partir do site r_capitals \u0026lt;- httr::GET( \u0026quot;https://lab.lmnixon.org/4th/worldcapitals.html\u0026quot;, httr::user_agent(\u0026quot;Mozilla/5.0 (X11; Linux x86_64)\u0026quot;) ) Em seguida, usamos nosso queridinho {xml2} para encontrar a tabela, e o não tão queridinho {rvest} para transformar essa tabela (temos uma discussão sobre isso aqui). Os pacotes {tibble} e {janitor} foram usados para deixar a tabela formatada.\n## Parse do resultado do site da_countries_raw \u0026lt;- r_capitals %\u0026gt;% xml2::read_html() %\u0026gt;% xml2::xml_find_first(\u0026quot;//table\u0026quot;) %\u0026gt;% rvest::html_table(header = TRUE) %\u0026gt;% tibble::as_tibble() %\u0026gt;% janitor::clean_names() Arrumando os dados Como bom faxineiro de dados, eu não poderia deixar se mostrar a parte mais divertida da ciência de dados: organizar os dados brutos! Primeiro, as coordenadas de latitude e longitude estavam em texto e, ao invés de mostrar valores positivos e negativos, mostrava os valores N (norte), S (sul), E (leste), W (oeste). Além disso, a latitude e longitude de Jerusalém (Israel) estava incorreta.\nlibrary(tidyverse) da_countries_tidy \u0026lt;- da_countries_raw %\u0026gt;% filter(country != \u0026quot;\u0026quot;) %\u0026gt;% # transforma (N,S) (E,W) em (1,-1), (1,-1) mutate( lat_num = str_detect(latitude, \u0026quot;N\u0026quot;) * 2 - 1, lng_num = str_detect(longitude, \u0026quot;E\u0026quot;) * 2 - 1 ) %\u0026gt;% # transforma em numérico mutate( across(c(latitude, longitude), parse_number), lat = latitude * lat_num, lng = longitude * lng_num ) %\u0026gt;% # arruma Israel mutate( lat = if_else(country == \u0026quot;Israel\u0026quot;, 31.7683, lat), lng = if_else(country == \u0026quot;Israel\u0026quot;, 35.2137, lng) ) %\u0026gt;% select(country, capital, lat, lng) Transformando os dados Com os dados arrumados em mãos, calculei as distâncias através da distância geodésica1, usando latitude e longitude como base e o maravilhoso pacote {sf}. São duas funções principais: a sf::st_point() cria um objeto especial do tipo ponto, e a sf::st_distance() calcula a distância entre dois pontos. Utilizamos map2() e map() do pacote {purrr} para fazer aplicar essas operações em todos os países. No final, temos a base ordenada pelas distâncias. As distâncias são calculadas em metros, que transformamos para quilômetros.\nda_countries \u0026lt;- da_countries_tidy %\u0026gt;% # cria pontos com base em lat lng e sf::st_point() # sf::st_sfc() transforma a lista num objeto POINT do {sf} # crs = 4326 serve para o {sf} saber que # são coordenadas no planeta Terra. mutate(pt = sf::st_sfc( map2(lng, lat, ~sf::st_point(c(.x, .y, 1))), crs = 4326 )) %\u0026gt;% mutate( across(c(lat, lng), list(br = ~.x[country == \u0026quot;Brazil\u0026quot;])), pt_br = sf::st_sfc( list(sf::st_point(c(lng_br[1], lat_br[1], 1))), crs = 4326 ) ) %\u0026gt;% mutate( dist_br = sf::st_distance(pt, pt_br, by_element = TRUE), dist_br = as.numeric(dist_br / 1000) ) %\u0026gt;% # ordena a base pelas distâncias arrange(dist_br) Visualizando As capitais mais próximas estão na Tabela 1. Sem muitas surpresas aqui: como Brasília fica na região central do país, a capital mais próxima é a do Paraguai, seguida por outros países da América do Sul.\nTabela 1: Capitais mais próximas de Brasília. País Capital Distância (km) Paraguay Asuncion 1473 Bolivia La Paz (administrative) / Sucre (legislative) 2202 Uruguay Montevideo 2276 French Guiana Cayenne 2326 Suriname Paramaribo 2464 Argentina Buenos Aires 2618 Guyana Georgetown 2695 Chile Santiago 3028 Peru Lima 3206 Barbados Bridgetown 3409 As coisas ficam mais interessantes quando visualizamos as capitais mais distantes, na Tabela 2. E temos nosso resultado: Koror (Palau) é a capital mais distante da capital, Brasília, seguida por Manila (Filipinas) e Saipan (Ilhas Mariana do Norte).\nTabela 2: Capitais mais distantes de Brasília. País Capital Distância (km) Palau Koror 19069 Philippines Manila 18801 Northern Mariana Islands Saipan 18639 Macao, China Macau 17887 Brunei Darussalam Bandar Seri Begawan 17765 Republic of Korea Seoul 17517 North Korea Pyongyang 17296 East Timor Dili 17264 Viet Nam Hanoi 17126 Micronesia (Federated States of) Palikir 17066 Mas será mesmo? Vamos usar o pacote {leaflet} para visualizar:\nlibrary(leaflet) # Cria as labels dos popups make_label \u0026lt;- function(pais, capital) { txt \u0026lt;- stringr::str_glue( \u0026quot;\u0026lt;b\u0026gt;País\u0026lt;/b\u0026gt;: {pais}\u0026lt;br\u0026gt;\u0026quot;, \u0026quot;\u0026lt;b\u0026gt;Capital\u0026lt;/b\u0026gt;: {capital}\u0026quot; ) htmltools::HTML(txt) } p_leaflet \u0026lt;- da_countries %\u0026gt;% mutate(lab = map2(country, capital, make_label)) %\u0026gt;% # cria mapa leaflet() %\u0026gt;% # adiciona a casquinha addTiles() %\u0026gt;% # adiciona os pontos addMarkers( clusterOptions = markerClusterOptions(), lat = ~lat, lng = ~lng, popup = ~lab ) p_leaflet Acesse o mapa dinâmico neste link.\nOlhando o mapa (e considerando que a terra é esférica), parece mesmo que esses países estão bem longe, mesmo tentando acessar pelo leste ou pelo oeste.\nE, já que a terra é esférica, que tal criar um mapa 3D? Fiz isso usando o {plotly}, com base no tutorial disponível neste link.\nlibrary(plotly) p_plotly \u0026lt;- plot_ly(height = 1000) %\u0026gt;% # adiciona o mapa mundi add_sf( data = world, x = ~coord_x(x, y), y = ~coord_y(x, y), z = ~coord_z(y), color = I(\u0026quot;gray80\u0026quot;), size = I(2), hoverinfo = \u0026quot;none\u0026quot; ) %\u0026gt;% # adiciona as linhas add_sf( data = da_lines_sf, name = \u0026quot;linhas\u0026quot;, x = ~coord_x(x, y), y = ~coord_y(x, y), z = ~coord_z(y), color = ~dist_br, size = I(3), text = ~label_plotly(country, capital, dist_br), hoverinfo = \u0026quot;text\u0026quot; ) %\u0026gt;% layout(showlegend = FALSE) p_plotly Acesse o mapa dinâmico neste link.\nWrap-up Nesse post vimos como usar ferramentas do {tidyverse}, {sf} para transformação de dados para mapas, além de utilizar o {leaflet} e o {plotly} para visualizações interativas.\nO código completo para construir as visualizações do zero está neste link.\nÉ isso pessoal. Happy coding ;)\nKarney, Charles FF, 2013, Algorithms for geodesics, Journal of Geodesy 87(1), 43–55.↩︎\n","permalink":"https://blog.curso-r.com/posts/2020-09-19-map-dist/","tags":["visualizacao","mapas","leaflet","plotly"],"title":"Qual a capital mais distante de Brasília?"},{"author":["Caio"],"categories":["conceitos"],"contents":" Essa semana o Julio fez uma pergunta aparentemente inocente no nosso fórum. Achei a resposta fosse ser simples, mas depois de 1 hora percebi que tinha virado quase um detetive da computação…\nAbaixo copiei a pergunta do Julio e a minha resposta, ligeiramente editadas por questões didáticas.\nA Pergunta Quero aplicar essa funcao_chata() em uma coluna do meu data frame. Mas quero aplicar isso somente a uma parte do vetor. E não queria os warnings.\nUsando as funções do {dplyr}, tenho warnings, mas usando um simples ifelse() não tenho warnings. Nesse caso tudo bem, uso ifelse(), mas se eu tiver um caso com várias condições (que é quando uso o case_when()), qual seria o melhor jeito de fazer?\nlibrary(tidyverse) funcao_chata \u0026lt;- function(x) { if (any(x \u0026gt; 10)) warning(\u0026quot;não gosto de vc\u0026quot;) 1 / x } # usando if_else(), com warnings resultado \u0026lt;- mtcars %\u0026gt;% mutate(res = if_else( mpg \u0026lt; 10, funcao_chata(mpg), mpg )) #\u0026gt; Warning: Problem with `mutate()` input `res`. #\u0026gt; ℹ não gosto de vc #\u0026gt; ℹ Input `res` is `if_else(mpg \u0026lt; 10, funcao_chata(mpg), mpg)`. #\u0026gt; Warning in funcao_chata(mpg): não gosto de vc # usando case_when(), com warnings resultado \u0026lt;- mtcars %\u0026gt;% mutate(res = case_when( mpg \u0026lt; 10 ~ funcao_chata(mpg), TRUE ~ mpg )) #\u0026gt; Warning: Problem with `mutate()` input `res`. #\u0026gt; ℹ não gosto de vc #\u0026gt; ℹ Input `res` is `case_when(mpg \u0026lt; 10 ~ funcao_chata(mpg), TRUE ~ mpg^2)`. #\u0026gt; Warning: não gosto de vc # usando ifelse(), sem warnings resultado \u0026lt;- mtcars %\u0026gt;% mutate(res = ifelse( mpg \u0026lt; 10, funcao_chata(mpg), mpg )) Thanks.\nA Resposta A sua pergunta é muito pertinente e já foi feita outras vezes, mas, para ficar bem claro, esse comportamento é proposital. Veja o que o Hadley fala na vignette sobre estabilidade do {vectrs}:\nUnlike ifelse() this implies that if_else() must always evaluate both yes and no in order to figure out the correct type. I think this is consistent with \u0026amp;\u0026amp; (scalar operation, short circuits) and \u0026amp; (vectorised, evaluates both sides).\nComo fica claro pelas próprias palavras do Hadley, esse tipo de comportamento tem precedentes no R, mas para entender exatamente o que ele quer dizer vamos ter que aprender sobre alguns conceitos de linguagens de programação. Infelizmente vou aproveitar a sua pergunta para fazer o meu diploma valer alguma coisa…\nExecução especulativa Execução especulativa uma técnica de otimização na qual um programa executa uma tarefa que talvez não seja necessária. Isso pode ser útil por uma série de razões apesar de parecer um desperdício! Se o seu computador consegue processar comandos em paralelo, ele pode executar a condição do if, o resultado caso ela seja TRUE e o resultado caso ela seja FALSE ao mesmo tempo, permitindo uma resposta até 2x mais rápida.\nEssa técnica é tão comum que aqueles famosos bugs de 2018 (Spectre e Meltdown) acontecem principalmente por causa dela.\nVoltando para o if_else(), a sua implementação de execução especulativa é diferentemente da de outras linguagens que tentam “adivinhar” se o if vai retornar TRUE ou FALSE: ele usa avaliação ansiosa, ou seja, ele sempre executa os dois ramos do condicional independentemente do resultado do if. A motivação disso é bem diferente de “otimizar” a computação (como vimos no exemplo anterior), mas sim garantir que ambos os lados da resposta vão ter o mesmo comprimento e o mesmo tipo.\nVeja o código do if_else() e perceba que nele não existe nenhum if ou else, ou seja, ambos os ramos do condicional necessariamente vão ser executados:\nif_else \u0026lt;- function(condition, true, false, missing = NULL) { if (!is.logical(condition)) { bad_args(\u0026quot;condition\u0026quot;, \u0026quot;must be a logical vector.\u0026quot;) } out \u0026lt;- true[rep(NA_integer_, length(condition))] out \u0026lt;- replace_with( out, condition, true, fmt_args(~ true), glue(\u0026quot;length of {fmt_args(~condition)}\u0026quot;) ) out \u0026lt;- replace_with( out, !condition, false, fmt_args(~ false), glue(\u0026quot;length of {fmt_args(~condition)}\u0026quot;) ) out \u0026lt;- replace_with( out, is.na(condition), missing, fmt_args(~ missing), glue(\u0026quot;length of {fmt_args(~condition)}\u0026quot;) ) out } Avaliação de curto-circuito O conceito de avaliação de curto-circuito é mais simples e foi citado diretamente pelo Hadley. Ele basicamente quer dizer que, em uma operação booleana, o segundo argumento somente será executado se o valor do primeiro não for suficiente para determinar o valor da expressão (por exemplo, se temos A \u0026amp;\u0026amp; B e A for FALSE, não precisamos saber o valor de B para saber que a resposta da expressão é FALSE).\nArmados desse conhecimento, podemos entender finalmente a frase do Hadley: \" I think this is consistent with \u0026amp;\u0026amp; (scalar operation, short circuits) and \u0026amp; (vectorised, evaluates both sides)\". A implementação do if_else() foi feita para ser consistente com o operador \u0026amp;, ou seja, vetorizada e com avaliação especulativa (ansiosa), enquanto um if-else comum é consistente com o \u0026amp;\u0026amp;, a saber, escalar e com avaliação de curto-circuito.\nAgora vamos ver alguns exemplos para tentar entender como cada um desses operadores se comporta:\n# Função que retorna TRUE f \u0026lt;- function() { warning(\u0026quot;ANSIOSO\u0026quot;) TRUE } # Preguiçoso (só escalares) if (TRUE) TRUE else f() #\u0026gt; [1] TRUE # Ansioso (funciona para vetores) dplyr::if_else(TRUE, TRUE, f()) #\u0026gt; Warning in f(): ANSIOSO #\u0026gt; [1] TRUE # Com curto-circuito (só escalares) FALSE \u0026amp;\u0026amp; f() #\u0026gt; [1] FALSE # Sem curto-circuito (fuciona para vetores) FALSE \u0026amp; f() #\u0026gt; Warning in f(): ANSIOSO #\u0026gt; [1] FALSE Acho que com esses códigos fica claro que, na verdade, o ifelse() é a exceção e não a regra! Note que aqui eu usei sempre entradas escalares (comprimento 1) por questão didática, mas estão marcados os operadores que podem receber vetores.\n# Preguiçoso (funciona para vetores) ifelse(TRUE, TRUE, f()) #\u0026gt; [1] TRUE Conclui-se que, no que diz respeito à sua pergunta, não existe um jeito óbvio de fazer o if_else() e o case_when() trabalharem com execução preguiçosa sem mudar fundamentalmente o comportamento (e a lógica por trás) dessas funções. Se você quiser uma alternativa, aparentemente o data.table::fcase() funciona como um case_when() sem avaliação ansiosa.\n","permalink":"https://blog.curso-r.com/posts/2020-09-11-if_else_ifelse/","tags":["programação"],"title":"Warnings no if_else(), mas não no ifelse()?"},{"author":["Julio","Athos"],"categories":["Tutoriais"],"contents":" ","permalink":"https://blog.curso-r.com/posts/2020-08-27-applause/","tags":["manipulação","tidyr"],"title":"Aplauda-me"},{"author":["Nicole Luduvice"],"categories":["Tutoriais"],"contents":" O tidyr é um pacote muito útil para arrumar os dados, ou seja, deixá-los tidy. Dizemos que os dados estão arrumados quando\ncada coluna é uma variável; cada linha é uma observação; cada célula representa um único valor. O lançamento da versão 1.0.0 (em 09/11/19) apresentou uma grande mudança do pacote: a substituição das funções gather() e spread() pelas novas funções pivot_longer() e pivot_wider().\nNeste post, vamos apresentar o que mudou nessas novas funções e mostrar como aplicar as operações de pivotagem nesse novo framework.\nOs dados Para ilustrar a utilização das funções, vamos criar uma tabelinha bem simples, que nos permita visualizar a mudança entre os formatos long e wide.\nSuponhamos que os dados a seguir representem as vendas de duas lojas nos meses de janeiro a junho de um determinado ano. É muito comum recebermos uma tabela no formado wide.\ndados \u0026lt;- tibble::tribble( ~loja, ~jan, ~fev, ~mar, ~abr, ~mai, ~jun, 1, 20, 30, 23, 10, 40, 55, 2, 30, 43, 29, 15, 40, 60 ) dados ## # A tibble: 2 x 7 ## loja jan fev mar abr mai jun ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 20 30 23 10 40 55 ## 2 2 30 43 29 15 40 60 Repare que, embora nessa tabela existam 3 variáveis, apenas 2 estão explícitas: a loja e o mês. As vendas estão apenas no corpo da tabela e, sem contexo, não saberíamos o que esses valores significam.\nAlém disso, essa tabela não está no formato tidy, pois cada coluna não represta uma variável: a variável mês está espalhada nas colunas e a variável vendas não é uma coluna.\nA seguir, vamos ver como utilizar as antigas funções gather() e spread() para deixar essa tabela tidy (ou para, uma vez tidy, voltar ao formato original).\nAs antigas funções gather() e spread() Para deixar os dados arrumados com as três variáveis (mês vendas e loja) como colunas, usamos a função gather(). Essa função vai transformar as colunas de janeiro a junho em valores de uma única coluna e vai criar uma nova coluna com os dados de vendas.\nlibrary(tidyr) dados_gather \u0026lt;- dados %\u0026gt;% gather(jan:jun, key = \u0026quot;mes\u0026quot;, value = \u0026quot;vendas\u0026quot;) dados_gather ## # A tibble: 12 x 3 ## loja mes vendas ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 jan 20 ## 2 2 jan 30 ## 3 1 fev 30 ## 4 2 fev 43 ## 5 1 mar 23 ## 6 2 mar 29 ## 7 1 abr 10 ## 8 2 abr 15 ## 9 1 mai 40 ## 10 2 mai 40 ## 11 1 jun 55 ## 12 2 jun 60 Aqui estamos selecionando as colunas de janeiro a junho e aplicando gather(). O argumento key recebe o nome da variável com os nomes das colunas (no nosso exemplo, meses) e value recebe o nome da variável com as observações das colunas, vendas.\nOk, conseguimos deixar nossos dados no formato tidy. Será que é possivel retorná-los ao formato original? Para isso, precisamos de uma função que faça o inverso de gather(), “espalhando” as observações da variável mês em várias colunas e transformando vendas em observações dessas colunas. No pacote {tidyr} essa função se chama spread().\ndados_spread \u0026lt;- dados_gather %\u0026gt;% spread(key = mes, value = vendas) dados_spread ## # A tibble: 2 x 7 ## loja abr fev jan jun mai mar ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 10 30 20 55 40 23 ## 2 2 15 43 30 60 40 29 Observe que o argumento key indica o nome da variável que será espalhada em várias colunas e o argumento value indica o nome da variável que vai ser transformada em observações.\nNo {tidyr} 1.0.0, essas funções foram aprimoradas, recebendo nomes e argumentos mais intuitivos e fáceis de lembrar, além de apresentarem mais possibilidade de uso a fim de auxiliar na tarefa de estruturar os dados.\nAs novas funções pivot_longer() e pivot_wider() O pivot_longer(), em casos mais simples, equivale a função gather(). Esse nome foi dado pois, ao rodar a função, o banco de dados se torna mais longo (longer) em relação aos dados originais.\ndados_longer \u0026lt;- dados %\u0026gt;% pivot_longer( cols = jan:jun, names_to = \u0026quot;mes\u0026quot;, values_to = \u0026quot;vendas\u0026quot; ) dados_longer ## # A tibble: 12 x 3 ## loja mes vendas ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 jan 20 ## 2 1 fev 30 ## 3 1 mar 23 ## 4 1 abr 10 ## 5 1 mai 40 ## 6 1 jun 55 ## 7 2 jan 30 ## 8 2 fev 43 ## 9 2 mar 29 ## 10 2 abr 15 ## 11 2 mai 40 ## 12 2 jun 60 A função spread() foi substituida por pivot_wider(). Ela vai transformar os dados em um formato mais largo. Veja que as duas funções funcionam de forma equivalente.\ndados_wider \u0026lt;- dados_longer %\u0026gt;% pivot_wider(names_from = mes, values_from = vendas) dados_wider ## # A tibble: 2 x 7 ## loja jan fev mar abr mai jun ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 20 30 23 10 40 55 ## 2 2 30 43 29 15 40 60 Essas novas funções de pivotagem trazem várias outras funcionalidades para remodelar as bases de dados. Vamos falar delas em novos posts. Em quanto isso, se você quiser conferir, basta acessar o vignette do tidyr (inglês).\n","permalink":"https://blog.curso-r.com/posts/2020-08-13-pivotagem/","tags":["manipulação","tidyr"],"title":"As novas funções de pivotagem"},{"author":["Caio"],"categories":["Tutoriais"],"contents":"A terminologia master/slave (mestre/escravo) existe há muitas décadas na computação e parece ter migrado para a indústria da tecnologia já no início do século XX. Esses conceitos normalmente estão associados a um modelo de comunicação assimétrico no qual um processo ou aparelho (o \u0026ldquo;mestre\u0026rdquo;) controla outros processos ou aparelhos (os \u0026ldquo;escravos\u0026rdquo;). Não é necessário dizer que os termos vêm sendo contestados há anos devido à evidente conotação racista, mas o debate ganhou nova vida após os protestos do Black Lives Matter em 2020.\nVárias linguagens de programação já abandonaram essa terminologia. Em 2018, no caso em que possivelmente houve maior repercussão até hoje, o Python abandonou a palavra master por parent process (processo pai) e a palavra slave por worker (trabalhador) ou helper (ajudante) após um debate acalorado.\nEnquanto isso, já em 2014, o Django passou a adotar a terminologia primary/replica (primário/réplica) e foi seguido pelo Drupal. Em 2017, o Internet Systems Consortium optou por primary/secondary (primário/secundário). Como se pode ver, não faltam alternativas para uma referência à escravidão.\nEnfim, este ano foi a vez do GitHub. Desde sua criação, a plataforma de controle de versão tem utilizado o termo master para tratar do branch principal de um repositório e, apesar de nem existir uma analogia para algo como um slave branch, a palavra continua até hoje. Antes tarde do que nunca, a empresa anunciou que vai fazer uma transição em breve para o termo main (principal) e aposentar seu antecessor.\nJustamente por pretender ser uma comunidade inclusiva e aberta, alguns programadores de R já começaram a transferir seus repositórios do GitHub para o novo modelo. Em seu blog, Steven Mortimer forneceu o passo-a-passo para fazermos o mesmo. Os comandos são bastante simples e estão listados abaixo:\n# Passo 1 # Crie o branch \u0026#39;main\u0026#39; localmente trazendo o histórico do \u0026#39;master\u0026#39; git branch -m master main # Passo 2 # Faça o push do novo branch para o GitHub git push -u origin main # Passo 3 # Substitua o HEAD do \u0026#39;master\u0026#39; para o \u0026#39;main\u0026#39; git symbolic-ref refs/remotes/origin/HEAD refs/remotes/origin/main # Passo 4 # Troque o branch padrão no GitHub para o \u0026#39;main\u0026#39; # https://docs.github.com/pt/github/administering-a-repository/setting-the-default-branch # Passo 5 # Delete o \u0026#39;master\u0026#39; git push origin --delete master Note apenas que o Passo 4 deve ser realizado diretamente no GitHub de acordo com a documentação (disponível em português).\nCaso você queira que o processo seja completamente automatizado, existe um aplicativo web que faz tudo para você. Para saber mais sobre os esforços do próprio GitHub, acesse a página na qual estão sendo discutidas as mudanças.\n","permalink":"https://blog.curso-r.com/posts/2020-07-27-github-main-branch/","tags":["github","git"],"title":"Abandonando o Termo 'master' no GitHub"},{"author":["José de Jesus Filho"],"categories":["tutoriais"],"contents":" Introdução Este tutorial irá mostrar como instalar o R e o PostgreSQL num mesmo servidor Ubuntu 18.04 ou 16.04. Em seguida, falaremos sobre como chamar o R a partir do Postgres.\nFarei uma série de tutoriais. Este primeiro é voltado para aqueles com familiaridade com funções do R e que gostariam de rodá-las no PostgreSQL, sem necessariamente conhecer muito SQL.\nNum próximo, iremos mostrar como criar queries e declarações do PostgreSQL do R, ou seja, será mais voltado para quem tem familiaridade com o SQL, mas não necessariamente versada em R.\nOs tutoriais posteriores serão voltados para aqueles com bastante familiaridade em tidyverse e que gostariam de realizar as mesmas coisas em SQL. Igualmente, servirão para aqueles que sabem manipular dados em SQL, mas gostariam de fazer as mesmas coisas no R.\nEm futuros tutoriais, falaremos quando compensa iniciar no PostgreSQL e terminar no R. Por exemplo, quando é mais vantajoso dar um join no R em vez de fazê-lo no PostgreSQL e vice-versa.\nInstalando o PostgreSQL A primeira coisa a fazer é atualizar os pacotes do sistema:\nsudo apt update sudo apt -y install vim bash-completion wget sudo apt -y upgrade Feito isso, dê um reboot:\nsudo reboot Importe a chave GPG:\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - Adicione a chave GPG ao systema:\necho \u0026quot;deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\u0026quot; |sudo tee /etc/apt/sources.list.d/pgdg.list Atualize os pacotes do sistema e instale os pacotes do PostgreSQL necessários. Os dois primeiros são necessários para rodar o PostgreSQL, os três últimos são necessários para instalar o plr, o RPostgres e outras extensões. Falaremos deles mais adiante.\nsudo apt update sudo apt install postgresql-12 postgresql-client-12 postgresql-server-dev-all libpq-dev postgresql-contrib Instalação do R sudo echo \u0026quot;deb https://cloud.r-project.org/bin/linux/ubuntu `lsb_release -cs`-cran40/\u0026quot; | sudo tee -a /etc/apt/sources.list Adicionar a chave GPG sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 Instale o R sudo apt-get update sudo apt-get install -y r-base r-base-dev Instale também o git\nsudo apt install git Agora use o git para clonar o plr. Não importa o local onde você irá cloná-lo. Desde que você tenha seguido fielmente os passos anteriores, tudo terminará bem.\ngit clone https://github.com/postgres-plr/plr Feito isso, entre no diretório plr e rode os seguintes comandos para instalá-lo como extensão.\ncd plr USE_PGXS=1 make USE_PGXS=1 make install Criando uma base de dados Vamos para o Postgres a fim de criar uma base de dados.\nsudo -u postgres psql CREATE DATABASE datasets; Além disso, você deve incluir a extensão plr na base de dados recentemente criada:\n\\c datasets -- conectar-se à base CREATE EXTENSION plr; \\q -- sair do psql De volta ao R.\nR Instale os pacotes RPostgres e broom Instalando esses dois pacotes é suficiente para instalar também outras dependências como o DBI e o dplyr, as quais igualmente usaremos.\ninstall.packages(c(\u0026quot;RPostgres\u0026quot;,\u0026quot;broom\u0026quot;)) Connexão do R ao Postgres Eventualmente, você terá de autorizar a conexão local. Vá para o arquivo:\nvim /etc/postgresql/12/main/pg_hba.conf E altere a seguinte linha de:\nlocal all all peer Para:\nlocal all all trust Colocando uma tabela na base de dados Admitindo que você ainda se encontra no R, estamos em condições de incluir uma tabela na base de dados.\nPrimeiramente, vamos conectar-nos à base:\nconn \u0026lt;- DBI::dbConnect(RPostgres::Postgres(), dbname=\u0026quot;datasets\u0026quot;) Estou admidindo com o código acima que você está usando o R na mesma máquina do Postgres, usando o usuário postgres e dispensou o uso de senha para conexão local.\nVamos enviar o dataframe mtcars para a base de dados. O exemplo do mtcars não é muito feliz porque ele poderia ser chamado do próprio R quando rodado no Postgres, mas apenas a título de exemplo, iremos assumir que ele seja qualquer outro data.frame.\nDBI::dbWriteTable(conn,\u0026quot;tabela\u0026quot;, mtcars) De volta ao Postgres Mostraremos num próximo tutorial como realizar os procedimentos a seguir sem sair do R, mas o propósito deste tutorial é justamente ilustrar como podemos chamar o R do Postgres. Assim, faremos tudo no Postgres mesmo.\nVoltando para o shell, vamos conectar-nos à base datasets:\nsudo -u postgres psql datasets Verifique se a tabela chamada “tabela” se encontra na base de dados:\n\\d+ -- ou \\d+ tabela Preparando o terreno Vamos criar uma tabela que servirá de referência para receber os resultados de uma regressão linear. Veja que as colunas são as mesmas do tibble retornado pela função tidy do pacote broom, com a diferença de que os pontos foram substituídos pelo sublinhado e os nomes das colunas passados para o português.\ncreate table modelo (termo text, estimativa float8, erro_padrao float8, estatistica float8, p_valor float8); Criando uma função plr Enfim estamos em condições de criar uma função no PostgreSQL que chama o R para rodar uma regressão linear em uma tabela contida no próprio Postgres:\nCREATE OR REPLACE FUNCTION lm_teste() RETURNS SETOF modelo AS $$ base \u0026lt;\u0026lt;- pg.spi.exec(\u0026#39;select mpg, wt, qsec, am from tabela\u0026#39;) df \u0026lt;- lm(mpg ~ wt + qsec + factor(am), data=base) df \u0026lt;- broom::tidy(df) names(df) \u0026lt;- c(\u0026#39;termo\u0026#39;,\u0026#39;estimativa\u0026#39;,\u0026#39;erro_padrao\u0026#39;,\u0026#39;estatistica\u0026#39;,\u0026#39;p_valor\u0026#39;) df \u0026lt;- dplyr::mutate_at(df,dplyr::vars(2:5), ~round(.,2)) return(df) $$ language \u0026#39;plr\u0026#39;; Note que o esqueleto da função é o mesmo para qualquer outra função do PostgreSQL. A diferença é que, para importar um objeto da base de dados para nossa função, devemos usar a função pg.spi.exec.\nRodando a regressão linear Agora ficou fácil. Basta chamar a função e ver os resultados:\nselect * from lm_teste(); datasets=# select * from lm_teste(); termo | estimativa | erro_padrao | estatistica | p_valor -------------+------------+-------------+-------------+--------- (Intercept) | 9.62 | 6.96 | 1.38 | 0.18 wt | -3.92 | 0.71 | -5.51 | 0 qsec | 1.23 | 0.29 | 4.25 | 0 factor(am)1 | 2.94 | 1.41 | 2.08 | 0.05 (4 rows) Vantagens Eu apontaria duas principais vantagens em usar o plr:\n1 - Uma vez que a base se encontra no PostgreSQL, você não precisa mais transferi-la para o R a fim de rodar o modelo e retornar o resultado ao PostgreSQL. Essa viagem dos dados torna-se dispensável. No exemplo mostrado, porém, a base irá para uma sessão do R de qualquer forma. Veremos como solucionar isso em tutoriais futuros.\n2 - Você pode continuar trabalhando no R, enquanto seu modelo roda no PostgreSQL. Se o modelo tomar horas, este se torna um problema menor.\n","permalink":"https://blog.curso-r.com/posts/2020-06-25-integracao-r-postgresql/","tags":["sql","postgresql"],"title":"Chamando o R do PostgreSQL"},{"author":["William"],"categories":["Tutoriais"],"contents":" O shinyapps.io é um serviço do RStudio que nos possibilita hospedar aplicativos em Shiny e compartilhá-los online com qualquer pessoa no mundo. A conta gratuita permite até 5 aplicativos simultâneos e 25 horas mensais de uso. Além disso, utilizando o shinyapps.io e o RStudio, a tarefa de subir um app para a internet pode ser feita em poucos minutos, com apenas alguns cliques.\nNo vídeo abaixo, eu mostro como conectar o RStudio com a sua conta do shinyapps.io e, com as contas conectadas, como é fácil hospedar um aplicativo no serviço.\n","permalink":"https://blog.curso-r.com/posts/2020-06-18-shinyappsio/","tags":["shiny","rstudio","deploy"],"title":"Conectando o RStudio com o shinyapps.io"},{"author":["William"],"categories":["Divulgação"],"contents":" Ao fim do no nosso curso de R para Ciência de dados I, apresentamos aos alunos (que antes do curso tinham pouca ou nenhuma experiência com R) a tarefa de realizar uma análise estatística (descritiva) e comunicar os resultados utilizando um relatório produzido em R Markdown ou um dashboard feito com o pacote flexdashboard.\nA análise tinha o seguinte contexto: dois produtores que não sabem nada de cinema querem investir seus milhões em um filme, mas não sabem qual tipo de filme ou quais atrizes, atores e diretoras(res) teriam mais chance de gerar um potencial sucesso de crítica e bilheteria. Para responder essa pergunta, eles contrataram cientistas de dados para analisar uma base de filmes do IMDB e responder essa pergunta. O Athos e eu (William) fizemos o papel dos produtores, no sentido de que não demos nenhuma direção sobre a análise a ser realizada (apenas ajudamos com a programação em R, é claro) e os alunos fizeram o papel dos cientistas de dados.\nPara motivar, resolvemos premiar os melhores trabalhos com vagas em qualquer um dos nossos cursos.\nO nosso objetivo, de início, era trazer aos alunos um pouco da experiência do dia-a-dia do cientista de dados: usar análise crítica (nesse caso, conhecimento sobre cinema) e uma ferramenta (no nosso caso, o R) para transformar uma base de dados em uma reposta para uma pergunta difícil1.\nAo final, recebemos trabalhos tão maravilhosos que, além de parabenizar cada um dos participantes, decidimos compartilhar os 4 ganhadores com a comunidade. Esse é o objetivo desse post.\nAbaixo, apresentamos as análises, os autores e os principais pontos pelos quais decidimos destacar esses trabalhos. Todos os textos estão muito claros e bem escritos, então vamos focar apenas no que mais nos chamou atenção em cada trabalho.\nLouise Mamedio Destaques do trabalho da Louise Mamedio:\nCriatividade. A análise conta uma história. Bom humor. A parte técnica muito bem diluída em um texto muito gostoso de ser lido, o que é muito convidativo para leitores pouco acostumados com análises de dados. Direto ao ponto. Todos os seus textos são informativos, sem enrolação ou mera descrição do que já está sendo mostrado no gráfico. Veja o trabalho completo aqui: https://curso-r.github.io/202005-r4ds-1/trabalhos_vencedores/concursolouise.html\nCamila Bertini Destaques do trabalho da Camila Bertini:\nTécnica. Dashboard muito bem construído e organizado. Visual. Gráficos e tabelas bem formatados, com cores, labels e legendas que ajudam a interpretação. Veja o trabalho completo aqui: https://curso-r.github.io/202005-r4ds-1/trabalhos_vencedores/Desafio_flexdasboard.html\nRodrigo Almeida Destaques do trabalho do Rodrigo Almeida:\nOrganização. O texto explica a análise de maneira muito bem organizada. Análise crítica. As visualizações escolhidas para apresentar cada informação e a definição das variáveis com poder de atuação mostram como técnica e conhecimento do tema (negócio) devem se relacionar em uma análise de dados. Veja o trabalho completo aqui: https://curso-r.github.io/202005-r4ds-1/trabalhos_vencedores/Projeto_Curso-R.html\nMariana Costa Destaques do trabalho da Mariana Costa.\nEstrutura. Análise muito bem estruturada, com objetivo bem estabelecido e desenvolvimento coerente. Visual. Gráficos e tabelas bem formatados, com cores, labels e legendas que ajudam a interpretação.s Conclusão clara e direta ao ponto. Veja o trabalho completo aqui: https://curso-r.github.io/202005-r4ds-1/trabalhos_vencedores/analise_descritiva.pdf\nE claro: em tempo limitado. Os alunos tiveram uma semana para fazer o trabalho.↩︎\n","permalink":"https://blog.curso-r.com/posts/2020-06-20-melhores-trabalhos-r4ds1/","tags":["relatórios"],"title":"Melhores trabalhos do curso R para Ciência de dados I (maio de 2020)"},{"author":["Athos"],"categories":["análises"],"contents":" Fui apresentado por uma amiga aO Teste de Bechdel, que avalia se um filme possui três regras básicas:\nAo menos duas mulheres (com nome) … que conversam entre si … sobre algo que não seja homem Esse teste foi nomeado em homenagem à cartunista Alison Bechdel que, em um de seus quadrinhos, apresentou essa ideia e hoje é utilizado para medir o grau de discriminação de gênero no cinema.\nO site bechdeltest.com oferece espaço para qualquer um contribuir com filmes avaliados no teste de Bechdel e disponibiliza fácil acesso a um banco de dados de 8.440 filmes de 1888 até os dias de hoje.\nUm pitaco pessoal (Athos): Imagine as mesmas regras, mas invertendo homem por mulher. Quantos filmes você acha que não passariam no teste? Na minha opinião? Menos de 1%!\nAchados Países como Canadá e Japão parecem estarem melhorando; Brasil tem poucos dados, mas não se destaca positivamente; Prêmios e Notas IMDB não são diferentes entre filmes que passaram no teste e filmes que não passaram no teste. Pode-se argumentar que a discriminação de gênero, mesmo que sempre injustificável, não sustenta nem a retórica de que vale a pena financeiramente; Dados O site disponibiliza uma API para acessar os dados. Então para trazer para o R é simples como a linha abaixo.\nbechdel \u0026lt;- fromJSON(\u0026quot;http://bechdeltest.com/api/v1/getAllMovies\u0026quot;) glimpse(bechdel) ## Rows: 8,440 ## Columns: 5 ## $ year \u0026lt;int\u0026gt; 1888, 1892, 1895, 1895, 1896, 1896, 1896, 1896, 1898, 1898, 18… ## $ title \u0026lt;chr\u0026gt; \u0026quot;Roundhay Garden Scene\u0026quot;, \u0026quot;Pauvre Pierrot\u0026quot;, \u0026quot;Tables Turned on t… ## $ imdbid \u0026lt;chr\u0026gt; \u0026quot;0392728\u0026quot;, \u0026quot;0000003\u0026quot;, \u0026quot;0000014\u0026quot;, \u0026quot;0132134\u0026quot;, \u0026quot;0000091\u0026quot;, \u0026quot;000013… ## $ rating \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ id \u0026lt;int\u0026gt; 8040, 5433, 5444, 6200, 4982, 5406, 5445, 6199, 5411, 5410, 49… OBS: reparem na coluna imdbid. Ela permite coletar informações do IMDB para maiores análises!\nRating O rating vai de 0 a 3, que representa quantas regras são atendidas no filme. Vamos rotular para melhor leitura.\nbechdel \u0026lt;- bechdel %\u0026gt;% mutate( indice_bechdel = case_when( rating == 0 ~ \u0026quot;Menos de duas mulheres\u0026quot;, rating == 1 ~ \u0026quot;Pelo menos duas mulheres\u0026quot;, rating == 2 ~ \u0026quot;... que conversam entre si\u0026quot;, rating == 3 ~ \u0026quot;... sobre algo que não seja homem\u0026quot; ), indice_bechdel = fct_reorder(indice_bechdel, rating), decada = (year %/% 10)*10 ) bechdel %\u0026gt;% count(indice_bechdel, rating) %\u0026gt;% mutate( rating_p = scales::percent(n/sum(n), accuracy = 1) ) %\u0026gt;% ggplot(aes(y = indice_bechdel, x = n)) + geom_segment(aes(xend = 0, yend = indice_bechdel)) + geom_point(size = 12) + geom_text(aes(label = rating_p), color = \u0026quot;white\u0026quot;, fontface = \u0026quot;bold\u0026quot;) + labs(x = \u0026quot;contagem\u0026quot;, y = NULL, title = \u0026quot;Filmes por Rating Bechdel\u0026quot;) + theme( panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_blank() ) Evolução nas Décadas bechdel_por_decada \u0026lt;- bechdel %\u0026gt;% count(decada, indice_bechdel, .drop = FALSE) bechdel_por_decada %\u0026gt;% ggplot(aes(x = decada, fill = indice_bechdel, y = n)) + geom_col(position = \u0026quot;fill\u0026quot;, colour = \u0026quot;black\u0026quot;, width = 10) + geom_text(aes(label = indice_bechdel), position = \u0026quot;fill\u0026quot;, data = filter(bechdel_por_decada, decada == 1950), vjust = 2.5, colour = \u0026quot;white\u0026quot;, fontface = \u0026quot;bold\u0026quot;) + scale_fill_brewer(palette = \u0026quot;RdBu\u0026quot;, guide = \u0026quot;none\u0026quot;) + labs(x = \u0026quot;Década\u0026quot;, y = \u0026quot;Proporção\u0026quot;, title = \u0026quot;Evolução dos Ratings Bechdel pelas Décadas\u0026quot;) + scale_y_continuous(labels = scales::percent) + theme( panel.grid = element_blank() ) O Teste de Bechdel vem melhorando com o tempo, então as perspectivas são positivas. Em um mundo cada vez mais conectado, espera-se que a informação chegue a cada vez mais pessoas e a discriminação encrustrada na nossa cultura se dissolva.\nInformações do IMDB A API do IMDB é muito burocrática de usar. Eu encontrei o site OMDB que foi bem mais fácil de usar, mas talvez tenha menos informação legal. Vou mostrar como usar:\nAPI Key Primeiro eu peguei uma API KEY no site deles e coloquei no meu .Renviron porque daí eu posso usar o Sys.get() pra usá-la sem precisar mostrar pra ninguém.\nSys.getenv(\u0026#39;OMDB_APIKEY\u0026#39;) # [1] as48732f1d (de mentirinha) Chamando a API Depois eu chamei a API do OMDB usando tanto minha API KEY quanto o IMDB ID. A chamada tem que ser assim: http://www.omdbapi.com/?apikey={Sys.getenv('OMDB_APIKEY')}\u0026amp;i=tt{imdbid}. Ela vai devolver um json que pode ser facilmente formatado pelo jsonlite::fromJSON().\nlibrary(glue) pega_info_imdb \u0026lt;- function(imdbid) { glue(\u0026quot;http://www.omdbapi.com/?apikey={Sys.getenv(\u0026#39;OMDB_APIKEY\u0026#39;)}\u0026amp;i=tt{imdbid}\u0026quot;) %\u0026gt;% fromJSON() %\u0026gt;% as_tibble() } # safely porque a API pode dar erro 522 (no servidor do OMDB) pega_info_imdb_safe \u0026lt;- safely(pega_info_imdb) # demora ~40 min bechdel_com_imdb \u0026lt;- bechdel %\u0026gt;% as_tibble() %\u0026gt;% mutate( infos_imdb = map(imdbid, pega_info_imdb_safe) ) Arrumando os dados Os dados precisam de uma massagem antes porque vem em uma lista aninhada e tudo texto.\n# Arruma as tabelas aninhadas arruma_infos_imdb \u0026lt;- function(infos_imdb) { if(!is.null(infos_imdb$Ratings)) { infos_imdb %\u0026gt;% nest(Ratings = c(Ratings)) %\u0026gt;% mutate(Ratings = map(Ratings, ~.x$Ratings %\u0026gt;% filter(Source %in% \u0026quot;Internet Movie Database\u0026quot;))) %\u0026gt;% unnest(Ratings) } else { infos_imdb } } # cria bechdel com as infos do imdb e arruma o tipo das variáveis bechdel_com_imdb \u0026lt;- bechdel_com_imdb %\u0026gt;% mutate( deu_errado = map_lgl(infos_imdb, ~ !is.null(.x$error)), infos_imdb = infos_imdb %\u0026gt;% map(\u0026quot;result\u0026quot;) %\u0026gt;% map(arruma_infos_imdb) ) %\u0026gt;% unnest(infos_imdb) %\u0026gt;% mutate( imdbRating_num = imdbRating %\u0026gt;% parse_number() ) Tabela final glimpse(bechdel_com_imdb) ## Rows: 8,414 ## Columns: 41 ## $ year \u0026lt;int\u0026gt; 1888, 1892, 1895, 1895, 1896, 1896, 1896, 1896, 1898, … ## $ title \u0026lt;chr\u0026gt; \u0026quot;Roundhay Garden Scene\u0026quot;, \u0026quot;Pauvre Pierrot\u0026quot;, \u0026quot;Tables Tur… ## $ imdbid \u0026lt;chr\u0026gt; \u0026quot;0392728\u0026quot;, \u0026quot;0000003\u0026quot;, \u0026quot;0000014\u0026quot;, \u0026quot;0132134\u0026quot;, \u0026quot;0000091\u0026quot;,… ## $ rating \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, … ## $ id \u0026lt;int\u0026gt; 8040, 5433, 5444, 6200, 4982, 5406, 5445, 6199, 5411, … ## $ indice_bechdel \u0026lt;fct\u0026gt; Menos de duas mulheres, Menos de duas mulheres, Menos … ## $ decada \u0026lt;dbl\u0026gt; 1880, 1890, 1890, 1890, 1890, 1890, 1890, 1890, 1890, … ## $ Title \u0026lt;chr\u0026gt; \u0026quot;Roundhay Garden Scene\u0026quot;, \u0026quot;Poor Pierrot\u0026quot;, \u0026quot;The Sprinkle… ## $ Year \u0026lt;chr\u0026gt; \u0026quot;1888\u0026quot;, \u0026quot;1892\u0026quot;, \u0026quot;1895\u0026quot;, \u0026quot;1895\u0026quot;, \u0026quot;1896\u0026quot;, \u0026quot;1896\u0026quot;, \u0026quot;1896\u0026quot;… ## $ Rated \u0026lt;chr\u0026gt; \u0026quot;NOT RATED\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;Not Rated\u0026quot;, \u0026quot;Not Rated\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;… ## $ Released \u0026lt;chr\u0026gt; \u0026quot;14 Oct 1888\u0026quot;, \u0026quot;28 Oct 1892\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;28 Aug 1895\u0026quot;, \u0026quot;N… ## $ Runtime \u0026lt;chr\u0026gt; \u0026quot;1 min\u0026quot;, \u0026quot;4 min\u0026quot;, \u0026quot;1 min\u0026quot;, \u0026quot;1 min\u0026quot;, \u0026quot;3 min\u0026quot;, \u0026quot;1 min\u0026quot;, … ## $ Genre \u0026lt;chr\u0026gt; \u0026quot;Documentary, Short\u0026quot;, \u0026quot;Animation, Comedy, Short, Roman… ## $ Director \u0026lt;chr\u0026gt; \u0026quot;Louis Aimé Augustin Le Prince\u0026quot;, \u0026quot;Émile Reynaud\u0026quot;, \u0026quot;Lou… ## $ Writer \u0026lt;chr\u0026gt; \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;Georges Méliès\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;… ## $ Actors \u0026lt;chr\u0026gt; \u0026quot;Annie Hartley, Adolphe Le Prince, Joseph Whitley, Sar… ## $ Plot \u0026lt;chr\u0026gt; \u0026quot;In the garden, a man asks his friends to do something… ## $ Language \u0026lt;chr\u0026gt; \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A… ## $ Country \u0026lt;chr\u0026gt; \u0026quot;UK, France\u0026quot;, \u0026quot;France\u0026quot;, \u0026quot;France\u0026quot;, \u0026quot;USA\u0026quot;, \u0026quot;France\u0026quot;, \u0026quot;Fr… ## $ Awards \u0026lt;chr\u0026gt; \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;… ## $ Poster \u0026lt;chr\u0026gt; \u0026quot;https://m.media-amazon.com/images/M/MV5BOGE3YjczMTQtZ… ## $ Metascore \u0026lt;chr\u0026gt; \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;… ## $ imdbRating \u0026lt;chr\u0026gt; \u0026quot;7.5\u0026quot;, \u0026quot;6.6\u0026quot;, \u0026quot;7.1\u0026quot;, \u0026quot;6.7\u0026quot;, \u0026quot;6.7\u0026quot;, \u0026quot;5.8\u0026quot;, \u0026quot;5.2\u0026quot;, \u0026quot;7.4\u0026quot;… ## $ imdbVotes \u0026lt;chr\u0026gt; \u0026quot;4,609\u0026quot;, \u0026quot;1,144\u0026quot;, \u0026quot;4,287\u0026quot;, \u0026quot;1,755\u0026quot;, \u0026quot;2,149\u0026quot;, \u0026quot;1,040\u0026quot;, … ## $ imdbID \u0026lt;chr\u0026gt; \u0026quot;tt0392728\u0026quot;, \u0026quot;tt0000003\u0026quot;, \u0026quot;tt0000014\u0026quot;, \u0026quot;tt0132134\u0026quot;, \u0026quot;t… ## $ Type \u0026lt;chr\u0026gt; \u0026quot;movie\u0026quot;, \u0026quot;movie\u0026quot;, \u0026quot;movie\u0026quot;, \u0026quot;movie\u0026quot;, \u0026quot;movie\u0026quot;, \u0026quot;movie\u0026quot;, … ## $ DVD \u0026lt;chr\u0026gt; \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;… ## $ BoxOffice \u0026lt;chr\u0026gt; \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;… ## $ Production \u0026lt;chr\u0026gt; \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;… ## $ Website \u0026lt;chr\u0026gt; \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;, \u0026quot;N/A\u0026quot;… ## $ Response \u0026lt;chr\u0026gt; \u0026quot;True\u0026quot;, \u0026quot;True\u0026quot;, \u0026quot;True\u0026quot;, \u0026quot;True\u0026quot;, \u0026quot;True\u0026quot;, \u0026quot;True\u0026quot;, \u0026quot;True\u0026quot;… ## $ Source \u0026lt;chr\u0026gt; \u0026quot;Internet Movie Database\u0026quot;, \u0026quot;Internet Movie Database\u0026quot;, … ## $ Value \u0026lt;chr\u0026gt; \u0026quot;7.5/10\u0026quot;, \u0026quot;6.6/10\u0026quot;, \u0026quot;7.1/10\u0026quot;, \u0026quot;6.7/10\u0026quot;, \u0026quot;6.7/10\u0026quot;, \u0026quot;5.8… ## $ Error \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ Season \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ Episode \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ seriesID \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ totalSeasons \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ Ratings \u0026lt;lgl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ deu_errado \u0026lt;lgl\u0026gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ imdbRating_num \u0026lt;dbl\u0026gt; 7.5, 6.6, 7.1, 6.7, 6.7, 5.8, 5.2, 7.4, 6.0, 7.5, 7.5,… Bechdel por País Japão e Canadá são os países que apresentem tendências mais claras de mudanças. Os demais parecem estagnados. Brasil tem 45 filmes analisado e não parece diferente da Alemanha, Reino Unido, EUA ou França.\nFilmes brasileiros bechdel_com_imdb %\u0026gt;% filter(str_detect(Country, \u0026quot;Brazil\u0026quot;)) %\u0026gt;% select(year, Title, rating) %\u0026gt;% arrange(rating) %\u0026gt;% knitr::kable() year Title rating 2007 Estomago: A Gastronomic Story 0 2007 You, Me and Him 0 2009 The Secret of Kells 0 2011 Samsara 0 2013 Boy and the World 0 2014 Ardor 0 2002 City of God 1 2003 Fear X 1 2011 Rio 1 2012 Two Rabbits 1 2013 Vai que dá Certo 1 2014 Trash 1 2014 Futuro Beach 1 2018 Tito and the Birds 1 2019 Bacurau 1 1985 Kiss of the Spider Woman 2 2003 The Man Who Copied 2 2009 From Beginning to End 2 2019 Turma da Mônica: Laços 2 1986 Hour of the Star 3 1998 Central Station 3 2004 Nina 3 2005 Æon Flux 3 2007 Elite Squad 3 2008 Blindness 3 2008 Lion’s Den 3 2010 So Hard to Forget 3 2011 Fast Five 3 2011 Found Memories 3 2012 Neighboring Sounds 3 2012 Tabu 3 2012 Snezhnaya koroleva 3 2013 Reaching for the Moon 3 2013 Open Road 3 2013 My Mom Is a Character 3 2013 Brazilian Western 3 2013 Time and the Wind 3 2014 August Winds 3 2014 The Way He Looks 3 2014 Last Night 3 2015 Neon Bull 3 2015 The Second Mother 3 2016 Aquarius 3 2017 Call Me by Your Name 3 2018 The Heiresses 3 Filmes Premiados A base traz informação de premiações. Filmes sem premiação não destoam dos filmes premiados em termos de avaliação de Bechdel, o que indica que o machismo não está relacionado com o sucesso de um filme.\nbechdel_por_decada_e_premiacao \u0026lt;- bechdel_com_imdb %\u0026gt;% mutate( premiacao = case_when( str_detect(Awards, \u0026quot;N/A\u0026quot;) | is.na(Awards) ~ \u0026quot;z Sem Premiação\u0026quot;, str_detect(Awards, \u0026quot;Won.*Oscar\u0026quot;) ~ \u0026quot;a Vencedor de Oscar\u0026quot;, str_detect(Awards, \u0026quot;Won.*(BAFTA|Golden Globe)\u0026quot;) ~ \u0026quot;b Vencedor de BAFTA/Golden Globe\u0026quot;, str_detect(Awards, \u0026quot;Nominated\u0026quot;) ~ \u0026quot;c Nomeado ao Oscar/BAFTA/Golden Globe\u0026quot;, str_detect(Awards, \u0026quot;win\u0026quot;) ~ \u0026quot;d Vencedor de outros prêmios\u0026quot;, str_detect(Awards, \u0026quot;nomination\u0026quot;) ~ \u0026quot;d Nomeado a outros prêmios\u0026quot;, TRUE ~ \u0026quot;e Outro\u0026quot; ) ) %\u0026gt;% count(premiacao, decada, indice_bechdel, .drop = FALSE) bechdel_por_decada_e_premiacao %\u0026gt;% filter(decada \u0026gt;= 1940) %\u0026gt;% ggplot(aes(x = decada, fill = indice_bechdel, y = n)) + geom_col(position = \u0026quot;fill\u0026quot;, colour = \u0026quot;black\u0026quot;, width = 10) + scale_fill_brewer(palette = \u0026quot;RdBu\u0026quot;, guide = \u0026quot;none\u0026quot;) + labs(x = \u0026quot;Década\u0026quot;, y = \u0026quot;Proporção\u0026quot;, title = \u0026quot;Evolução dos Ratings Bechdel pelas Décadas\u0026quot;) + scale_y_continuous(labels = scales::percent) + theme( panel.grid = element_blank() ) + facet_wrap(vars(premiacao), ncol = 3) Nota IMDB Notas do IMDB também não diferem entre filmes que passaram no teste e filmes que não passaram no teste de Bechdel.\nbechdel_por_decada_e_pais \u0026lt;- bechdel_por_decada_e_pais %\u0026gt;% mutate( imdbVotes = parse_number(imdbVotes), passou_no_teste = case_when( rating %in% 3 ~ \u0026quot;Sim\u0026quot;, TRUE ~ \u0026quot;Não\u0026quot; ) ) bechdel_nota_por_decada \u0026lt;- bechdel_por_decada_e_pais %\u0026gt;% group_by(pais) %\u0026gt;% filter(n() \u0026gt; 10) %\u0026gt;% group_by(pais, decada, passou_no_teste) %\u0026gt;% summarise( imdb_media = mean(imdbRating_num, na.rm = TRUE), imdb_dp = sd(imdbRating_num, na.rm = TRUE), imdb_inf = imdb_media - 2*imdb_dp, imdb_sup = imdb_media + 2*imdb_dp ) bechdel_nota_por_decada %\u0026gt;% filter(decada \u0026gt;= 1940) %\u0026gt;% ggplot(aes(x = decada, y = imdb_media, colour = passou_no_teste)) + geom_line() + geom_point() + geom_jitter(data = bechdel_por_decada_e_pais %\u0026gt;% filter(decada \u0026gt;= 1940), aes(y = imdbRating_num), alpha = 0.05) + geom_ribbon(aes(ymin = imdb_inf, ymax = imdb_sup, fill = passou_no_teste), alpha = 0.1) + facet_wrap(vars(pais)) + labs(x = \u0026quot;Década\u0026quot;, y = \u0026quot;Nota IMDB\u0026quot;, colour = \u0026quot;Passou no Teste\u0026quot;, fill = \u0026quot;Passou no Teste\u0026quot;, title = \u0026quot;Notas do IMDB vs Teste de Bechdel\u0026quot;) + scale_x_continuous(breaks = c(1950, 2020)) + theme(legend.position = \u0026quot;top\u0026quot;) O que você achou das análises? Conte para a gente! É um tema que vale a pena ser discutido por todos.\n","permalink":"https://blog.curso-r.com/posts/2020-06-09-bechdel/","tags":["ggplot2","tidyverse","api","imdb"],"title":"Bechdel: Filme e Machismo"},{"author":["José de Jesus Filho"],"categories":["pacotes"],"contents":" Introdução Há alguns anos, eu tenho me dedicado a aperfeiçoar ferramentas de coleta, limpeza, organização e análise de dados processuais. Posso afirmar, com segurança, que tenho bem elaborado um processo que dá conta eficientemente de todo o ciclo da ciência de dados utilizando apenas dois softwares livres: R e PostgreSQL.\nEste tutorial mostrará como estruturar uma base de dados de textos no PostgreSQL, tokenizá-los e montar um índice invertido a fim de realizar buscas textuais em grande volumes de documentos em poucos segundos. Os textos serão organizados a partir do R, enviados para o PostgreSQL, indexados e, de lá,́ chamados a partir de um aplicativo shiny.\nBusca textual A busca textual confere a capacidade de identificar documentos em linguagem natural que atendam a uma consulta e, opcionalmente, classificá-los por relevância para a busca. O tipo mais comum de pesquisa é encontrar todos os documentos que contenham os termos da consulta e retorná-los em ordem de semelhança com a consulta. As noções de consulta e semelhança são muito flexíveis e dependem da aplicação específica.\nOs operadores de pesquisa textual existem nos bancos de dados há anos. O PostgreSQL possui operadores ~, ~ *, LIKE e ILIKE para tipos de dados textuais, mas eles não possuem muitas propriedades essenciais exigidas pelos modernos sistemas de informação:\nNão há suporte linguístico, mesmo para o inglês. Expressões regulares não são suficientes porque não conseguem lidar facilmente com palavras derivadas, por exemplo, satisfazer e satisfeito.\nEles não ordenam (classificação) os resultados da pesquisa conforme a relevância, o que os torna ineficazes quando milhares de documentos correspondentes são encontrados.\nEles tendem a ser lentos porque não há suporte de índice; portanto, eles devem processar todos os documentos para cada pesquisa.\nEm outras palavras, para uma busca textual eficiente, é importante tomar em consideração ferramentas de NLP e um pré-processamente dos textos.\nA indexação de texto permite que os documentos sejam pré-processados e um índice salvo para posterior busca rápida. O pré-processamento inclui:\nTokenização dos documentos;\nConversão dos tokens em lexemas;\nSalvar documentos pré-processados e otimizados para pesquisa;\nPorque usar o R Software live; Acolhedor da diversidade; Composto por uma comunidade acadêmica exigente e cientificamente rigorosa; Possui um grande número de pacotes para coleta, limpeza e estruturação de dados. É importante lembrar que esse trabalho chega a tomar 80% do ciclo de ciência de dados; Dá conta tanto de estatística quanto de machine learning; Facilita a publicação de resultados tanto com relatórios (Rmarkdown) quanto com aplicativos (shiny); Porque usar o PostgreSQL Software livre; Fácil de instalar; Bem documentado; Ampla comunidade; Funciona como motor de busca textual, dispensando o uso do Solr ou do Elasticsearch; Assumindo que você já tem o R, o RStudio, e o shiny-server instalados, irei mostar apenas como instalar o PostgreSQL. Caso queira instalar os três primeiros, você pode seguir este script para instalá-los no Ubuntu.\nInstalação do PostgreSQL Supondo um ambiente de desenvolvimento, irei considerar a instalação do PostgreSQL, do RStudio e do Shiny numa única máquina. Em produção, eu criaria uma rede privada de máquinas virtuais e distribuiria as funcionalidades em diferentes máquinas numa mesma central de dados (data-center).\n- PostgreSQL instalado (irei mostrar como instalar no Ubuntu); - R, RStudio e Shiny instalados; - Pacotes RPostgres, dbx, pool, glue, abjutils e DT instalados; - Tidyverse Instalar o PostgreSQL O procedimento abaixo mostra como instalar o PostgreSQL\nAdicionar a chave GPG A instalação da chave GPG preserva uma comunicação segura entre o cliente eo servidor. Ela é importante para assegurar a integridade dos dados e a autencidade da fonte. Ou seja, os dados são criptografados antes de serem baixados por seu computador e decriptografados pela chave previamente instalada. Isso reduz significativamente as chances de que um terceiro intervenha no processo de transmissão e instale algo nocivo na sua máquina.\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - Em seguida, adicione o repositório com o comando abaixo:\nsudo sh -c \u0026#39;echo \u0026quot;deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\u0026quot; \u0026gt;\u0026gt; /etc/apt/sources.list.d/pgdg.list\u0026#39; Feito isso, o passo seguinte é instalar o PostgreSQL:\nsudo apt update sudo apt install postgresql postgresql-contrib Configuração do locale O PostgreSQL adota o locale da sua máquina, então é importante assegurar que o locale está configurado para pt_BR.UTF-8. Crie um arquivo e adicione o script abaixo para configuração do locale:\nsudo touch set_locale.sh ## criação do arquivo Script:\n#!/bin/bash # Set locales in /etc/default/locale file echo \u0026quot;Setting locale...\u0026quot; echo \u0026quot;# Locale settings export LANGUAGE=pt_BR.UTF-8 export LANG=pt_BR.UTF-8 export LC_ALL=pt_BR.UTF-8\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile locale-gen pt_BR.UTF-8 sudo dpkg-reconfigure locales source ~/.bash_profile sudo chmod +x set_locale.sh sudo ./set_locale.sh Trabalhando com o PostgreSQL Para fins de completude, estou admitindo que você não tem familiaridade com o PostgreSQL. Isso não significa que darei explicação de cada passo, mas apenas que não os deixarei implícitos.\nHá muitos clientes que permitem acesso ao PostgreSQL para envio de queries e statements. Nós usaremos dois, o psql e o próprio R. Com o psql você acessa e trabalha com o Posgres via linha de comando. Quando você instalou o PostgreSQL, o psql também foi instadado. Dito isso, vamos realizar nosso primeiro acesso.\nsudo -u postgres psql TO_TSVECTOR, TO_TSQUERY e @@ As funções to_tsvector, to_tsquery e o operador @@ (match) fazem a mágica da busca textual.\nSELECT to_tsvector(\u0026#39;portuguese\u0026#39;, \u0026#39;Alma minha gentil, que te partiste Tão cedo desta vida descontente, Repousa lá no Céu eternamente, E viva eu cá na terra sempre triste.(Camões)\u0026#39;) @@ to_tsquery(\u0026#39;céu\u0026#39;); ?column? ---------- t (1 row) SELECT to_tsvector(\u0026#39;portuguese\u0026#39;, \u0026#39;Minha mãe me deu ao mundo e, sem ter mais o que me dar, me ensinou a jogar palavra no vento pra ela voar. Dizia: “Filho, palavra Tem que saber como usar. Aquilo é que nem remédio: Cura, mas pode matar.(Aleixo)\u0026#39;) @@ to_tsquery(\u0026#39;filho \u0026amp; remédio\u0026#39;); ?column? ---------- t (1 row) SELECT to_tsvector(\u0026#39;portuguese\u0026#39;, \u0026#39;No fundo, no fundo, bem lá no fundo, a gente gostaria de ver nossos problemas resolvidos por decreto a partir desta data, aquela mágoa sem remédio é considerada nula e sobre ela — silêncio perpétuo extinto por lei todo o remorso, maldito seja quem olhar pra trás, lá pra trás não há nada, e nada mais mas problemas não se resolvem, problemas têm família grande, e aos domingos saem todos a passear o problema, sua senhora e outros pequenos probleminhas.(Leminski)\u0026#39;) @@ to_tsquery(\u0026#39;remorso | probleminhas\u0026#39;); ?column? ---------- t (1 row) Trabalhando com tabelas No seguinte repositório consta uma base de 48 mil notícias do G1, a qual utilizaremos para fins de demonstração.\nInicialmente, vamos criar um usuário (role) e uma base de dados para receber essas notícias:\nCREATE ROLE saturday WITH PASSWORD \u0026#39;RshinesWithPostgres\u0026#39;; CREATE DATABASE noticias OWNER = saturday; Agora nos conectamos à base, adicionamos a extensão unaccent retirar acentos das palavras.\n\\c noticias CREATE EXTENSION unaccent; Configurações necessárias Vamos agora configurar a busca para que ela lide adequadamente com palavras acentuadas, maiúsculas e minúsculas, bem como, de suas variações.\nCREATE TEXT SEARCH CONFIGURATION pt (COPY = pg_catalog.portuguese); ALTER TEXT SEARCH CONFIGURATION pt ALTER MAPPING FOR hword, hword_part, word with unaccent, portuguese_stem; Indexação dos documentos De agora em diante, passaremos a executar os queries e statemants a partir do próprio R, colocando-os dentro de funções.\nA primeira coisa a fazer é conectar-se à base e adicionar a tabela. Veja que eu apenas crio a tabela, mas não insiro os documentos. Quando você tem muitos documentos, isso pode travar.\nconn \u0026lt;- DBI::dbConnect(RPostgres::Postgres(), dbname = \u0026quot;noticias\u0026quot;, host = \u0026quot;localhost\u0026quot;, user=\u0026quot;saturday\u0026quot;, password = \u0026quot;RshinesWithPostgres\u0026quot;) DBI::dbCreateTable(conn,\u0026quot;g1\u0026quot;,g1) Inserindo os documentos Para inserir os documentos, eu prefiro usar o pacote dbx porque ele permite a inserção em batches. Inserir centenas de milhares de documentos pode sobrecarregar sua máquina. Coloquei mil, mas 50 mil tem suportado bem.\ndbx::dbxInsert(con = conn, table = \u0026quot;g1\u0026quot;, records = g1, batch_size = 1000) Indexando os documentos Hora de indexar os documentos. Há dois indexadores, o GIN e o GIST, usaremos o GIN pq é mais rápido, porém mais intenso. A função a seguir cria o index estabelecendo pesos diferentes para duas colunas.\npsql_tokenize \u0026lt;- function(con, tbl, config = \u0026quot;pt\u0026quot;) { source \u0026lt;- list(a = c(\u0026quot;intro\u0026quot;, \u0026quot;A\u0026quot;), j = c(\u0026quot;corpo\u0026quot;, \u0026quot;B\u0026quot;)) target \u0026lt;- \u0026quot;document_tokens\u0026quot; idx \u0026lt;- paste0(tbl,\u0026quot;_idx\u0026quot;) query \u0026lt;- glue::glue_sql(\u0026quot;ALTER TABLE {`tbl`} ADD COLUMN {`target`} TSVECTOR\u0026quot;, .con = con) res \u0026lt;- DBI::dbSendQuery(con, query) DBI::dbClearResult(res) query \u0026lt;- glue::glue_sql(\u0026quot;UPDATE {`tbl`} SET {`target`} = setweight(to_tsvector({config},coalesce({`source$a[1]`},\u0026#39;\u0026#39;)), {source$a[2]}) || setweight(to_tsvector({config},coalesce({`source$j[1]`}, \u0026#39;\u0026#39;)), {source$j[2]})\u0026quot;, .con = con) res \u0026lt;- DBI::dbSendQuery(con, query) DBI::dbClearResult(res) query \u0026lt;- glue::glue_sql(\u0026quot;CREATE INDEX {`idx`} ON {`tbl`} USING GIN ({`target`})\u0026quot;, .con = con) res \u0026lt;- DBI::dbSendQuery(con, query) DBI::dbClearResult(res) } Criando gatilho (trigger) A função a seguir cria um gatilho para indexar novos documentos inseridos:\npsql_trigger \u0026lt;- function(con,tbl,config=\u0026quot;pt\u0026quot;){ a\u0026lt;-\u0026quot;A\u0026quot; b\u0026lt;-\u0026quot;B\u0026quot; intro\u0026lt;-\u0026quot;new.intro\u0026quot; corpo=\u0026quot;new.corpo\u0026quot; f_name\u0026lt;-paste0(tbl,\u0026quot;_trigger()\u0026quot;) q\u0026lt;-glue::glue_sql(\u0026quot;CREATE FUNCTION {DBI::SQL(f_name)} RETURNS trigger AS $$ begin new.document_tokens := setweight(to_tsvector({config},coalesce({intro},\u0026#39;\u0026#39;)), {a}) || setweight(to_tsvector({config},coalesce({corpo},\u0026#39;\u0026#39;)), {b}); return new; end $$ LANGUAGE plpgsql;\u0026quot;,.con=con) RPostgres::dbExecute(con,q) q \u0026lt;- glue::glue_sql(\u0026quot; CREATE TRIGGER tsvectorupdate BEFORE INSERT OR UPDATE ON {`tbl`} FOR EACH ROW EXECUTE FUNCTION {DBI::SQL(f_name)}\u0026quot;,.con=con) RPostgres::dbExecute(con,q) } Realizando buscas Por fim, montamos a função para realizar as buscas\npsql_query \u0026lt;- function (con, tbl, query = \u0026quot;\u0026quot;) { target \u0026lt;- \u0026quot;document_tokens\u0026quot; q \u0026lt;- glue::glue_sql( \u0026quot;SELECT * FROM {`tbl`} WHERE {`tbl`}.{`target`} @@ websearch_to_tsquery(\u0026#39;pt\u0026#39;,{query})\u0026quot;, .con = con ) DBI::dbGetQuery(con, q) } Inclusão no aplicativo Shiny O repositório FullTextSearch contém template de aplicativo para realizar as buscas.\nIncluí uma função psql_g1_dt.R para criar um datatable htmlwidget com ajustes na aparência.\n","permalink":"https://blog.curso-r.com/posts/2020-04-18-fts/","tags":["sql","postgresql","text mining","tokenization"],"title":"Busca textual com R, Shiny e PostgreSQL"},{"author":["Caio"],"categories":["pacotes"],"contents":" Se você estiver interessado em web scraping, considere participar do nosso workshop no dia 25/04/2020! Lá vou ensinar tudo sobre httr, xml2 e muito mais.\nSe você já trabalhou com web scraping, então provavelmente você já ouviu falar de três pacotes: httr, xml2 e rvest. Talvez você não conheça ainda o xml2, mas o rvest foi por muito tempo o divulgado como o principal pacote do R para raspagem de dados. A realidade, entretanto, é que seu reinado acabou.\nComo este post é voltado para pessoas que já têm um pouco de experiência com web scraping em R, não vou me alongar em explicar o que cada função do rvest faz. Meu objetivo aqui é apresentar para o leitor as principais alternativas do xml2 (e do httr) para o rvest.\nO fim de uma era Acredite, eu digo isso com muito pesar, mas o rvest está morto. Ele pode ter sido muito útil em um passado distante, mas hoje em dia a nossa melhor opção para a raspagem de dados é o bom e velho xml2.\nA realidade é que o rvest nunca passou de um wrapper em torno do xml2 e do httr; esta é inclusive a sua descrição oficial: Wrappers em torno dos pacotes ‘xml2’ e ‘httr’ para facilitar o download e a manipulação de HTML e XML. Mas se o rvest está uma camada acima do xml2, então por que abandoná-lo por essa alternativa mais “rústica”?\nO grande problema do rvest é que ele foi majoritariamente abandonado. É verdade que ele teve três novas atualizações em 2019, mas estas não passaram de pequenos ajustes. O último lançamento relevante do rvest (a versão 0.3.0) foi em 2015, praticamente dois séculos atrás em anos da Internet.\nNestes últimos 4 anos e pouco, o xml2 continuou sendo atualizado e acabou se tornando tão simples de usar quanto o seu aparente sucessor. Por isso, na minha opinião, hoje em dia é mais fácil aprender web scraping direto com o original.\nPequenas diferenças A principal diferença entre os dois é que o xml2 trabalha com XPath e não seletores CSS. Na minha opinião, o XPath é muito mais poderoso que os seletores, mas a verdade é que trabalhar com ambos é praticamente igual! Quando você estiver no seu navegador explorando a estrutura HTML de uma página a ser raspada, basta clicar com o botão direito e copiar um ao invés do outro. Inclusive existem até alguns guias de conversão de um para o outro; o XPath é naturalmente mais verborrágico, mas ele compensa com algumas capacidades a mais.\nDepois que você tiver se acostumado com o XPath, basta entender qual é o nome da nova função a utilizar.\nrvest xml2/httr rvest::html_session() Desnecessário com o httr rvest::follow_link() httr::GET() rvest::read_html() xml2::read_html() rvest::html_nodes() xml2::xml_find_all() rvest::html_node() xml2::xml_find_first() rvest::html_text() xml2::xml_text() rvest::html_table() rvest::html_attr() xml2::xml_attr() rvest::html_children() xml2::xml_children() xml2::xml_parents() xml2::xml_contents() xml2::xml_siblings() Como fica claro acima, o xml2 possui praticamente todas as funções que o rvest possui e mais algumas. A grande vantagem de usar o primeiro é precisar de uma dependência a menos: o rvest já importa o xml2, então porque não fazer tudo direto em xml2?\nA única grande ausência do xml2 é o html_table(), mas isso pode ser facilmente corrigido com o código abaixo:\n#\u0026#39; Parse an html table into a data frame #\u0026#39; #\u0026#39; @param x A node, node set or document. #\u0026#39; @param header Use first row as header? If NA, will use first row if it consists of th tags. #\u0026#39; @param trim Remove leading and trailing whitespace within each cell? #\u0026#39; @param fill If TRUE, automatically fill rows with fewer than the maximum number of columns with NAs. #\u0026#39; @param dec The character used as decimal mark. #\u0026#39; #\u0026#39; @export xml_table \u0026lt;- function(x, header = NA, trim = TRUE, fill = FALSE, dec = \u0026quot;.\u0026quot;) { if (\u0026quot;xml_nodeset\u0026quot; %in% class(x)) { return(lapply(x, xml_table, header = header, trim = trim, fill = fill, dec = dec)) } stopifnot(xml2::xml_name(x) == \u0026quot;table\u0026quot;) rows \u0026lt;- xml2::xml_find_all(x, \u0026quot;.//tr\u0026quot;) n \u0026lt;- length(rows) cells \u0026lt;- lapply(rows, xml2::xml_find_all, xpath = \u0026quot;.//td|.//th\u0026quot;) ncols \u0026lt;- lapply(cells, xml2::xml_attr, \u0026quot;colspan\u0026quot;, default = \u0026quot;1\u0026quot;) ncols \u0026lt;- lapply(ncols, as.integer) nrows \u0026lt;- lapply(cells, xml2::xml_attr, \u0026quot;rowspan\u0026quot;, default = \u0026quot;1\u0026quot;) nrows \u0026lt;- lapply(nrows, as.integer) p \u0026lt;- unique(vapply(ncols, sum, integer(1))) maxp \u0026lt;- max(p) if (length(p) \u0026gt; 1 \u0026amp; maxp * n != sum(unlist(nrows)) \u0026amp; maxp * n != sum(unlist(ncols))) { if (!fill) { stop(\u0026quot;Table has inconsistent number of columns. \u0026quot;, \u0026quot;Do you want fill = TRUE?\u0026quot;, call. = FALSE) } } values \u0026lt;- lapply(cells, xml2::xml_text, trim = trim) out \u0026lt;- matrix(NA_character_, nrow = n, ncol = maxp) for (i in seq_len(n)) { row \u0026lt;- values[[i]] ncol \u0026lt;- ncols[[i]] col \u0026lt;- 1 for (j in seq_len(length(ncol))) { out[i, col:(col + ncol[j] - 1)] \u0026lt;- row[[j]] col \u0026lt;- col + ncol[j] } } for (i in seq_len(maxp)) { for (j in seq_len(n)) { rowspan \u0026lt;- nrows[[j]][i] colspan \u0026lt;- ncols[[j]][i] if (!is.na(rowspan) \u0026amp; (rowspan \u0026gt; 1)) { if (!is.na(colspan) \u0026amp; (colspan \u0026gt; 1)) { nrows[[j]] \u0026lt;- c( utils::head(nrows[[j]], i), rep(rowspan, colspan - 1), utils::tail(nrows[[j]], length(rowspan) - (i + 1)) ) rowspan \u0026lt;- nrows[[j]][i] } for (k in seq_len(rowspan - 1)) { l \u0026lt;- utils::head(out[j + k, ], i - 1) r \u0026lt;- utils::tail(out[j + k, ], maxp - i + 1) out[j + k, ] \u0026lt;- utils::head(c(l, out[j, i], r), maxp) } } } } if (is.na(header)) { header \u0026lt;- all(xml2::xml_name(cells[[1]]) == \u0026quot;th\u0026quot;) } if (header) { col_names \u0026lt;- out[1, , drop = FALSE] out \u0026lt;- out[-1, , drop = FALSE] } else { col_names \u0026lt;- paste0(\u0026quot;X\u0026quot;, seq_len(ncol(out))) } df \u0026lt;- lapply(seq_len(maxp), function(i) { utils::type.convert(out[, i], as.is = TRUE, dec = dec) }) names(df) \u0026lt;- col_names class(df) \u0026lt;- \u0026quot;data.frame\u0026quot; attr(df, \u0026quot;row.names\u0026quot;) \u0026lt;- .set_row_names(length(df[[1]])) if (length(unique(col_names)) \u0026lt; length(col_names)) { warning(\u0026quot;At least two columns have the same name\u0026quot;) } df } Apesar de ser uma função bastante complicada, ela não passa de uma cópia do código de rvest::html_table() utilizando apesar funções do xml2; com isso você terá a sua própria implementação de xml_table(). E depois de uma ou duas semanas, você já estará pronto para abandonar o rvest e voltar a usar o bom e velho xml2!\nQuer saber mais? Para mais informações sobre este e outros assuntos do mundo R, siga a Curso-R no GitHub, no Facebook e no Twitter. Além disso, participe da nossa comunidade no Discord e do nosso canal no Telegram onde compartilhamos as novidades mais quentes do R e respondemos as suas perguntas pessoalmente.\n","permalink":"https://blog.curso-r.com/posts/2020-04-09-xml2/","tags":["xml2","rvest"],"title":"Abaixo ao rvest!"},{"author":["Daniel"],"categories":["divulgação"],"contents":" Lançamos a Comunidade Curso-R!\nEste fórum foi criado para qualquer um tenha interesse em aprender, compartilhar conhecimento e discutir assuntos relacionados à linguagem R e assuntos correlacionados.\nAqui você pode tirar suas dúvidas, responder dúvidas de outras pessoas, compartilhar links interessantes, iniciar discussões relevantes para a comunidade, divulgar eventos e até mesmo divulgar vagas de emprego - desde que sempre respeite o nosso código de conduta.\nAqui também é o lugar que alunos e ex-alunos da Curso-R usam para tirar dúvidas com os nossos professores, fazendo com que nossos cursos permaneçam o mais abertos possível.\nCadastrem-se aqui e sejam bem vindos!\nNota: o banner deste post também é uma arte da Allison Horst\nArte da Allison Horst.\n","permalink":"https://blog.curso-r.com/posts/2020-02-21-discourse/","tags":["comunidade"],"title":"Anunciando a comunidade Curso-R"},{"author":["Daniel"],"categories":["pacotes"],"contents":" O gghighlight é uma das extensões do ggplot2 que eu mais gosto. Este post serve como uma pequena introdução é apresentação do que é possível fazer com ele.\nComo próprio nome indica, o gghighlight serve para realçar partes de um gráfico feito com o ggplot.\nRealçando pontos Muitas vezes temos um gráfico de dispersão em que queremos realçar alguns pontos de acordo com alguma característica. Por exemplo, abaixo estamos realçando os pontos que possuem carat \u0026gt; 4, além disso colocamos uma label em cada um.\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.4 ✓ dplyr 1.0.2 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() diamonds %\u0026gt;% ggplot(aes(x = carat, y = price)) + geom_point() + gghighlight::gghighlight(carat \u0026gt; 4, label_key = carat) Também é possível configurar a cor dos pontos que serão realçados e dos que não serão, bem como o estilo dos labels.\ndiamonds %\u0026gt;% ggplot(aes(x = carat, y = price)) + geom_point(color = \u0026quot;red\u0026quot;) + gghighlight::gghighlight( carat \u0026gt; 4, label_key = carat, unhighlighted_colour = \u0026quot;black\u0026quot;, label_params = list(size = 10, fill = \u0026quot;grey\u0026quot;) ) ## Warning: The `unhighlighted_colour` argument of `gghighlight()` is deprecated as of gghighlight 0.2.0. ## Please use the `unhighlighted_params` argument instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. Realçando linhas Com o gghighlight também é possível realçar linhas em um gráfico que possui varias linhas. Isso é interessante quando você quer ver como uma série temporal se compara com relação à um conjunto de outras séries.\nd \u0026lt;- cranlogs::cran_downloads( packages = tidyverse::tidyverse_deps()$package, from = \u0026quot;2019-01-01\u0026quot;, to = \u0026quot;2019-12-31\u0026quot; ) No gráfico a seguir mostramos o número de downloads de cada um dos pacotes do tidyverse no ano de 2019. Uma das séries se destaca por mudar de padrão no meio do ano. Usamos o gghighlight para destacá-la no gráfico.\nd %\u0026gt;% ggplot(aes(x = date, y = count, group = package)) + geom_line() + gghighlight::gghighlight(max(count) \u0026gt; 100000) ## label_key: package É isso aí! Esse pacote existe e é muito útil! A documentação é bem boa também.\n","permalink":"https://blog.curso-r.com/posts/2020-02-20-gghighlight/","tags":["ggplot2"],"title":"gghighlight"},{"author":["Caio"],"categories":["conceitos"],"contents":" Nesta série de posts, apresentamos a todos a nossa primeira tentativa de escrever um livro: O Zen do R! Durante algumas semanas de 2019, todas as quartas, trouxemos para o nosso blog os capítulos do livro e respondemos qualquer pergunta sobre o conteúdo.\nDepois de algum tempo na geladeira, voltamos a escrever! Vamos falar sobre como manter as versões de um pacote sobre controle.\nVersões A maior parte dos softwares que utilizamos no dia-a-dia possuem versões, até mesmo aqueles que não costumamos associar com esse tipo de prática. Sistemas operacionais, como Windows 10 e iOS 13 , têm as versões embutidas em seus próprios nomes, enquanto muitos aplicativos, como WhatsApp 2.20.22, só exibem esse tipo de informação no fundo da página de configurações.\nPacotes do R também possuem versões. Ao executar o comando update.packages() (inclusive faça isso agora caso você nunca o tenha feito), o R é forçado a procurar por vesões mais recentes dos pacotes instalados na sua máquina. É importante deixar claro que “versão” não é nada mais que o identificador de uma atualização do software; se eu mudar uma linha do código do meu pacote e atualizar esse fonte no GitHub, pode-se dizer que criei uma nova versão do meu pacote.\nMas por que marcar e dar nomes a essas atualizações? É bastante mais fácil simplesmente continuar programando e exigir que seus usuários baixem a versão mais recente do seu código. Essa prática, entretanto, é ruim para o usuário e para o programador.\nVocê, como programador, quer poder fazer alterações no seu código que não necessariamente estão prontas para o público em geral. Erros acontecem e, muitas vezes, é preciso fazer uma série de modificações antes que o pacote volte a ter certa estabilidade. Por outro lado, o usuário também só quer atualizar seu pacote quando algo suficientemente diferente estiver disponível (sejam correções de erros, sejam novas funcionalidades). Além disso, caso uma nova versão do seu código gere problemas para ele, é necessário ter uma versão estável anterior claramente rotulada para que o usuário possa fazer o downgrade do pacote.\nE justamente são esses rótulos que precisamos utilizar para ter um bom sistema de versões nos nossos pacotes. Ao contrário de “controle de versão”, para o qual utilizamos o Git, “versionamento semântico” é a prática de dar nomes fáceis de entender para as versões de um projeto. Trazendo isso para termos concretos, o pacote dplyr está na versão 0.8.4, o ggplot2 está na 3.2.1 e o shiny está na 1.4.0.\nVersionamento semântico Em teoria, existe um padrão ouro para a nomenclatura de versões de um software. Alguns dos seus preceitos, como o uso de 3 números inteiros não-negativos separados por pontos, são extremamente valiosos, enquanto outros nem sempre são seguidos ao pé da letra. Aqui tratamos do protocolo padronizado, mas apenas você pode determinar quanto dele o seu esquema de nomenclatura seguirá.\nUma versão é denotada pela forma X.Y.Z, onde: - X é a versão maior (denominada major); - Y é a versão menor (denominada minor) e - Z é a versão do conserto (denominada patch).\nUm patch não passa de uma versão atualizada na qual apenas bugs foram corrigidos; nenhuma funcionalidade pode ser alterada e qualquer código escrito utilizando a versão sem o patch deve continuar funcionando. Quando alguma nova funcionalidade é introduzida ao programa, mas ele continua sendo retrocompatível (ou seja, compatível com as suas versões anteriores), deve ser incrementada a versão minor. Por fim, se for introduzida alguma mudança que quebra a retrocompatibilidade, deve ser increvementada a versão major. É importante dizer também que, depois de lançada, uma versão nunca deverá ser alterada, pois isso confundirá os usuários da mesma.\nAlguns pontos do versionamento semântico não são seguidos sempre, mas podem ser úteis. Por exemplo, o patch pode ser omitido se ele for 0, a major 0 normalmente é reservada para software em fase de testes (o dplyr, portanto, não segue essa regra) e às vezes sufixos em texto podem ser adicionados (como “-alpha”, “-beta”, etc.).\nPara propósitos ilustrativos, imaginemos um pacote chamado pacotr que possui apenas uma função com protótipo f(x, y, z = TRUE). Um caminho imaginário para seu desenvolvimento pode ser o seguinte: - Versão 0.1.0: a primeira versão com código do pacote contém uma função f(x, y); - Versão 0.1.1: um bug é consertado em f(x, y); - Versão 0.1.2: mais um bug é consertado em f(x, y); - Versão 1.0.0: o pacote sai do beta quando a função ganha um novo argumento z, agora necessário para o seu funcionamento, e vira f(x, y, z); - Versão 1.0.1: um bug é consertado em f(x, y, z); - Versão 1.2.0: o argumento z agora tem um valor padrão, transformando a função em f(x, y, z = FALSE); - Versão 2.0.0: o argumento z muda seu valor padrão, quebrando qualquer código escrito utilizando a versão 1.x.x, se tornando f(x, y, z = TRUE)\nUtilizando o exemplo acima sem perda de generalidade, entre qualquer dois passos subsequentes haveriam versões denominadas de desenvolvimento, como por exemplo 0.1.1.9000, 1.0.0.9000 e assim por diante. Isso não está no padrão ouro do versionamento semântico, mas é comumente utilizado em programas de R.\nUma versão de desenvolvimento é qualquer “subversão” que não deveria ser utilizada pelo usuário final; qualquer commit entre patches, minors ou majors é uma versão de desenvolvimento. Por convenção, a primeira versão de desenvolvimento é marcada como 9000, podendo chegar até 9999. Alguns programadores incrementam esse número a cada commit realizado, mas é mais comum manter o 9000 até que o código esteja pronto para se tornar um verdadeiro patch, minor ou major.\nNa prática, é muito simples trabalhar com versões. Assim que um novo pacote é criado através de usethis::create_package(), o arquivo DESCRIPTION tem a seguinte cara:\nPackage: demo Title: What the Package Does (One Line, Title Case) Version: 0.0.0.9000 Authors@R: person(given = \u0026quot;First\u0026quot;, family = \u0026quot;Last\u0026quot;, role = c(\u0026quot;aut\u0026quot;, \u0026quot;cre\u0026quot;), email = \u0026quot;first.last@example.com\u0026quot;, comment = c(ORCID = \u0026quot;YOUR-ORCID-ID\u0026quot;)) Description: What the package does (one paragraph). License: What license it uses Encoding: UTF-8 LazyData: true Note como o campo Version já está populado com a primeira versão possível para um pacote: 0.0.0.9000. Depois de alguns commits, se acharmos que o código está preparado para a primeira minor, utilizamos:\nusethis::use_version(\u0026quot;minor\u0026quot;) #\u0026gt; ✔ Setting Version field in DESCRIPTION to \u0026#39;0.1.0\u0026#39; A segunda linha já deixa claro que agora o arquivo DESCRIPTION contém uma linha Version: 0.1.0 (note como a versão de desenvolvimento é removida automaticamente). Mas atenção, antes de fazer qualquer outra alteração no pacote, você deve se certificar de que esta alteração receberá um commit só para ela. Atualmente o usethis faz isso automaticamente.\nO primeiro commit após a alteração da versão deve trazer o pacote de volta para o estado de desenvolvimento. Ou seja, deve existir um único commit no qual o pacote é considerado estável para cada sequência de commits de desenvolvimento.\nusethis::use_dev_version() #\u0026gt; ✔ Setting Version field in DESCRIPTION to \u0026#39;0.1.0.9000\u0026#39; Depois que o pacote voltar para desenvolvimento, a programação pode continuar desempedida. Por completude, usethis::use_version() pode ser utilizada para qualquer versão cheia:\nusethis::use_version(\u0026quot;patch\u0026quot;) #\u0026gt; ✔ Setting Version field in DESCRIPTION to \u0026#39;0.1.1\u0026#39; usethis::use_dev_version() #\u0026gt; ✔ Setting Version field in DESCRIPTION to \u0026#39;0.1.1.9000\u0026#39; usethis::use_version(\u0026quot;major\u0026quot;) #\u0026gt; ✔ Setting Version field in DESCRIPTION to \u0026#39;1.0.0\u0026#39; Releases Uma parte interessante do processo de versionamento é que o GitHub permite marcar commits específicos como estáveis e atribuir um rótulo aos mesmos. No fundo isso não passa de um versionamento semântico integrado ao site.\nComo exemplo, podemos ver a aba “Releases” do pacote decryptr:\nÉ bom fazer esse tipo de versionamento externo porque os usuários passam a ter um lugar de fácil acesso com o código-fonte de absolutamente todas as versões do seu pacote. Além disso, a função devtools::install_github() é capaz de utilizar esses releases como indicador de qual versão de um pacote deve ser baixada.\nPara criar releases de um pacote seu, antes você deve dar push no commit da versão atual (depois de usethis::use_version() e antes de usethis::use_dev_version()), pois assim estará pública no GitHub exatamente a versão do seu pacote que deve ser baixada pelos usuários. Depois basta ir para a aba das releases e clicar em Draft a new release. Preencha os campos correspondentes e publique a versão.\nUma forma eficiente de manter todas as alterações realizadas no pacote de forma organizada é com um arquivo NEWS. Ele pode ser criado com usethis::use_news_md() e criar um novo título para cada versão. Observe como esse arquivo é utilizado no ggplot2.\n","permalink":"https://blog.curso-r.com/posts/2020-02-21-zen-do-r-8/","tags":["Zen do R"],"title":"Versões e Releases (Zen do R parte 8)"},{"author":["Caio"],"categories":["análises"],"contents":" Uma Bela Herança O motivador da discussão deste post é meio estranho: em 17 de dezembro de 2020 serão comemorados 250 anos desde o nascimento do grande compositor alemão Ludwig van Beethoven. Não é incomum ver pessoas abrindo poupanças para seus filhos logo quando nascem (para que eles tenham um pé-de-meia quando crescerem) e isso me fez pensar como ficaria uma poupaça aberta para Beethoven 250 anos atrás…\nSendo assim, em pleno 2020, uma carta chega à casa do último herdeiro vivo do músico, você, lhe dando acesso à recém-descoberta poupança do seu tatara-tataravô. Como poderia existir uma poupança em 17 de dezembro de 1770 ou como ela durou até hoje é um mistério, mas sabe-se que ela foi aberta com o que atualmente seriam R$ 500,00.\nVisualizando a Poupança Para saber quanto dinheiro há hoje na poupança, basta utilizar a fórmula de juros compostos:\n\\[ V_f=V_p(1+j)^n \\]\nOnde \\(V_f\\) é o valor final, \\(V_p\\) é o valor inicial, \\(j\\) é a taxa de juros e \\(n\\) é o número de períodos que o dinheiro ficou guardado (número de anos se os juros forem anuais, número de meses se os juros forem mensais, etc.).\nMas, muito mais do que simplesmente obter o valor final, é interessante ver o crescimento dos fundos da poupança ao longo do tempo. Isso pode ser feito com o pacote ggplot2 facilmente: basta utilizar a fórmula acima para cada um dos 250 anos e plotar esses pontos em um scatterplot. Entretanto, temos uma opção ainda melhor.\nUm recurso pouco conhecido do ggplot2 é o stat_function(), capaz de construir uma linha contínua no gráfico a partir de uma função que recebe o valor de uma abscissa e retorna o valor da ordenada correspondente. Ou seja, se tivermos uma função f(x) = y, podemos passá-la como argumento para o stat_function() desenhá-la (com o nível de resolução mais apropriado).\nSendo assim, precisamos transformar a função ternária Vp*(1+j)**n em uma função que recebe apenas um argumento. Para isso devemos usar uma técnica denominada currying, que essencialmente “pré-preenche” argumentos de uma função; nesse caso, vamos fixar o valor de Vp e j de modo que somente o n varie. A função unária final, que podemos construir com purrr::partial(), nos mostrará quanto tinha na poupança depois de n unidades de tempo (a saber, anos).\nO código final para a visualização é o seguinte:\nlibrary(magrittr) # Fórmula de juros compostos juro_composto \u0026lt;- function(Vp, j, n) Vp*(1+j)**n # Gerar um gráfico de juros compostos grafico_juros \u0026lt;- function(Vp, j, n, SI = TRUE) { # Atalho para o ggplot (currying) juro_fixado \u0026lt;- purrr::partial(juro_composto, Vp = Vp, j = j) # Exibir eixos de forma abreviada scale_real \u0026lt;- if (SI) { scales::label_number_si(prefix = \u0026quot;R$ \u0026quot;) } else { I } # Mostrar no gráfico c(0, n) %\u0026gt;% dplyr::tibble(x = .) %\u0026gt;% ggplot2::ggplot(ggplot2::aes(x)) + ggplot2::stat_function(fun = juro_fixado) + ggplot2::scale_y_continuous(labels = scale_real) + ggplot2::xlab(\u0026quot;Anos\u0026quot;) + ggplot2::ylab(\u0026quot;Valor Final\u0026quot;) + ggplot2::theme_minimal() } Agora vamos para os detalhes da poupança. Para desconsiderarmos a inflação, digamos que a poupança começou com o que nos valores de hoje seriam R$ 500,00; uma poupança rende, em média, módicos 6% ao ano, então vamos usar isso como referência; por fim, o dinheiro ficou parado lá por 250 anos. Quanto você acha que terá na conta agora em 2020?\ngrafico_juros(500, 0.06, 250) Como é de se esperar, juros compostos são uma função exponencial. Depois desses 250 anos, os R$ 500,00 se tornaram mais de R$ 1B. Nada mal para uma mera poupança.\nBônus Aproveitando que já temos todo o código pronto, podemos visualizar outras séries exponenciais. Se colocássemos 1 centavo na poupança no ano 1 D.C., hoje em dia teríamos aproximadamente R$ 1.311740210^{49}, ou mais ou menos R$ 0,10 para cada átomo da Terra. Essa é a mágica dos juros compostos.\ngrafico_juros(0.01, 0.06, 2020, FALSE) Uma propriedade interessante de frases como “crescimento de 3%” é que raramente notamos que essa porcentagem se acumuma ano-a-ano. Um país cujo PIB cresce 3% ao ano tem que dobrar a sua economia a cada 25 anos. Atualmente o PIB do Brasil é de R$ 7T, vejamos o gráfico de nossa economia crescendo a 3% todo ano:\ngrafico_juros(7e12, 0.03, 25) Isso pode não impressionar muito, mas, se começármos a falar em escalas globais, a coisa muda de figura. Os EUA têm um PIB de US$ 17T, enquanto a soma de todos os PIBs do mundo dá aproximadamente US$ 80T; se a economia estadunidense crescesse a 3% pelos próximos 100 anos, precisaríamos de mais de 4 PIBs mundiais para chegar perto desse novo PIB dos EUA.\ngrafico_juros(17e12, 0.03, 100) Uma coisa que não cresce exponencialmente, entretanto, é a quantidade de recursos disponíveis na natureza…\n","permalink":"https://blog.curso-r.com/posts/2020-02-20-poupanca-beethoven/","tags":["ggplot2","finanças"],"title":"A Poupança de Beethoven"},{"author":["Caio"],"categories":["análises"],"contents":" Eu Tenho um Problema Esse problema se chama fake news. Não importa a fonte, a inclinação ideológica, o estilo ou o grau da mentira; eu preciso me segurar muito para não passar a tarde toda desmascarando as notícias falsas (antigamente conhecidas simplesmente como mentiras) que chegam no meu WhatsApp diariamente. Eu não ligo tanto para análises mal feitas ou mesmo tendenciosas (já que na minha opinião isso faz parte de viver em sociedade), mas quando o conteúdo parece deliberadamente montado para ludibriar o leitor eu perco o controle.\nNormalmente eu ignoro a mensagem/foto/montagem e sigo com o meu dia, mas há ocasiões raras em que o nível da mentira é grande o bastante e eu estou com a agenda livre o suficiente, que corro para o computador com o único objetivo de desmentir a fake news. Geralmente em menos de meia hora já tenho uma resposta pronta com dados confiáveis, uma imagem sem Photoshop ou simplesmente um link para um site de fact-checking. Mas ontem aconteceu algo inédito: a mentira lutou contra mim.\nA “Notícia” A “notícia” falsa em questão é na verdade uma imagem. Uma imagem alterada, por sinal.\nVamos por partes. Meu espanhol não é dos melhores, mas acho que qualquer lusófono consegue compreender bem o que está sendo mostrado: o gráfico representa a posição de dois países no ranking de PIB per capita. Além disso, destacam-se o período de 1882 a 1950 (quando a Argentina nunca ficou abaixo do 10º maior PIB per capita), o período entre 1895 e 1896 (quando a Argentina estava em 1º lugar) e o ranking atual dos nossos vizinhos (59º lugar).\nDe acordo com o que me parece ser a imagem original, nesta versão da notícia ganhamos de brinde também uma pequena intervenção “artística” na forma de duas figuras da política Argentina: Juan Bautista Alberdi (um dos principais responsáveis pela primeira Constituição argentina) e Juan Domingo Perón (três vezes presidente da Argentina). O texto que acompanha a foto de Perón nos “informa” sobre a Constituição de 1949, quando o governo peronista reforma a Carta Magna e aparentemente dá início à “justiça social”.\nO que esta imagem parece estar sugerindo é que Perón é o grande culpado pela queda da Argentina no ranking do PIB per capita. A comparação com a Austrália (que evidentemente não teve nenhum governo peronista) reforça essa ideia porque ela permaneceu aproximadamente na camada superior do gráfico. Implicitamente, o leitor parece ser encorajado a equivaler PIB per capita com desenvolvimento econômico ou qualidade de vida.\nMas, como dito anteriormente, esta imagem não é uma fake news qualquer: para desmentí-la será necessário explorar algumas camadas da narrativa sendo traçada.\nOs Dados Não Mentem… Diferentemente da maior parte das fake news, esta possui uma fonte. Os dados foram retirados do Maddison Project, um projeto acadêmico da Universidade de Groningen cujo objetivo é estimar o valor de certas estatísticas econômicas para o passado longínquo (mais sobre isso em breve).\nSendo assim, acessei o site do projeto e coletei os dados. Eles não passam de uma planilha Excel com um país por coluna e um ano por linha, com cada célula contendo o PIB per capita daquele país naquele ano em dólares constantes de 2011.\nUsando o ggplot2 podemos reproduzir a imagem com facilidade:\nQuando vi o gráfico, fiquei boquiaberto. Uma imagem em baixa resolução, com Photoshop mal feito, linguagem tendenciosa e veiculada via WhatsApp parecia não estar errada. Analisando o ranking da Argentina em 2016, o país de fato estava na 62ª posição e não na 59ª, mas isso está longe de desqualificar um argumento.\n…Mas Podem Enganar Mesmo estando tecnicamente certo, um gráfico não necessariamente está correto. A próxima visualização que fiz pretendia explorar não o ranking dos países, mas os seus PIBs per capita diretamente; isso é relevante porque a suposta alegação do gráfico diz respeito à economia da Argentina e não sobre ela em relação às de todos os outros países.\nO que me chamou a atenção no gráfico acima foi que os PIBs per capita da Argentina e da Austrália parecem estar se descolando consistentemente desde 1905. Por que então o descolamento dos rankings só ocorre após 1950?\nContexto Importa… Depois de mexer mais no gráfico dos valores, notei que a grande maioria dos países da tabela não tinham nenhum dado para antes de 1950. Isso me fez procurar outras fontes para dados do PIB, mas não encontrei nada que contemplasse um período tão grande quanto o do projeto Maddison. Sendo assim, fui saber mais sobre o conceito de Produto Interno Bruto.\nEm primeiro lugar, descobri que a definição moderna de PIB só existe desde 1934 e que ele só começou a ser utilizado para medir a economia dos países a partir de 1944.\nE agora entra em jogo o que de fato é o projeto Maddison. Já comentei que ele estima alguns índices socioeconômicos para o passado, mas não falei sobre um detalhe vital que só descobri depois: o projeto estima esses dados somente para um conjunto limitado de países! Ou seja, antes de meados da década de 1950, só temos acesso à informação de um grupo seleto de nações.\nVoltando para a base, percebi que isso explicava a quantidade de países com linhas vazias antes de 1950, pois apenas 28 nações possuíam estimativas para o PIB per capita a desde 1875. É muito diferente dizer que a Argentina tinha o maior PIB per capita do mundo em 1895 e dizer que ela tinha o maior PIB per capita (estimado) entre 28 países selecionados. Abaixo é possível ver o ranking até 2016 se filtrarmos apenas os 28 países iniciais:\nFiz questão de manter a escala deste gráfico igual à do anterior porque assim fica fácil de ver o problema com a visualização da “notícia”. De fato a Argentina perdeu posições no ranking, isso é indiscutível, mas é muito diferente perder algo em torno de 10 posições em 50 anos e perder 50 posições no mesmo período.\nPara entender ainda melhor o que a imagem original está escondendo, basta exibir no gráfico os dados de todos os países contemplados pela base.\nNote como a quantidade de países presentes na tabela cresce significativamente depois do início da década de 1950 dado que a métrica foi colocada em vigor apenas 6 anos antes. Sendo assim, é de se esperar que a Argentina e até a Austrália percam posições no ranking. No fundo, estamos visualizando uma base para antes de 1950 (as estimativas do projeto Maddison) e uma para depois (as estatísticas reais coletadas pela comunidade internacional).\nMais interessante ainda do que tentar visualizar todos os países em um mesmo gráfico, é possível utilizar a função dplyr::percent_rank() que, ao invés de atribuir números naturais como ranking, retorna o ranking dividido pelo total de elementos na comparação.\nAqui fica claro que, mesmo depois de piorar muito no ranking do PIB per capita, a Argentina ainda não está tão ruim quanto estava no início dos anos 1880 (em relação a todos os países disponíveis para cada ano). Conclusão muito diferente da sugerida pela imagem original.\n…E História Também A última camada dessa fake news é um pouco mais sutil e não pode ser identificada olhando somente para os dados. Na verdade, só fiquei sabendo disso enquanto pesquisava mais sobre a história da Argentina na Wikipédia.\nSem querer entrar na discussão “peronistas vs. antiperonistas”, mas nossos vizinhos possuem uma herança muito forte dos três governos de Perón e a sociedade é fortemente dividida entre aqueles que apoiam seu legado e aqueles que o rejeitam (de forma similar ao nosso sentimento em relação a Vargas, mas certamente mais relevante para a política contemporânea).\nA imagem é claramente montada para manchar o legado de Perón e, para isso, destaca a data na qual ele reformou a Constituição argentina. A adição das fotos e do texto posteriormente em Photoshop é o que mais faz soar meu alarme de notícia falsa.\nO que aprendi sobre a história argentina, entretanto, parece colocar em cheque a ideia de que teria sido a Constituição peronista que derrubou a Argentina no ranking dos PIBs per capita: Perón foi deposto por um golpe militar em 1955 e no ano seguinte foi publicada uma nova Constituição que anulava a anterior. A Constituição de 1956 declarava “vigente la Constitución Nacional sancionada en 1853, con las reformas de 1860, 1866 y 1898, y exclusión de la de 1949”.\nAgora acho importante fazer uma visualização final, destacando a vigência da Constituição que deu início à “justiça social”:\nSugiro que os autores da imagem original façam a mesma montagem com o gráfico acima.\n","permalink":"https://blog.curso-r.com/posts/2020-01-21-fake-news/","tags":["fake news"],"title":"Camadas de Fake News"},{"author":["Julio"],"categories":["análises"],"contents":" A base de dados de CNPJ da Receita Federal do Brasil (RFB) é, na minha opinião, uma das maiores conquistas de dados abertos do Brasil. Trata-se de uma base com quase 42 milhões de linhas repleta de informações de todas as empresas do Brasil. Essa base pode ser útil para desenvolver diversos tipos de produtos e serviços, bem como abre espaço para realizaçãod e estudos para políticas públicas.\nA base de dados da RFB pode ser obtida de duas fontes distintas:\nO fantástico canal Brasil.io, do Álvaro Justen, ou turicas. Diretamente do site da RFB. Lendo os dados em csv Para carregar a partir de (1), também é bem tranquilo. O Turicas gentilmente disponibilizou as bases em CSV, facilitando a leitura em qualquer software de análise de dados.\nPor exemplo, podemos fazer a leitura usando o {vroom}:\nsocio \u0026lt;- vroom::vroom(\u0026quot;~/Downloads/rfb/socio.csv.gz\u0026quot;) socio Observations: 26,188,771 Variables: 10 chr [7]: cnpj, nome_socio, cnpj_cpf_do_socio, codigo_qualificacao_socio, cpf_representante_legal, nome_representante_legal, codigo_qualificacao_r... dbl [2]: identificador_de_socio, percentual_capital_social date [1]: data_entrada_sociedade Call `spec()` for a copy-pastable column specification Specify the column types with `col_types` to quiet this message Se você não conhece o vroom, trata-se de um pacote para leitura de dados de texto que carrega as bases de forma extremamente rápida, pois ele não carrega a base na memória, apenas indexa. Mais detalhes na documentação do pacote.\nUsando o pacote {qsacnpj} Para carregar a partir de (2), você precisará ler arquivos do tipo Fixed Width, que geralmente dá um pouco de trabalho, pois é necessário criar um arquivo de configuração. Felizmente, o George Santiago já resolveu esse problema para nós, criando o pacote {qsacnpj}.\nPara ler os arquivos da RFB usando o pacote {qsacnpj}, não tem segredo. Basta seguir as instruções dadas pelo George no README do pacote, que copiei aqui:\nBaixe os arquivos (.zip) da base de dados do CNPJ no site da Receita Federal. Salve o arquivo .zip no diretório que será utilizado para o processamento dos dados. Descompacte os arquivos no diretório. Talvez os arquivos tenhom nomes semelhantes a K3241.K03200DV.D90607.L00001. OBS: Os arquivos descompactados têm mais de 85Gb. Verifique se há espaço suficiente no seu HD Adicione a extensão .txt no final do nome do arquivo. Ex: K3241.K03200DV.D90607.L00001.txt Crie uma pasta específica para armazenar somente os arquivos .txt. Instale e execute o pacote. Por exemplo:\nmeu_arquivo \u0026lt;- \u0026quot;~/Downloads/rfb/originais/K3241.K03200DV.D90805.L00001.txt\u0026quot; qsacnpj::gerar_bd_cnpj(meu_arquivo) [1] \u0026quot;Pasta \u0026#39;bd_cnpj_tratados\u0026#39; criada com sucesso!\u0026quot; [1] \u0026quot;Iniciando o tratamento e consolidação dos dados do CNPJ. Esse processo pode levar entre 2h a 4h, dependenndo da configuração do computador!\u0026quot; [1] \u0026quot;Base de Dados do CNPJ gerada com Sucesso! Tabelas geradas: `dados_cadastrais_pj`, `dados_socios_pj` e `dados_cnae_secundario_pj`\u0026quot; [1] \u0026quot;Adicionando na base a tabela com dados dos Entes Públicos Federais, Estaduais e Municipais!\u0026quot; [1] \u0026quot;Tabela `tab_cnpj_entes_publicos_br` gerada com Sucesso!\u0026quot; [1] \u0026quot;Adicionando na base a tabela com Código e Nome da Qualificação dos Responsáveis!\u0026quot; [1] \u0026quot;Tabela `tab_qualificacao_responsavel_socio` gerada com Sucesso!\u0026quot; [1] \u0026quot;Adicionando na base a tabela com Código e Nome da Situação Cadastral!\u0026quot; [1] \u0026quot;Tabela `tab_situacao_cadastral` gerada com Sucesso!\u0026quot; [1] \u0026quot;Adicionando na base a tabela com Código e Nome da Natureza Jurídica!\u0026quot; [1] \u0026quot;Tabela `tab_natureza_juridica` gerada com Sucesso!\u0026quot; [1] \u0026quot;Adicionando na base a tabela com os CNAEs!\u0026quot; [1] \u0026quot;Tabela `tab_cnae` gerada com Sucesso!\u0026quot; Fim do Processamento: Base de Dados do CNPJ gerada com Sucesso! Em seguida, é possível ler os arquivos em .csv gerados. Um detalhe importante é que os arquivos foram gerados com o separador #.\nUma coisa que eu gostei do {qsacnpj} é que ele já tem algumas bases auxiliares carregadas no pacote. Essas bases podem ser usadas para fazer o match com a base principal, com o objetivo de obter informações complementares.\nqsacnpj::tab_cnae: nomes e códigos de atividades das empresas qsacnpj::tab_cnpj_entes_publicos_br: nome e código dos Entes Públicos Federais, Estaduais e Municipais qsacnpj::tab_natureza_juridica: nome e código de Classificação da Natureza Jurídica de 2018. qsacnpj::tab_qualificacao_responsavel_socio: nome e código de Qualificação do Responsável e/ou Sócio no Quadro Societário da Pessoa Jurídica. qsacnpj::tab_situacao_cadastral: nome e código da Situação Cadastral da Pessoa Jurídica. Análises Para dar um cheirinho de como essa base é rica, fiz alguns gráficos descritivos considerando apenas as informações do estado de SP. Usei como base o CSV do Turicas e usei as bases carregadas do {qsacnpj}.\nA Figura 1 mostra o volume de empresas na base por ano de abertura. É interessante notar a evolução a partir do ano de 2004.\n## Warning in knitr::include_graphics(\u0026quot;/images/posts/conteudo/qsacnpj/ ## abertura.png\u0026quot;): It is highly recommended to use relative paths for images. You ## had absolute paths: \u0026quot;/images/posts/conteudo/qsacnpj/abertura.webp\u0026quot; Figura 1: Quantidade de empresas abertas por ano. A Figura 2 mostra as atividades principais das empresas. Eu fiquei surpreso com o fato de mais um quarto das empresas serem relacionadas a comércio, reparação de veículos automotores e motocicletas. Seria interessante aprofundar essa análise criando um Treemap, por exemplo, para entender o que acontece nos subníveis dessa categoria.\n## Warning in knitr::include_graphics(\u0026quot;/images/posts/conteudo/qsacnpj/ ## atividade.png\u0026quot;): It is highly recommended to use relative paths for images. You ## had absolute paths: \u0026quot;/images/posts/conteudo/qsacnpj/atividade.webp\u0026quot; Figura 2: Distribuição da atividade principal (CNAE), considerando apenas o nível mais genérico. A Figura 3 mostra a distribuição do capital social das empresas. Quase metade das empresas tem capital social de até mil reais! Essa análise, no entanto, pode estar distorcida, pois existem várias empresas na base que não têm capital social por conta do tipo societário.\n## Warning in knitr::include_graphics(\u0026quot;/images/posts/conteudo/qsacnpj/ ## capital.png\u0026quot;): It is highly recommended to use relative paths for images. You ## had absolute paths: \u0026quot;/images/posts/conteudo/qsacnpj/capital.webp\u0026quot; Figura 3: Distribuição do capital social das empresas. A Figura 4 mostra os municípios de registro das empresas. Aqui não tem nada de impressionante, apenas o esperado: cidades mais populosas têm mais empresas. Seria interessante aprofundar a análise para taxa de empresas por 100 mil habitantes, por exemplo.\n## Warning in knitr::include_graphics(\u0026quot;/images/posts/conteudo/qsacnpj/mapa.webp\u0026quot;): ## It is highly recommended to use relative paths for images. You had absolute ## paths: \u0026quot;/images/posts/conteudo/qsacnpj/mapa.webp\u0026quot; Figura 4: Mapa dos municípios de origem. A Figura 5 mostra a distribuição da natureza jurídica das empresas. Eu fiquei surpreso com o fato de que 60% das empresas são empresários individuais!\n## Warning in knitr::include_graphics(\u0026quot;/images/posts/conteudo/qsacnpj/ ## natureza.png\u0026quot;): It is highly recommended to use relative paths for images. You ## had absolute paths: \u0026quot;/images/posts/conteudo/qsacnpj/natureza.webp\u0026quot; Figura 5: Distribuição da natureza jurídica das empresas. A Figura 6 mostra a distribuição da quantidade de sócios. É esperado que a maioria das empresas tenham dois sócios, já que as LTDAs devem ter pelo menos dois sócios.\n## Warning in knitr::include_graphics(\u0026quot;/images/posts/conteudo/qsacnpj/ ## qtd_socios1.png\u0026quot;): It is highly recommended to use relative paths for images. ## You had absolute paths: \u0026quot;/images/posts/conteudo/qsacnpj/qtd_socios1.webp\u0026quot; Figura 6: Distribuição da quantidade de sócios. A Figura 7 mostra a distribuição da quantidade de sócios, considerando apenas Limitada e SA. É possível notar que as Sociedades Anônimas possuem mais sócios.\n## Warning in knitr::include_graphics(\u0026quot;/images/posts/conteudo/qsacnpj/ ## qtd_socios2.png\u0026quot;): It is highly recommended to use relative paths for images. ## You had absolute paths: \u0026quot;/images/posts/conteudo/qsacnpj/qtd_socios2.webp\u0026quot; Figura 7: Distribuição da quantidade de sócios, comparando Limitada e SA. É isso pessoal. Happy coding ;)\nO código para gerar os gráficos segue abaixo:\nlibrary(formattable) library(tidyverse) library(lubridate) library(sf) # # precisei selecionar as colunas para não estourar a memória do # # meu computador # empresa \u0026lt;- vroom::vroom( # \u0026quot;empresa.csv.gz\u0026quot;, # col_select = c( # cnpj, identificador_matriz_filial, # razao_social, nome_fantasia, # situacao_cadastral, # codigo_natureza_juridica, data_inicio_atividade, # cnae_fiscal, descricao_tipo_logradouro, logradouro, # numero, complemento, # bairro, cep, uf, codigo_municipio, municipio, # qualificacao_do_responsavel, capital_social, # porte, opcao_pelo_simples # )) # socios \u0026lt;- vroom::vroom(\u0026quot;socio.csv.gz\u0026quot;, col_types = \u0026quot;cccccccccc\u0026quot;) empresa_sp \u0026lt;- read_rds(\u0026quot;empresa_sp.rds\u0026quot;) %\u0026gt;% filter( # apenas ativos situacao_cadastral == \u0026quot;2\u0026quot;, # apenas filiais identificador_matriz_filial == 1 ) socios_sp \u0026lt;- read_rds(\u0026quot;socios_sp.rds\u0026quot;) # 1. Natureza jurídica: Ltda, SA, outros. ------------------------ # # Apenas uma curiosidade que acabei não usando # empresa_sp %\u0026gt;% # count(porte, sort = TRUE) %\u0026gt;% # mutate(porte = case_when( # porte == \u0026quot;00\u0026quot; ~ \u0026quot;Não informado\u0026quot;, # porte == \u0026quot;01\u0026quot; ~ \u0026quot;Micro empresa\u0026quot;, # porte == \u0026quot;03\u0026quot; ~ \u0026quot;Empresa de Pequeno Porte\u0026quot;, # porte == \u0026quot;05\u0026quot; ~ \u0026quot;Demais\u0026quot; # )) %\u0026gt;% # mutate(prop = percent(n/sum(n))) natureza \u0026lt;- qsacnpj::tab_natureza_juridica %\u0026gt;% mutate(codigo_natureza_juridica = as.numeric(cod_subclass_natureza_juridica)) natureza_sp \u0026lt;- empresa_sp %\u0026gt;% left_join(natureza, \u0026quot;codigo_natureza_juridica\u0026quot;) %\u0026gt;% replace_na(list(nm_natureza_juridica = \u0026quot;Vazio\u0026quot;, nm_subclass_natureza_juridica = \u0026quot;Vazio\u0026quot;)) %\u0026gt;% mutate( tipo = if_else( nm_natureza_juridica == \u0026quot;Administração Pública\u0026quot;, \u0026quot;Administração Pública\u0026quot;, nm_subclass_natureza_juridica ), tipo = fct_lump(tipo, 10, other_level = \u0026quot;Outros\u0026quot;) ) %\u0026gt;% count(tipo, sort = TRUE) %\u0026gt;% mutate(prop = percent(n/sum(n))) p_natureza \u0026lt;- natureza_sp %\u0026gt;% mutate(tipo = fct_reorder(str_wrap(tipo, 40), n)) %\u0026gt;% ggplot(aes(x = tipo, y = n / 1e3)) + geom_col(fill = vistrnv::trnv_colors()[1], alpha = .9) + coord_flip() + geom_text(aes(label = prop), nudge_y = 150) + theme_minimal(14) + labs(x = \u0026quot;Natureza Jurídica\u0026quot;, y = \u0026quot;Quantidade de empresas (milhares)\u0026quot;) # 2. Data da abertura. ------------------------------------------- # li a base novamente sem o filtro de ativos # é ineficiente, mas eu estava com preguiça d_abertura \u0026lt;- read_rds(\u0026quot;empresa_sp.rds\u0026quot;) %\u0026gt;% filter(identificador_matriz_filial == 1) %\u0026gt;% mutate( mes_atividade = floor_date(data_inicio_atividade, \u0026quot;quarter\u0026quot;), ano_atividade = floor_date(data_inicio_atividade, \u0026quot;year\u0026quot;) ) %\u0026gt;% filter(mes_atividade \u0026gt; \u0026quot;1950-01-01\u0026quot;, mes_atividade \u0026lt; \u0026quot;2019-06-01\u0026quot;) %\u0026gt;% count(ano_atividade) p_abertura \u0026lt;- d_abertura %\u0026gt;% mutate(mes_atividade = ano_atividade) %\u0026gt;% filter(year(mes_atividade) \u0026gt; 1950, year(mes_atividade) \u0026lt; 2019) %\u0026gt;% ggplot(aes(x = mes_atividade, y = n/1000)) + geom_line(size = 1) + theme_minimal(14) + scale_x_date(breaks = scales::date_breaks(\u0026quot;3 year\u0026quot;), labels = scales::date_format(\u0026quot;%Y\u0026quot;)) + scale_y_continuous(limits = c(0, 800)) + labs(x = \u0026quot;Ano de início das atividades\u0026quot;, y = \u0026quot;Quantidade de empresas (milhares)\u0026quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1), panel.grid.minor.x = element_blank()) # 3. Atividade econômica principal. ------------------------------ tab_cnae \u0026lt;- empresa_sp %\u0026gt;% mutate(cod_cnae = str_pad(cnae_fiscal, 7, \u0026quot;left\u0026quot;, \u0026quot;0\u0026quot;)) %\u0026gt;% left_join(qsacnpj::tab_cnae, \u0026quot;cod_cnae\u0026quot;) %\u0026gt;% replace_na(list(nm_secao = \u0026quot;Vazio\u0026quot;, nm_divisao = \u0026quot;Vazio\u0026quot;, nm_grupo = \u0026quot;Vazio\u0026quot;, nm_classe = \u0026quot;Vazio\u0026quot;, nm_cnae = \u0026quot;Vazio\u0026quot;)) %\u0026gt;% count(nm_secao, nm_divisao, nm_grupo, nm_classe, nm_cnae) d_cnae \u0026lt;- empresa_sp %\u0026gt;% mutate(cod_cnae = str_pad(cnae_fiscal, 7, \u0026quot;left\u0026quot;, \u0026quot;0\u0026quot;)) %\u0026gt;% left_join(qsacnpj::tab_cnae, \u0026quot;cod_cnae\u0026quot;) %\u0026gt;% replace_na(list(nm_secao = \u0026quot;Vazio\u0026quot;, nm_divisao = \u0026quot;Vazio\u0026quot;, nm_grupo = \u0026quot;Vazio\u0026quot;, nm_classe = \u0026quot;Vazio\u0026quot;, nm_cnae = \u0026quot;Vazio\u0026quot;)) # montei uma função pois estava copiando muito código p_cnae \u0026lt;- function(d_cnae, quebra, lump = 14) { d_cnae %\u0026gt;% mutate(tipo = fct_lump({{quebra}}, lump)) %\u0026gt;% count(tipo) %\u0026gt;% mutate(tipo = fct_reorder(str_wrap(tipo, 40), n)) %\u0026gt;% mutate(prop = percent(n/sum(n))) %\u0026gt;% ggplot(aes(x = tipo, y = n / 1e3)) + geom_col(fill = vistrnv::trnv_colors()[1], alpha = .9) + coord_flip() + geom_text(aes(label = prop), nudge_y = 100) + theme_minimal(12) + labs(x = \u0026quot;Atividade principal\u0026quot;, y = \u0026quot;Quantidade de empresas (milhares)\u0026quot;) } graficos_cnae \u0026lt;- list( secao = d_cnae %\u0026gt;% p_cnae(nm_secao), divisao = d_cnae %\u0026gt;% p_cnae(nm_divisao), grupo = d_cnae %\u0026gt;% p_cnae(nm_grupo), classe = d_cnae %\u0026gt;% p_cnae(nm_classe), cnae = d_cnae %\u0026gt;% p_cnae(nm_cnae) ) # acabei usando só esse p_cnae \u0026lt;- graficos_cnae$secao # 4. Município da sede. ------------------------------------------- d_muni \u0026lt;- brazilmaps::get_brmap( \u0026quot;City\u0026quot;, geo.filter = list(State = 35) ) %\u0026gt;% mutate(nome = abjutils::rm_accent(nome)) p_muni \u0026lt;- empresa_sp %\u0026gt;% count(municipio) %\u0026gt;% mutate(nome = case_when( # precisei arrumar alguns nomes na mão, pois não consegui # dar match pelo código do município municipio == \u0026quot;BIRITIBA-MIRIM\u0026quot; ~ \u0026quot;BIRITIBA MIRIM\u0026quot;, municipio == \u0026quot;MOGI-GUACU\u0026quot; ~ \u0026quot;MOGI GUACU\u0026quot;, TRUE ~ municipio )) %\u0026gt;% inner_join(d_muni, \u0026quot;nome\u0026quot;) %\u0026gt;% mutate(ncat = cut(n/1000, ceiling(c(0, 1e3, 2e3, 1e4, 1e5, max(n))/1000), dig.lab = 10)) %\u0026gt;% st_as_sf() %\u0026gt;% ggplot(aes(fill = ncat)) + geom_sf(colour = \u0026quot;black\u0026quot;, size = .3) + scale_fill_viridis_d(option = \u0026quot;A\u0026quot;, begin = 0.1, alpha = .9) + theme_void(14) + theme(legend.position = c(.85,.8)) + labs(fill = \u0026quot;Quantidade de\\nempresas (milhares)\u0026quot;) # 5. Quantidade de sócios. ---------------------------------------- qtd_socios \u0026lt;- socios_sp %\u0026gt;% count(cnpj) empresa_natureza \u0026lt;- empresa_sp %\u0026gt;% left_join(natureza, \u0026quot;codigo_natureza_juridica\u0026quot;) %\u0026gt;% replace_na(list(nm_natureza_juridica = \u0026quot;Vazio\u0026quot;, nm_subclass_natureza_juridica = \u0026quot;Vazio\u0026quot;)) %\u0026gt;% mutate( tipo = if_else( nm_natureza_juridica == \u0026quot;Administração Pública\u0026quot;, \u0026quot;Administração Pública\u0026quot;, nm_subclass_natureza_juridica ), tipo = fct_lump(tipo, 10, other_level = \u0026quot;Outros\u0026quot;) ) p_qtd_socios1 \u0026lt;- qtd_socios %\u0026gt;% mutate(n = fct_lump(str_pad(n, 2, \u0026quot;left\u0026quot;, 0), 10, other_level = \u0026quot;11 ou mais\u0026quot;)) %\u0026gt;% count(n) %\u0026gt;% mutate(prop = percent(nn/sum(nn)), n = fct_rev(n)) %\u0026gt;% ggplot(aes(x = n, y = nn / 1e3)) + geom_col(fill = vistrnv::trnv_colors()[1], alpha = .9) + coord_flip() + geom_text(aes(label = prop), nudge_y = 30) + theme_minimal(14) + labs(x = \u0026quot;Quantidade de sócios\u0026quot;, y = \u0026quot;Quantidade de empresas (milhares)\u0026quot;) d_plot_qtd_socios \u0026lt;- qtd_socios %\u0026gt;% inner_join(select(empresa_natureza, cnpj, tipo), \u0026quot;cnpj\u0026quot;) %\u0026gt;% filter(str_detect(tipo, \u0026quot;Limitada|Anôn\u0026quot;)) %\u0026gt;% mutate(tipo = if_else(str_detect(tipo, \u0026quot;Limitada\u0026quot;), \u0026quot;Limitada\u0026quot;, \u0026quot;SA\u0026quot;)) %\u0026gt;% mutate(n = fct_lump(str_pad(n, 2, \u0026quot;left\u0026quot;, 0), 10, other_level = \u0026quot;11 ou mais\u0026quot;)) %\u0026gt;% count(tipo, n) %\u0026gt;% group_by(tipo) %\u0026gt;% mutate(prop = nn/sum(nn), n = fct_rev(n)) %\u0026gt;% ungroup() %\u0026gt;% mutate(prop = percent(prop, 1)) p_qtd_socios2 \u0026lt;- d_plot_qtd_socios %\u0026gt;% ggplot(aes(x = n, y = prop, fill = tipo)) + geom_col(position = \u0026quot;dodge\u0026quot;) + coord_flip() + scale_fill_viridis_d(begin = 0.3, end = 0.8) + theme_minimal(14) + scale_y_continuous(labels = scales::percent) + labs(x = \u0026quot;Quantidade de sócios\u0026quot;, y = \u0026quot;Proporção das empresas\u0026quot;, fill = \u0026quot;Tipo\u0026quot;) + guides(fill = guide_legend(reverse = TRUE)) + theme(legend.position = c(.8, .2)) # 6. Capital social. ----------------------------------------------- p_capital \u0026lt;- empresa_sp %\u0026gt;% mutate(capital_social_cat = cut( capital_social/1e3, breaks = c(0, 1e3, 1e4, 1e5, 1e6, Inf)/1e3, include.lowest = TRUE, dig.lab = 10) ) %\u0026gt;% count(tipo = capital_social_cat) %\u0026gt;% mutate(prop = percent(n/sum(n)), tipo = fct_rev(tipo)) %\u0026gt;% ggplot(aes(x = tipo, y = n / 1e3)) + geom_col(fill = vistrnv::trnv_colors()[1], alpha = .9) + coord_flip() + geom_text(aes(label = prop), nudge_y = 100) + theme_minimal(14) + labs(x = \u0026quot;Capital social (milhares de reais)\u0026quot;, y = \u0026quot;Quantidade de empresas (milhares)\u0026quot;) # FIM ------------------------------------------------------------- todos_graficos \u0026lt;- list( natureza = p_natureza, abertura = p_abertura, atividade = p_cnae, mapa = p_muni, qtd_socios1 = p_qtd_socios1, qtd_socios2 = p_qtd_socios2, capital = p_capital ) fs::dir_create(\u0026quot;graficos\u0026quot;) iwalk(todos_graficos, ~ggsave(paste0(.y, \u0026quot;.png\u0026quot;), .x, path = \u0026quot;graficos\u0026quot;)) ","permalink":"https://blog.curso-r.com/posts/2019-09-20-qsacnpj/","tags":["rfb","banco de dados"],"title":"Explorando a base de dados de CNPJ da Receita Federal"},{"author":["Daniel"],"categories":["análises"],"contents":" A partir do vestibular de 2019 a FUVEST alterou o seu sistema cotas. Antes de 2019, o sistema beneficiava com uma bonificação os candidatos que cursaram o ensino médio na rede pública de ensino e passou a contar com reserva de vagas para os estudantes cotistas.\nPara cada curso as vagas são divididas da seguinte forma:\n60% reservadas para ampla concorrência 40% reservadas para candidatos cotistas das 40% reservadas p/ cotistas, 37,5% são reservadas para os candidatos de escolas públicas autodeclarados pretos, pardos e indígenas. Segundo a resolução do Conselho Universitário (CoG) ainda:\nO candidato concorrerá apenas com os candidatos que tenham optado pela mesma modalidade de concorrência no Concurso Vestibular FUVEST 2019.\nÉ aí que aparece o problema. Algumas carreiras da FUVEST já possuiam historicamente proporção maior de 40% de alunos oriundos de escola pública. Esse ponto foi levantado em janeiro de 2019 pela ADUSP e mesmo em 2018 pela imprensa.\nVejamos os dados.\nPara 37% das carreiras na primeira fase da FUVEST a nota de corte para ser convocado para a segunda fase para candidatos cotistas foi maior do que a nota para candidatos que se candidataram em ampla concorrência. Consideramos nas análises apenas as modalidades que tiveram todas as vagas preenchidas - isto é, foram convocados para a segunda fase mais candidatos do que o número de vagas disponíveis.\nO gráfico a seguir mostra a diferença entre a nota de corte para candidatos cotistas de escola pública (EP) e candidatos da ampla concorrência (AC).\nVeja que para o curso de Filosofia, a nota de corte para alunos cotistas de escolas públicas foi 11 pontos maior do que a nota para alunos da ampla concorrência.\nQuando comparamos candidatos da modalidade PPI (estudaram na rede pública e se autodeclaram pretos, pardos ou indígenas) com candidatos da modalidade EP, a situação se repete.\nAs notas de corte para candidatos PPI dos cursos de Saúde Pública e de Ciências dos alimentos foram 4 pontos superiores às notas dos cotistas do ensino público.\nAs tabelas a seguir mostram os cursos que apresentaram maiores diferenças:\n# vagas # inscritos # convocados 2ª fase Pontuação Mínima Máxima Filosofia AC 102 431 331 27 73 EP 17 179 72 38 60 PPI 0 0 0 NA NA Pedagogia AC 108 469 377 27 64 EP 18 215 77 35 67 PPI 0 0 0 NA NA Ciências Físicas e Biomoleculares − São Carlos AC 24 62 57 27 75 EP 2 15 8 35 51 PPI 2 5 5 27 47 Geologia AC 30 144 122 29 65 EP 5 49 20 37 57 PPI 0 0 0 NA NA Informática Biomédica − Ribeirão Preto AC 12 49 44 27 67 EP 3 17 12 33 45 PPI 1 3 2 27 48 Química − Licenciatura − Ribeirão Preto AC 20 68 57 27 66 EP 4 17 12 33 52 PPI 4 4 2 33 35 Licenciatura em Educomunicação AC 16 64 55 27 61 EP 3 20 13 32 43 PPI 2 4 2 33 40 Pedagogia − Ribeirão Preto AC 30 100 75 27 65 EP 2 35 12 32 40 PPI 3 25 12 27 45 Licenciatura em Ciências Exatas − São Carlos AC 30 106 88 27 61 EP 2 17 8 32 50 PPI 3 7 3 33 37 Sistemas de Informação − São Carlos AC 25 173 103 34 67 EP 6 79 25 39 56 PPI 4 23 12 28 54 A relação candidato vaga também aumenta quando comparamos candidatos de cada modalidade do vestibular. Por exemplo, o gráfico abaixo mostra a diferença percentual entre a relação candidato/vaga para da ampla concorrência e cotistas de escola pública. Note que em alguns casos a relação aumentou mais de 400%.\nEm suma, este artigo tem por objetivo apresentar dados que contestam a eficácia do sistema de cotas da fuvest. É notável que para a maior parte dos cursos o sistema de cotas teve efeito benéfico, no entanto, não faz sentido melhorar de um lado e piorar de outro. Além disso, a decisão de se candidatar pelo sistema de cotas ou não é uma decisão difícil para o candidato - que no momento da inscrição não possui dados tal como a relação candidato vaga para cada uma das modalidades.\nTodo o código para replicar os gráficos e análises deste artigo estão diosponíveis aqui.\n","permalink":"https://blog.curso-r.com/posts/2019-11-27-fuvest/","tags":["pdf","banco de dados"],"title":"A (não) eficácia do sistema de cotas na FUVEST"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" O pacote {arrow} é uma atualização importante para o mundo da ciência de dados. Essa tecnologia possibilitará uma integração ainda mais suave entre as principais linguagens de programação voltadas à ciência de dados, como R/Python, bem como as diversas ferramentas que utilizamos na parte de engenharia de dados, como Spark, por exemplo.\nCada vez mais, o que percebo é que a ciência de dados está conseguindo atingir uma meta que parece contraditória: unificar as ferramentas e, ao mesmo tempo, dar liberdade ao profissional. Isso acontece pois a parte que se quer libertar é a do pensamento crítico, a modelagem e o problema de negócio, e o que se quer padronizar é o armazenamento, o deploy e a escalabilidade.\nO que é o Arrow? Segundo o site da Apache:\nApache Arrow is a development platform for in-memory analytics. It contains a set of technologies that enable big data systems to process and move data fast. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware.\nA instalação do {arrow} ainda é um pouco complicada e não convencional. Para usuários linux, como eu, é necessário baixar o código fonte do github e mandar compilar na mão. O tutorial para isso é relativamente simples de seguir, mas acredito que ainda pode melhorar.\nUma simples utilização do {arrow} é salvando e lendo objetos em extensões que podem ser utilizadas em diferentes linguagens. Um exemplo de extensão parquet, que pode ser lida em diversas linguagens, carregada pelo Google BigQuery e trabalhada em projetos envolvendo Spark.\nEstamos caminhando para um futuro unificado! Pretendo escrever mais sobre o arrow no futuro.\nÉ isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2019-08-30-arrow/","tags":["arrow","dados"],"title":"O pacote arrow"},{"author":["Julio"],"categories":["tutoriais"],"contents":" Um oráculo é uma função que diz para se uma predição está correta ou incorreta. Ou seja, ela diz a verdade sobre a variável resposta de um novo caso da sua base no qual você só observa as explicativas.\nO oráculo nem sempre existe em problemas reais, pois a verdade não costuma estar disponível no momento em que fazemos a predição. Por exemplo, se estamos predizendo as vendas do próximo mês, só saberemos a taxa de acerto com certeza no próximo mês. Antes disso, só podemos fazer estimativas.\nEntão o oráculo é como se fosse minha base de testes? Bem, sim, mas é um tipo muito especial de base de teste. No contexto de captchas, nós podemos gerar infinitas observações novas da base, criando uma base de dados de treino virtualmente infinita.\nO problema é que a resposta do oráculo não é sempre clara: usualmente, o oráculo não diz o quê você errou, mas somente se você errou. Ou seja, é uma informação incompleta, censurada.\nE como nós fazemos para tratar informação incompleta nos nossos estudos? Botamos na verossimilhança! Essa é uma possível ideia para atingir nosso objetivo final: criar modelos que aprendem a resolver captchas automaticamente.\nNa prática, no entanto, isso é uma tarefa difícil! Nesse post vou me ater em mostrar um toy model de oráculo e, nos próximos posts sobre o tema, vou desenvolver mais a parte estatística.\nConstruindo um oráculo Um oráculo precisa ter três partes implementadas: i) obtenção de uma imagem nova, ii) teste do modelo e iii) retorno, que deve envolver a matriz de entrada, o valor predito e a resposta (completa ou incompleta).\nVamos então ao código:\n#\u0026#39; Oráculo do MNIST #\u0026#39; #\u0026#39; @param model modelo que resolve o captcha do MNIST. Por padrão #\u0026#39; é \u0026quot;ask\u0026quot;, significando que ele vai perguntar para o usuário. #\u0026#39; @param type se for 0, fala só se acertou. Se for 1, #\u0026#39; informa quais letras acertou e quais errou. Se for 2, #\u0026#39; informa o gabarito. #\u0026#39; oracle_mnist \u0026lt;- function(model = \u0026quot;ask\u0026quot;, type = 0) { mnist \u0026lt;- keras::dataset_mnist() # gerando um captcha N \u0026lt;- nrow(mnist$train$x) sample_nums \u0026lt;- sample(N, 5) sample_list \u0026lt;- purrr::map(sample_nums, ~mnist$train$x[.x,,]) X \u0026lt;- abind::abind(sample_list, along = 2) Y \u0026lt;- as.numeric(mnist$train$y[sample_nums]) # teste do modelo if (is.character(model)[1] \u0026amp;\u0026amp; model == \u0026quot;ask\u0026quot;) { par(mar=rep(0,4)) plot(as.raster(X/256)) ans \u0026lt;- readline(\u0026quot;Answer: \u0026quot;) ans \u0026lt;- as.numeric(unlist(strsplit(ans, \u0026quot;\u0026quot;))) } else { pred \u0026lt;- predict(model, tensorflow::array_reshape(X, c(1, dim(X), 1))) ans \u0026lt;- apply(pred[1,,], 1, which.max) - 1 } # retorno res \u0026lt;- list( predito = ans, acertou = all(ans == Y), quais = if (type \u0026gt; 0) (ans == Y), gabarito = if (type \u0026gt; 1) Y ) list(x = X, res = res) } Vamos testar! Esse é um caso que acertei\nresult \u0026lt;- oracle_mnist(type = 0) Answer: 87685 result$res $predito [1] 8 7 6 8 5 $acertou [1] TRUE $quais NULL $gabarito NULL E esse é um caso que errei, e mandei mostrar todos os outputs possíveis:\nresult \u0026lt;- oracle_mnist(type = 2) Answer: 56198 result$res $predito [1] 5 6 1 9 8 $acertou [1] FALSE $quais [1] TRUE TRUE TRUE TRUE FALSE $gabarito [1] 5 6 1 9 0 Em seguida, mostro como podemos ajustar um modelo preditivo e como utilizar o oráculo com esse modelo. Essa é a operação que utilizaremos para aprimorar nosso modelo no futuro.\nDados Os dados que vou utilizar para esse exemplo vêm da base MNIST, colando horizontalmente os caracteres de cinco em cinco para formar os captchas.\nlibrary(keras) mnist \u0026lt;- dataset_mnist() # cola as imagens lado a lado montar_x \u0026lt;- function(x) { pos_x \u0026lt;- (seq_len(nrow(x))-1) %% 5 + 1 xizes \u0026lt;- purrr::map(1:5, ~x[pos_x == .x,,]/256) X \u0026lt;- abind::abind(xizes, along = 3) tensorflow::array_reshape(X, c(dim(X), 1)) } # monta a variável resposta usando # decryptr:::resize_answer() # a variável resposta de um captcha # é uma matriz nesse formato # (lab = label, pos = posição) # # lab|0 1 2 3 4 5 6 7 8 9 # pos|------------------- # 1 |0 0 0 0 0 1 0 0 0 0 # 2 |1 0 0 0 0 0 0 0 0 0 # 3 |0 0 0 0 1 0 0 0 0 0 # 4 |0 1 0 0 0 0 0 0 0 0 # 5 |0 0 0 0 0 0 0 0 0 1 montar_y \u0026lt;- function(y) { pos_y \u0026lt;- seq(0, ceiling(length(y)/5) - 1) y \u0026lt;- as.numeric(y) yizes \u0026lt;- purrr::map( .x = pos_y, .f = ~decryptr:::resize_answer(y[.x * 5 + 1:5], 0:9) ) abind::abind(yizes, along = 0) } # montando os dados numa lista data \u0026lt;- list( train = list( x = montar_x(mnist$train$x), y = montar_y(mnist$train$y) ), test = list( x = montar_x(mnist$test$x), y = montar_y(mnist$test$y) ) ) Modelo Meu modelo é uma rede neural convolucional simples, igual ao que está implementado no decryptr. Essa rede foi treinada com as 12 mil observações de treino montadas anteriormente. O modelo possui uma taxa de acerto de 92% para o captcha completo, ou seja, para todas as 5 letras.\n# meu input, começando com as dimensões de X input \u0026lt;- layer_input(shape = dim(data$train$x)[-1]) # uma camada convolucional customizada, # já com o max pooling my_conv \u0026lt;- function(obj, f, k) { obj %\u0026gt;% layer_conv_2d( filters = f, kernel_size = c(k,k), padding = \u0026quot;same\u0026quot;, activation = \u0026quot;relu\u0026quot; ) %\u0026gt;% layer_max_pooling_2d() } # dimensões das bases dims \u0026lt;- list( x = dim(data$train$x)[-1], y = dim(data$train$y)[-1] ) # rede completa output \u0026lt;- input %\u0026gt;% my_conv(16, 5) %\u0026gt;% my_conv(32, 3) %\u0026gt;% my_conv(64, 3) %\u0026gt;% layer_flatten() %\u0026gt;% layer_dense(units = 64) %\u0026gt;% layer_dropout(.1) %\u0026gt;% # fixar o número de parâmetros # para o output do modelo layer_dense(units = prod(dims$y)) %\u0026gt;% layer_reshape(target_shape = dims$y) %\u0026gt;% layer_activation(\u0026quot;softmax\u0026quot;) # definição do modelo model \u0026lt;- keras_model(input, output) # métrica customizada para o captcha metric_captcha \u0026lt;- custom_metric( name = \u0026quot;captcha\u0026quot;, metric_fn = function(y_true, y_pred) { k_equal(k_argmax(y_true), k_argmax(y_pred)) %\u0026gt;% k_all(axis = -1) %\u0026gt;% k_cast(k_floatx()) }) # adicionando regras de otimização, # função de perda e métrica a acompanhar model %\u0026gt;% compile( optimizer = optimizer_adam(), loss = loss_categorical_crossentropy, metrics = metric_captcha ) # ajustando modelo model %\u0026gt;% fit( x = data$train$x, y = data$train$y, validation_data = list(data$test$x, data$test$y), batch_size = 64, epochs = 10 ) A estrutura do modelo ajustado é dada abaixo:\n__________________________________________________________ Layer (type) Output Shape Param # ========================================================== input_1 (InputLayer) (None, 28, 140, 1) 0 __________________________________________________________ conv2d (Conv2D) (None, 28, 140, 16) 416 __________________________________________________________ max_pooling2d (MaxPooling (None, 14, 70, 16) 0 __________________________________________________________ conv2d_1 (Conv2D) (None, 14, 70, 32) 4640 __________________________________________________________ max_pooling2d_1 (MaxPooli (None, 7, 35, 32) 0 __________________________________________________________ conv2d_2 (Conv2D) (None, 7, 35, 64) 18496 __________________________________________________________ max_pooling2d_2 (MaxPooli (None, 3, 17, 64) 0 __________________________________________________________ flatten (Flatten) (None, 3264) 0 __________________________________________________________ dense (Dense) (None, 64) 208960 __________________________________________________________ dropout (Dropout) (None, 64) 0 __________________________________________________________ dense_1 (Dense) (None, 50) 3250 __________________________________________________________ reshape (Reshape) (None, 5, 10) 0 __________________________________________________________ activation (Activation) (None, 5, 10) 0 ========================================================== Total params: 235,762 Trainable params: 235,762 Non-trainable params: 0 __________________________________________________________ Agora, podemos utilizar o oráculo com o modelo:\nresult \u0026lt;- oracle_mnist(model, 0) result$res $predito [1] 8 6 6 0 1 $acertou [1] TRUE $quais NULL $gabarito NULL set.seed(13) result \u0026lt;- oracle_mnist(model, 2) result$res $predito [1] 7 2 2 7 7 $acertou [1] FALSE $quais [1] FALSE TRUE TRUE TRUE TRUE $gabarito [1] 1 2 2 7 7 Veja qual foi o caso que o modelo errou:\nNão parece tão fácil de ver que é um 7, né?\nPacote mimir Estou montando um pacote chamado {mimir} para construir oráculos e trabalhar com eles. O nome Mimir vem da figura da mitologia nórdica, conhecida por ser muito sábia. Eu achei o nome legal pois i) mimi lembra gatos, o que interage bem com as brincadeiras purrr e forcats e ii) por ter um R no final, que é sempre legal em pacotes! O pacote ainda não está disponível, mas assim que estiver eu aviso nas redes.\nNo próximo post do tema, vou apresentar algumas formas de aproveitar a informação do oráculo na verossimilhança e alguns testes que venho desenvolvendo.\nÉ isso pessoal. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2019-09-20-captcha-oraculo/","tags":["captcha"],"title":"Quebrando Captchas - Parte VII: Oráculos"},{"author":["Julio"],"categories":["análises"],"contents":" O que faz o R ser tão legal é a comunidade que está por trás dela. Existem várias formas de acessar essa comunidade: pelo stackoverflow, twitter, facebook, entre outras. Nesse post vou falar um pouquinho sobre a comunidade RBrasil no telegram, usando dados!\nA comunidade @rbrasiloficial foi criada em 2015 pelo Felipe Barros e já tem mais de mil inscritos. O canal é utilizado para tirar dúvidas, discutir sobre o futuro do R, divulgar trabalhos e dar pitacos sobre as coisas que estão acontecendo.\nNesse post vou mostrar como importar e arrumar os dados, e algumas visualizações da base de conversas do Telegram.\nImportando os dados O Telegram já possui uma ferramenta para exportar os chats. Basta clicar nas opções do chat e mandar exportar. Então vou assumir que esses dados já estão em mãos.\nOs dados estão disponíveis no formato HTML. Para ler um arquivo, montei o seguinte código:\nlibrary(magrittr) ler_html_telegram \u0026lt;- function(html_file) { # pega todas as mensagens divs \u0026lt;- xml2::read_html(html_file) %\u0026gt;% xml2::xml_find_all(\u0026quot;//div[@class=\u0026#39;message default clearfix\u0026#39;]\u0026quot;) # nome da pessoa nomes \u0026lt;- divs %\u0026gt;% xml2::xml_find_all(\u0026quot;./div/div[@class=\u0026#39;from_name\u0026#39;]\u0026quot;) %\u0026gt;% xml2::xml_text() %\u0026gt;% stringr::str_squish() # data e hora da mensagem data_horas \u0026lt;- divs %\u0026gt;% xml2::xml_find_all(\u0026quot;./div/div[@class=\u0026#39;pull_right date details\u0026#39;]\u0026quot;) %\u0026gt;% xml2::xml_attr(\u0026quot;title\u0026quot;) %\u0026gt;% lubridate::dmy_hms() # texto da mensagem textos \u0026lt;- divs %\u0026gt;% purrr::map(xml2::xml_find_first, \u0026quot;./div/div[@class=\u0026#39;text\u0026#39;]\u0026quot;) %\u0026gt;% purrr::map_chr(xml2::xml_text) %\u0026gt;% stringr::str_squish() # retorna numa tabela tibble::tibble( data_hora = data_horas, nome = nomes, texto = textos ) } Depois, basta listar todos os arquivos e carregar cada arquivo usando purrr::map_dfr():\npath \u0026lt;- \u0026quot;~/Downloads/Telegram Desktop/ChatExport_17_08_2019/\u0026quot; todos_arquivos \u0026lt;- fs::dir_ls(path, regexp = \u0026quot;messages\u0026quot;) d_msg \u0026lt;- purrr::map_dfr( todos_arquivos, ler_html_telegram, .id = \u0026quot;arquivo\u0026quot; ) No final, fiquei com uma base de dados com 26.980 linhas e 4 colunas.\nContas deletadas ganharam no volume de mensagens… Mas acho que não queremos considerá-las pois temos várias pessoas nessa contagem. O grande Marcelo Ventura Freire venceu no número de mensagens! Eu também estou no páreo :)\nd_msg %\u0026gt;% dplyr::count(nome, sort = TRUE) %\u0026gt;% dplyr::mutate(prop = scales::percent(n/sum(n))) %\u0026gt;% head(10) %\u0026gt;% knitr::kable() nome n prop Deleted Account 3101 11.5% Marcelo Ventura Freire 2313 8.6% Charles Lula da Silva 1897 7.0% Leonard de Assis 1677 6.2% Sillas Gonzaga 1540 5.7% Julio Trecenti 1130 4.2% Bruna Wundervald 951 3.5% Andre Mesquita 818 3.0% Fernando Barbalho 696 2.6% George Santiago 611 2.3% E no tempo, como que fica? Parece que no início o volume de mensagens era mais alto, e depois entramos num patamar estável.\nlibrary(ggplot2) d_msg %\u0026gt;% dplyr::mutate(mes = lubridate::floor_date(data_hora, \u0026quot;month\u0026quot;)) %\u0026gt;% dplyr::count(mes) %\u0026gt;% ggplot(aes(x = mes, y = n)) + geom_line() + geom_point() + theme_minimal(16) No gráfico seguinte, peguei as doze pessoas que falaram mais historicamente e avaliei a evolução mensal de mensagens. Temos de tudo: os constantes, os que sumiram, os que ressurgiram das cinzas e os que chegaram para ficar.\nd_msg %\u0026gt;% dplyr::filter(nome != \u0026quot;Deleted Account\u0026quot;) %\u0026gt;% dplyr::mutate(nome = forcats::fct_lump(nome, 12), nome = as.character(nome), mes = lubridate::floor_date(data_hora, \u0026quot;month\u0026quot;)) %\u0026gt;% dplyr::filter(nome != \u0026quot;Other\u0026quot;) %\u0026gt;% dplyr::count(mes, nome, sort = TRUE) %\u0026gt;% tidyr::complete(mes, nome, fill = list(n = 0)) %\u0026gt;% ggplot(aes(x = mes, y = n)) + geom_line() + facet_wrap(~nome) + labs(x = \u0026quot;Mês\u0026quot;, y = \u0026quot;Quantidade de mensagens\u0026quot;) + theme_bw() E qual é o horário que as pessoas interagem mais? Parece que é às 16 horas, aquele horário que a pessoa está ferrada no trabalho e precisa pedir uma ajuda aos amigos…\nd_msg %\u0026gt;% dplyr::mutate(hora = factor(lubridate::hour(data_hora))) %\u0026gt;% ggplot(aes(x = hora)) + geom_bar(fill = \u0026quot;royalblue\u0026quot;) + theme_minimal(14) + ggtitle(\u0026quot;Hora das mensagens\u0026quot;) E o dia da semana? Terça-feira wins!\nd_msg %\u0026gt;% dplyr::mutate(wd = lubridate::wday(data_hora, label = TRUE)) %\u0026gt;% ggplot(aes(x = wd)) + geom_bar(fill = \u0026quot;pink2\u0026quot;) + theme_minimal(14) + ggtitle(\u0026quot;Dia da semana das mensagens\u0026quot;) E claro, não poderia faltar um wordcloud…\n# dá pra criar funções anônimas assim ;) # esse é um limpador bem safado que fiz em 1 min limpar \u0026lt;- . %\u0026gt;% abjutils::rm_accent() %\u0026gt;% stringr::str_to_title() %\u0026gt;% stringr::str_remove_all(\u0026quot;[^a-zA-Z0-9 ]\u0026quot;) %\u0026gt;% stringr::str_squish() # tirar palavras que nao quero banned \u0026lt;- tidytext::get_stopwords(\u0026quot;pt\u0026quot;) %\u0026gt;% dplyr::mutate(palavra = limpar(word)) cores \u0026lt;- viridis::viridis(10, begin = 0, end = 0.8) d_msg %\u0026gt;% tidytext::unnest_tokens(palavra, texto) %\u0026gt;% dplyr::mutate(palavra = limpar(palavra)) %\u0026gt;% dplyr::anti_join(banned, \u0026quot;palavra\u0026quot;) %\u0026gt;% dplyr::count(palavra, sort = TRUE) %\u0026gt;% with(wordcloud::wordcloud( palavra, n, scale = c(5, .1), min.freq = 80, random.order = FALSE, colors = cores )) Fiz poucos gráficos para te deixar com vontade de brincar mais. Gostou da brincadeira? Então faça suas próprias análises do seu chat preferido do Telegram!\nÉ isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2019-09-10-rbrasil/","tags":["comunidade"],"title":"Comunidade R Brasil no Telegram"},{"author":["Julio"],"categories":["análises"],"contents":" Uma forma poderosa de participar socialmente é utilizando dados abertos. Dados abertos tornam a sociedade mais democrática pois com eles podemos verificar afirmações baseadas em estatísticas e também buscar novas narrativas para os fenômenos estudados.\nNesse post, mostramos um exemplo de como isso pode ser feito, utilizando os dados da Sistema Nacional de Informações de Segurança pública, o SINESP. O objetivo com esse texto é empoderar as pessoas que trabalham com ciência de dados e mostrar como podemos participar socialmente usando o R.\nNo dia 16/julho o Ministro da Justiça, Sérgio Moro, fez a seguinte declaração no Twitter:\nAbaixo notícia de verdade. Dados do Sinesp, estes, sim, oficiais, apontam queda no primeiro trimestre de 2019 do número dos principais crimes em todo o país em comparação com o mesmo período de 2018. Vamos trabalhar para aprofundar essa queda e levar segurança a cada cidadão. pic.twitter.com/srnJZF4X6K — Sergio Moro (@SF_Moro) July 16, 2019 O legal de existirem dados abertos é que podemos checar esses informações e criar análises mais aprofundadas sobre o tema. Infelizmente, ainda não foram disponibilizados os dados do primeiro bimestre de 2019, que fazem parte do Tweet do Moro.\nMas isso não é um problema, pois podemos fazer diversos cruzamentos com os dados que estão disponíveis. Nesta rápida análise, mostro como baixar os dados a partir do portal de dados abertos https://dados.gov.br. Para isso, utilizarei o tidyverse, httr e xml2.\nlibrary(tidyverse) Parte 1: Download Como o dados.gov.br é um portal muito bem feito, é possível baixar todos os dados em CSV com um pipeline simples:\n# caminho do diretório path_sinesp \u0026lt;- \u0026quot;~/data-raw/sinesp/\u0026quot; # cria o diretório fs::dir_create(path_sinesp) # link do SINESP no dados.gov.br u_sinesp \u0026lt;- paste0(\u0026quot;http://dados.gov.br/dataset\u0026quot;, \u0026quot;/sistema-nacional-de-estatisticas-de-seguranca-publica\u0026quot;) u_sinesp %\u0026gt;% # carrega a página httr::GET() %\u0026gt;% # parseia o código fonte da página xml2::read_html() %\u0026gt;% # identifica todas as tags que contêm os links xml2::xml_find_all(\u0026quot;//ul[@class=\u0026#39;dropdown-menu\u0026#39;]//a\u0026quot;) %\u0026gt;% # extrai a lista todos os links xml2::xml_attr(\u0026quot;href\u0026quot;) %\u0026gt;% # seleciona apenas os arquivos xlsx da lista str_subset(\u0026quot;xlsx\u0026quot;) %\u0026gt;% # essa parte faz o loop de download. Demora ~1 minuto walk(~httr::GET(.x, httr::write_disk(paste0(path_sinesp, basename(.x))))) Nada impede o usuário de baixar esses arquivos CSV manualmente. Pode ser até que seja mais rápido baixar manualmente se for uma vez só. A vantagem desse código é que ele é reprodutível, a menos que o site mude sua estrutura. Ou seja, quando chegarem os dados de 2019, provavelmente poderemos baixar tudo novamente usando o mesmo código.\nParte 2: Importação e arrumação O código abaixo carrega e arruma os dados para análise. Sempre existem inconsistências nos dados e devemos ficar atentos a eles.\nd_sinesp \u0026lt;- path_sinesp %\u0026gt;% # lista os arquivos fs::dir_ls() %\u0026gt;% # lê todos os arquivos xlsx e empilha map_dfr(readxl::read_excel, col_types = \u0026quot;text\u0026quot;, .id = \u0026quot;file\u0026quot;) %\u0026gt;% # define os nomes das colunas set_names(c(\u0026quot;file\u0026quot;, \u0026quot;uf\u0026quot;, \u0026quot;tipo_crime\u0026quot;, \u0026quot;mes_ano\u0026quot;, \u0026quot;valor\u0026quot;)) %\u0026gt;% # transforma os valores para os tipos corretos transmute( uf, # transformando string em data mes_ano = lubridate::dmy(paste0(\u0026quot;01/\u0026quot;, mes_ano)), # arrumando tipo de crime tipo_crime = str_to_title(tipo_crime), valor = as.numeric(valor), # cria uma coluna com o bimestre bimestre = lubridate::floor_date(mes_ano, \u0026quot;bimonth\u0026quot;) ) No caso, o maior problema era um tipo de crime que estava escrito com nomes ligeiramente diferentes. Um stringr::str_to_title() foi suficiente para resolver. A base de dados ficou assim:\nuf mes_ano tipo_crime valor bimestre Acre 2015-01-01 Estupro 6 2015-01-01 Acre 2015-01-01 Furto De Veículo 0 2015-01-01 Acre 2015-01-01 Homicídio Doloso 13 2015-01-01 Acre 2015-01-01 Lesão Corporal Seguida De Morte 0 2015-01-01 Acre 2015-01-01 Roubo A Instituição Financeira 0 2015-01-01 Acre 2015-01-01 Roubo De Carga 0 2015-01-01 Acre 2015-01-01 Roubo De Veículo NA 2015-01-01 Acre 2015-01-01 Roubo Seguido De Morte (Latrocínio) 0 2015-01-01 Acre 2015-01-01 Tentativa De Homicídio 2 2015-01-01 Acre 2015-02-01 Estupro 8 2015-01-01 Parte 3: Transformação Nesse código, meu objetivo foi reproduzir a tabela do Ministro da Justiça, mas adicionando as mudanças ocorridas entre 2015-2016, 2016-2017 e 2017-2018. Não fiz a análise por trimestre, e sim por bimestre. O leitor interessado pode replicar as análises para trimestres.\nresults \u0026lt;- d_sinesp %\u0026gt;% # agrupa por bimestre e sumariza group_by(bimestre, tipo_crime) %\u0026gt;% summarise(valor = sum(valor, na.rm = TRUE)) %\u0026gt;% ungroup() %\u0026gt;% # apenas primeiro bimestre e tirar dados de antes de 2015 filter(lubridate::month(bimestre) == 1, lubridate::year(bimestre) \u0026gt;= 2015) %\u0026gt;% # ordenar e agrupar por tipo de crime arrange(tipo_crime, bimestre) %\u0026gt;% group_by(tipo_crime) %\u0026gt;% # adicionar bimestre anterior mutate(vl_lag = lag(valor)) %\u0026gt;% ungroup() %\u0026gt;% # tirar 2015 filter(!is.na(vl_lag)) %\u0026gt;% # calcula a razão mutate(razao = scales::percent(valor / vl_lag - 1), bim1 = lubridate::year(bimestre)) %\u0026gt;% # seleciona as colunas importantes select(bim1, tipo_crime, razao) %\u0026gt;% # joga os bimestres nas colunas spread(bim1, razao, sep = \u0026quot;_\u0026quot;) knitr::kable(results) tipo_crime bim1_2016 bim1_2017 bim1_2018 Estupro 4.3% 1.2% 11.5% Furto De Veículo 5.5% -7.3% -6.4% Homicídio Doloso 3.3% 6.5% -10.6% Lesão Corporal Seguida De Morte 21.6% 23.0% -7.8% Roubo A Instituição Financeira -4.8% -28.9% -1.4% Roubo De Carga -1.5% 13.1% 11.2% Roubo De Veículo 9.8% 7.0% -5.9% Roubo Seguido De Morte (Latrocínio) 12.5% 13.7% -13.1% Tentativa De Homicídio -7.7% -2.0% -9.9% A tabela mostra que o primeiro bimestre de 2018, se comparado ao de 2017, também apresenta quedas em todos os tipos de crime, com exceção de estupro e roubo de cargas. Além disso, como será possível ver no próximo gráfico, as variações percentuais acima de 20% na tabela ocorrem em crimes com pequeno volume absoluto de ocorrências. Isso é esperado, pois quando o valor absoluto é pequeno, pequenas mudanças podem significar grandes variações percentuais. Por exemplo, se uma contagem vai de 150 para 120 (queda de 30), a queda é de 20%, mas se outra contagem vai de 1000 para 900 (queda de 100), a queda percentual é de 10%\nParte 4: Visualização Nesse gráfico, mostrei a série bimestral de ocorrências por tipo de crime, buscando identificar tendências de subida ou queda.\nd_sinesp %\u0026gt;% # soma por bimestre e tipo de crime group_by(bimestre, tipo_crime) %\u0026gt;% summarise(valor = sum(valor, na.rm = TRUE)) %\u0026gt;% ungroup() %\u0026gt;% # apenas 2015 para frente filter(lubridate::year(bimestre) \u0026gt;= 2015) %\u0026gt;% # monta o grafico ggplot(aes(x = bimestre, y = valor)) + # adiciona linhas geom_line() + # adiciona uma linha vertical par 2018 geom_vline(xintercept = as.Date(\u0026quot;2018-01-01\u0026quot;), colour = \u0026quot;red\u0026quot;, linetype = 2) + # divide o grafico por tipo de crime facet_wrap(~tipo_crime, scales = \u0026quot;free_y\u0026quot;, ncol = 3) + theme_bw() Pelo gráfico, é possível identificar que, em diversos tipos de crime, existe uma tendência de queda após o primeiro bimestre de 2018. Ou seja, não se pode atribuir a queda nas estatísticas de ocorrências ao novo governo de 2019. A queda vertiginosa no volume de estupros é bem curiosa.\nWrap-up Dado aberto é a melhor forma de tornar as informações disponibilizadas publicamente auditáveis. Com o R, é fácil ingerir esses dados para realizar análises simples e complexas. Não se engane: dado aberto não é dado arrumado. O trabalho de faxina de dados é contínuo. É isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2019-09-03-sinesp/","tags":["dados abertos"],"title":"Verificando afirmações com dados abertos"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" Este é o último post na sequência que estou chamando de “shiny em produção”. Já falamos sobre como usar o pacote golem para facilitar o desenvolvimento de shiny apps, sobre como transformar eles em pacotes e sobre como dockerizá-los para obter compatibilidade máxima.\nE é justamente essa compatibilidade que nos permite fazer o deploy desses apps na nuvem (mais especificamente no Google Cloud Platform) em menos de 20 minutos! Como esse tutorial é muito visual, resolvemos fazer um vídeo para que tudo ficasse mais claro:\nSe você quiser acessar o repositório com o conteúdo, deixamos ele aberto a todos. Por fim, segue abaixo o código em Node utilizado para criar as Cloud Functions:\nvar http = require(\u0026#39;http\u0026#39;); var Compute = require(\u0026#39;@google-cloud/compute\u0026#39;); var compute = Compute(); exports.startInstance = function startInstance(req, res) { var zone = compute.zone(\u0026#39;{SUA ZONA}\u0026#39;); var vm = zone.vm(\u0026#39;{SUA VM}\u0026#39;); vm.start(function(err, operation, apiResponse) { console.log(\u0026#39;instance start successfully\u0026#39;); }); res.status(200).send(\u0026#39;Success start instance\u0026#39;); }; { \u0026quot;name\u0026quot;: \u0026quot;sample-http\u0026quot;, \u0026quot;dependencies\u0026quot;: { \u0026quot;@google-cloud/compute\u0026quot;: \u0026quot;0.7.1\u0026quot;}, \u0026quot;version\u0026quot;: \u0026quot;0.0.1\u0026quot; } var http = require(\u0026#39;http\u0026#39;); var Compute = require(\u0026#39;@google-cloud/compute\u0026#39;); var compute = Compute(); exports.stopInstance = function stopInstance(req, res) { var zone = compute.zone(\u0026#39;{SUA ZONA}\u0026#39;); var vm = zone.vm(\u0026#39;{SUA VM}\u0026#39;); vm.stop(function(err, operation, apiResponse) { console.log(\u0026#39;instance stop successfully\u0026#39;); }); res.status(200).send(\u0026#39;Success stop instance\u0026#39;); }; { \u0026quot;name\u0026quot;: \u0026quot;sample-http\u0026quot;, \u0026quot;dependencies\u0026quot;: { \u0026quot;@google-cloud/compute\u0026quot;: \u0026quot;0.7.1\u0026quot;}, \u0026quot;version\u0026quot;: \u0026quot;0.0.1\u0026quot; } ","permalink":"https://blog.curso-r.com/posts/2019-09-06-app-deploy/","tags":["gcp","shiny"],"title":"Shiny em Produção: Deploy seu App em 20min"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" Recentemente, o Julio discutiu aqui no blog como usar o pacote golem para facilitar o desenvolvimento de shiny apps em forma de pacotes e como transformar esses pacotes em “executáveis” de apenas uma linha. Nesse post eu vou continuar essa série e falar sobre como pegar esse app de uma linha e embrulhá-lo em um contêiner docker para que o seu deploy seja instantâneo.\nInst Essencialmente toda a magia da transformação de um app em um docker ocorre na pasta /inst. Esse diretório, pouco utilizado no dia-a-dia, é útil quando precisamos adicionar dados, templates ou qualquer conteúdo que pode ser necessário em algum momento para o pacote e, portanto, devem ser facilmente acessíveis.\nQuando você transformar seu shiny app em pacote utilizando o tutorial do Julio, você terá uma estrutura de diretórios padrão na raiz do seu app: pastas /R, /man e assim por diante. Crie a pasta /inst e dentro dela /app; aqui é onde seu app irá morar.\nEm princípio, você só precisa de um arquivo app.R dentro de /inst/app contendo aquela linha mágica que executa seu app todo:\nshiny::shinyApp(meuPacote:::app_ui(), meuPacote:::app_server) Se você estiver desenvolvendo um app mais complexo, talvez você precise criar também uma pasta /www ou um arquivo _auth0.yml, mas o resumo da ópera é que você deve ser capaz de rodar shiny::runApp() nesta pasta e ver o seu app funcionando perfeitamente. Durante o desenvolvimento, não se esqueça de sempre reinstalar o seu pacote para que meuPacote:::app_ui() e meuPacote:::app_server mantenham-se atualizados!\nO último arquivo que você pode querer adicionar a /inst/app é a configuração do seu servidor shiny. No meu caso, eu desenvolvo apps dockerizados para subí-los em máquinas virtuais na núvem que podem ser acessadas por qualquer um, então preciso deixar clara qual porta deve ser utilizada pelo meu shiny. Como minhas máquinas suportam o protocolo HTTP, meu shiny-server.conf é o seguinte:\nrun_as shiny; # Log all Shiny output to files in this directory log_dir /var/log/shiny-server; # Define a server that listens on port 80 server { listen 80; # Define a location at the base URL location / { # Host the directory of Shiny Apps stored in this directory site_dir /srv/shiny-server; # When a user visits the base URL rather than a particular application, # an index of the applications available in this directory will be shown. directory_index off; } } Fazer com que o app seja ouvido na porta 80 permite que outra pessoas o acessem sem precisar especificar uma porta no URL! Sendo assim, você pode disponibilizar seu shiny app em meuApp.dominio.com.br.\nDockerfile Agora que a sua pasta /inst está devidamente configurada, você precisa criar um Dockerfile na raiz do seu app. Isso irá incomodar o devtools::check(), mas basta adicionar o nome desse arquivo ao seu .Rbuildignore para que ele seja ignorado durante a verificação.\nSe todas as dependências do seu aplicativo estiverem devidamente organizadas no DESCRIPTION e todas as suas bases internas estiverem propriamente exportadas como documentado pelo Julio, então o seu Dockerfile deve ser parecido com o seguinte:\nFROM rocker/shiny-verse # Instalar bibliotecas para o tidyverse RUN apt-get update -qq \u0026amp;\u0026amp; apt-get -y --no-install-recommends install \\ build-essential \\ libcurl4-gnutls-dev \\ libxml2-dev \\ libssl-dev \\ r-cran-curl \\ r-cran-openssl \\ curl \\ gnupg1 \\ r-cran-xml2 # Instalar seu próprio app (e suas dependências) COPY ./ /tmp/app/ RUN R -e \u0026quot;remotes::install_local(\u0026#39;/tmp/app/\u0026#39;)\u0026quot; # Copiar arquivos para o lugar certo EXPOSE 80/tcp RUN rm /srv/shiny-server/index.html COPY ./inst/app /srv/shiny-server/ COPY ./inst/app/shiny-server.conf /etc/shiny-server/shiny-server.conf # Run CMD [\u0026quot;/usr/bin/shiny-server.sh\u0026quot;] Deploy Agora você já pode construir sua imagem docker e executá-la ou subí-la para alguma máquina virtual. Para testar localmente o app, execute os comandos a seguir no terminal e acesse localhost:8080 no navegador:\ndocker build -t meuApp pasta/para/meuApp docker run -p 8080:80 meuApp ","permalink":"https://blog.curso-r.com/posts/2019-09-04-app-docker/","tags":["docker","shiny"],"title":"Shiny em Produção: dockerize seu app com 1 linha"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Esse post é um tutorial rápido de como você pode criar um livro usando o pacote {bookdown}. Eu uso esse pacote para escrever todos os meus grandes relatórios e até minha tese de mestrado. A filosofia dele é a mesma que a do markdown: o foco é o conteúdo; o formato fica pra depois.\nEsse é um pacote maravilhoso, mas um pouco espinhudo, pois não é fácil lidar com vários formatos distintos. Para dominar o {bookdown}, recomendo que você leia o livro feito pelo Yihui Xie: https://bookdown.org/yihui/bookdown/. O Yihui é também o autor do {knitr}, {rmarkdown}, {pagedown}, e do excepcional {xaringan}. Os pacotes dele têm todos a mesma característica: são extremamente úteis, mas às vezes te mordem.\nNesse post, apresento um vídeo-tutorial faço apenas alguns comentários adicionais\nVídeo Prometo que vou melhorar o áudio nos próximos tutoriais.\nO que faz o que? Para criar um livro com {bookdown}, você só precisa de dois componentes, descritos abaixo\nUm arquivo index.Rmd ou index.md, com o seguinte conteúdo: --- site: bookdown::bookdown_site --- # Rodar a operação O primeiro componente diz que seu markdown deve ser compilado como um livro do {bookdown}. Isso substitui a função rmarkdown::default_site(), que é o padrão para desenvolver sites usando RMarkdown. Isso fica para outro post!\nJá o segundo componente é o que compila o site de fato. O parâmetro output_format é o que controla a saída. Existem três opções principais de formatos no {bookdown}: gitbook (HTML), pdf_book (PDF) e gitbook (E-pub).\nOutra forma mais enxuta, mas menos geral de obter o mesmo é utilizando a função bookdown::render_book() no lugar de rmarkdown::render_site(). Nesse caso, não é necessário adicionar a tag site: bookdown::bookdown_site no arquivo index.Rmd/index.md.\nE se eu quiser que meu site tenha vários formatos? Você pode fazer isso trocando output_format por um vetor. No entanto, existem formas mais organizadas. Nesse caso, a recomendação é criar um arquivo _output.yml, contendo os outputs que você quer ter, por exemplo\nbookdown::gitbook: caption: false bookdown::pdf_book Note que é possível adicionar parâmetros ao livro, que podem controlar o template que vamos utilizar, por exemplo.\nOutra forma de fazer isso é diretamente no index.Rmd:\n--- site: bookdown::bookdown_site output: bookdown::gitbook: caption: false bookdown::pdf_book --- # meu livro E se eu quiser quebrar meu livro em vários arquivos? Sem problemas. Basta criar arquivos com uma ordem específica, como 01_introducao.Rmd, 02_introducao.Rmd, etc. Esses arquivos são agregados num grande Rmd antes de ser compilado.\nPor enquanto é isso. Farei mais posts sobre markdown, pois é muito útil e divertido! Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2019-08-28-bookdown/","tags":["bookdown"],"title":"Escrevendo livros com R e bookdown"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" No meu último post sobre shiny, mostrei como é possível usar o pacote {golem} como framework para desenvolvimento de shiny apps como pacotes. O processo de desenvolvimento se torna ligeiramente mais burocrático, mas os ganhos em consistência e reprodutibilidade são enormes.\nMas eu acho que faltou uma coisa no post. Eu não deixei clara qual é a grande vantagem de desenvolver um app usando o golem: o poder de rodar seu app com uma linha de código.\nNosso objetivo será chegar num código assim:\nshiny::shinyApp(meuPacote:::app_ui(), meuPacote:::app_server) Isso é tudo que você precisará subir em uma máquina na nuvem ou no shinyapps.io!\nPara conseguir isso, no entanto, você precisará fazer duas coisas:\nCertifique-se de que seu app funciona com o pacote instalado numa sessão nova. (para o shinyapps.io) coloque seu app no github e instale o pacote de lá. Criar um arquivo app.R com o código shiny::shinyApp(meuPacote:::app_ui(), meuPacote:::app_server) e subir para seu servidor ou para o shinyapps.io. Certifique-se de que seu app funciona. Uma boa forma de garantir que seu app funciona é fazendo-o passar no devtools::check(). Se tudo der certo nessa etapa, a probabilidade de dar algum problema no passo (1) colocado acima é realmente baixa.\nRecomendo fortemente a leitura do livro Zen do R para trilhar o caminho do desenvolvimento de pacotes sem dor nem sofrimento.\nTrabalhando com bases exportadas dentro do pacote Se o seu Shiny App utiliza bases de dados, sua base deve ser adicionada na pasta data/ de seu pacote. O problema é que, como as bases na pasta data/ são exportadas, elas não são diretamente acessíveis por funções internas do pacote. Para resolver esse problema, existem duas alternativas:\nUtilizar :: sempre que for acessar sua base nas funções de seu Shiny App. Usualmente isso ocorrerá apenas algumas vezes, pois é uma boa prática criar um reactive() que carrega e filtra os dados, e fazer todas as outras funcionalidades do app dependerem desse reactive(). Por exemplo: app_ui \u0026lt;- function() { tagList( plotOutput(\u0026quot;grafico\u0026quot;) ) } app_server \u0026lt;- function(input, output, session) { # esse é o reactive que controla seus dados. # se você tiver muitas bases no seu app, uma boa # ideia é colocá-los numa lista neste reactive() dados \u0026lt;- reactive({ meuPacote::cars }) # a partir daqui, você não usa mais a base do pacote, # mas sim o que vem do reactive() output$grafico \u0026lt;- renderPlot({ plot(dados()) }) } Tratar seus dados como dados internos. Para isso, você deverá usar usethis::use_data(dados, internal = TRUE). Nessa alternativa, seus dados ficarão num arquivo chamado sysdata.rda. A vantagem nesse caso é que você não precisará usar o :: sempre que for acessar seus dados. A desvantagem é que todas suas bases ficarão em um arquivo só, o que pode causar confusões. É isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2019-08-27-app-pacote/","tags":["golem","shiny"],"title":"Shiny em Produção: Rodando seu App com 1 Linha"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Quando você acessa um pacote no GitHub, muitas vezes você se depara com essas coisas verdinhas:\nEssas coisas são chamadas distintivos (“badges”). Indo da esquerda para a direita: a primeira delas serve para garantir ao leitor que o pacote é instalável no Linux, a segunda serve para garantir que o pacote é instalável no Windows, e a terceira serve para garantir que o pacote está no CRAN. Esses distintivos dão maior confiabilidade ao pacote e é considerado como boa prática de programação.\nNesse post, vou discutir sobre o primeiro distintivo, e mostrar um exemplo de como você pode conseguir criá-la usando o pacote {usethis} e o serviço Travis.\nO que é o Travis? Travis é um serviço de integração contínua (Continuous Integration, CI), que automatiza a aplicação de um conjunto de testes do seu código. De forma muito simplificada, o Travis funciona assim:\nVocê dá um push do seu código no GitHub O Travis pega seu novo código e testa ele numa nova máquina Linux, criada na hora. Se o teste der certo, o Travis permite que seu distintivo fique verde. Se não der certo, ele fica vermelho. Se você não conhece muito de GitHub, dê uma olhada nesse post do Athos ou veja esse maravilhoso livro da Jenny Bryan\nMas algumas coisas precisam ser configuradas para que esses passos sejam de fato aplicados.\nPara (1), é necessário que você i) tenha uma conta no serviço travis-ci, ii) dê permissão para que o Travis monitore seus repositórios do GitHub e iii) adicione o seu pacote no Travis para monitoramento. Você só precisa fazer (i) e (ii) uma vez.\nPara (2), é necessário definir que testes você quer fazer. Por padrão, o R já tem uma rotina na documentação do Travis. O que essa rotina faz é passar o R CMD CHECK no seu pacote, que é basicamente o que seu pacote precisa passar para que seja aceito no CRAN.\nPara (3), você precisa adicionar o distintivo do seu pacote corretamente no seu arquivo README.md. A melhor forma de fazer isso é utilizando a função usethis::use_travis()\nVídeo Nesse vídeo, usei minha conta do GitHub e criei um pacote do zero usando o pacote usethis, vinculei ao Travis, e fiz ele dar build.\nDetalhes Se o seu pacote depende da instalação de bibliotecas em C++, como libpoppler, libgsl, entre outras, você precisará adicioná-las manualmente no seu arquivo .travis.yml. Acredito que a melhor forma de fazer isso é copiando de outros pacotes que já fizeram isso antes, como esse aqui que eu fiz algumas semanas atrás. Você também pode fazer o mesmo trabalho para testar seu código no Windows. Para isso, uma alternativa legal é o AppVeyor. Para testar seu pacote em múltiplas plataformas, principalmente quando for submeter seu pacote no CRAN, use o R-Hub. É isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2019-08-21-travis/","tags":["travis","ci"],"title":"Fazendo seu pacote passar no Travis"},{"author":["Caio"],"categories":["tutoriais"],"contents":" Qual a relação entre as duas partes do título deste post? O que a remoção de acentos de uma palavra em português pode ter a ver com o Ano Novo muçulmano? A resposta é transliteração.\nTransliteração é uma operação em strings pouco discutida, mas bastante importante quando lidamos com idiomas que não o inglês. Ela implica na conversão de texto de um sistema de escrita para outro, substituindo letras (ou caracteres) de formas previsíveis, como, por exemplo, α → a ou æ → ae.\nEm português é comum transliterarmos palavras sem mesmo saber o que isso significa. Por exemplo, a palavra חנוכה (/ˡχanuka/) é normalmente escrita como “chanucá” apesar de essa não ser uma correspondência da pronúncia hebraica para os sons do português. Se quiséssemos uma transposição de pronúncia, provavelmente escreveríamos “ranucá”, mas o mais correto é justamente transliterar a palavra de modo a preservar uma certa correspondência entre as letras do hebraico e as letras do português; sendo assim, a primeira letra é transformada em um CH e não em um R. O mesmo ocorre com a romanização do japonês e do mandarim, etc.\nLetras estrangeiras E qual a importância da transliteração no dia a dia? Em primeiro lugar, quando lidamos com idiomas que apresentam caracteres distintos dos nossos, muitas vezes as funções padrão do R não são capazes de tratar eles corretamente!\nVejamos por exemplo uma palavra do alemão e uma do holandês: “groß” e “ijsvrij”. No alemão, a versão capitalizada do beta (ß) é SS, enquanto no holandês o dígrafo IJ é na verdade uma letra só. Vejamos o que acontece se usamos as funções padrões:\ntoupper(\u0026quot;groß\u0026quot;) ## [1] \u0026quot;GROß\u0026quot; stringr::str_to_title(\u0026quot;ijsvrij\u0026quot;) ## [1] \u0026quot;Ijsvrij\u0026quot; Para corrigir esses problemas (o “ß” e o “Ij”), devemos utilizar funções do pacote stringi que são capazes de transliterar as strings antes de aplicar as outras transformações:\nlibrary(stringi) stri_trans_toupper(\u0026quot;groß\u0026quot;, locale = \u0026quot;de_DE\u0026quot;) ## [1] \u0026quot;GROSS\u0026quot; stri_trans_totitle(\u0026quot;ijsvrij\u0026quot;, locale = \u0026quot;nl_NL\u0026quot;) ## [1] \u0026quot;IJsvrij\u0026quot; Acentos do português O segundo uso da transliteração (e provavelmente mais comum no dia a dia) é a remoção dos acentos do português. Muitas vezes recebemos arquivos e tabelas nos quais a acentuação das palavras está quebrada ou incorreta, tornando necessária a remoção dos diacríticos (acentos ortográficos + til + cedilha + trema + etc.) de todas as strings.\nJamais devemos criar uma regex para realizar essa tarefa porque a chance de ela não funcionar é muito grande! Suponha que esquecemos de um acento na regex, uma forma capitalizada, ou mesmo a existência de palavras em outros idiomas no meio de um texto em português. A solução para esse problema é, você adivinhou, a transliteração.\nstri_trans_general(\u0026quot;Stríng cõm müìtôs açëntòs\u0026quot;, \u0026quot;Latin-ASCII\u0026quot;) ## [1] \u0026quot;String com muitos acentos\u0026quot; A função stri_trans_general() recebe dois argumentos: uma string e um identificador de transformação. Neste caso e quando estivermos lidando com português em geral, o que queremos é passar uma string no alfabeto latino (o nosso alfabeto) para ASCII (o conjunto restrito de caracteres sem nenhum tipo de diacrítico) e, portanto, utilizamos o ID \"Latin-ASCII\".\nAno Novo muçulmano O terceiro e último uso da transliteração serve para quando lidamos com línguas que apresentam ligaduras (duas ou mais letras unidas em um único glifo). O árabe é o melhor exemplo disso porque grande parte da sua arte caligráfica envolve a composição de diversas palavras em uma só figura [observe o círculo central da imagem do Alcorão deste post e repare como ele é preenchido por diversas palavras entrelaçadas].\nComo o Ano Novo muçulmano deste ano é amanhã (ao pôr-do-sol do dia 30/08/19), acho que vale a pena aprender como lidar com a ligadura mais famosa do árabe: a frase conhecida como Sallallahou Alayhe Wasallam que significa “bênçãos de Deus estejam sobre Ele e Sua família e paz”. Esta frase é comumente conectada ao nome dos profetas do Islã em sinal de respeito e, pela frequência de seu uso, foi convertida em apenas um caractere Unicode:\nﷺ Faça um teste e copie o símbolo acima. Você pode notar que ele é de fato apenas um glifo como qualquer outro e que pode ser enviado por qualquer WhatsApp da vida!\nA questão é que se estivermos analisando texto em árabe, esse tipo de ligadura pode atrapalhar, por exemplo, a contagem de palavras. Para lidar com essas situações (especificamente em árabe) usamos a função stri_trans_nfkc().\nstri_trans_nfkc(\u0026quot;\\ufdfa\u0026quot;) ## [1] \u0026quot;صلى الله عليه وسلم\u0026quot; E pronto! Agora nenhum feriado internacional vai pegar a sua programação de surpresa.\nObs.: Acima utilizei o código Unicode da ligadura porque senão ela atrapalharia a formatação da caixa de código, mas basta executar cat(\"\\ufdfa\") no seu R para ver que ele realmente representa o S.A.W.\n","permalink":"https://blog.curso-r.com/posts/2019-08-29-transliteracao/","tags":["strings"],"title":"Remoção de Acentos e o Ano Novo Muçulmano"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" O {dplyr} é um pacote incrível pois permite realizar operações difíceis de forma iterada e intuitiva. Uma grande facilidade do {dplyr} é a possibilidade de utilizar os nomes das variáveis sem encapsular com aspas, o que torna a programação mais fluida e compreensível.\nPor exemplo, é fácil argumentar que\nmtcars %\u0026gt;% summarise(soma = sum(cyl ^ 2)) é mais intuitivo que\nmtcars %\u0026gt;% summarise(cyl = sum(mtcars$cyl ^ 2)) ou ainda\nmtcars %\u0026gt;% mutate(cyl = sum(mtcars[[\u0026quot;cyl\u0026quot;]] ^ 2)) No entanto, ao usar o {dplyr} com frequência, passamos a querer colocá-lo em todos os contextos possíveis, como novas função ou Shiny apps. Assim, gostaríamos de fazer algo do tipo\nminha_fn \u0026lt;- function(dados, variavel) { dados %\u0026gt;% summarise(nova_variavel = sum(variavel ^ 2)) } No entanto, ao experimentar isso, temos o erro\nmtcars %\u0026gt;% minha_fn(cyl) # \u0026gt; Error: object \u0026#39;cyl\u0026#39; not found Você já se deparou com essa situação? É bem frustrante. A verdade é que o tidyverse foi desenvolvido com foco em facilitar o trabalho de análise, com o custo de dificultar o trabalho de programação.\nPorém, graças a avanços recentes no pacote {rlang}, isso está ficando mais fácil. Nesse post vou mostrar três casos casos comuns de programação com o tidyverse, e suas soluções. Para casos mais complicados, recomendo dar uma olhada no livro sobre Tidyeval.\nPara reproduzir esse post, você precisará ter pelo menos a versão 0.4.0 do {rlang} instalado na sua máquina!\nQuero que minha função receba um nome sem aspas Para isso, podemos usar o quentíssimo operador {{}}, que foi oficialmente apresentado na useR!2019, em Tolouse. Esse operador informa as funções do {dplyr} (e seus amigos {tidyr}, {ggplot2} etc) que olhem para a variável de dentro da base de dados, ao invés de um objeto supostamente passado como argumento da função.\nCom isso, a função anterior fica simples assim:\nminha_fn_sem_aspas \u0026lt;- function(dados, variavel) { dados %\u0026gt;% summarise(nova_variavel = sum({{variavel}} ^ 2)) } E sua utilização:\nmtcars %\u0026gt;% minha_fn_sem_aspas(cyl) nova_variavel 1324 Zero trauma.\nQuero que minha função receba uma string Para isso, teremos de usar o objeto especial .data. Ele permite que você acesse a informação dos dados antes de aplicar a nova função. É muito similar ao . do pacote {magrittr}, para quem já conhece.\nminha_fn_com_aspas \u0026lt;- function(dados, variavel) { dados %\u0026gt;% summarise(nova_variavel = sum(.data[[variavel]] ^ 2)) } E sua utilização:\nmtcars %\u0026gt;% minha_fn_com_aspas(\u0026quot;cyl\u0026quot;) nova_variavel 1324 Show! Esse provavelmente é o caso da maioria dos Shiny apps, pois acessamos as informações através de input$id_input, que geralmente é uma string.\nObservação: uma diferença essencial entre usar .data e . é que o primeiro consegue lidar com grupos, e o segundo não. Por exemplo, esses códigos têm resultados diferentes:\nmtcars %\u0026gt;% group_by(cyl) %\u0026gt;% summarise(nova_variavel = sum(.data[[\u0026quot;cyl\u0026quot;]] ^ 2)) cyl nova_variavel 4 176 6 252 8 896 mtcars %\u0026gt;% group_by(cyl) %\u0026gt;% summarise(nova_variavel = sum(.[[\u0026quot;cyl\u0026quot;]] ^ 2)) cyl nova_variavel 4 1324 6 1324 8 1324 Para a lista completa de diferenças, veja ?rlang::.data.\nQuero que minha função crie uma coluna com nome variável E se você quiser mudar o nome de nova_variavel e incluir isso como argumento da função? Nesse caso, é necessário introduzir o operador :=, e o resto é resolvido com {{}}:\nminha_fn_sem_aspas_novo_nome \u0026lt;- function(dados, variavel, nome) { dados %\u0026gt;% summarise({{nome}} := sum({{variavel}} ^ 2)) } E sua utilização:\nmtcars %\u0026gt;% minha_fn_sem_aspas_novo_nome(cyl, novo_nome) novo_nome 1324 Curiosamente, essa solução também funciona com as aspas:\nmtcars %\u0026gt;% minha_fn_sem_aspas_novo_nome(cyl, \u0026quot;novo_nome\u0026quot;) novo_nome 1324 Wrap-up O pacote que está por trás da programação com {dplyr} e amigos é o {rlang}. Use {{variavel}} quando não quiser colocar aspas no argumento da função. Use .data[[\"variavel\"]] quando quiser colocar aspas no argumento da função. Use {{nome}} := ... quando quiser criar colunas com nomes que estão no argumento da função. É isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2019-08-21-nse/","tags":["rlang"],"title":"Programando com o tidyverse"},{"author":["Julio"],"categories":["discussões"],"contents":" Um dos posts mais legais da Curso-R, na minha opinião, é esse aqui, feito pelo William Amorim. O texto não tem nada de códigos, mas cita motivos pelos quais é muito bom aprender estatística e programação.\nMas eu colocaria um tópico adicional no texto, que apesar de ter sido abordado, talvez não tenha ficado com o devido destaque. Eu adicionaria a existência da comunidade como elemento primordial para o sucesso no aprendizado de tópicos difíceis, especialmente a ciência – e a estatística, que é a meta ciência – e linguagens de programação, com destaque especial para o R.\nEsse post é uma tentativa de desenvolver esse tópico adicional. Me baseei na palestra dada pela Julia Stewart Lowndes na useR!2019, que foi fantástica. Recomendo o texto abaixo para todos que se sentem perdidos quando falamos de “comunidade”.\nO que é a comunidade? Todos nós fazemos parte da comunidade. Ela é o motor que faz a ciência aberta andar e os pacotes do R serem criados. Ninguém é dono da comunidade – e nunca será. A única forma de tirar proveito dela, é contribuindo, pois ao contribuir, colocamos atraímos a atenção dos trabalhos das outras pessoas. É como se fosse o modelo da colcha de retalhos da teoria da relatividade do Einstein: cada pessoa atrai os trabalhos das outra pessoas, que seguem em diferentes direções.\n## Warning in knitr::include_graphics(\u0026quot;/images/posts/conteudo/comunidade/rubber- ## sheet.png\u0026quot;, : It is highly recommended to use relative paths for images. You had ## absolute paths: \u0026quot;/images/posts/conteudo/comunidade/rubber-sheet.webp\u0026quot; Figura 1: Quem será que é a bolinha amarela? Contribuir com a comunidade tem duas vantagens principais, ambas indiretas:\nEconomizar tempo. Pessoas vão trabalhar para você, de graça, do nada. Você ganhará, também de graça, o feedback necessário para aprimorar seus trabalhos. Ganhar dinheiro. As pessoas vão começar a te chamar para participar de eventos, workshops e projetos. Esses espaços têm um grande impacto na carreira e trazem oportunidades. Isso, é claro, sem mencionar todas as vantagens sociais envolvidas. Preferi me ater somente às motivações egoísticas para não cair em argumentos baseados em emoção.\nMas é claro que essas vantagens vêm com um custo. Para contribuir de forma efetiva com a comunidade, precisamos trabalhar.\nSó abertura gera abertura É necessário ajudar a comunidade a te ajudar. Ser aberto não é fácil, pois exige o uso de certos padrões de organização que são utilizados por boa parte da comunidade.\nNa comunidade do R, isso envolve, por exemplo: i) usar do GitHub, ii) os princípios tidy e iii) estilos de programação, como o styleguide do tidyverse, dentre muitos outros.\n## Warning in knitr::include_graphics(\u0026quot;/images/posts/conteudo/comunidade/ ## gentileza.png\u0026quot;, : It is highly recommended to use relative paths for images. You ## had absolute paths: \u0026quot;/images/posts/conteudo/comunidade/gentileza.webp\u0026quot; É importante destacar que esses padrões não são regras definitivas; são apenas convenções! Isso significa que não devemos ficar bravos com quem não usa, até porque nós mesmos podemos acabar não usando no futuro.\nMarketing é importante A comunidade é densa e dispersa. Por isso, às vezes é necessário dar um empurrãozinho nas nossas soluções e divulgar nosso trabalho para atrair atenção.\nNa comunidade do R, uma boa forma de divulgar nossos trabalhos é usando do Twitter. Um possível caminho seria i) fazer um novo pacote, ii) criar um blog post sobre isso, e iii) divulgar as atualizações do seu pacote de tempos em tempos e iv) interagir/retuittar com todos que citam você ou seus trabalhos. Para interagir, use a hashtag #rstats, emojis e piadas.\nMas tome cuidado. As redes socias tentam de todas as formas se tornar o objetivo, e não o meio para um fim:\n“Social media, it’s just the market’s answer to a generation that demanded to perform. So the market said, ‘Here, perform everything to each other, all the time, for no reason.’ It’s prison. It’s horrific. It is performer and audience melded together (…) I know very little about anything, but what I do know is that if you can live your life without an audience, you should do it.”\nBo Burnham (Make Happy, 2016)\nA rede social pode ser tóxica e nos tornar improdutivos. Por isso, temos de usá-la com parcimônia e consciência.\n## Warning in knitr::include_graphics(\u0026quot;/images/posts/conteudo/comunidade/ ## bad_opinions.png\u0026quot;, : It is highly recommended to use relative paths for images. ## You had absolute paths: \u0026quot;/images/posts/conteudo/comunidade/bad_opinions.webp\u0026quot; Figura 2: https://xkcd.com/2051/ A comunidade é inclusiva No passado, a comunidade de programadores era machista, grossa e antipática. Ela ainda é assim, mas hoje em dia isso não é mais legal.\nAgora, regra da comunidade é ser gentil, mesmo com quem não é gentil com a gente. Então se você acha engraçado quando um programador “raiz” tira sarro ou é grosso com algum iniciante, está na hora de mudar. Felizmente isso aconteceu pois a comunidade finalmente chegou à – agora óbvia – conclusão de que, quando somos inclusivos, aumentamos a quantidade de pessoas que contribuem com a gente.\nNa comunidade do R, isso acontece de forma muito especial. É só ler o meu post sobre a useR ou o post da Bruna sobre a rstudio::conf para notar o quanto isso é importante. E é impossível não mencionar aqui o impacto que o RLadies tem nesse grande avanço. Talvez um gráfico seja melhor do que palavras para explicar esse fenômeno:\n## Warning in knitr::include_graphics(\u0026quot;/images/posts/conteudo/comunidade/rladies- ## growth.jpeg\u0026quot;, : It is highly recommended to use relative paths for images. You ## had absolute paths: \u0026quot;/images/posts/conteudo/comunidade/rladies-growth.webp\u0026quot; Figura 3: https://twitter.com/RLadiesGlobal/status/1148600179236442112/photo/1 É trabalho mesmo? Cá entre nós, é fácil confundir esses trabalhos adicionais para que a comunidade nos ajude com simples boas práticas. Ser organizado, comunicar bem as ideias e ser gentil me parecem ser habilidades que realmente queremos ter na vida.\nMas pode ser difícil dar o pontapé inicial. Por isso, separei um pequeno conjunto de sugestões para quem quiser entrar nesse mundo:\nSe você é uma R lady ou faz parte de alguma minoria de gênero, participe do RLadies. Acredito que o jeito mais efetivo de entrar seja falando com alguém que faz parte, mas se você quiser, também é possível acessar por esse link. Comece a postar coisas de #rstats no Twitter. Sua primeira função em R, seu primeiro pacote, seu resultado de sucesso, ou sua enorme falha. As pessoas vão começar a interagir com você e você estará falando com pessoas famosas da comunidade antes que perceba. Fale com a gente. A Curso-R é uma empresa voltada a cursos de programação em R. Nossa missão é descomplicar a ciência de dados. Todos nossos materiais são públicos por padrão e podem ser acessados pelo GitHub. Pode me procurar no Twitter por jtrecenti ou fale com todo mundo da Curso-R através de contato@curso-r.com. São apenas sugestões! Também estou aprendendo.\nÉ isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2019-07-15-comunidade/","tags":["comunidade"],"title":"Programando com a comunidade"},{"author":["Beatriz Milz","Angélica Custódio"],"categories":["divulgação"],"contents":" Tidyverse developer day 2019 (tidy-dev-day) Este post tem como objetivo relatar a experiência de participar do Tidyverse Developer Day 2019, e tem como autoras a Beatriz Milz e a Angélica Custódio. A Bea é doutoranda em Ciência Ambiental na USP, e é uma das co-organizadoras do R-Ladies São Paulo, que é uma organização que tem como objetivo promover a diversidade de gênero na comunidade da linguagem R. A Angélica é formada em estatística pela UFSCar, e mestranda em Ciência da Computação na UFABC, também faz parte do R-Ladies São Paulo e do PyLadies São Paulo.\nO tidy-dev-day O Tidyverse Developer Day (tidy-dev-day) é um evento com duração de um dia, onde os participantes aprendem mais sobre como contribuir com o tidyverse. O evento relatado aqui foi a segunda edição, sendo que a primeira aconteceu na rstudio::conf2019.\nNesta edição, o tidy-dev-day foi à parte da conferência useR!2019, que ocorreu em Toulouse, na França. O evento ocorreu um dia antes do início da conferência, reunindo diversas pessoas que participaram dela.\nAlgo a destacar é que a organização da useR!2019 selecionou pessoas para serem contempladas com bolsas de diversidade, fornecendo ajuda financeira a membros de grupos sub-representados na comunidade de R, ou historicamente marginalizados. Essa iniciativa foi fundamental para a participação de diversas pessoas, de diferentes partes do mundo. Inclusive para as duas pessoas que estão escrevendo agora sobre a experiência lá.\nComo funcionou o evento? O evento aconteceu no mesmo espaço da useR!2019, e tinha um código de conduta, com o objetivo de oferecer um ambiente seguro e amigável para todos os participantes. O evento não foi gratuito: foi necessário pagar uma taxa de $10 (dez dólares), com o intuito de diminuir o número de pessoas que se inscreve e não comparece no evento. No entanto, a organização aponta que o dinheiro será doado.\nComo o objetivo do evento é proporcionar um aprendizado sobre como contribuir com desenvolvimento e manutenção do tidyverse, estavam presentes diversos desenvolvedores e mantenedores do tidyverse, e também um time para ajudar os participantes, tirar dúvidas, etc.\nO material utilizado (instruções, apresentações, sugestões de material, etc) está disponível neste repositório no Github.\nEm uma parede no fundo da sala, haviam diversos post-its que indicavam issues a serem trabalhadas, e estavam separadas por pacotes, e também por cor, onde a cor indicava o tipo de issue (documentação, bug, feature). Os participantes escolhiam uma issue para trabalhar durante o evento, até a realização de um pull request. Em outra parede, os post-its estavam separados em: post-its de pull requests para serem avaliados pelo Hadley Wickham e outros contribuidores, e os post-its que já tinham sido avaliados e aceitos!\nOutra coisa legal é que o espaço disponibilizou diversas mesas agrupadas, o que ajudava a trabalhar em conjunto.\nLegenda: Foto tirada no tidy-dev-day, por [@Dale_Masch](https://twitter.com/Dale_Masch).\nPara quem estava participando pela primeira vez eram recomendadas as issues de documentação, pois elas são mais naturais para quem nunca fez esse tipo de atividade antes. Para as pessoas que já tinham mais familiaridade eram recomendadas issues de bug ou feature que já exigem um conhecimento maior não só do pacote, mas também de como desenvolver a melhoria.\nExperiência pessoal - Bea Eu nunca desenvovi um pacote, e ao chegar no evento nem sabia como pacotes eram estruturados. Na issue que eu escolhi, o objetivo era para melhorar uma mensagem de erro da função filter(), do pacote dplyr, para que a mensagem estivesse de acordo com o style guide do tidyverse. Legenda: Foto da issue que estava trabalhando, e também da moeda especial que recebemos, por [@Bea_Milz](https://twitter.com/Bea_Milz).\nInicialmente, eu fiz o fork do repositório do dplyr, o clone no meu computador, e fiquei sem saber o que fazer depois. Eu não sabia em qual diretório ficavam as funções, estava com receio de alterar algo e causar algum erro em outras funções do pacote… Mas ainda bem que tive ajuda! A Ildi Czeller me mostrou em qual diretório as funções estavam (aliás, é no /R), e e então consegui encontrar o arquivo que precisava alterar, e qual parte do código era correspondente à função filter(). Uma coisa que conheci no evento e foi muito interessante é o conceito de testes, pois tive que alterar o teste referente à mensagem de erro que eu estava trabalhando.\nApós fazer as alterações que entendi que eram necessárias, fiz um pull request. O Hadley respondeu que outra pessoa também estava trabalhando neste tópico (haviam duas issues criadas para o mesmo problema), e indicou onde a pessoa estava sentada. Então fui lá e perguntei para o Colin (que estava trabalhando nesse mesmo problema) se poderíamos trabalhar na issue juntos. Ele foi super receptivo! Ele explicou como ele estava resolvendo, e eu também mostrei o que havia feito. O Hadley pediu para mudar uma função usada no teste, e ficamos trabalhando nisso juntos (no caso, eu estava mais aprendendo e perguntando para o Colin, pois os testes eram novidade para mim).\nLegenda: O pull request foi aceito!\nFoi uma experiência única, e aprendi muito nesse dia!\nExperiência pessoal - Angélica Durante o tidy-dev-day trabalhei em issues do ggplot2, foi uma experiência totalmente nova para mim e foi super gratificante a sensação de que eu aprendi e também pude contribuir.\nNo evento havia vários monitores facilitando a atividade, eles ajudavam desde como fazer o fork no github e trabalhar a issue no R até como fazer o pull request para fechar a issue. Contei com a ajuda da Ildi Czeller que era uma das monitoras e me auxiliou em todas as etapas, dando dicas desde configurações no git até ensinando atalhos no Rstudio que facilitaram a vida (estão documentados aqui).\nLegenda: Criando um pull request no pacote ggplot2.\nA atividade promoveu o engajamento e contribuição de todos, desde de quem era novato até quem já tinha bastante experiência. Era um espaço aberto para tirar dúvidas e pedir ajuda, em um determinado momento pude conversar com o Hadley e tirar dúvida sobre uma issue que eu estava trabalhando. Entender melhor qual era a interpretação de melhoria daquele ponto com um dos autores do pacote foi um aprendizado incrível.\nConclusão Segundo o Hadley, mais de 40 pull requests foram aceitos, e quase 30 precisavam ser revisados, o que mostra como o evento :\nHuge thanks to everyone who contributed to tidyverse developer day! We merged over 40 PRs and have almost 30 more to review over the next few days pic.twitter.com/uM9Tg1dMeX — Hadley Wickham (@hadleywickham) July 8, 2019 Foi uma ótima oportunidade para conhecer pessoas novas também! E para conhecer pessoalmente muitas referências que temos na comunidade.\nLegenda: Angélica e Bea mostrando as issues que estavam trabalhando no tidy-dev-day, por AngCustodio.\n","permalink":"https://blog.curso-r.com/posts/2019-08-12-tidydevday/","tags":["eventos"],"title":"Tidyverse developer day 2019"},{"author":["Caio"],"categories":["conceitos"],"contents":" Nesta série de posts, apresentamos a todos a nossa primeira tentativa de escrever um livro: O Zen do R! Durante as últimas semanas, todas as quartas, trouxemos para o nosso blog os capítulos que já escrevemos do livro e respondemos qualquer pergunta sobre o conteúdo.\nEste é o último capítulo (por enquanto)! Vamos falar sobre como armazenar dados em um pacote: os diretórios data e data-raw.\nData e data-raw Na seção anterior, foi discutida a importância de empacotar uma análise. Seja para organizar dependências, reutilizar código, manter testes automatizados, ou qualquer outra razão, pacotes são a melhor forma de guardar e compartilhar código em R. Mas, apesar de toda a conversa sobre programação, pouco foi abordado sobre outro elemento essencial de uma análise de dados: dados.\nFelizmente, pacotes em R têm lugares específicos para guardar dados brutos e dados tratados. São as pastas data e data-raw, cada uma com as suas propriedades e possibilidades. Ambas podem ser criadas com facilidades por funções do pacote usethis, então elas se encaixam perfeitamente no fluxo de análise descrito até agora.\nComo indicado anteriormente, existem dois tipos de dados: brutos e tratados. Normalmente dados brutos estão em formatos comumente compartilhados em ambientes de trabalho: planilhas Excel, arquivos CSV, etc. Os pacotes readxl e readr permitem que esses formatos sejam importados para dentro do R, mas normalmente essas funções são mais lentas e menos padronizadas do que readRDS(), por exemplo, que lê arquivos no formato nativo do R.\nAlém disso, raramente os dados recebidos durante uma análise estarão perfeitamente organizados e padronizados. É comum precisar de múltiplos fluxos de tratamento para poder transformar os dados brutos naquilo que de fato pode ser utilizado durante uma análise.\nO programador é encorajado a separar essas planilhas brutas daquelas resultantes do processo de limpeza e tratamento. Junto com os dados crús, é importante também guardar os arquivos que fazem o processo de limpeza; caso haja uma mudança nas demandas ou nas bases, o analista precisa ser capaz de alterar os scripts de tratamento e gerar novas bases consolidadas.\nNo exemplo abaixo, supõe-se um diretório com um pacote R e uma base bruta denominada dados.xlsx. Primeiramente deve-se executar a função usethis::use_data_raw() para criar a pasta data-raw e um arquivo de tratamento para a base em questão.\nusethis::use_data_raw(\u0026quot;dados\u0026quot;) #\u0026gt; ✔ Setting active project to \u0026#39;~/Documents/demo\u0026#39; #\u0026gt; ✔ Creating \u0026#39;data-raw/\u0026#39; #\u0026gt; ✔ Adding \u0026#39;^data-raw$\u0026#39; to \u0026#39;.Rbuildignore\u0026#39; #\u0026gt; ✔ Writing \u0026#39;data-raw/dados.R\u0026#39; #\u0026gt; ● Modify \u0026#39;data-raw/dados.R\u0026#39; #\u0026gt; ● Finish the data preparation script in \u0026#39;data-raw/dados.R\u0026#39; #\u0026gt; ● Use `usethis::use_data()` to add prepared data to package Como indicado pelos três últimos pontos da saída do comando, agora basta colocar o código de tratamento da base dados em data-raw/dados.R e por fim utilizar usethis::use_data() para adicionar os dados preparados ao pacote. Para prosseguir o exemplo, o arquivo dados.xlsx foi copiado para o diretório data-raw e o código abaixo foi inserido em data-raw/dados.R.\nlibrary(magrittr) # Limpar a base dados.xlsx dados \u0026lt;- \u0026quot;data-raw/dados.xlsx\u0026quot; %\u0026gt;% readxl::read_xlsx() %\u0026gt;% dplyr::filter(cyl \u0026gt; 4) %\u0026gt;% dplyr::mutate( brand = stringr::str_extract(model, \u0026quot;^[A-z]+\u0026quot;) ) %\u0026gt;% dplyr::group_by(brand) %\u0026gt;% dplyr::summarise( mean_mpg = mean(mpg), prop_6_cyl = sum(cyl == 6)/dplyr::n() ) %\u0026gt;% dplyr::arrange(brand) # Salvar a base para uso no pacote usethis::use_data(dados) #\u0026gt; ✔ Creating \u0026#39;data/\u0026#39; #\u0026gt; ✔ Saving \u0026#39;dados\u0026#39; to \u0026#39;data/dados.rda\u0026#39; Neste caso o arquivo Excel foi criado de dentro do prṕrio R com o comando writexl::write_xlsx(tibble::rownames_to_column(mtcars, \"model\"), \"data-raw/dados.xlsx\"), mas isso é só um exemplo ilustrativo. O importante é saber o que acontece quando a função use_data() é executada para um objeto do ambiente global, ou seja, as duas últimas linhas do bloco de código acima.\nPor trás das câmeras, use_data() está chamando a função save() do R para gerar um arquivo RDA a partir de um objeto do ambiente global. Arquivos RDA são extremamente estáveis, compactos e podem ser carregados rapidamente pelo R, tornando este formato o principal meio de guardar dados de um pacote. Se os dados do pacote forem guardados assim, eles ficarão disponíveis para serem chamados pelo usuário (você mesmo durante a análise)! Para entender como ficam os dados uma vez que eles são incluídos na pasta data, basta dar uma olhada no objeto dplyr::starwars; neste caso, a base tratada e exportada se chama starwars.\nPara carregar os dados na sua sessão e poder utilizá-los na análise, basta executar pkgload::load_all() ou pressionar a combinação CTRL + SHIFT + L no RStudio. Independentemente do número de tabelas que estiverem salvas na pasta data, todas serão carregadas instantaneamente.\nA título de curiosidade, existem algumas situações em que as bases brutas são grandes demais para serem sincronizadas com o GitHub. A plataforma tem um (razoável) limite de 1GB por repositório que pode ser insuficiente para armazenar dados brutos e tratados. Para não sincronizar as bases brutas com o Git, basta adicioná-las ao arquivo .gitignore do pacote; no caso do exemplo acima, bastaria adicionar a esse arquivo uma linha com o texto data-raw/dados.xlsx.\nDocumentação Além de funções, também é possível documentar bases de dados com o pacote roxygen2. Para isso, crie um arquivo data.R na pasta R/ do pacote e crie um objeto entre aspas com o nome de cada base de dados exportada. Documentar dados é extremamente útil quando o pacote vai ser compartilhado com múltiplas pessoas da mesma organização, pois assim não é necessário compartilhar uma planilha Excel separada descrevendo cada uma das colunas da tabela.\nUma boa documentação de bases de dados não precisa de muita coisa. Abaixo é exemplificado como seria documentada dados:\n#\u0026#39; Dados sobre 15 marcas de carros #\u0026#39; #\u0026#39; A tabela, gerada a partir de `mtcars`, apresenta algumas poucas #\u0026#39; informações sobre carros com mais de 4 cilindros de 15 marcas #\u0026#39; americanas de carros. #\u0026#39; #\u0026#39; @format Uma tabela com 3 colunas e 15 linhas: #\u0026#39; \\describe{ #\u0026#39; \\item{brand}{Marca} #\u0026#39; \\item{mean_mpg}{Milhas por galão médias para aquela marca} #\u0026#39; \\item{prop_6_cyl}{Proporção dos carros que apresentam 6 cilindros} #\u0026#39; } #\u0026#39; @source Henderson and Velleman (1981) \u0026quot;dados\u0026quot; ","permalink":"https://blog.curso-r.com/posts/2019-08-13-zen-do-r-7/","tags":["zen-do-r"],"title":"Data e data-raw (Zen do R parte 7)"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Alguns meses atrás o Daniel fez um post demonstrando como colocar autenticação em shiny apps, baseado numa implementação do José Jesus Filho. Depois disso, trabalhamos bastante e conseguimos desenvolver o pacote auth0, que foi apresentado na useR!2019.\nNesse post, darei uma ideia básica de como o pacote funciona, seus desafios atuais e próximos passos. Novamente, a melhor forma de utilizar o pacote é lendo a documentação completa, então aqui vou me ater a alguns detalhes.\nVídeo Fiz um vídeo demonstrando a utilização do auth0 aqui:\nO vídeo está bem simples e mostra os aspectos iniciais para configurar o Auth0 e fazer funcionar na plataforma shinyapps.io, da RStudio.\nDicas Configuração com _auth0.yml O arquivo yml pode ser criado com auth0::use_auth0() e começa assim:\nname: myApp shiny_config: local_url: http://localhost:8100 remote_url: \u0026#39;\u0026#39; auth0_config: api_url: !expr paste0(\u0026#39;https://\u0026#39;, Sys.getenv(\u0026#39;AUTH0_USER\u0026#39;), \u0026#39;.auth0.com\u0026#39;) credentials: key: !expr Sys.getenv(\u0026quot;AUTH0_KEY\u0026quot;) secret: !expr Sys.getenv(\u0026quot;AUTH0_SECRET\u0026quot;) O ideal é que você use usethis::edit_r_environ() para adicionar as variáveis de configuração AUTH0_USER, AUTH0_KEY e AUTH0_SECRET, por questões de segurança.\nArrumando as informações no Auth0 Pelo que notei dos feedbacks da comunidade, a parte mais difícil de configurar o auth0 é colocar as URLs no aplicativo. Primeiro, lembre-se sempre de que você precisa colocar essas informações em três lugares:\nAllowed Callback URLs Allowed Web Origins Aqui, se a URL do seu app tiver algo depois do domínio e da porta, você deve tirar essas informações. Por exemplo, https://rseis.shinyapps.io/auth0Example vira https://rseis.shinyapps.io somente. Logout URLs É importante notar também que você precisa da URL completa, incluindo os https://, tando no arquivo .yml quanto na configuração do Auth0. Esse é o erro mais comum que notei nos aplicativos dos meus alunos.\nDesafios Por enquanto o auth0 não funciona no paradigma ui.R/server.R. Preciso de ajuda para fazer funcionar! O problema principal aqui é que talvez seja necessário alterar algumas coisas no ambiente ao carregar o pacote, e eu gostaria de fazer isso da maneira menos intrusiva possível. Seria interessante implementar toda a extensa API do auth0 no R, para ficar mais fácil de setar as web origins e configurar usuários. Mas isso dá bastante trabalho e eu também aceito ajuda. É isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2019-07-19-auth0-pkg/","tags":["auth0"],"title":"Autenticação em Shiny Apps com o pacote auth0"},{"author":["Caio"],"categories":["conceitos"],"contents":" Nesta série de posts, estamos apresentamos a todos a nossa primeira tentativa de escrever um livro: O Zen do R! Durante as últimas semanas, todas as quartas, trouxemos para o nosso blog os capítulos que já escrevemos do livro e respondemos qualquer pergunta sobre o conteúdo.\nEste é o penúltimo capítulo, mas provavelmente é o mais importante! Falamos sobre pacotes: por que e como fazê-los.\nPacotes Nas palavras do maior guru do R, Hadley Wickham, “pacotes são a unidade fundamental de código R reprodutível”. Toda vez que você utiliza a função library(), algum pacote está sendo carregado na sessão. Muitas vezes criar uma biblioteca de funções pode parecer uma tarefa árdua e confusa, restrita a grandes conhecedores da linguagem, mas essa impressão não poderia estar mais distante da realidade: pacotes para o R são bastante simples e intuitivos de fazer.\nNo início deste livro foi abordado o conceito de projeto. Ele não passa de um arquivo .Rproj que indica para o RStudio que aquele diretório é um ambiente de trabalho estruturado. Nesse sentido, pacotes iguais a projetos porque eles também têm um .Rproj; pacotes na verdade são projetos.\nA diferença entre os dois é que pacotes podem ser documentados e instalados, permitindo toda uma gama de novas possibilidades para o programador. Muitas vezes uma análise de dados pode envolver dezenas de funções e diversas pessoas, fazendo com que o compartilhamento de código seja vital para que a análise não fuja do controle. Pacotes permitem gerenciar dependências, manter documentação, executar testes unitários e muito mais com o objetivo de deixar todos os analistas na mesma página.\nSendo assim, recomenda-se criar um pacote para qualquer análise que envolva pelo menos meia dúzia de funções complexas e mais de uma pessoa; caso contrário, um projeto já é suficiente. Outra motivação para criar um pacote é compartilhar conjuntos úteis de funções com outras pessoas; isso acaba sendo menos comum para a maioria dos usuários, mas é importante ressaltar que o R não seria a linguagem popular que é hoje se não fossem pelas famosas bibliotecas ggplot2 e dplyr.\nusethis::create_package(\u0026quot;~/Documents/demo\u0026quot;) #\u0026gt; ✔ Setting active project to \u0026#39;~/Documents/demo\u0026#39; #\u0026gt; ✔ Creating \u0026#39;R/\u0026#39; #\u0026gt; ✔ Writing \u0026#39;DESCRIPTION\u0026#39; #\u0026gt; Package: demo #\u0026gt; Title: What the Package Does (One Line, Title Case) #\u0026gt; Version: 0.0.0.9000 #\u0026gt; Authors@R (parsed): #\u0026gt; * First Last \u0026lt;first.last@example.com\u0026gt; [aut, cre] #\u0026gt; Description: What the package does (one paragraph). #\u0026gt; License: What license it uses #\u0026gt; Encoding: UTF-8 #\u0026gt; LazyData: true #\u0026gt; ✔ Writing \u0026#39;NAMESPACE\u0026#39; #\u0026gt; ✔ Writing \u0026#39;demo.Rproj\u0026#39; #\u0026gt; ✔ Adding \u0026#39;.Rproj.user\u0026#39; to \u0026#39;.gitignore\u0026#39; #\u0026gt; ✔ Adding \u0026#39;^demo\\\\.Rproj$\u0026#39;, \u0026#39;^\\\\.Rproj\\\\.user$\u0026#39; to \u0026#39;.Rbuildignore\u0026#39; #\u0026gt; ✔ Opening \u0026#39;~/Documents/demo/\u0026#39; in new RStudio session #\u0026gt; ✔ Setting active project to \u0026#39;demo\u0026#39; A função executada acima é exatamente análoga à função de criação de projetos. A principal diferença é que ela cria um arquivo DESCRIPTION e assume que o nome do pacote é igual ao nome da pasta onde o mesmo está sendo criado (neste caso, “demo”). Alguns outros arquivos também são criados (como .Rbuildignore e NAMESPACE), mas eles não vêm ao caso. De resto, o pacote é idêntico a um projeto e pode ser sincronizado com o Git exatamente da mesma maneira.\nO primeiro passo para começar a usar um pacote é atribuir a ele uma licença (caso um dia você resolva compartilhá-lo com o mundo) e preencher a descrição. Abaixo encontra-se uma função simples que adiciona uma licença MIT ao pacote.\nusethis::use_mit_license(\u0026quot;Seu Nome\u0026quot;) #\u0026gt; ✔ Setting active project to \u0026#39;~/Documents/demo\u0026#39; #\u0026gt; ✔ Setting License field in DESCRIPTION to \u0026#39;MIT + file LICENSE\u0026#39; #\u0026gt; ✔ Writing \u0026#39;LICENSE.md\u0026#39; #\u0026gt; ✔ Adding \u0026#39;^LICENSE\\\\.md$\u0026#39; to \u0026#39;.Rbuildignore\u0026#39; #\u0026gt; ✔ Writing \u0026#39;LICENSE\u0026#39; O arquivo de descrição, no entanto, é um pouco mais complexo porque ele tem alguns campos que precisam ser preenchidos manualmente. Quando o pacote for criado, eles já estarão populados com instruções para facilitar a vida do programador. Abaixo está um exemplo de como DESCRIPTION deve ficar depois de completo:\nPackage: demo Title: O Que o Pacote Faz (Uma Linha) Version: 0.0.0.9000 Authors@R: person(given = \u0026quot;Seu\u0026quot;, family = \u0026quot;Nome\u0026quot;, role = c(\u0026quot;aut\u0026quot;, \u0026quot;cre\u0026quot;), email = \u0026quot;seunome@dominio.com\u0026quot;) Description: O que o pacote faz (um paragrafo curto terminado em ponto final). License: MIT + file LICENSE Encoding: UTF-8 LazyData: true A partir deste ponto, os metadados do pacote estão essencialmente prontos e não precisam mais ser modificados. Assim como em um projeto, o que resta é adicionar arquivos com funções à pasta R/.\nDocumentação Para poder programar pacotes com mais facilidade, é necessário instalar o devtools. Assim como o tidyverse, este é um conjunto de pacotes (que inclui o usethis por sinal) que auxiliam no processo de criar e testar um pacote de R.\ninstall.packages(\u0026quot;devtools\u0026quot;) A partir de agora você pode, por exemplo, criar documentações para as funções do seu pacote. Quando outras pessoas o instalarem, elas poderão consultar esses manuais da mesma forma que fazem com qualquer outra função: ?funcao().\nA documentação mais simples (e obrigatória) envolve dar um título para a função e descrever o que cada parâmetro significa. Para documentar uma função qualquer, basta adicionar comentários em cima dela com #' assim como no exemplo abaixo:\n#\u0026#39; Função demonstrativa que soma e imprime #\u0026#39; #\u0026#39; @param x Um número ou vetor numérico #\u0026#39; @param y Um número ou vetor numérico #\u0026#39; @param ... Outros argumentos passados para [print()] #\u0026#39; #\u0026#39; @export funcao_demo \u0026lt;- function(x, y, ...) { z \u0026lt;- x + y print(z, ...) return(z) } No RStudio esse tipo de documentação é tratado diferentemente de outros comentários, então certas palavras-chave ficam coloridas. @param por exemplo indica a documentação de um dos parâmetros e @export indica que aquela função será exportada pelo pacote, ou seja, ficará disponível ao usuário quando ele executar library(demo).\nPara gerar a documentação do pacote, basta chamar uma outra função do devtools:\ndevtools::document() #\u0026gt; Updating demo documentation #\u0026gt; Updating roxygen version in ~/Documents/demo/DESCRIPTION #\u0026gt; Writing NAMESPACE #\u0026gt; Loading demo #\u0026gt; Writing NAMESPACE #\u0026gt; Writing funcao_demo.Rd ?funcao_demo() #\u0026gt; Rendering development documentation for \u0026#39;funcao_demo\u0026#39; Conforme o número de funções no pacote for crescendo, basta iterar nesse ciclo descrito até aqui. Além disso, é importante lembrar (como destacado na sessão anterior) que qualquer função utilizada de outro pacote deve ser invocada na forma pacote::funcao(); neste momento, o pacote em questão se tornará uma dependência do seu pacote e deve ser declarado como tal com usethis::use_package(\"pacote\").\nPara garantir que o R não encontrará nenhum problema no seu pacote, basta executar a função de verificação devtools::check(). Se nenhum defeito for encontrado, basta compartilhar o pacote com os seus colegas e instalá-lo com devtools::install_local().\ndevtools::check() #\u0026gt; Updating demo documentation #\u0026gt; Writing NAMESPACE #\u0026gt; Loading demo #\u0026gt; Writing NAMESPACE #\u0026gt; ── Building ───────────────────────────────────────────────────────── demo ── #\u0026gt; Setting env vars: #\u0026gt; ● CFLAGS : -Wall -pedantic -fdiagnostics-color=always #\u0026gt; ● CXXFLAGS : -Wall -pedantic -fdiagnostics-color=always #\u0026gt; ● CXX11FLAGS: -Wall -pedantic -fdiagnostics-color=always #\u0026gt; ───────────────────────────────────────────────────────────────────────────── #\u0026gt; ✔ checking for file ‘/home/clente/Documents/demo/DESCRIPTION’ ... #\u0026gt; #\u0026gt; [... omitido por brevidade ...] #\u0026gt; #\u0026gt; ── R CMD check results ───────────────────────────────── demo 0.0.0.9000 ──── #\u0026gt; Duration: 8.2s #\u0026gt; #\u0026gt; 0 errors ✔ | 0 warnings ✔ | 0 notes ✔ Testes automatizados Antes de concluir a sessão sobre pacotes, se faz necessária uma breve menção aos testes automatizados. Eles são disponibilizados pelo pacote testthat e permitem que um programador verifique que seu código está atendendo às especificações. Testes unitários garantem que uma alteração pontual em algum ponto do código não vai alterar o comportamento de nenhuma outra parte, já que as outras funções ainda terão que passar nos seus próprios testes.\nPara criar um conjunto de testes é necessário primeiro criar o ambiente para tal dentro do pacote. Depois disso, basta criar conjuntos individuais de testes para cada função.\nusethis::use_testthat() #\u0026gt; ✔ Adding \u0026#39;testthat\u0026#39; to Suggests field in DESCRIPTION #\u0026gt; ✔ Creating \u0026#39;tests/testthat/\u0026#39; #\u0026gt; ✔ Writing \u0026#39;tests/testthat.R\u0026#39; #\u0026gt; ● Call `use_test()` to initialize a basic test file and open it for editing. usethis::use_test(\u0026quot;funcao_demo\u0026quot;) #\u0026gt; ✔ Increasing \u0026#39;testthat\u0026#39; version to \u0026#39;\u0026gt;= 2.1.0\u0026#39; in DESCRIPTION #\u0026gt; ✔ Writing \u0026#39;tests/testthat/test-funcao_demo.R\u0026#39; #\u0026gt; ● Modify \u0026#39;tests/testthat/test-funcao_demo.R\u0026#39; Como é possível notar, o pacote testthat permite criar um arquivo de testes para funcao_demo() (neste caso tests/testthat/test-funcao_demo.R). Esse arquivo já vem com um teste padrão a título de demonstração, mas, depois de reescrito manualmente, um possível conjunto de testes para funcao_demo() seria o seguinte:\nlibrary(demo) test_that(\u0026quot;funcao_demo funciona\u0026quot;, { expect_equal(funcao_demo(1, 2), 3) expect_equal(funcao_demo(-1, -2), -3) expect_equal(funcao_demo(1, -2), -1) expect_output(funcao_demo(1, 2), \u0026quot;3\u0026quot;) }) E o resultado da execução dos testes é o seguinte:\ndevtools::test() #\u0026gt; Loading demo #\u0026gt; Testing demo #\u0026gt; ✔ | OK F W S | Context #\u0026gt; ✔ | 4 | funcao_demo #\u0026gt; #\u0026gt; ══ Results ══════════════════════════════════════════════════════════════════ #\u0026gt; OK: 4 #\u0026gt; Failed: 0 #\u0026gt; Warnings: 0 #\u0026gt; Skipped: 0 #\u0026gt; #\u0026gt; Keep up the good work. ","permalink":"https://blog.curso-r.com/posts/2019-08-07-zen-do-r-6/","tags":["zen-do-r"],"title":"Pacotes (Zen do R parte 6)"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Desenvolver shiny apps é muito divertido, mas dá trabalho. Os apps começam com uma ideia simples, mas vão crescendo até o ponto que não conseguimos mais entender onde estão os pedaços do app.\nPara resolver esse problema, os módulos foram criados. Com módulos, é possível separar pedaços de um shiny em scripts separados, que são adicionados como funções dentro do app principal. Os módulos facilitam muito a manutenção dos apps.\nNo entanto, novos problemas podem surgir em apps complexos. Um módulo pode usar funções de certo pacote, e às vezes esquecemos de checar se ele existe quando o app for colocado em produção. Além disso, como existem vários jeitos de organizar os arquivos que contém os scripts dos módulos dentro do projeto, cada app acaba ficando com uma estrutura diferente.\nPara resolver esse problema, uma alternativa muito útil é desenvolver o shiny dentro de um pacote. Dessa forma, as dependências são checadas automaticamente, os módulos se tornam funções do pacote e tudo deve ficar documentado e organizado por padrão. Eu tenho essa prática desde 2018, quando percebi que meus apps estavam ficando muito complicados de manter.\nO pacote golem é a generalização dessa ideia. O pacote sugere um fluxo de trabalho excelente para desenvolver seu app dentro de um pacote do R. O pacote ainda não está no CRAN, então use com cuidado. Várias coisas podem mudar!\nEu acredito que a melhor documentação possível do golem é a que está na própria página do pacote. Eu simplesmente segui o tutorial indicado e em 5 minutos estava com meu app modular e funcionando perfeitamente. Por isso, vou apenas destacar alguns detalhes que considero importantes e podem acabar despercebidos numa primeira leitura.\nComo o golem trata de shiny apps dentro de pacotes, o ideal é que você tenha algum conhecimento em desenvolvimento de pacotes. Se tiver interesse, o melhor material possível sobre isso é o http://r-pkgs.org.\nComo o golem funciona? O pacote é uma espécie de usethis para shiny apps. Aliás, se você não conhece o usethis, pare tudo o que você está fazendo e acesse essa página. Ao criar um shiny app com o template do golem, você verá uma estrutura de arquivos assim\nDESCRIPTION ¦--dev/ # desenvolvimento ¦--01_start.R ¦--02_dev.R ¦--03_deploy.R ¦--run_dev.R ¦--inst/ # aqui fica o aplicativo ¦--app ¦--server.R ¦--ui.R ¦--www/ ¦--favicon.ico ¦--man/ # documentação das funções/módulos do pacote ¦--run_app.Rd NAMESPACE # informa as funções que o pacote importa e exporta myapp.Rproj # projeto ¦--R/ # aqui ficam os módulos e as funções que geram ui e server ¦--app_server.R ¦--app_ui.R ¦--run_app.R Essa é uma estrutura de pacotes do R, com algumas coisas a mais.\nA pasta dev/, que geralmente não existe em pacotes, contém o tutorial que o usuário deve seguir para fazer o setup e o desenvolvimento do app. O ideal é ler esses scripts e ir rodando tudo com calma, verificando o que cada função altera nos seus arquivos. A paste inst/ é uma pasta especial que é copiada diretamente na pasta do pacote, sem nenhuma modificação. É graças à ela que conseguimos acessar arquivos através da função system.file(), por exemplo. Por isso, é nessa pasta que fica o nosso shiny app de verdade. O mais interessante de um app criado dessa forma é que os arquivos ui.R e server.R ficam praticamente vazios. Isso acontece porque todo o trabalho sujo está sendo feito pelas funções app_ui() e app_server(), que agora são funções documentadas do seu pacote.\nCaracterísticas interessantes Com o golem, é fácil de criar uma imagem Docker do seu app, o que facilita o deploy em escala. Atualmente, estou testando formas de integrar essa funcionalidade com a Google Cloud Platform, e parece bem promissor. O golem te obriga a documentar tudo o que você faz. Isso pode parecer entediante no início, mas se paga no futuro. Existem funções que facilitam a utilização de javascript e css no app, o que geralmente é algo doloroso no desenvolvimento de apps no shiny. Vídeo Montei um vídeo rápido mostrando a utilização do golem na construção de um app simples, colocado no shinyapps.io. Se quiser acessar esse app, o link está na descrição do vídeo!\nO áudio ficou um pouco baixo e o meu computador estava fazendo um pouco de barulho. Mas você pode simplesmente ver sem o áudio, ele nem importa muito.\nÉ isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2019-07-16-golem/","tags":["golem","shiny"],"title":"Shiny Apps Empacotados com o golem"},{"author":["Caio"],"categories":["conceitos"],"contents":" Nesta série de posts, estamos apresentamos a todos a nossa primeira tentativa de escrever um livro: O Zen do R! Durante as próximas semanas, todas as quartas, traremos para o nosso blog os capítulos que já escrevemos do livro e responderemos qualquer pergunta sobre o conteúdo.\nHoje o assunto é ideal para quem precisa escrever código reprodutível: funções e dependências.\nFunções e dependências Até este momento, foi abordada apenas uma forma de organizar os arquivos de uma análise: projetos. Entretanto existe ainda outra maneira, ainda mais interessante, de guardar análises. Se você programou em R, com certeza já se deparou com essa ferramenta, os bons e velhos pacotes ou bibliotecas. É surpreendentemente fácil criar um diretório que pode ser completamente acessado através da função library().\nQuando uma tarefa de análise de dados aumenta em complexidade, o número de funções e arquivos necessários para manter tudo em ordem cresce exponencialmente. Um arquivo para ler os dados, outro para limpar os nomes das colunas, mais um para fazer joins… Cada um deles com incontáveis blocos de código que rapidamente se transformam em uma macarronada.\nO primeiro passo para sair dessa situação é transformar tudo em funções. Essa tarefa está longe de simples, mas os benefícios são imensos; ao encontrar um erro no resultado, fica bem mais fácil depurar a função culpada do que uma coleção desordenada de código. Funções têm argumentos e saídas, enquanto código solto pode modificar globais e criar resultados tardios que são impossíveis de acompanhar sem conhecer profundamente a tarefa sendo realizada.\nlibrary(dplyr) library(tibble) # Limpar dados mtcars_clean \u0026lt;- mtcars %\u0026gt;% rownames_to_column(var = \u0026quot;model\u0026quot;) %\u0026gt;% as_tibble() %\u0026gt;% filter(cyl \u0026lt; 8) # Selecionar carros com 4 cyl e tirar média de mpg e wt mtcars_clean %\u0026gt;% filter(cyl == 4) %\u0026gt;% group_by(cyl) %\u0026gt;% summarise( mpg = mean(mpg), wt = mean(wt) ) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; cyl mpg wt #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 4 26.7 2.29 # Selecionar carros com 6 cyl e tirar média de drat e disp mtcars_clean %\u0026gt;% filter(cyl == 6) %\u0026gt;% group_by(cyl) %\u0026gt;% summarise( drat = mean(drat), disp = mean(disp) ) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; cyl drat disp #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 6 3.59 183. O código acima é somente um exemplo de análise. Como descrito pelos comentários, mtcars é limpa e depois são extraídas as médias de diferentes variáveis para duas seleções da tabela (número de cilindros igual a 4 e 6). Abaixo está descrita uma forma de transformar a maioria deste código em funções. É verdade que pela natureza simples do exemplo, fica difícil ver os benefícios do encapsulamento das tarefas de limpeza e resumo, mas perceba, por exemplo, que, se fosse necessário trocar mean() por median(), antes seria necessário alterar quatro linhas e agora apenas uma. Esse tipo de ganho a longo prazo pode salvar análises inteiras do caos.\nlibrary(dplyr) library(tibble) # Limpa tabela, filtrando cyl \u0026lt; cyl_max clean \u0026lt;- function(data, cyl_max = 8) { data %\u0026gt;% rownames_to_column(var = \u0026quot;model\u0026quot;) %\u0026gt;% as_tibble() %\u0026gt;% filter(cyl \u0026lt; cyl_max) } # Resume tabela onde cyl == cyl_max, tirando média das colunas em ... summarise_cyl \u0026lt;- function(data, cyl_num, ...) { data %\u0026gt;% filter(cyl == cyl_num) %\u0026gt;% group_by(cyl) %\u0026gt;% summarise_at(vars(...), mean) } # 4 cyl, média de mpg e wt mtcars %\u0026gt;% clean(cyl_max = 8) %\u0026gt;% summarise_cyl(cyl_num = 4, mpg, wt) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; cyl mpg wt #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 4 26.7 2.29 # 6 cyl, média de drat e disp mtcars %\u0026gt;% clean(cyl_max = 8) %\u0026gt;% summarise_cyl(cyl_num = 6, drat, disp) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; cyl drat disp #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 6 3.59 183. Um código bem encapsulado reduz a necessidade de objetos intermediários ( base_tratada, base_filtrada, etc.) pois para gerar um deles basta a aplicação de uma função. Além disso, programas com funções normalmente são muito mais enxutos e limpos do que scripts soltos, pois estes estimulam repetição de código. Às vezes é mais rápido copiar e colar um pedaço de código e adaptá-lo ao novo contexto do que criar uma função que generalize a operação desejada para as duas situações, mas os benefícios das funções são de longo prazo: ao encontrar um bug, haverá apenas um lugar para concertar; se surgir a necessidade de modificar uma propriedade, haverá apenas um lugar para editar; se aquele código se tornar obsoleto, haverá apenas um lugar para deletar.\nPense na programação funcional1 como ir à academia. No início o processo é difícil e exige uma quantidade considerável de esforço, mas depois de um tempo se torna um hábito e traz benefícios consideráveis para a saúde (neste caso, do código). As recomendações para quando criar uma nova função ou separar uma função em duas variam muito, mas normalmente é uma boa ideia não deixar uma única função ser encarregada de mais uma tarefa ou ficar longa/complexa demais.\nNo mundo ideal, na pasta R/ do seu projeto haverá uma coleção de arquivos, cada um com uma coleção de funções relacionadas e bem documentadas, e apenas alguns arquivos que utilizam essas funções para realizar a análise em si. Como dito anteriormente, isso fica muito mais fácil se você já tiver esse objetivo em mente desde o momento de criação do novo projeto.\n:: No exemplo da seção anterior, é possível notar as chamadas para as bibliotecas dplyr e tibble. Elas têm inúmeras funções úteis, mas aqui somente algumas poucas foram utilizadas. Além disso, se o código fosse muito maior, ficaria impossível saber de uma biblioteca ainda está sendo utilizada; se não fosse mais necessário utilizar rownames_to_column(), qual seria a melhor forma de saber que pode ser removida a chamada library(tibble)?\nA resposta para essa pergunta pode assustar: no código ideal, a função library() nunca seria chamada, todas as funções teriam seus pacotes de origem explicitamente referenciados pelo operador ::.\nEsta subseção está separada porque ela de fato é um pouco radical demais. É excessivamente preciosista pedir para que qualquer análise em R seja feita sem a invocação de nenhuma biblioteca, apenas com chamadas do tipo biblioteca::funcao(). Muitas pessoas inclusive nem sabem que é possível invocar uma função diretamente através dessa sintaxe!\nSe algum leitor estiver tendendo a seguir o caminho do TOC da programação, existem dois grandes benefícios em chamar todas as funções diretamente: - O código, no total, executa um pouco mais rápido porque são carregadas menos funções no ambiente global (isso é especialmente importante em aplicações interativas feitas em Shiny). - As dependências do código estão sempre atualizadas porque elas estão diretamente atreladas às próprias funções sendo utilizadas.\nExiste um terceiro e importante benefício, mas este será abordado apenas no próximo capítulo. A título de curiosidade, o código anterior ficaria assim caso fosse escrito sem as chamadas para library():\n# Referência ao pipe `%\u0026gt;%` \u0026lt;- magrittr::`%\u0026gt;%` # Limpa tabela, filtrando cyl \u0026lt; cyl_max clean \u0026lt;- function(data, cyl_max = 8) { data %\u0026gt;% tibble::rownames_to_column(var = \u0026quot;model\u0026quot;) %\u0026gt;% dplyr::as_tibble() %\u0026gt;% dplyr::filter(cyl \u0026lt; cyl_max) } # Resume tabela onde cyl == cyl_max, tirando média das colunas em ... summarise_cyl \u0026lt;- function(data, cyl_num, ...) { data %\u0026gt;% dplyr::filter(cyl == cyl_num) %\u0026gt;% dplyr::group_by(cyl) %\u0026gt;% dplyr::summarise_at(dplyr::vars(...), mean) } # 4 cyl, média de mpg e wt mtcars %\u0026gt;% clean(cyl_max = 8) %\u0026gt;% summarise_cyl(cyl_num = 4, mpg, wt) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; cyl mpg wt #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 4 26.7 2.29 # 6 cyl, média de drat e disp mtcars %\u0026gt;% clean(cyl_max = 8) %\u0026gt;% summarise_cyl(cyl_num = 6, drat, disp) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; cyl drat disp #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 6 3.59 183. Se serve de consolo, o RStudio facilita muito esse tipo de programação por causa da sua capacidade de sugerir continuações para código interativamente. Para escrever dplyr::, por exemplo, basta digitar d, p, l e apertar TAB uma vez. Com os ::, as sugestões passarão a ser somente de funções daquele pacote.\nAqui o termo “programação funcional” é usado de forma figurativa. Na computação linguagens denominadas “funcionais” tem um modus operandi bastante específico não abordado neste capítulo.↩︎\n","permalink":"https://blog.curso-r.com/posts/2019-07-30-zen-do-r-5/","tags":["Zen do R"],"title":"Funções e Dependências (Zen do R parte 5)"},{"author":["Caio"],"categories":["conceitos"],"contents":" Nesta série de posts, estamos apresentamos a todos a nossa primeira tentativa de escrever um livro: O Zen do R! Durante as próximas semanas, todas as quartas, traremos para o nosso blog os capítulos que já escrevemos do livro e responderemos qualquer pergunta sobre o conteúdo.\nHoje o assunto é como usar projetos no R para nunca mais perder um arquivo importante.\nGit e GitHub Há poucas coisas mais frustrantes no mundo do que ter que refazer um trabalho. Perder progresso já feito por algum erro ou acidente transforma qualquer pacifista em um vulcão prestes a entrar em erupção. Quando se trata de programação, há várias formas de isso acontecer: um disco rígido que falha, o copo de café derramado no lugar errado, aquela alteração que não pode ser desfeita.\nEste problema está longe de ser novo. Em 2005, Linus Torvalds (o criador do Linux) se deparava com essas questões durante o seu desenvolvimento do kernel de seu sistema operacional. Muitas pessoas contribuindo para um mesmo código, fazendo alterações que deveriam ser revistas e possivelmente revertidas, não é uma tarefa facilmente solucionável com métodos convencionais de armazenamento de arquivos. Com isso em mente, Torvalds criou o sistema de controle de versão distribuído conhecido como Git.\nEm termos leigos, o Git permite gerenciar versões de arquivos texto (outros tipos também são suportados, mas o foco principal é em arquivos de código). Ele não passa de um programa para linha de comando que observa as mudanças nos arquivos de um diretório e vai guardando essas informações para que seja possível reverter qualquer alteração indesejada. O Git também pode se conectar a um serviço de hospedagem e armazenar todas as versões de um código fora do seu computador; o mais utilizado atualmente se chama GitHub.\nNa prática, a utilização do Git e do GitHub tem dois principais benefícios: - Nunca mais precisar controlar versões com analise.R, analise_v2.R, analise_v3.R, analise_final.R, analise_final_final.R, analise_final_revisada.R… - Nunca mais precisar se preocupar em perder seus projetos por causa de falhas no seu computador.\nNada mal para dois serviços gratuitos!\nNo capítulo anterior, é apresentado o conceito de projeto. Agora o segundo passo é entender como esses projetos podem ser utilizados em conjunto com controle de versão para manter seu trabalho sempre sincronizado na nuvem. Criar uma conta no GitHub e instalar o programa git no seu computador são necessários para poder utilizar os recursos descritos a seguir. A partir daqui assume-se que ambos os requisitos foram cumpridos.\nPara permitir que os comandos do R acessem a sua conta do GitHub, é essencial criar um Personal Access Token (PAT). Tendo logado no GitHub, clique na sua imagem no canto direito superior e siga para Settings \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new token. Nesta página basta descrever o seu uso para o token e selecionar o primeiro box de todos; por fim, gere e copie o seu token (uma sequência com uns 40 letras e números). Se você estiver sem nenhuma paciência, execute o comando abaixo:\nusethis::browse_github_token() #\u0026gt; ✔ Opening URL \u0026#39;https://github.com/settings/tokens/new?scopes=repo,gist\u0026amp;description=R:GITHUB_PAT\u0026#39; #\u0026gt; ● Call `usethis::edit_r_environ()` to open \u0026#39;.Renviron\u0026#39;. #\u0026gt; ● Store your PAT with a line like: #\u0026gt; GITHUB_PAT=xxxyyyzzz #\u0026gt; ● Make sure \u0026#39;.Renviron\u0026#39; ends with a newline! Independentemente de como um PAT foi gerado, o importante é seguir as instruções descritas no comando. Execute a função usethis::edit_r_environ() e crie uma nova linha na forma GITHUB_PAT={SEU_TOKEN}. Assim que isso estiver feito, você não precisará mais se preocupar com nenhum tipo de configuração.\nAgora, ao criar um novo projeto, é possível associar imediatamente a ele um repositório no GitHub. O comando para criar projetos não muda, mas torna-se possível usar dois outros comando para associar aquela pasta com o sistema de controle de versões.\nusethis::create_project(\u0026quot;~/Documents/demo\u0026quot;) #\u0026gt; ✔ Creating \u0026#39;~/Documents/demo/\u0026#39; #\u0026gt; ✔ Setting active project to \u0026#39;~/Documents/demo\u0026#39; #\u0026gt; ✔ Creating \u0026#39;R/\u0026#39; #\u0026gt; ✔ Writing \u0026#39;demo.Rproj\u0026#39; #\u0026gt; ✔ Adding \u0026#39;.Rproj.user\u0026#39; to \u0026#39;.gitignore\u0026#39; #\u0026gt; ✔ Opening \u0026#39;~/Documents/demo/\u0026#39; in new RStudio session #\u0026gt; ✔ Setting active project to \u0026#39;demo\u0026#39; # No console do novo projeto usethis::use_git() #\u0026gt; ✔ Setting active project to \u0026#39;~/Documents/demo\u0026#39; #\u0026gt; ✔ Initialising Git repo #\u0026gt; ✔ Adding \u0026#39;.Rhistory\u0026#39;, \u0026#39;.RData\u0026#39; to \u0026#39;.gitignore\u0026#39; #\u0026gt; There are 2 uncommitted files: #\u0026gt; * \u0026#39;.gitignore\u0026#39; #\u0026gt; * \u0026#39;demo.Rproj\u0026#39; #\u0026gt; Is it ok to commit them? #\u0026gt; #\u0026gt; 1: Negative #\u0026gt; 2: Not now #\u0026gt; 3: I agree #\u0026gt; #\u0026gt; Selection: 3 #\u0026gt; ✔ Adding files #\u0026gt; ✔ Commit with message \u0026#39;Initial commit\u0026#39; #\u0026gt; ● A restart of RStudio is required to activate the Git pane #\u0026gt; Restart now? #\u0026gt; #\u0026gt; 1: Absolutely not #\u0026gt; 2: No way #\u0026gt; 3: Yes #\u0026gt; #\u0026gt; Selection: 3 usethis::use_github() #\u0026gt; ✔ Checking that current branch is \u0026#39;master\u0026#39; #\u0026gt; Which git protocol to use? (enter 0 to exit) #\u0026gt; #\u0026gt; 1: ssh \u0026lt;-- presumes that you have set up ssh keys #\u0026gt; 2: https \u0026lt;-- choose this if you don\u0026#39;t have ssh keys (or don\u0026#39;t know if you do) #\u0026gt; #\u0026gt; Selection: 2 #\u0026gt; ● Check title and description #\u0026gt; Name: demo #\u0026gt; Description: #\u0026gt; Are title and description ok? #\u0026gt; #\u0026gt; 1: Nope #\u0026gt; 2: No way #\u0026gt; 3: Yup #\u0026gt; #\u0026gt; Selection: 3 #\u0026gt; ✔ Creating GitHub repository #\u0026gt; ✔ Setting remote \u0026#39;origin\u0026#39; to \u0026#39;https://github.com/curso-r/demo.git\u0026#39; #\u0026gt; ✔ Pushing \u0026#39;master\u0026#39; branch to GitHub and setting remote tracking branch #\u0026gt; ✔ Opening URL \u0026#39;https://github.com/curso-r/demo\u0026#39; Depois de ter executado estes dois novos comandos, será inicializada uma nova aba no RStudio denominada Git. Através dela é possível controlar todas as versões do código e enviá-las ao GitHub para que sejam armazenadas com segurança na nuvem.\nPara testar se está tudo funcionando, crie um arquivo na pasta R/ e abra a aba Git. Clique na caixa em branco que lá se encontra, aperte o botão Commit (escreva uma mensagem que descreva aquilo que você fez) e então aperte o botão Push. Em poucos segundos o repositório deve ser atualizado na sua página correspondente no GitHub.\nExplicar todas as funcionalidades do Git e do GitHub estão além do escopo deste material. Os botões mais importantes dessa aba são as checkboxes, o Commit e o Push. Para saber mais sobre esse assunto, recomendo o livro Happy Git with R (especialmente este capítulo) que aborda em detalhes minuciosos todo o processo e uso e manutenção do Git com o RStudio.\n","permalink":"https://blog.curso-r.com/posts/2019-07-23-zen-do-r-4/","tags":["zen do R","github"],"title":"Git e GitHub (Zen do R parte 4)"},{"author":["Caio"],"categories":["conceitos"],"contents":" Nesta série de posts, estamos apresentamos a todos a nossa primeira tentativa de escrever um livro: O Zen do R! Durante as próximas semanas, todas as quartas, traremos para o nosso blog os capítulos que já escrevemos do livro e responderemos qualquer pergunta sobre o conteúdo.\nHoje o assunto é como usar projetos no R para nunca mais perder um arquivo importante.\nRproj e diretórios Um programador iniciante corre o risco de não gerenciar seus projetos. Muitas vezes seus arquivos de código ficarão espalhados pelos infinitos diretórios de seu computador, esperando a primeira oportunidade de sumir para sempre. No R isso não é diferente: organizar arquivos é uma parte integral do processo de programação.\nFelizmente o RStudio possui uma ferramenta incrível que auxilia na tarefa de consolidar todos os recursos necessários para uma análise. Denominados “projetos”, eles não passam de pastas comuns com um arquivo .Rproj.\nO primeiro passo para organizar um bom ambiente de trabalho para análises de dados é criar um diretório onde todos os seus programas R podem viver. No meu computador eu tenho a pasta ~/Documents/Dev/R/, mas não importa onde ela está, apenas que seja um lugar o qual você lembre sem dificuldades. Tendo criado um esse workspace, é importante registrá-lo como o seu ambiente de trabalho no RStudio: basta selecioná-lo em Tools \u0026gt; Global Options… \u0026gt; Browse.\nDesta forma, toda vez que você iniciar um novo projeto no R, ele por parão usará essa nova pasta como ambiente padrão! Se você já tiver alguns arquivos espalhados pelo seu computador, traga eles para o seu diretório de programas.\nO segundo passo no processo de organização dos seus projetos é um pouco mais complexo e demanda mais atenção. Não basta juntar todos os arquivos em um só lugar, é importante colocá-los em subdiretórios para que a sua pasta não vire um equivalente virtual a uma mesa desorganizada. Assim como em uma mesa cada papel e cada utensílio tem uma gaveta, cada arquivo precisa fazer parte de um projeto.\nÉ nesse ponto que os “projetos” do RStudio dialogam com os projetos da vida real. Em uma empresa, cada cliente é um projeto; na academia, cada pesquisa é um projeto; e assim por diante. Cada projeto seu deve ter a sua própria pasta para que seja fácil encontrar todos os códigos e dados pertencentes a um único assunto. Mas esta não deve ser uma pasta comum, ela deve ser um projeto.\nO código listado abaixo demonstra como criar um projeto no RStudio. Basta apenas um comando e ele já fará tudo que for necessário para preparar o seu ambiente de desenvolvimento.\nusethis::create_project(\u0026quot;~/Documents/Dev/R/Proj/\u0026quot;) #\u0026gt; ✔ Creating \u0026#39;~/Documents/Dev/R/Proj/\u0026#39; #\u0026gt; ✔ Setting active project to \u0026#39;~/Documents/Dev/R/Proj\u0026#39; #\u0026gt; ✔ Creating \u0026#39;R/\u0026#39; #\u0026gt; ✔ Writing \u0026#39;Proj.Rproj\u0026#39; #\u0026gt; ✔ Adding \u0026#39;.Rproj.user\u0026#39; to \u0026#39;.gitignore\u0026#39; #\u0026gt; ✔ Opening \u0026#39;~/Documents/Dev/R/Proj/\u0026#39; in new RStudio session #\u0026gt; ✔ Setting active project to \u0026#39;Proj\u0026#39; Cada linha da saída do comando representa algo que ele fez para preparar o projeto. A mais importante é a quarta linha, que cria o arquivo Proj.Rproj; ele indica para o RStudio que aquele diretório será a raiz de um projeto e que, portanto, várias outras funcionalidades podem ser ativadas. Por exemplo, clicando duas vezes neste arquivo já carrega o RStudio com os arquivos de Proj.\nImportante também é a pasta R/ criada. Nela você deve colocar todos os seus arquivos de código referentes àquele projeto com nomes que descrevam bem o que cada um faz. Se você seguiu o conselho anterior e juntou todos os seus códigos no seu diretório de trabalho, crie um projeto novo para cada grupo de programas que vocẽ tiver detectado. Talvez um para exercícios de R, um para cada cliente, um para uma nova ideia, etc. Cada um deles deve ter um nome descritivo e conter, em sua pasta R, todos os arquivos necessários para aquela análise.\nDiretório de trabalho Mas a funcionalidade mais importante dentre todas as já citadas é o conceito do working directory ou diretório de trabalho. No canto esquerdo superior do Console do RStudio existe um caminho denominado diretório de trabalho, que é essencialmente a raiz do seu projeto. Muitos programadores que aprenderam R há muito tempo conhecem uma função chamada setwd(); se você nunca ouviu falar disso, não se preocupe e continue assim, mas se você costuma usá-la, siga prestando atenção.\nO R dá a possibilidade de mudar, instantaneamente, o diretório de trabalho. Isso que dizer que os caminhos relativos para arquivos podem mudar em questão de linhas. O exemplo abaixo demonstra superficialmente como isso funciona:\n# Abrindo dois arquivos em diretórios diferentes setwd(\u0026quot;~/Downloads\u0026quot;) a \u0026lt;- read.csv(\u0026quot;a.csv\u0026quot;) setwd(\u0026quot;~/Documents/Dev/R/Proj\u0026quot;) b \u0026lt;- read.csv(\u0026quot;b.csv\u0026quot;) write.csv(a, \u0026quot;c.csv\u0026quot;) Isso não parece tão problemático à primeira vista, mas usar setwd() cria um vício difícil de abandonar. Essa função estimula que os seus projetos continuem desorganizados com arquivos espalhados pelo computador, mas também confunde o programador na hora de salvar arquivos. Onde será salvo o arquivo c.csv? De onde veio o arquivo a.csv ou de onde veio o b.csv? E se essa linha fosse mudada de lugar para antes do segundo setwd()? São perguntas difíceis de responder caso você não esteja atento ao código todo.\nA solução que os projetos oferecem para isso é fazer com que o diretório de trabalho seja sempre a pasta do projeto. Neste caso é como se, ao abrir o RStudio, ele executasse o comando setwd(\"~/Documents/Dev/R/Proj\") automaticamente. Mas como ler então o arquivo a.csv?\n# Duas formas de ler a.csv a \u0026lt;- read.csv(\u0026quot;~/Downloads/a.csv\u0026quot;) file.copy(\u0026quot;~/Downloads/a.csv\u0026quot;, \u0026quot;a.csv\u0026quot;) a \u0026lt;- read.csv(\u0026quot;a.csv\u0026quot;) A primeira forma deixa explícito que aquele arquivo não faz parte do projeto e que portanto deve ser tratado como temporário. A segunda forma, mais indicada, é trazer o arquivo para dentro do projeto! Se ele é importante, é essencial que ele esteja junto com todos os outros dados de Proj. Com o código acima, o comando write.csv(a, \"c.csv\") salvaria c.csv dentro do projeto sem sombra de dúvidas.\nOs principais benefícios de não usar setwd() são dois: saber sempre onde os arquivos utilizados estão/serão salvos e poder compartilhar um projeto com qualquer pessoa. setwd() depende que seja explicitado um caminho dentro do seu computador e isso nem sempre é verdade no computador de outra pessoa; fazendo com que todos os arquivos estejam no projeto e com caminhos relativos nos códigos permite que outro usuário replique a sua análise sem ter que modificar nem uma linha do programa.\n","permalink":"https://blog.curso-r.com/posts/2019-07-16-zen-do-r-3/","tags":["zen do R"],"title":"Rproj e diretórios (Zen do R parte 3)"},{"author":["Julio"],"categories":["divulgação"],"contents":" Entre os dias 8 e 12 de julho de 2019 aconteceu a useR, em Toulouse, França. A useR é um evento bem tradicional e ocorre desde o ano de 2004. Tive a felicidade de participar pela primeira vez esse ano, juntamente com William Amorim, Daniel Falbel, Beatriz Milz e Angélica Custódio. Também encontrei o professor Elias Krainski, da UFPR, que nos introduziu à professora Laurie Baker, que também andou com a gente durante o evento.\nSe eu pudesse resumir minha experiência a uma palavra, seria pertencimento. Sinto que a comunidade do R é verdadeiramente inclusiva e se esforça o máximo possível para que as pessoas de todos os lugares do mundo, de todas as culturas, sintam-se bem vindas e relevantes para produção do conhecimento científico. Essa sensação de pertencimento é tão importante quanto os avanços técnicos – que com certeza foram mostrados, e foram incríveis – pois potencializa a produção de soluções e incentiva a colaboração, dentre muitas outras vantagens. É óbvio que ainda temos um caminho enorme a seguir nesse sentido, mas acredito que estamos indo muito bem.\nHighlights de cada dia Dia -1: Tidy dev day! Já pensou em sentar com o Hadley Wickham, a Jenny Bryan e outros para ajudar a resolver issues do dplyr e do ggplot2? Bom, o tidy dev day é exatamente isso!\nInfelizmente eu não participei do evento, então não tenho muito para escrever. O que sei é que as pessoas colocaram post-its na lousa para as pessoas irem pegando e fazendo pull requests nas issues dos pacotes do tidyverse.\nQuem sabe a Bea ou a Angélica explicam melhor pra gente!\nFigura 1: Bea e Angélica no tidy dev day. Dia 0: Cursos! Também não participei dos cursos, mas não importa muito, pois vi todos os materiais apresentados ao mesmo tempo que meus amigos. Todos os cursos estão abertos por aí, basta fuçar no twitter e entrar no github dos autores.\nE como nossa comunidade é maravilhosa, é claro que alguém já criou um repositório para colocar todos os links:\nhttps://github.com/sowla/useR2019-materials/\nDia 1: Uma palestra cheia de \u0026lt;3 O que dizer da palestra da Julia Stewart Lowndes? Acho que a palavra é: inspiradora. Ela mostrou como fazer ciência aberta potencializa os trabalhos e gera resultados efetivos. E mostrou como o ecossistema do R é essencial para isso. E sério, usar desenhos com o tema Star Wars foi uma ideia genial.\nFigura 2: Que vontade de abraçar esses slides. Figura 3: Que vontade de abraçar esses slides. Dia 2: shiny, ensino e comunidade No dia 2, ficou claro o impacto do shiny na comunidade R. Muitas apresentações corporativas sobre como usar o shiny de forma escalável, como monitorar desempenho etc. Algumas pessoas chegaram até a comentar que a quantidade de falas sobre shiny foi excessiva, até porque a useR é, tradicionalmente, um evento acadêmico. Eu não pude sentir isso pois o shiny é uma parte essencial do meu trabalho, mas creio que essa seja uma possível crítica construtiva ao evento.\nOutra coisa que ficou clara é como a comunidade R quer ajudar o resto do mundo a entrar na ciência de dados. É muito bonito. Ver a apresentação do Colin Rundel sobre o pacote ghclass, bem como diversos case studies sobre ensino de R em comunidades diversas e outras ferramentas e desafios para ensinar pessoas de todas as idades renovou meu compromisso com a missão da Curso-R, que é descomplicar a ciência de dados aqui no Brasil.\nDestaque para a palestra da Bea Milz, feita em conjunto com a Bruna Wundervald, sobre a experiência do RLadies no capítulo de São Paulo. Foi muito tocante, e nós da Curso-R ficamos muito felizes com o slide de agradecimento \u0026lt;3\nFigura 4: Palestra da Bea Milz sobre o RLadies São Paulo. Dia 3: mais shiny, vroom, auth0! A palestra do Joe Cheng foi excelente. É muito legal ver como ele se mantém humilde sendo um dos fundadores da RStudio e criador do shiny. Ele falou sobre o pacote shinymeta, ainda em construção, para adicionar a funcionalidade de gerar relatórios reprodutíveis diretamente do shiny. O mais legal foi ver o processo de desenvolvimento dele, que mostra que mesmo os grandes programadores – talvez principalmente? – sofrem ao não saber como fazer/planejar as soluções e pedem ajuda da comunidade para conseguir encontrar um caminho.\nOutro highlight do dia foi a apresentação do pacote vroom, pelo Jim Hester. Trata-se de um pacote que lê arquivos de texto muito rápido, ganhando muitas vezes do próprio data.table::fread(). O mais legal foi ver a cara de incredulidade de todos que estavam assistindo, pois antes isso nem parecia possível. Claro que os benchmarks do vroom precisam ser olhados com cuidado, pois é um sistema de leitura preguiçoso, diferente do readr e do data.table. Mas mesmo assim parece que o vroom, em média, é mais eficiente que os concorrentes.\nE fala sério, olha que capa legal dos slides\nFigura 5: Life’s too short to read slow. Nesse dia eu também apresentei sobre o auth0, um pacote que fizemos para facilitar a vida de quem quer adicionar autenticação em shiny apps. Foi uma experiência muito legal! Apesar de serem apenas 5 minutos e do meu inglês ruim, acho que deu para passar a ideia geral do pacote e tenho certeza de que mais pessoas vão utilizar!\nFigura 6: Eu falando sobre o auth0, provavelmente a palavra ‘Awesome’, pois disseram que eu abusei dessa palavra na fala. Pacotes que definitivamente quero estudar Tentei separar uma lista pessoal de coisas que definitivamente quero estudar mais e contribuir nos próximos tempos.\nTidyverse Pacote vroom: O autor do pacote, Jim Hester, me convenceu de que o vroom é real deal. Pacote usethis: O filhinho da Jenny Bryan é realmente muito legal. É um dos meus pacotes preferidos. Pacote rray: Pacote mega interessante do Davis Vaughan para tratar arrays no R de uma forma mais intuitiva. Definitivamente vale tentar! Workflow, ensino e comunicação Pacote remedy: com esse pacote, podemos adicionar diversos atalhos no RStudio para acelerar a escrita de documentos RMarkdown. Farei um post sobre esse pacote em breve. Pacote ghactions: é uma espécie de generalização do Travis CI, aplicado diretamente e gratuitamente pelo GitHub. Eu tenho até dificuldade de pensar nas possibilidades desse pacote, que são muitas! Pacote crrri: esse pacote permite abrir o chrome dentro do RStudio, o que pode tornar a experiência de desenvolver web scrapers muito mais fluida e interativa. Vale à pena tentar. Pacote ghclass: achei esse pacote interessante para ser usado em salas de aula. Shiny: Pacote shinyEventLogger: Esse pacote pode ser utilizado para verificar o que o seu shiny está fazendo e otimizar nas partes que ele está lento. Com um pouco de programação ele pode ser muito útil para apps complexos. Pacote shinymeta: O pacote do Joe Cheng parece realmente muito útil para gerar relatórios reprodutíveis a partir dos apps. Provavelmente esse é o melhor jeito de fazer isso, e resolve um problema bem difícil, que supera diversas ferramentas de BI. Ainda está incipiente, mas quando ficar maduro, será um diferencial absurdo do shiny. Pacote golem: O golem é uma forma estruturada de organizar apps do shiny na forma de pacotes. É uma espécie de usethis para shiny. Antes da useR, eu achava o golem bacana. Agora eu acho essencial. Extra Outra coisa legal da useR é que as famosas da comunidade estão a uma cutucada de distância. Se você gosta de tietar seus ídolos, assim como eu, deveria fazer isso pelo menos uma vez. Tirei foto com o Hadley, falei com a filha do Romain François, troquei ideia com o Dirk Eddelbuetel e outras pessoas da comunidade que parecem tão distantes, mas são tão próximas.\nPor isso, vou encerrar meu post com algumas fotinhos legais\nFigura 7: Fiquei tão nervoso na foto com o Hadley que coloquei o copo na frente da câmera. Figura 8: Depois pedi para tirar foto com ele e tampei o William, além de ter ficado com um sorriso horrível. O Hadley, obviamente, estava ótimo. Olha esses óculos, muito estilosos! Figura 9: Lexie, filha do Romain François (o do cabelo colorido), brincando com o Daniel Falbel. Coisa mais fofa! Figura 10: Playmobil com um pipe no escudo, da ThinkR Figura 11: Turma dos BR (+Laurie) na useR! É isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2019-07-14-user2019/","tags":["comunidade","eventos"],"title":"Minha experiência na useR!2019"},{"author":["Caio"],"categories":["conceitos"],"contents":" Nesta série de posts, estamos apresentamos a todos a nossa primeira tentativa de escrever um livro: O Zen do R! Durante as próximas semanas, todas as quartas, traremos para o nosso blog os capítulos que já escrevemos do livro e responderemos qualquer pergunta sobre o conteúdo.\nHoje o assunto é “.RData e .Rhistory”, como eles funcionam e porque você não deveria usá-los.\n.RData e .Rhistory O fluxo ideal de análise de dados começa na escolha da ferramenta. Por ser uma linguagem especializada em estatística, o R é a primeira escolha de muitos usuários. Normalmente optar por programar em R também implica na escolha de uma IDE (Integrated Development Environment) que, em 90%1 dos casos, será o RStudio.\nO R, em combinação com o RStudio, possui um conjunto de funcionalidades cuja intenção é ajudar no processo de desenvolvimento. Entretanto, isso acaba deixando os programadores de R mal acostumados.\nComo um pai coruja, o RStudio faz questão de lembrar tudo o que você fez anteriormente. Em sua configuração padrão, a IDE manterá na “memória” todos os últimos comandos executados, todos os dados utilizados e todos os objetos criados. Ao fechar e abrir o RStudio, essas informações serão recarregadas na memória como se o usuário nunca tivesse saído do programa.\nEsse recurso é tornado possível pela criação de dois arquivos ocultos: .RData e .Rhistory. O primeiro abriga absolutamente todos os objetos criados por uma sessão R, enquanto o segundo contém uma lista com os últimos comandos executados. Ao reabrir o RStudio, o conteúdo armazenados nestes arquivos será carregado no ambiente de trabalho atual como se nada tivesse acontecido.\nPorque desistir desse recurso Apesar de ser uma ótima conveniência, assim como o pai coruja, esse tipo de funcionalidade pode deixar o programador mal acostumado. Se todos os resultados parciais de uma análise estiverem disponíveis a qualquer momento, diminui o incentivo para a escrita de código reprodutível e, se todo o histórico de comandos for acessível, acaba a necessidade de experimentos controlados.\nUm usuário que dependa ativamente do .RData para recuperar seus dados estará aos poucos contando cada vez mais com a sorte. Caso ele acidentalmente sobrescreva o objeto relevante e o código para recriá-lo já tenha sido apagado, não haverá nenhuma forma confiável de recuperar esses dados. Idealmente, todo o código necessário para uma análise de dados deve estar salvo em um arquivo .R perfeitamente reprodutível; assim, caso o programador cometa um engado, é possível executar aquele arquivo do início e obter novamente os objetos que estavam sendo utilizados.\nArquivos reprodutíveis também tem uma outra vantagem: facilidade de compartilhamento. A menos que o programador pretenda sentar com seu colega para explicar como utilizar os objetos do .RData e do .Rhistory, não pode-se esperar que outra pessoa seja capaz de reproduzir uma análise a partir de arquivos incompletos. Deste modo, abandonar essa funcionalidade permite utilizar ferramentas de compartilhamento e controle de versão da maneira como elas foram idealizadas.\nPor fim, é importante notar uma desvantagem sutil mas muito relevante do uso do .RData. O R trata todos os objetos guardados na memória igualmente, sem levar em conta sua utilidade ou tamanho. Isso significa que ele também irá armazenar nos arquivos ocultos todas as bases de dados da sessão (não importando quão grande sejam). Isso faz com que o .RData normalmente seja um arquivo de múltiplos gigabytes.\nAo reabrir o RStudio, todos esses dados serão recarregados e provavelmente farão com que o programador espere vários minutos até que ele possa voltar ao seu trabalho. Com o .RData é impossível ter controle sobre quais dados devem ser utilizados em cada sessão de programação.\nComo desativar essa funcionalidade O processo de desabilitar o .RData e o .Rhistory é bastante simples e afeta todos os projetos do computador, então só é necessário passar por ele uma vez. Basta selecionar Tools \u0026gt; Global Options… na aba de ferramentas do RStudio e então ajustar três configurações. No final a página de opções gerais deve ficar similar à da imagem abaixo:\nSe acostumar com sessões efêmeras não é uma tarefa fácil e um desconforto inicial é esperado. Pode ser que o programador ache entediante executar o mesmo código toda vez que abrir seu RStudio, mas é importante ter em mente que esse programa só está lá e pode ser executado inúmeras vezes porque o R não estará mais fazendo o trabalho do analista.\nNão tenho nenhuma estatística confiável sobre esse número, mas sei que ele não é 100% porque conheço pelo menos uma pessoa que programa R no neovim.↩︎\n","permalink":"https://blog.curso-r.com/posts/2019-07-10-zen-do-r-2/","tags":["zen do R"],"title":".RData e .Rhistory (Zen do R parte 2)"},{"author":["Caio"],"categories":["Tutoriais"],"contents":"O objetivo principal do pacote Kuber é ajudar com computações massivamente paralelas. Ele usa o kubernetes e o docker de modo a criar um contêiner que automaticamente executa tarefas em paralelo via expansão. Se você já usa o Google Cloud Platform, o Kuber também consegue automaticamente criar clusters, executar computações e gerenciar o seu progresso com o Google cloud SDK.\nSe você nunca ouviu falar sobre orquestração de contêineres, armazenamento persistente na nuvem ou computação paralela, pode ser que esse tutorial pareça avançado demais. Você não precisa ser nenhum especialista nesses assuntos, mas ajuda pelo menos saber o que significam esses termos.\nEsse tutorial vai te ajudar a criar a sua primeira tarefa do Kuber. Antes de começar, certifique-se de que você instalou todos os requisitos corretamente com a vignette \u0026ldquo;Getting started\u0026rdquo;\nA tarefa em si A principal vantagem do Kuber em relação a outros pacotes de paralelização (como Parallel ou Future/Furrr) é que ele automaticamente cria um cluster de computadores que executa a sua tarefa via orquestração de contêineres. Isso pode ser muito útil para web scraping, por exemplo, porque (1) cada máquina tem um IP diferente, (2) salvar os HTMLs raspados é fácil com o Google Cloud Storage e (3) o processo pode ser facilmente ativado/desativado a qualquer momento.\nNeste tutorial a função a ser paralelizada é a seguinte:\n# Scrapear um vetor de caracteres de URLs scrape_urls \u0026lt;- function(urls) { # Criar um diretório dir \u0026lt;- fs::dir_create(\u0026#34;scraped\u0026#34;) # Iterar nos URLs paths \u0026lt;- c() for (url in urls) { path \u0026lt;- paste0(dir, \u0026#34;/\u0026#34;, stringr::str_remove_all(url, \u0026#34;[^a-z]\u0026#34;), \u0026#34;.html\u0026#34;) paths \u0026lt;- append(paths, path) httr::GET(url, httr::write_disk(path, overwrite = TRUE)) } return(paths) } Em suma, essa função recebe um vetor de URLs, os raspa e salva os HTMLs resultantes em um diretório local.\nCriando o cluster Agora para o Kuber. Se tudo estiver instalado corretamente, você deveria ser capaz de criar um cluster simples com o seguinte comando:\nlibrary(kuber) kub_create_cluster(\u0026#34;toy-cluster\u0026#34;, machine_type = \u0026#34;f1-micro\u0026#34;) #\u0026gt; ✔ Creating cluster Vá para o Kubernetes console para ver se tudo funcionou corretamente. Não se preocupe se você receber um monte de alertas, a maioria deles é referente à versão do SDK.\nCriando a tarefa A função mais importante do Kuber provavelmente é a próxima. Ela cria um diretório na sua máquina local que descreve a computação paralela e seus cluster, pacote, imagem e conta de serviço. Para executar o comando abaixo, apenas toy-key.json (a chave da conta de serviço baixada na vignette \u0026ldquo;Getting started\u0026rdquo;) já precisa existir no caminho indicado; o resto é todo criado para você.\nkub_create_task(\u0026#34;~/toy-dir\u0026#34;, \u0026#34;toy-cluster\u0026#34;, \u0026#34;toy-bucket\u0026#34;, \u0026#34;toy-image\u0026#34;, \u0026#34;~/toy-key.json\u0026#34;) #\u0026gt; ✔ Fetching cluster information #\u0026gt; ✔ Fetching bucket information #\u0026gt; ✔ Creating bucket #\u0026gt; ● Edit `~/toy-dir/exec.R` #\u0026gt; ● Create `~/toy-dir/list.rds` with usable parameters #\u0026gt; ● Run `kub_push_task(\u0026#34;~/toy-dir\u0026#34;)` Editando o exec.R e o list.rds O diretório criado por kub_create_task() tem alguns arquivos que são explorados em detalhe na documentação da própria função, mas os dois mais importantes são exec.R e list.rds. O primeiro contém o arquivo R a ser executado pela imagem docker, enquanto o segundo tem todos os objetos que cada máquina precisa para rodar o seu próprio exec.R.\nComeçando pelo exec.R, o arquivo já está populado com um exemplo simples:\n#!/usr/bin/env Rscript args \u0026lt;- commandArgs(trailingOnly = TRUE) # Arguments idx \u0026lt;- as.numeric(args[1]) bucket \u0026lt;- as.character(args[2]) # Use this function to save your results save_path \u0026lt;- function(path) { system(paste0(\u0026#34;gsutil cp -r \u0026#34;, file_, \u0026#34; gs://\u0026#34;, bucket, \u0026#34;/\u0026#34;, gsub(\u0026#34;/.+\u0026#34;, \u0026#34;\u0026#34;, file_))) do.call(file.remove, list(list.files(path, full.names = TRUE))) return(path) } # Get object passed in list[[idx]] obj \u0026lt;- readRDS(\u0026#34;list.rds\u0026#34;)[[idx]] ########################### ## INSERT YOUR CODE HERE ## ########################### Como você pode ver, é um Rscript que recebe dois argumentos: um índice e o nome de um bucket GCS. O código em seguida descreve uma função a ser usada quando salvando os resultados; ela envia o arquivo/diretório no path para o bucket especificado e então o deleta do disco da máquina. Finalmente, o script lê list.rds e seleciona o objeto guardado no índice idx.\nAgora é hora de adicionar scrape_urls() para o arquivo. Não há muitas mudanças na função em si, apenas em como os arquivos resultantes são gerenciados. Aqui está a versão final do exec.R:\n#!/usr/bin/env Rscript args \u0026lt;- commandArgs(trailingOnly = TRUE) # Arguments idx \u0026lt;- as.numeric(args[1]) bucket \u0026lt;- as.character(args[2]) # Use this function to save your results save_path \u0026lt;- function(path) { system(paste0(\u0026#34;gsutil cp -r \u0026#34;, file_, \u0026#34; gs://\u0026#34;, bucket, \u0026#34;/\u0026#34;, gsub(\u0026#34;/.+\u0026#34;, \u0026#34;\u0026#34;, file_))) do.call(file.remove, list(list.files(path, full.names = TRUE))) return(path) } # Get object passed in list[[idx]] obj \u0026lt;- readRDS(\u0026#34;list.rds\u0026#34;)[[idx]] # Scrapear um vetor de caracteres de URLs scrape_urls \u0026lt;- function(urls) { # Criar um diretório dir \u0026lt;- fs::dir_create(\u0026#34;scraped\u0026#34;) # Iterar nos URLs paths \u0026lt;- c() for (url in urls) { path \u0026lt;- paste0(dir, \u0026#34;/\u0026#34;, stringr::str_remove_all(url, \u0026#34;[^a-z]\u0026#34;), \u0026#34;.html\u0026#34;) paths \u0026lt;- append(paths, path) httr::GET(url, httr::write_disk(path, overwrite = TRUE)) } return(paths) } # Rodar o scraper paths \u0026lt;- scrape_urls(obj) # Salvar os HTMLs no GCS for (path in paths) { save_path(path) } Como você já pode imaginar pelos chamados acima, obj contém os URLs a serem raspados. Isso faz sentido porque, como descrito anteriormente, list.rds tem todo objeto que cada máquina precisa para seu próprio exec.R; neste caso, cada máquina precisa de um vetor de URLs a serem scrapeados e idx é simplesmente o índice de cada máquina (para que duas máquinas nunca raspem os mesmos URLs). É só isso.\nAgora a única coisa que falta é criar o list.rds, ou seja, a lista de URLs quebrada em um bloco para cada máquina. Como neste exemplo toy-cluster foi criado com o número padrão de máquinas (3), list.rds vai ser uma lista com 3 elementos. Os comandos a seguir devem ser rodados na sua máquina local:\n# URLs a serem raspados, blocados por máquina url_list \u0026lt;- list( c(\u0026#34;google.com\u0026#34;, \u0026#34;duckduckgo.com\u0026#34;), c(\u0026#34;wikipedia.org\u0026#34;), c(\u0026#34;facebook.com\u0026#34;, \u0026#34;twitter.com\u0026#34;, \u0026#34;instagram.com\u0026#34;) ) # Sobrescrever o list.rds com a lista de URLs readr::write_rds(url_list, \u0026#34;~/toy-dir/list.rds\u0026#34;) Com esse list.rds, o primeiro nó vai raspar motores de busca, o segundo vai raspar a Wikipédia e o terceiro vai raspar mídias sociais.\nDando push e executando a tarefa Por último mas não menos importante, a tarefa deve ser pushada para o Google Container Registry (GCR), que é onde as imagens docker do Kuber vão ficar guardadas. Isso garante controle de versão para todas as tarefas e permitem que elas sejam executadas de outro computador, mas pode levar um bom tempo para rodar da primeira vez que você cria uma tarefa.\nkub_push_task(\u0026#34;~/toy-dir\u0026#34;) #\u0026gt; ✔ Building image #\u0026gt; ✔ Authenticating #\u0026gt; ✔ Pushing image #\u0026gt; ✔ Removing old jobs #\u0026gt; ✔ Creating new jobs Se tudo funcionou até agora, o último comando obrigatório é executar a tarefa:\nkub_run_task(\u0026#34;~/toy-dir\u0026#34;) #\u0026gt; ✔ Authenticating #\u0026gt; ✔ Setting cluster context #\u0026gt; ✔ Creating jobs #\u0026gt; ● Run `kub_list_pods()` to follow up on the pods Gerenciando o progresso da tarefa A duas formas principais de gerenciar o progresso de uma tarefa: listando os pods atualmente ativos e listando os arquivos guardados em um bucket. As letras estranhas no nome de cada processo é um identificador único gerado pelo Kuber para gerenciar aqueles pods.\nkub_list_pods(\u0026#34;~/toy-dir\u0026#34;) #\u0026gt; ✔ Setting cluster context #\u0026gt; ✔ Fetching pods #\u0026gt; NAME READY STATUS RESTARTS AGE #\u0026gt; 1 process-mkewsr-item-1-8kpg7 1/1 Running 0 1m #\u0026gt; 2 process-mkewsr-item-2-cph8z 1/1 Running 0 1m #\u0026gt; 3 process-mkewsr-item-3-kpn5f 1/1 Running 0 1m Se o status dos seus pods indicar algo ruim, pode ser que você precise depurar o seu arquivo exec.R. Isso é absolutamente normal e pode ser que sejam necessárias várias tentativas até que a sua tarefa esteja rodando corretamente. Se você precisar de ajuda na depuração da sua tarefa, dê uma olhada na vignette \u0026ldquo;Debugging exec.R\u0026rdquo;.\nO comando abaixo lista todo arquivo em um bucket. Você também pode especificar o diretório dentro do bucket e se a listagem deve ser feita recursivamente. Aqui é possível ver que todo download terminou de rodar corretamente.\nkub_list_bucket(\u0026#34;~/toy-dir\u0026#34;, folder = \u0026#34;scraped\u0026#34;) #\u0026gt; ✔ Listing content #\u0026gt; [1] \u0026#34;googlecom.html\u0026#34; \u0026#34;duckduckgocom.html\u0026#34; \u0026#34;wikipediaorg.html\u0026#34; #\u0026gt; [4] \u0026#34;facebookcom.html\u0026#34; \u0026#34;twittercom.html\u0026#34; \u0026#34;instagramcom.html\u0026#34; ","permalink":"https://blog.curso-r.com/posts/2019-05-14-scraper-orquestrado/","tags":["web-scraping"],"title":"Web Scraping Orquestrado"},{"author":["Julio"],"categories":["análises"],"contents":" Nesse post vou trabalhar com Captchas e GANs. Se você não sabe o que é um Captcha e como estamos trabalhando para resolvê-los até agora, recomendo que você veja a série de posts da Curso-R sobre o tema. Se você não sabe nada de GAN, acredito que seja possível acompanhar esse post. No entanto, não acho que esse é o exemplo mais simples possível de GAN: se você quiser um “hello world” no tema, recomendo ver esse exemplo e esse paper.\nObjetivo: criar um modelo capaz de, ao mesmo tempo, gerar novos Captchas e também resolver Captchas existentes.\nMotivo: o motivo principal de usar modelos generativos é aproveitar o fato de existirem muito mais Captchas não classificados do que classificados. De certa forma, saber criar novos Captchas pode auxiliar no trabalho de resolvê-los.\nBase de dados: ao invés de utilizar um Captcha real, vamos trabalhar com um Captcha sintético criado partir do MNIST. Chamarei esses carinhas de MNIST-Captcha. A ideia foi simplificar o problema para facilitar o desenvolvimento da solução. Um problema que enfrentamos no Deep Learning é que existem tantos hiperparâmetros a serem escolhidos que é muito difícil achar configurações que funcionam bem. Em problemas mais simples, é mais fácil achar esses parâmetros.\nFigura 1: Exemplo de imagem do MNIST. Figura 2: Exemplo de MNIST-Captcha. O que é GAN? A sigla GAN significa Generative Adversarial Networks. Não confunda com GAM (Generalized Additive Models)! Trata-se de um modelo recente, introduzido em 2014 pelo Ian Goodfellow, autor do famoso livro DeepLearningBook, que está ganhando cada vez mais espaço na comunidade científica, por conta de seus impressionantes resultados.\nGAN, na verdade, são dois modelos co-dependentes, o gerador e o discriminador. O gerador tenta gerar novos Captchas a partir de nada, indistinguíveis dos originais, e o discriminador tenta discriminar se os Captchas criados pelo gerador são reais ou não. Ou seja, um fica brigando com o outro, e no final temos um excelente gerador de novos Captchas e um excelente discriminador de Captchas.\nO que é AC-GAN? Quando nosso interesse está na predição, simples GANs não ajudam, pois o discriminadores só sabem verificar se uma imagem é original ou fake. No entanto, podemos adicionar respostas no discriminador, tornando-o um modelo duplo, capaz de discriminar imagens reais e falsas e, ao mesmo tempo, capaz de predizer o valor do Captcha.\nOK, mas não está claro Não mesmo. Esses carinhas não são simples. O ideal é sentar na cadeira e tentar entender o que o modelo faz. Eu fiz minha lição de casa, e consegui extrair esses passos:\nColetar uma amostra das imagens de treino Gerar Captchas fake. Juntar com Captchas reais. Andar 1 iteração no ajuste do discriminador. Gerar mais Captchas fake. Criar uma variável resposta sintética para avaliar a qualidade do gerador. Andar 1 iteração no ajuste do gerador, usando o discriminador para avaliar se as imagens foram bem geradas ou não. Voltar ao passo 1 até coletar a totalidade das imagens de treino Aplicar os passos 1 até 7 com as imagens de teste, para avaliar a performance dos modelos. Os passos são até tranquilos de implementar, mas são difíceis de entender. Particularmente, as partes que tive mais dificuldade foram 6 e 7.\nVamos então repassar esses pontos, só que agora assumindo que temos as funções generator() e discriminator() em mãos.\nEstrutura do AC-GAN O generator() O generator() é uma função que recebe como input i) um vetor de valores aleatórios e ii) uma label de um Captcha, e retorna a imagem de um Captcha. Os valores aleatórios são variáveis latentes que ajudam a criar os ruídos do Captcha: se não fossem eles, todos os Captchas gerados para uma dada label seriam iguais.\nMatematicamente, o gerador \\(g\\) de uma imagem \\(i\\) se comporta da seguinte forma:\n\\[ \\mathbf X_i = g(\\mathbf Y_i, \\boldsymbol \\varepsilon_i), \\]\nonde\n\\(\\mathbf X_i\\) é uma imagem. No caso da RFB, uma imagem 50x180. \\(\\mathbf Y_i\\) é a label do Captcha, encodada na forma 0-1, como mostrada por esse post \\(\\boldsymbol \\varepsilon_i\\) é um erro aleatório, usado para que os resultados gerados sejam diferentes toda vez. \\(g\\) é uma função altamente não linear, capaz de receber os inputs mencionados e retornar uma imagem. Uma forma interessante de representar g() é através de uma rede convolucional ao contrário. Veja os comentários da função para entender o que ela está fazendo.\nbuild_generator \u0026lt;- function() { # ruído latent \u0026lt;- layer_input(c(2, 10), name = \u0026quot;noise\u0026quot;) # resposta label \u0026lt;- layer_input(c(2, 10), name = \u0026quot;sampled_labels\u0026quot;) # obtém o input a partir da soma da resposta e do ruído input \u0026lt;- layer_add(list(label, latent)) output \u0026lt;- input %\u0026gt;% # destrói as dimensões existentes e coloca num vetor layer_flatten() %\u0026gt;% # remodela as dimensões iniciais da imagem # aqui, ela fica com 4 \u0026quot;cores\u0026quot; (canais), # 7 linhas e 14 colunas layer_dense(4 * 7 * 14, activation = \u0026quot;tanh\u0026quot;) %\u0026gt;% layer_reshape(c(4, 7, 14)) %\u0026gt;% # A rede convolucional transpose com strides aumenta # o tamanho da imagem. Agora está em 14 x 28 layer_conv_2d_transpose(64, 3, padding = \u0026quot;same\u0026quot;, strides = c(2, 2)) %\u0026gt;% layer_activation_leaky_relu() %\u0026gt;% # Agora está em 28 x 56 layer_conv_2d_transpose(64, 3, padding = \u0026quot;same\u0026quot;, strides = c(2, 2)) %\u0026gt;% layer_activation_leaky_relu() %\u0026gt;% # Agora está em 28 x 112 (note que strides = c(1, 2)) layer_conv_2d_transpose(32, 3, padding = \u0026quot;same\u0026quot;, strides = c(1, 2)) %\u0026gt;% layer_activation_leaky_relu() %\u0026gt;% # mais algumas convolucionais para gerar não-linearidade layer_conv_2d(16, 3, padding = \u0026quot;same\u0026quot;) %\u0026gt;% layer_activation_leaky_relu() %\u0026gt;% layer_conv_2d(16, 3, padding = \u0026quot;same\u0026quot;) %\u0026gt;% layer_activation_leaky_relu() %\u0026gt;% layer_conv_2d(8, 3, padding = \u0026quot;same\u0026quot;) %\u0026gt;% layer_activation_leaky_relu() %\u0026gt;% # Reduzir o número de cores para 1 (imagem preto e branco) layer_conv_2d( 1, 3, padding = \u0026quot;same\u0026quot;, activation = \u0026quot;tanh\u0026quot;) # coloca os dois inputs e o output no modelo keras_model(list(latent, image_class), output) } build_generator() Model ___________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to =========================================================================================== noise (InputLayer) (None, 4, 10) 0 ___________________________________________________________________________________________ sampled_labels (InputLayer) (None, 4, 10) 0 ___________________________________________________________________________________________ add_2 (Add) (None, 4, 10) 0 noise[0][0] sampled_labels[0][0] ___________________________________________________________________________________________ flatten_4 (Flatten) (None, 40) 0 add_2[0][0] ___________________________________________________________________________________________ dense_10 (Dense) (None, 392) 16072 flatten_4[0][0] ___________________________________________________________________________________________ reshape_5 (Reshape) (None, 4, 7, 14) 0 dense_10[0][0] ___________________________________________________________________________________________ conv2d_transpose_4 (Conv2DTra (None, 64, 14, 28) 2368 reshape_5[0][0] ___________________________________________________________________________________________ leaky_re_lu_10 (LeakyReLU) (None, 64, 14, 28) 0 conv2d_transpose_4[0][0] ___________________________________________________________________________________________ conv2d_transpose_5 (Conv2DTra (None, 64, 28, 56) 36928 leaky_re_lu_10[0][0] ___________________________________________________________________________________________ leaky_re_lu_11 (LeakyReLU) (None, 64, 28, 56) 0 conv2d_transpose_5[0][0] ___________________________________________________________________________________________ conv2d_transpose_6 (Conv2DTra (None, 32, 28, 112) 18464 leaky_re_lu_11[0][0] ___________________________________________________________________________________________ leaky_re_lu_12 (LeakyReLU) (None, 32, 28, 112) 0 conv2d_transpose_6[0][0] ___________________________________________________________________________________________ conv2d_8 (Conv2D) (None, 1, 28, 112) 289 leaky_re_lu_12[0][0] =========================================================================================== Total params: 74,121 Trainable params: 74,121 Non-trainable params: 0 ___________________________________________________________________________________________ No caso, nossa resposta é 4747:\n[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] 0 0 0 1 0 0 0 0 0 0 [2,] 0 0 0 0 0 0 1 0 0 0 [3,] 0 0 0 1 0 0 0 0 0 0 [4,] 0 0 0 0 0 0 1 0 0 0 E temos uma matriz de números aleatórios do mesmo tamanho:\n[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] -0.06 0.02 -0.08 0.16 0.03 -0.08 0.05 0.07 0.06 -0.03 [2,] 0.15 0.04 -0.06 -0.22 0.11 0.00 0.00 0.09 0.08 0.06 [3,] 0.09 0.08 0.01 -0.20 0.06 -0.01 -0.02 -0.15 -0.05 0.04 [4,] 0.14 -0.01 0.04 -0.01 -0.14 -0.04 -0.04 -0.01 0.11 0.08 Queremos gerar uma resposta com ruído a partir da resposta e de um vetor de números aleatórios. Fazemos isso simplesmente somando as duas quantidades:\nround(y + z , 2) [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] -0.06 0.02 -0.08 1.16 0.03 -0.08 0.05 0.07 0.06 -0.03 [2,] 0.15 0.04 -0.06 -0.22 0.11 0.00 1.00 0.09 0.08 0.06 [3,] 0.09 0.08 0.01 0.80 0.06 -0.01 -0.02 -0.15 -0.05 0.04 [4,] 0.14 -0.01 0.04 -0.01 -0.14 -0.04 0.96 -0.01 0.11 0.08 Observe que os valores da resposta continuam destacados. O resultado da aplicação do generator() é uma imagem com as mesmas dimensões do MNIST-Captcha e os números dados no input. Queremos que essa imagem seja o mais parecida possível com uma imagem real.\nO discriminator() O discriminador é um modelo preditivo, mas ele prevê duas coisas: i) se a imagem recebida é real ou fake e ii) qual é a label de uma imagem recebida. Se o gerador for bom, (i) terá dificuldades para funcionar. Se o gerador for muito ruim (i) conseguirá prever resultados com facilidade.\nVamos a um exemplo de discriminador, também usando redes convolucionais:\nbuild_discriminator \u0026lt;- function() { image \u0026lt;- layer_input(shape = c(1, 28, 28*4)) # rede convolucional LENET-5 bem comum output \u0026lt;- image %\u0026gt;% layer_conv_2d(32, 5, padding = \u0026quot;same\u0026quot;) %\u0026gt;% layer_activation_leaky_relu() %\u0026gt;% layer_max_pooling_2d() %\u0026gt;% layer_conv_2d(64, 5, padding = \u0026quot;same\u0026quot;) %\u0026gt;% layer_activation_leaky_relu() %\u0026gt;% layer_max_pooling_2d() %\u0026gt;% layer_conv_2d(64, 3, padding = \u0026quot;same\u0026quot;) %\u0026gt;% layer_activation_leaky_relu() %\u0026gt;% layer_flatten() %\u0026gt;% layer_dense(64, activation = \u0026quot;relu\u0026quot;) %\u0026gt;% layer_dropout(0.2) %\u0026gt;% layer_dense(128, activation = \u0026quot;relu\u0026quot;) cnn \u0026lt;- keras_model(image, output) features \u0026lt;- cnn(image) # chega a primeira resposta (fake, não fake) fake \u0026lt;- features %\u0026gt;% layer_dense(32, activation = \u0026quot;tanh\u0026quot;) %\u0026gt;% layer_dense(1, activation = \u0026quot;sigmoid\u0026quot;, name = \u0026quot;generation\u0026quot;) # chega na segunda resposta (Y) aux \u0026lt;- features %\u0026gt;% layer_dense(4 * 10, activation = \u0026quot;relu\u0026quot;) %\u0026gt;% layer_reshape(c(4, 10)) %\u0026gt;% layer_activation(\u0026quot;softmax\u0026quot;, name = \u0026quot;auxiliary\u0026quot;) # junta os dois no resultado keras_model(image, list(fake, aux)) } build_discriminator() Model ___________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to =========================================================================================== input_4 (InputLayer) (None, 1, 28, 112) 0 ___________________________________________________________________________________________ model_5 (Model) (None, 128) 900224 input_4[0][0] ___________________________________________________________________________________________ dense_9 (Dense) (None, 40) 5160 model_5[1][0] ___________________________________________________________________________________________ dense_8 (Dense) (None, 32) 4128 model_5[1][0] ___________________________________________________________________________________________ reshape_4 (Reshape) (None, 4, 10) 0 dense_9[0][0] ___________________________________________________________________________________________ generation (Dense) (None, 1) 33 dense_8[0][0] ___________________________________________________________________________________________ auxiliary (Activation) (None, 4, 10) 0 reshape_4[0][0] =========================================================================================== Total params: 1,819,090 Trainable params: 909,545 Non-trainable params: 909,545 ___________________________________________________________________________________________ Note que o resultado é uma lista de dois outputs.\nO gan() gan() é a função que junta o discriminador e o gerador. Em teoria, ela não serve para nada: é só um meio prático de ajustar o modelo do gerador. Fazemos isso gerando algumas imagens falsas e avaliando a qualidade do gerador por quanto ele é capaz de enganar o discriminador. Se o discriminador é muito ruim, qualquer gerador vai mandar bem. Se o discriminador é muito bom, o gerador terá de criar Captchas excepcionais para ganhar pontos.\nO gan() é definido de forma muito simples, com:\n# placeholder para ruído latent \u0026lt;- layer_input(shape = list(4L, 10L)) # placeholder para resposta image_class \u0026lt;- layer_input(shape = list(4L, 10L)) # imagem falsa fake \u0026lt;- generator(list(latent, image_class)) # Only want to be able to train generation for the combined model # Só queremos treinar o generator nessa parte; # utilizaremos o discriminator apenas para # avaliação da qualidade do gerador. # Por isso, congelamos o discriminator aqui. freeze_weights(discriminator) results \u0026lt;- discriminator(fake) gan \u0026lt;- keras_model(list(latent, image_class), results) gan Model ___________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to =========================================================================================== input_5 (InputLayer) (None, 4, 10) 0 ___________________________________________________________________________________________ input_6 (InputLayer) (None, 4, 10) 0 ___________________________________________________________________________________________ model_7 (Model) (None, 1, 28, 112) 74121 input_5[0][0] input_6[0][0] ___________________________________________________________________________________________ model_6 (Model) [(None, 1), (None, 909545 model_7[1][0] =========================================================================================== Total params: 983,666 Trainable params: 74,121 Non-trainable params: 909,545 ___________________________________________________________________________________________ Mas funciona? É uma pergunta natural. Esse modelo é meio maluco, pois usa uma parte do modelo para avaliar a qualidade do outro. Mas sim, temos alguns resultados matemáticos que garantem que estamos em um bom território.\nMeu desejo era colocá-los aqui, mas ainda não sei explicar todos os detalhes. Pretendo estudar mais e adicionar em novos posts.\nExemplo de aplicação nos MNIST-Captcha O exemplo completo que rodei com os MNIST-Captchas está neste link. Decidi não colocar o código completo aqui pois a minha implementação está muito feia o post ficaria muito longo.\nMuito bem, se o modelo serve para prever e para gerar, vamos avaliar a qualidade nesses quesitos.\nPrevê bem? A taxa de acerto de cada dígito do MNIST-Captcha foi de 95%. Como temos 4 dígitos nesse caso, o acerto de todo o Captcha seria de 81%. Comparado com os modelos funcionando em produção do pacote decryptr, esse resultado é ruim. Lá, as taxas de acerto do Captcha são de no mínimo 93% e tem casos que acertamos tudo. Mas eu acredito que o motivo disso é que não ajustamos bem os hiperparâmetros. Se fizermos isso, provavelmente ficará melhor.\nGera bem? Vamos ver! Para testar o gerador, montamos uma função que gera e plota uma imagem a partir de um vetor de respostas:\ngenerate_image \u0026lt;- function(num = sample(0:9, 4)) { # Gerando barulho noise \u0026lt;- rnorm(4 * 10, 0, .01) %\u0026gt;% array(dim = c(4, 10)) %\u0026gt;% array_reshape(c(1, dim(.))) # Resposta (aleatorizada ou input) sampled_labels \u0026lt;- num %\u0026gt;% matrix(ncol = 4) %\u0026gt;% # gera o one-hot dessa resposta transform_to_matrix() %\u0026gt;% array_reshape(c(1, dim(.))) # Aplicar generator img \u0026lt;- predict(generator, list(noise, sampled_labels))[1,1,,] # a imagem foi deixada em (-1,1). Arrumando par(mar = rep(0, 4)) plot(as.raster((img + 1) / 2)) } Rodando para alguns exemplos:\ngerar_imagem(c(1, 2, 4, 4)) gerar_imagem(c(4, 0, 4, 7)) gerar_imagem(c(9, 9, 9, 9)) O que achou? Note que a última está errada.\nPróximos passos Fazer o GAN funcionar para mais Captchas. Testar algumas dicas que o Athos me passou. Buscar uma forma de aproveitar a informação parcial advinda de oráculos. Fica o mistério para os próximos posts. Wrap-up GAN é um modelo interessante que pode nos ajudar a montar modelos não supervisionados (diretamente) e supervisionados (AC-GAN). Para montar um GAN, você precisa definir um gerador e um discriminador. O AC-GAN parece ser uma boa abordagem para Captchas, considerando que temos muitas imagens disponíveis e poucas classificadas. Ainda temos muito a descobrir sobre esse modelo. Agradecimentos Sempre ao Daniel Falbel, que é meu guru do Deep Learning e autor do exemplo inicial que foi adaptado para esse post. Também agradeço ao Athos pelos insights e links que me passou!\n","permalink":"https://blog.curso-r.com/posts/2019-03-04-gan-captcha/","tags":["captcha"],"title":"Quebrando Captchas - Parte VI: Redes Generativas Adversariais com Classificador Auxiliar"},{"author":["Julio"],"categories":["tutoriais"],"contents":" Esse é um post de matemática. Mesmo se você não curte essa faceta da ciência de dados, encorajo vocês a lerem e estudarem esse assunto. Nesse texto vou usar um pouco de teoria dos conjuntos e introduzir a teoria da medida, que é a base matemática de tudo que sabemos sobre probabilidade.\nAos matemáticos: eu tentei simplificar ao máximo a linguagem, então posso ter cometido alguns deslizes. Não sou o Billingsley, então não peço desculpas! ;)\nObjetivo Nosso objetivo é definir matematicamente a função probabilidade. Isso é diferente de definir a interpretação da probabilidade, como a interpretação frequentista e subjetivista, que é assunto para outro texto.\nPra quê? Durante meus estudos, o argumento mais comum usado para me convencer a estudar probabilidade e medida foi: se você constrói toda a estatística – e consequentemente boa parte da ciência – a partir de um objeto matemático, é bom que esse objeto esteja bem definido e funcione direito, né!? Mas nós sabemos que ele está bem definido, pois já disseram isso pra gente em algum momento no passado. Devem ter várias pessoas que já se preocuparam com isso, então deve funcionar!\nPor isso, eu vou te dar outro argumento: se algum dia você montar um novo modelo estatístico e quiser mostrar que ele prevê bem em bases de dados diferentes da sua, você precisará extrair algumas propriedades do seu modelo. E para extrair essas propriedades, você precisará manjar de probabilidade no nível de medida.\nNão adianta você ser dono de uma padaria, contratar um padeiro e não saber fazer pão. Um dia seu padeiro vai faltar e você estará ferrado.\n– WUNDERVALD, J.\nÉ difícil? SIM! A definição de probabilidade seria muito simples e intuitiva se não fossem algumas dificuldades associadas ao conjunto dos números reais \\(\\mathbb R\\), e o fato deles serem não contáveis1. Por isso, convido vocês a entrarem comigo nessa jornada com os seguintes passos:\nTentar definir a probabilidade intuitivamente. Verificar que essa definição tem problemas. Tirar da cachola uma tal de \\(\\sigma\\)-álgebra que, em tese, resolve (2). Mostrar que ela de fato resolve (2), mostrando mais um conjunto tirado da cachola: o conjunto dos mensuráveis. Mostrar que \\(\\sigma\\)-álgebras são difíceis de trabalhar. Mostrar que tem uma forma de trabalhar (5) sem nos deixar o cabelo em pé, graças ao nosso amigo Caratheodory. Definindo a probabilidade com intuição Probabilidade nada mais é que medir eventos, soltando números entre zero e um. Mas o que são eventos, e o que é medir?\nO exemplo mais fácil de medida que consigo pensar é a régua. A régua é uma função \\(\\lambda(\\cdot)\\) que recebe como input um intervalo \\((a,b]\\) e retorna \\(b-a\\). Ou seja, a função de medida é uma função que trabalha com conjuntos e retorna números reais não-negativos. Mas \\(b-a\\) pode ser maior que um, ou até infinito. Logo, a probabilidade é um caso particular de medida, que recebe como input um conjunto e como output um número no intervalo \\([0,1]\\).\nNo caso da régua, daria para resolver isso forçando \\(0 \\leq a \u0026lt; b \\leq 1\\). Assim todos os intervalos teriam medida entre zero e um. De fato, quando trabalhamos com variáveis aleatórias\nNo entanto, nem tudo na vida são intervalos na reta. Queremos poder calcular probabilidade de chover amanhã, ou então a probabilidade de um cliente ficar inadimplente após 100 dias ou mais. Para isso, precisamos definir um conjunto abstrato \\(\\Omega\\), que chamamos de espaço amostral. Esse carinha contém os eventos individuais que estamos interessados, e calculamos probabilidades sobre combinações de pedaços dele.\nEssas combinações de pedaços (ou subconjuntos) de \\(\\Omega\\) são os eventos. Por exemplo, no caso da chuva \\(\\Omega\\) seria \\(\\{\\text{chover},\\text{não chover}\\}\\) e calculamos probabilidades sobre \\(\\{\\text{não chover}\\}\\), \\(\\{\\text{chover}\\}\\), \\(\\{\\text{não chover}\\} \\text{ ou } \\{\\text{chover}\\}\\), e o vazio \\(\\emptyset\\). No caso da inadimplência, nosso \\(\\Omega\\) poderia ser os naturais \\(\\mathbb N\\), por exemplo, e o evento de interesse seria \\(\\{i\\in \\mathbb N: i \\geq 100\\}\\).\nPrimeiro, defina como \\(\\mathcal P(\\Omega)\\) como o conjunto de todos os eventos baseados em \\(\\Omega\\). Chamamos esse objeto de partes de \\(\\Omega\\). Esse cara é bem grande, pois tem não só o próprio \\(\\Omega\\) e o vazio \\(\\emptyset\\), como também todas as possíveis combinações de subconjuntos do \\(\\Omega\\). No caso da chuva, são apenas os quatro eventos citados no parágrafo anterior. No caso da inadimplência, temos todos os números naturais e ainda todas as combinações desses números. No caso da régua então, temos desde o intervalo \\((0, \\frac \\pi 4]\\) até o conjunto de Cantor, o que pode ser bem infinito!\nSegundo, uma propriedade desejável da probabilidade é a aditividade. Ela significa que se eu tiver em mãos dois eventos disjuntos (ou seja, com interseção vazia), a probabilidade de algum desses eventos ocorrer é igual à soma das probabilidades de cada um desses eventos. Essa é uma suposição bem intuitiva no caso da régua. Se você pegar dois intervalos separados de uma reta e medir os dois intervalos de uma vez, isso será equivalente a medir cada um dos intervalos e depois somar. Curiosamente, existem áreas grandes da matemática/estatística que trabalham com medidas que não seguem essa regra. Veja, por exemplo, a medida de possibilidade.\nAgora estamos prontos para definir a medida de probabilidade. Queremos uma função \\(\\mathbb P(\\cdot)\\) que tenha as seguintes características:\nRecebe um elemento de \\(\\mathcal P(\\Omega)\\) e retorna um número real não-negativo. (Normalização) \\(P(\\Omega) = 1\\). Isso obriga a probabilidade a ficar sempre entre zero e um, pois nada tem medida maior que \\(\\Omega\\). (Aditividade) Para a sequência \\(A_j\\), \\(j \\in \\mathbb N\\), com \\(A_j \\cap A_k = \\emptyset\\) (disjuntos) para todo \\(j\\) e \\(k\\), vale \\(\\mathbb P(\\cup_j A_j) = \\sum_j \\mathbb P(A_j)\\). Note que, se você pensar que \\(\\cup_j A_j\\) é como uma soma dos \\(A_j\\), a propriedade (3) é o que permite “passar a soma para fora” da probabilidade.\nNote também que eu coloquei aí uma união infinita e contável. Alguns pesquisadores gostam de restringir a medida de probabilidade para uniões finitas apenas. Isso ajuda bastante na parte matemática, mas pode levar a alguns problemas práticos. Isso gerou uma treta enorme entre os matemáticos: leia aqui para uma abordagem histórica do tema. No final, a versão mais popular é a com uniões infinitas.\nMuito bem, esses são os famosos Axiomas de Kolmogorov! Ou quase…\nO problema de usar as partes como a base de tudo Eu comentei alguns parágrafos atrás que a coleção \\(\\mathcal P(\\Omega)\\) pode ser um objeto muito infinito. O problema é que esse conjunto pode ser tão, mas tão grande, que pode bugar a função de probabilidade. Um exemplo famoso de bug se chama conjunto de Vitali: a medida desse evento leva a uma contradição.\nPara explicar esse conjunto, vamos voltar à medida régua \\(\\lambda\\), sem restringir ao zero e um. A régua tem mais uma propriedade interessante: se \\(x\\in \\mathbb R\\), o conjunto transladado \\(x+A\\), ou seja, o conjunto formado por todos os elementos de \\(A\\) somados de \\(x\\), tem a mesma medida que \\(A\\). Por exemplo, se \\(A = (a,b]\\), então \\(A+x = (a+x,b+x]\\), e a medida desse conjunto é\n\\[\\lambda(A+x) = \\lambda((a+x,b+x]) = (b+x)-(a+x) = b-a = \\lambda(A).\\]\nAgora vamos ao exemplo de conjunto não mensurável. O exemplo é um caso de matemágica, ou seja, é bem complicado tirar a intuição dele. Além disso, esse exemplo usa diretamente o axioma da escolha. Mas tudo bem, é um exemplo, e vamos ver que ele funciona.\nPasso 1. Para cada \\(x \\in \\mathbb R\\), defina \\([x] = \\{y\\in \\mathbb R, y-x \\in \\mathbb Q\\}\\). Ou seja, para cada \\(x\\) criamos um subconjunto de \\(\\mathbb R\\) que contém todos os números que subtraídos de \\(x\\) são racionais. Por exemplo, se \\(x = \\sqrt 2\\), \\([x] = \\{\\sqrt 2, 2\\sqrt 2, \\dots\\}\\) (são muitos casos possíveis). Defina o conjunto de todos os possíveis \\([x]\\) de \\(\\Lambda\\). ou seja, \\(\\Lambda\\) é o conjunto dos conjuntos \\([x]\\), e cada elemento \\(\\alpha \\in \\Lambda\\) é um conjunto que contém uma porção de números reais.\nPasso 2. Note que para cada conjunto \\(\\alpha \\in \\Lambda\\), sempre existe um elemento \\(a \\in \\alpha\\) que está no intervalo \\((0,1)\\). Agora, para cada \\(\\alpha \\in \\Lambda\\), pegamos um elemento \\(a_\\alpha \\in \\alpha\\) que está no intervalo \\((0,1)\\). Finalmente, chamamos \\(\\Omega\\) de todos esses conjuntos \\(a_\\alpha\\), e temos \\(\\Omega \\subseteq (0,1)\\). Numa imagem:\nMuito bem. Agora, note que se \\(p,q \\in \\mathbb Q\\), então \\((\\Omega+q)\\) e \\((\\Omega+p)\\) são disjuntos. Na verdade, se não fossem disjuntos, isso significaria que \\(p=q\\) e os conjuntos coincidiriam. Isso acontece porque, se \\(x \\in (\\Omega+q)\\cap (\\Omega+p)\\), então \\(x\\) é um elemento dos dois conjuntos:\n\\[x = a+p = b+q \\implies b-a=q-p\\]\nComo \\(q-p\\) é racional, então \\(b-a\\) é racional, logo \\(b \\in [a]\\). Mas nós montamos \\(\\Omega\\) com apenas um elemento de cada conjunto \\([a]\\) como esse, então \\(a=b\\). Logo, \\(p=q\\).\nAgora, vamos às contas. Estamos interessados em calcular, para \\(-1\u0026lt;p\u0026lt;1\\), \\(p \\in \\mathbb Q\\), a medida\n\\[ \\lambda\\left(\\bigcup_{p} (\\Omega + p)\\right) \\]\nPor um lado, temos que \\(\\bigcup_{p} (\\Omega + p) \\subseteq(-1,2)\\). Então,\n\\[ \\lambda\\left(\\bigcup_{p} (\\Omega + p)\\right) \\leq \\lambda((-1, 2]) = 3. \\]\nPor outro lado, como os \\((\\Omega + p)\\) são disjuntos, temos\n\\[\\lambda\\left(\\bigcup_{p} (\\Omega + p)\\right) = \\sum_p\\lambda(\\Omega+p) = \\sum_p\\lambda(\\Omega)\\]\nComo estamos somando várias vezes o mesmo termo, e como a soma deve ser menor ou igual a 3, então temos que \\(\\lambda(\\Omega) = 0\\). Logo,\n\\[ \\lambda\\left(\\bigcup_{p} (\\Omega + p)\\right) = 0. \\]\nE agora vem o problema. Para \\(x \\in (0,1)\\), temos que existe um \\(a \\in [x]\\) que está em \\(\\Omega\\), certo? E sabemos também que esse \\(a \\in (0,1)\\), porque definimos assim. Como \\(a \\in [x]\\), então \\(p^* = a - x\\) é racional. Além disso, \\(-1\u0026lt;p^*\u0026lt;1\\), por conta dos valores de \\(a\\) e \\(x\\). Logo, \\(x = a + p\\), onde \\(p=-p^*\\), então \\(x \\in (\\Omega + p)\\). Como isso vale para todo \\(x \\in (0,1)\\), temos que\n\\[(0,1) \\subseteq \\bigcup_{p} (\\Omega + p)\\] Logo,\n\\[\\lambda\\left(\\bigcup_{p} (\\Omega + p)\\right) \\geq \\lambda((0,1)) = 1\\]\nUé, mas um número não pode ser zero e maior ou igual a um ao mesmo tempo. Isso é uma contradição. Logo, como encontramos problemas em elementos de \\(\\mathcal P(\\Omega)\\), não podemos definir a probabilidade nesse conjunto.\nA solução: \\(\\sigma\\)-álgebras Uma \\(\\sigma\\)-álgebra \\(\\mathcal F\\) é um conjunto de subconjuntos, como as partes, mas um pouco menor. Ela tem condições restritivas restritivas:\n\\(\\Omega \\in \\mathcal F\\) se \\(A \\in \\mathcal F\\), então \\(A^c \\in \\mathcal F\\). se \\(A_j \\in \\mathcal F, j\\in \\mathbb N\\), então \\(\\cup_j A_j \\in \\mathcal F\\). [este post ainda não está finalizado. Volte depois…]\nUm conjunto contável \\(A\\) é um conjunto que podemos mapear nos naturais \\(\\mathbb N\\). Ou seja, \\(A\\) é contável se conseguirmos montar uma função que associe cada elemento \\(a \\in A\\) a um novo elemento \\(n \\in \\mathbb N\\). Por exemplo, o conjunto dos inteiros \\(\\mathbb I\\) é contável, pois, podemos montar uma função que mapeia os números positivos nos naturais pares e os negativos nos naturais ímpares. Outro exemplo de conjunto contável são os racionais \\(\\mathbb Q\\). Um conjunto não contável são os irracionais (aquele que tem \\(\\pi\\), \\(e\\), \\(\\sqrt 2\\), etc) e o conjunto dos reais \\(\\mathbb R\\), já que ele contém os irracionais. Vamos:↩︎\n","permalink":"https://blog.curso-r.com/posts/2019-03-10-probabilidade/","tags":["estatística"],"title":"Probabilidade e medida: intuição"},{"author":["Bruna Wundervald","Julio"],"categories":["análises"],"contents":" Introdução Esse ano, vamos participar da Escola de Modelos de Regressão (EMR-2019). Trata-se de um evento voltado a um nicho da comunidade de estatísticos, os regresseiros, e traz avanços importantes na área de Regressão.\nPorém ao olhar a programação do evento ficamos surpresos com a baixa proporção de mulheres dentre palestrantes e coordenadores. Isso é um problema recorrente na comunidade científica, especialmente em congressos: a maior parte dos eventos acabam apresentando proporções baixas de mulheres. Nos últimos anos, vários grupos têm monitorado e denunciado ocorrências desse fenômeno. Para enriquecer os debates aqui no Brasil, decidimos organizar os dados de três eventos (2 recentes e um que ainda vai acontecer este ano):\nEMR 2019: O evento que gerou a discussão. RBras 2018: Evento sobre estatística aplicada a agronomia. Sinape 2018: Maior evento sobre Estatística do Brasil. Obtenção dos dados Os scripts para obtenção e organização dos dados foram colocados nesse repositório. Utilizamos as funções do tidyverse como manda o figurino, e o pacote httr para acessar as páginas que contêm as informações. No caso da EMR e do Sinape, utilizamos o pacote xml2 para extração dos conteúdos nas páginas. No caso da RBras, foi necessário utilizar o pdftools, pois as informações estavam em um arquivo PDF.\nA classificação do sexo foi feita manualmente. Adotamos a seguinte estratégia: criamos uma coluna com “M” de masculino e fomos substituindo quando o nome era de uma mulher. O trabalho foi fácil, já que a proporção de mulheres era mais baixa em todos os casos…\nNo total, ficamos com 20 nomes para a EMR 2019 (que ainda não tem a programação completa), 68 nomes para o Sinape 2018 e 102 nomes para a RBras 2018. Quando a mesma pessoa participa de duas atividades, o nome foi duplicado.\nOBS: No caso da RBras, pegamos sempre o nome do primeiro autor ou primeiro palestrante das conferências. Isso pode ter causado alguma pequena distorção nos resultados, se houver um viés na escolha do primeiro nome dos trabalhos com relação ao sexo.\nResultados Observamos a maior discrepância na EMR 2019. A proporção de mulheres é menor 40% em todos os tipos de participação. No caso dos palestrantes, a participação feminina é menor que 20%.\nNo Sinape 2018, os resultados se repetem para palestrantes e conferencistas, mas se inverte para coordenadores. Dos 11 coordenadores, 8 são mulheres.\nNa RBras 2018, observamos novamente menor participação feminina, mas de forma menos gritante que o EMR 2019. No caso dos coordenadores, a participação é igual para homens e mulheres.\nConsiderando os totais de cada evento, temos que a RBras 2018 é a que se aproxima mais de ser paritária, mas ainda longe do ideal, com aproximadante 35% de participação feminina.\nConclusões Claramente, existe falta de diversidade nas programações principais dos mais importantes eventos de Estatística no Brasil. A desculpa mais comum que organizadores de congressos dão para isso é de que não existiam mulheres pra convidar, o que não é verdade. Talvez o que falte seja o reconhecimento da necessidade de consertar essa falha, e aplicar algumas simples medidas, como:\nBuscar atingir um mínimo realista de mulheres convidadas a palestrar nos eventos. Considerar outras minorias de gênero para as posicões. Abrir espaço nos eventos para tratar do tema, e evidenciar o quanto é importante combater o sexismo na academia. Exemplos de eventos de estatística no Brasil com iniciativas parecidas são o SeR 2018 e o RDay 2018. Ambos tiveram palestras da Gabriela de Queiroz, fundadora do R-Ladies, sobre o assunto em questão. E não é só isso, muito mais pode ser feito para promover a diversidade em eventos, desde o oferecimento de bolsas para minorias até a realização de eventos satélites focados apenas nos trabalhos delas. Medidas como essa estão cada vez mais populares em diversos ramos da ciência, como na ciência computação e inteligência artificial. Continuar ignorando esse problema da comunidade estatística só tem consequências ruins: perderemos relevância e ficaremos para trás.\nNo futuro, seria interessante colher e organizar os dados de mais eventos, para monitorar e pressionar os comitês organizadores, pois ainda temos muito a evoluir para que a comunidade científica deixe de priorizar o sexo masculino em conferências.\nAproveitando: A Gabriela de Queiroz é finalista do Women in Open Source Award. Você pode votar na sua finalista preferida nesse link.\n","permalink":"https://blog.curso-r.com/posts/2019-02-16-eventos-mf/","tags":["comunidade"],"title":"Precisamos falar sobre a participação de mulheres em eventos de estatística"},{"author":["Julio"],"categories":["tutoriais"],"contents":" AVISO: Infelizmente a função pvec() não está mais no pacote abjutils! No final deste texto deixamos o seu código para que o leitor possa implementá-la por conta própria.\nQuando usamos laços para rodar algoritmos complexos em uma lista de inputs, podemos pensar em power-ups. Tratam-se de funcionalidades que ajudam na aplicação dos laços, tanto do ponto de vista de eficiência do código quanto do ponto de vista de eficiência do trabalho do cientista de dados.\nAqui na Curso-R nós já vimos três desses power-ups:\nComo fazer laços em paralelo. Como usar barras de progresso Como fazer tratamento de erros. Mas será que tem um jeito de juntar essas três funcionalidades em apenas uma operação?\nSim, é claro que tem. E se algo é possível no R, o Caio Lente já fez. Trata-se da operação pvec(), do pacote abjutils.\nPara utilizá-la, você precisará instalar a versão de desenvolvimento do abjutils no GitHub:\ndevtools::install_github(\u0026quot;abjur/abjutils\u0026quot;) Pode ser que o pvec() não funcione muito bem no Windows. Isso é algo que vamos trabalhar no futuro.\nComo funciona O pvec() recebe duas informações de entrada: uma lista ou vetor de inputs e uma função a ser aplicada. O pvec() funciona exatamente como um purrr::map(), mas retorna um data.frame com os outputs.\nPor exemplo, digamos que nosso objetivo seja aplicar a função\nfuncao \u0026lt;- function(x) { # dorme 1s Sys.sleep(1) # aplica o log log(x) } em uma lista de entradas, dada por\ninput \u0026lt;- list(1, 2, -1, \u0026quot;a\u0026quot;) O resultado é dado por:\nresultado \u0026lt;- abjutils::pvec(input, funcao) resultado # A tibble: 4 x 3 id return output \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; 1 1 result \u0026lt;dbl [1]\u0026gt; 2 2 result \u0026lt;dbl [1]\u0026gt; 3 3 result \u0026lt;dbl [1]\u0026gt; 4 4 error \u0026lt;S3: simpleError\u0026gt; Ou seja, o resultado é um data.frame, que tem o número de linhas exatamente igual ao comprimento do vetor ou lista de entrada, e três colunas específicas.\nid, que guarda o índice de entrada. Se a lista de entrada é nomeada, id guarda esses nomes. return identifica se a aplicação retornou num resultado (result) ou erro (error) output é uma coluna-lista que contém os resultados. Quando o resultado é um erro, o erro é capturado e colocado no elemento correspondente. Ou seja, uma característica do pvec() é que ele nunca irá travar. Se essa operação travar, é porque o computador todo travou.\nÉ importante notar que alguns resultados nesse caso são NaN. Isso ocorre pois log(-1) resulta em NaN, acompanhado de um warning. O pvec() não trabalha com warnings.\nOutra característica importante do pvec() é que ele roda em paralelo. Você pode controlar a quantidade de núcleos de processamento com o parâmetro .cores. Por padrão, ele usará o número de núcleos da sua máquina.\nFinalmente, o que não poderia faltar no pvec() é a utilização de barras de progresso. Por exemplo, considerando como input\ninput \u0026lt;- list(a = 1, b = 2, c = -1, d = \u0026quot;a\u0026quot;, e = 2, f = 3, g = -2, h = \u0026quot;b\u0026quot;) O resultado é\nabjutils::pvec(input, funcao) Progress: ─────────────────────────────── 100% Progress: ──────────────────────────────────────────────────────────── 100% # A tibble: 8 x 3 id return output \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; 1 a result \u0026lt;dbl [1]\u0026gt; 2 b result \u0026lt;dbl [1]\u0026gt; 3 c result \u0026lt;dbl [1]\u0026gt; 4 d error \u0026lt;S3: simpleError\u0026gt; 5 e result \u0026lt;dbl [1]\u0026gt; 6 f result \u0026lt;dbl [1]\u0026gt; 7 g result \u0026lt;dbl [1]\u0026gt; 8 h error \u0026lt;S3: simpleError\u0026gt; Se você quiser desligar a barra de progresso, basta adicionar .progress = FALSE.\nO parâmetro .flatten Esse é o parâmetro dos preguiçosos (eu que pedi para o Caio adicionar). Em muitas operações, o resultado que sai no output é uma lista de data.frames ou uma lista de vetores. A opção .flatten faz tidyr::unnest(), empilhando os resultados e colando tudo num vetor ou data.frame.\nO único problema é que nesse caso não é possível guardar os erros. Por isso, o pvec() retorna um warning:\nabjutils::pvec(input, funcao, .flatten = TRUE) Progress: ──────────────────────────────────────────────────────────── 100% # A tibble: 6 x 2 id output \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; 1 a 0 2 b 0.693 3 c NaN 4 e 0.693 5 f 1.10 6 g NaN Warning message: Since \u0026#39;.flatten = TRUE\u0026#39;, a total of 2 errors are being ignored Note que o resultado tem 6 linhas, menor que a entrada, que tem 8 elementos. Por isso, use .flatten somente quando você tem certeza do que está fazendo.\nPor trás dos panos: o furrr O pvec() só funciona por conta de dois excelentes pacotes:\no future, que é um novo paradigma de computação em paralelo no R. o furrr, que faz todo o trabalho sujo e implementa a maioria das operações do purrr usando future. Se quiser estudar esses pacotes e implementar suas próprias soluções, recomendo acessar aqui e aqui. Não incluí detalhes desses pacotes aqui para não sair do foco.\nSe quiser adicionar opções do future no pvec(), basta adicioná-las na opção .options. Por padrão, passamos furrr::future_options() nesse argumento.\nDiscussão: o future é o futuro do purrr? O purrr contém uma série de discussões no GitHub sobre a possibilidade de rodar funções em paralelo e com barras de progresso. Pode ser que a funcionalidade do pvec() passe a ser parte oficial no futuro.\nVeremos!\nWrap-up abjutils::pvec() é um map() que roda em paralelo, tem barras de progresso e trata erros automaticamente. Você pode brincar com as opções .cores, .progress e .flatten para controlar o comportamento do pvec(). Tome muito cuidado com o .flatten, pois ele pode não tratar os erros da forma que você imagina! Estude future e furrr se quiser estender as funcionalidades do pvec(). É isso pessoal. Happy coding ;)\nP.S.: Código Como dito no começo deste post, a função pvec() foi removida do abjutils por causa de alguns problemas nas dependências que dificultavam a sua manutenção em um pacote. O código dela está preservado aqui para que você possa implementá-la por conta própria e ainda é possível acessá-la no arquivo do abjutils.\n#\u0026#39; @title Verbose, parallel, and safe map-like #\u0026#39; #\u0026#39; @description Using the same argument notation as [purrr::map()], this function #\u0026#39; iterates over a list of inputs `.x`, applying `.f` to each element. It #\u0026#39; returns a tibble with the id, whether the function returned an error #\u0026#39; and the output. #\u0026#39; #\u0026#39; @importFrom magrittr %\u0026gt;% #\u0026#39; #\u0026#39; @param .x A list or atomic vector #\u0026#39; @param .f A function, formula, or atomic vector (see [purrr::map()]) #\u0026#39; @param ... Other parameters passed on to `.f` #\u0026#39; @param .cores Number of cores to use when multiprocessing #\u0026#39; @param .progress Whether or not to display progress #\u0026#39; @param .flatten If `TRUE`, the errors are filtered from the output, #\u0026#39; and the returned object is flattened (a vector, a list, or a tibble) #\u0026#39; @param .options Options passed on to [furrr::future_map()] #\u0026#39; ([furrr::future_options()] by default) #\u0026#39; #\u0026#39; @seealso [purrr::map()], [furrr::future_map()], [furrr::future_options()] #\u0026#39; #\u0026#39; @return A tibble with 3 columns: input, return, and output #\u0026#39; @export pvec \u0026lt;- function(.x, .f, ..., .cores = get_cores(), .progress = TRUE, .flatten = FALSE, .options = future_options()) { .Deprecated(\u0026quot;furrr::future_map\u0026quot;) # Preserve execution plan oplan \u0026lt;- future::plan() on.exit(future::plan(oplan), add = TRUE) # Set execution plan to multicore future::plan(future::multicore, workers = .cores) # Capture function side-effects .f \u0026lt;- purrr::safely(purrr::as_mapper(.f)) # Run future map out \u0026lt;- furrr::future_map(.x, .f, ..., .progress = .progress, .options = .options) # Compact with care compact_ \u0026lt;- function(x) { if (is.null(x[[1]]) \u0026amp;\u0026amp; is.null(x[[2]])) { return(list(result = NULL)) } else { if (length(x$result) == 0) { return(list(result = NULL)) } return(purrr::compact(x)) } } # Process output pout \u0026lt;- out %\u0026gt;% purrr::map(compact_) %\u0026gt;% purrr::flatten() %\u0026gt;% dplyr::tibble( id = purrr::`%||%`(names(.x), seq_along(.x)), return = names(.), output = . ) # Flatten results if necessary if (.flatten) { n_error \u0026lt;- length(pout$return[pout$return == \u0026quot;error\u0026quot;]) if (n_error \u0026gt; 0) { warning( \u0026quot;Since \u0026#39;.flatten = TRUE\u0026#39;, a total of \u0026quot;, n_error, \u0026quot; errors are being ignored\u0026quot;, call. = FALSE ) } pout \u0026lt;- pout %\u0026gt;% dplyr::filter(return != \u0026quot;error\u0026quot;) %\u0026gt;% dplyr::select(-return) %\u0026gt;% tidyr::unnest() if (ncol(pout) == 1) { pout \u0026lt;- dplyr::pull(pout, output) } } return(pout) } # Get number of available cores get_cores \u0026lt;- purrr::partial(future::availableCores, constraints = \u0026quot;multicore\u0026quot;) # Import of future_options() future_options \u0026lt;- furrr::future_options ","permalink":"https://blog.curso-r.com/posts/2019-02-10-pvec/","tags":["algoritmos"],"title":"pvec: O laço perfeito"},{"author":["Julio"],"categories":["tutoriais"],"contents":" Se você gosta de fazer mapinhas, talvez já tenha visto nosso post para fazer mapas temáticos muito rápido. Mas esse post é tão… 2017!\nVamos ver agora como fazer isso ainda mais rápido, usando os excelentes pacotes sf e o brasileiríssimo brazilmaps.\nResposta Para quem não tem paciência de ler o post, a resposta está aqui, em 4 linhas de código! Carregue o tidyverse e o brazilmaps e rode o código abaixo.\nlibrary(tidyverse) library(brazilmaps) get_brmap(\u0026quot;City\u0026quot;, geo.filter = list(State = 33)) %\u0026gt;% left_join(pop2017, c(\u0026quot;City\u0026quot; = \u0026quot;mun\u0026quot;)) %\u0026gt;% ggplot() + geom_sf(aes(fill = pop2017/1e6)) No caso, estamos fazendo um mapa do Rio de Janeiro (Unidade Federativa de código 33) e colorindo o mapa dos municípios com a população das cidades em 2017 (pop2017, dividida por um milhão). A cidade em azul claro é a capital.\nOK, mas como fizemos isso, e como podemos criar mapas mais bonitos? É o que veremos a seguir!\nObjetivo Nosso objetivo é fazer mapas temáticos, que são mapas que levam alguma característica das regiões a cores dessas regiões. O nome específico desse gráfico é mapa coroplético (choropleth map). Vamos fazer dois deles: um de unidades federativas e um de municípios.\nPara fazer um mapa desse tipo, precisamos de dois insumos principais: i) os dados que vão compor as cores de cada região e ii) os polígonos para desenhar o mapa.\nGeralmente esses dois insumos vêm de bases de dados diferentes, então é importante que exista uma chave para ligá-las através de um join. Se você obteve seus dados de uma fonte oficial, provavelmente você terá o código do município ou da unidade federativa. Se não, provavelmente você terá de cruzar pelos nomes, o que pode ser uma fonte de dores de cabeça.\nPara reproduzir as análises que seguem, carregue os pacotes tidyverse e brazilmaps:\nlibrary(tidyverse) library(brazilmaps) Polígonos A melhor fonte oficial para obter os polígonos do Brasil é o Instituto Brasileiro de Geografia e Estatística (IBGE). Lá, você vai encontrar malhas territoriais em um formato chamado shapefile. Esse arquivo, que na verdade é um conjunto de arquivos com dados e metadados, contém as informações necessárias para desenhar os polígonos no seu mapa.\nPor muito tempo, a leitura desse tipo de arquivo no R foi uma dor de cabeça. Até 2015 pelo menos, eu utilizava o sp e o maptools, e sempre sofria um pouco para carregar a base e fazer mapas.\nAgora, com o pacote sf, isso ficou bem mais fácil. Atendendo aos princípios tidy, o sf armazena mapas em data.frames, onde cada cada linha é um território, cada coluna representa uma característica desse território, e a lista-coluna geometry contém as informações dos polígonos que representam esse território.\nComo objetos de classe sf são data.frames, estamos em casa. Todas operações dos amados pacotes dplyr e ggplot2 já estão preparadas para lidar com esses objetos, logo a tarefa de trabalhar com mapas se resume à tarefa de transformação e visualização de dados, do jeito que gostamos.\nE como faz para ler esses dados no R? Com o pacote brazilmaps, basta utilizar a função get_brmap(). Essa função recebe parâmetros do tipo de região que se deseja baixar, e também algum filtro que se deseja aplicar. Por exemplo, se você quiser carregar a base no nível Região, utilize\nlibrary(brazilmaps) mapa \u0026lt;- get_brmap(\u0026quot;Region\u0026quot;) glimpse(mapa) Note que a base de dados contém apenas cinco linhas, uma para cada região, e a coluna especial geometry. Se você deseja obter todos os municípios do estado de São Paulo, utilize\nmapa \u0026lt;- get_brmap(\u0026quot;State\u0026quot;, geo.filter = list(State = 35)) glimpse(mapa) O pacote brazilmaps permite que você obtenha as malhas em seis níveis diferentes: Brasil, Regiões, Estados, Municípios, Mesorregiões e Microrregiões.\nDados Para preencher os dados, vamos utilizar a base pnud_muni do pacote abjData. Esse pacote foi desenvolvido pela Associação Brasileira de Jurimetria (ABJ) para guardar e organizar dados úteis para a Jurimetria. O pacote não está disponível no CRAN, então para utilizá-lo você precisará instalar o pacote do GitHub com\ndevtools::install_github(\u0026quot;abjur/abjData\u0026quot;) A base de dados pnud_muni, em particular, contém as informações utilizadas para calcular o Índice de Desenvolvimento Humano (IDH) a nível municipal, com base nos dados dos Censos de 1991, 2000 e 2010. É uma base muito rica e pode ser utilizada para realizar diversas análises interessantes.\nNo caso, utilizaremos apenas as colunas identificadoras de município, unidade federativa, região e IDH, e apenas o ano de 2010. Faremos isso rodando:\npnud_minima \u0026lt;- abjData::pnud_muni %\u0026gt;% filter(ano == 2010) %\u0026gt;% select(cod_uf = uf, cod_muni = codmun7, starts_with(\u0026quot;idh\u0026quot;)) glimpse(pnud_minima) Essa base de dados contém somente 5565 municípios, que eram os que existiam na época que os dados foram levantados. Atualmente, temos 5570 municípios no Brasil.\nJuntando as bases Vamos primeiro calcular o IDH médio por unidade federativa, depois juntar com a base de estados. Rigorosamente falando, nós não deveríamos fazer isso, pois a média dos IDHs municipais de uma unidade federativa não é o IDH da unidade federativa. Mas vida que segue:\npnud_minima_estado \u0026lt;- pnud_minima %\u0026gt;% group_by(cod_uf) %\u0026gt;% summarise(idh = mean(idhm)) glimpse(pnud_minima_estado) Agora vamos juntar com o mapa de unidades federativas:\nuf_map \u0026lt;- get_brmap(\u0026quot;State\u0026quot;) %\u0026gt;% inner_join(pnud_minima_estado, c(\u0026quot;State\u0026quot; = \u0026quot;cod_uf\u0026quot;)) glimpse(uf_map) Parece que funcionou! Agora, temos uma coluna idh. Agora vamos fazer um mapa. Para isso, usaremos uma operação geométrica especial do ggplot2: o geom_sf(). Essa função permite desenhar o mapa diretamente, sem a necessidade de realizar transformações nos dados nem mapear longitude e latitude.\nuf_map %\u0026gt;% ggplot() + geom_sf(aes(fill = idh)) O gráfico mostra claramente o aspecto regional do IDH nos estados do Brasil.\nPara fazer o gráfico com os municípios, a lógica é a mesma: i) fazer um join das bases com dplyr e ii) usar o ggplot2 para desenhar o mapa. O único problema é que o gráfico demora para rodar1.\nmuni_map \u0026lt;- get_brmap(\u0026quot;City\u0026quot;) %\u0026gt;% left_join(pnud_minima, c(\u0026quot;City\u0026quot; = \u0026quot;cod_muni\u0026quot;)) muni_map %\u0026gt;% ggplot() + geom_sf(aes(fill = idhm)) Podemos também quebrar pelas regiões com um simples facet_wrap():\nmuni_map %\u0026gt;% ggplot() + geom_sf(aes(fill = idhm)) + facet_wrap(~Region) Parece que estamos indo bem. Alguns problemas que podemos notar:\nAs linhas estão muito largas. As cores ajudam a identificar tendências regionais, mas parece que a imagem está embaçada. Os mapas não estão centralizados em cada facet. E não adianta usar o parâmetro scales=\"free\", pois infelizmente o sistema de coordenadas do sf não suporta essa funcionalidade. Resolveremos esses problemas na próxima seção.\nMasterizando os mapas Tweaks e temas O ggplot2 é um sistema extremamente flexível e permite que você altere praticamente qualquer elemento do gráfico. No caso de mapas, eu gosto de trabalhar com três coisas específicas: i) deixar as linhas na cor preta e um pouco mais finas e ii) tirar o fundo cinza do mapa e iii) mudar as cores das regiões. Um exemplo de como fazer isso:\nmuni_map %\u0026gt;% ggplot() + geom_sf(aes(fill = idhm), # ajusta tamanho das linhas colour = \u0026quot;black\u0026quot;, size = 0.1) + # muda escala de cores scale_fill_viridis_c(option = 2) + # tira sistema cartesiano theme(panel.grid = element_line(colour = \u0026quot;transparent\u0026quot;), panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank()) Outra coisa legal de fazer é demarcar os estados com outra largura da linha. Podemos fazer isso adicionando mais um geom_sf():\nmuni_map %\u0026gt;% ggplot() + geom_sf(aes(fill = idhm), # ajusta tamanho das linhas colour = \u0026quot;black\u0026quot;, size = 0.1) + geom_sf(data = get_brmap(\u0026quot;State\u0026quot;), fill = \u0026quot;transparent\u0026quot;, colour = \u0026quot;black\u0026quot;, size = 0.5) + # muda escala de cores scale_fill_viridis_c(option = 2) + # tira sistema cartesiano theme(panel.grid = element_line(colour = \u0026quot;transparent\u0026quot;), panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank()) No caso, usei a paleta de cores viridis. Eu gosto muito dessa paleta e uso sempre que possível :)\nCategorizar ou não, eis a questão No contexto de mapas, geralmente nosso interesse está em mapear informações ordinais ou numéricas. Quando a informação é numérica, o mapa coroplético pode ficar um pouco difícil de interpretar para o cérebro, pois temos de fazer uma conta com as cores para entender a diferença entre duas regiões. Além disso, a legenda acaba não ajudando muito, pois é dificil de associar uma cor da legenda com a cor do gráfico.\nPor isso, quando a informação é numérica, uma boa prática é categorizar a variável que queremos observar em um conjunto pequeno de categorias (entre 3 e 7), para facilitar a visualização. Isso induz um erro na análise, mas facilita a interpretação de quem está vendo o gráfico.\nPodemos categorizar nosso mapa usando a função cut():\nmuni_map %\u0026gt;% mutate(idhm = cut(idhm, c(0, 0.5, 0.6, 0.75, 1.0))) %\u0026gt;% ggplot() + geom_sf(aes(fill = idhm), # ajusta tamanho das linhas colour = \u0026quot;black\u0026quot;, size = 0.1) + geom_sf(data = get_brmap(\u0026quot;State\u0026quot;), fill = \u0026quot;transparent\u0026quot;, colour = \u0026quot;black\u0026quot;, size = 0.5) + # muda escala de cores scale_fill_viridis_d(option = 2, begin = 0.2, end = 0.8) + # tira sistema cartesiano theme(panel.grid = element_line(colour = \u0026quot;transparent\u0026quot;), panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank()) Não sei vocês, mas para mim faz muito mais sentido olhar o gráfico categorizado do que o contínuo.\nFazendo facets de mapas Para fazer vários mapas menores e juntar em um só, podemos usar o fantástico pacote patchwork. Esse pacote permite somar vários mapas, como mágica. Primeiro, vamos montar uma função que gera o mapa para uma determinada região.\ngrafico_regiao \u0026lt;- function(regiao) { # titulo do mapa titulo \u0026lt;- switch(regiao, \u0026quot;1\u0026quot; = \u0026quot;Norte\u0026quot;, \u0026quot;2\u0026quot; = \u0026quot;Nordeste\u0026quot;, \u0026quot;3\u0026quot; = \u0026quot;Sudeste\u0026quot;, \u0026quot;4\u0026quot; = \u0026quot;Sul\u0026quot;, \u0026quot;5\u0026quot; = \u0026quot;Centro-Oeste\u0026quot;) muni_map %\u0026gt;% filter(Region == regiao) %\u0026gt;% mutate(idhm = cut(idhm, c(0, 0.5, 0.6, 0.75, 1.0))) %\u0026gt;% ggplot() + geom_sf(aes(fill = idhm), # ajusta tamanho das linhas colour = \u0026quot;black\u0026quot;, size = 0.1) + geom_sf(data = filter(get_brmap(\u0026quot;State\u0026quot;), Region == regiao), fill = \u0026quot;transparent\u0026quot;, colour = \u0026quot;black\u0026quot;, size = 0.5) + # muda escala de cores scale_fill_viridis_d(option = 2, begin = 0.2, end = 0.8) + guides(fill = FALSE) + # tira sistema cartesiano theme(panel.grid = element_line(colour = \u0026quot;transparent\u0026quot;), panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank()) + ggtitle(titulo) } mapas \u0026lt;- map(1:5, grafico_regiao) Agora, vamos somar esses mapas:\nlibrary(patchwork) reduce(mapas, magrittr::add) Pronto! Temos nossos mapas lindos maravilhosos no tidy way!\nWrap-up Use brazilmaps para carregar os mapas. Faça join com a sua base de interesse. Use geom_sf() do ggplot2 para plotar os mapas. Customize seus mapas com as opções do ggplot2 e com o pacote patchwork! Se quiserem ver uma aplicação legal disso, dêem uma olhada nessa análise da Bruna Wundervald elaborado durante a Datathon do RLadies-SP.\nÉ isso pessoal. Happy coding ;)\nNão por muito tempo! Veja aqui just more than tripled the speed of geom_sf… in case anyone is interested in spatial plotting… — Thomas Lin Pedersen (@thomasp85) February 4, 2019 ↩︎ ","permalink":"https://blog.curso-r.com/posts/2019-02-10-sf-miojo/","tags":["mapas"],"title":"Gráficos Miojo: mapas temáticos, the tidy way"},{"author":["Daniel"],"categories":["tutoriais"],"contents":" Esse post serve como introdução aos modelos de suavização exponencial e replica um exemplo disponível no livro Forecasting principles and practice (FPP) disponível aqui. Em seguida implementamos usando o pacote rstan uma abordagem bayesiana para o mesmo problema usando como referência o paper Fitting and Extending Exponential Smoothing Models with Stan.\nVamos portanto começar descrevendo o modelo de suavização exponencial simples. Fazer um exemplo inicial usando o conhecido pacote forecast e partir para a implementação com rstan.\nSuavização exponencial simples Suavização exponencial é uma das principais classes de modelos usados em previsão de séries temporais. Essa classe inclui diversos tipos de modelos como o bem conhecido Holt Winters e até o Theta Method - conhecido por ter ido bem em uma M3-competition.\nA suavização exponencial simples, com o próprio nome diz, é o caso particular mais simples de suavização exponencial. Esse tipo de modelo serve para quando não existe nenhum padrão claro de sazonalidade ou tendência na série.\nNesta parte introdutória vou parafrasear o próprio Hyndman no FPP.\nConsidere que \\(y_t\\) seja a observação no tempo t de uma série temporal. O modelo mais simples possível e que também é chamado de ingênuo (naive) seria:\n\\[y_{T+1|T} = y_{T}\\] Isto é, a previsão para \\(T+1\\) dado que conhecemos o valor da série até o instante \\(T\\) seria simplesmente o último valor observado da série \\(y_T\\). Neste modelo, a última observação da série é única importante para prever o seu próximo valor.\nAgora considere um modelo chamado Average method em que consideraremos que a previsão para a próxima observação da série é a média aritmética de todos os valores anteriores da série. Em notação matemática:\n\\[y_{T+1|T} = \\frac{1}{T}\\sum_{t=1}^{T}y_t\\]\nNeste modelo, todas as observações anteriores da série possuem o mesmo peso (\\(1/T\\)) na previsão do próximo valor da série.\nEm geral queremos um meio termo. Não queremos que todas as observações tenham o mesmo peso, nem que só a última tenha o maior peso. Poderíamos escrever um modelo assim:\n\\[y_{T+1|T} = \\sum_{t=1}^{T}\\beta_t*y_t\\] Com \\(\\sum_{t=1}^{T}\\beta_t = 1\\). Neste modelo cada observação do passado teria um pesinho \\(\\beta_t\\) que poderíamos estimar por meio de algum algoritmo. A previsão para o próximo período da série seria uma média ponderada das observações passadas.\nNo entanto, isso não é viável pois teríamos tantos parâmetros quanto observações.\nPara simplificar esse problema, a suavização exponencial faz a suposição de que o peso das observações passadas vai diminuindo conforme elas estão mais distantes do ponto que estamos tentando prever. Mais especificamente esses pesos decaem exponencialmente com o passar do tempo. Portanto, o modelo é reescrito para:\n\\[y_{T+1|T} = \\alpha y_t + \\alpha(1 - \\alpha)y_{t-1} + \\alpha(1 - \\alpha)^2y_{t-2} + ...\\]\nEm que \\(0 \\le \\alpha \\le 1\\) é o parâmetro de suavização. Neste modelo a previsão para o próximo período da série é uma média ponderada das observações passadas em que os pesos decrescem exponencialmente com uma taxa \\(\\alpha\\).\nQuando \\(\\alpha \\approx 0\\) então as observações mais distantes damos mais peso para as observações mais distantes. Quando \\(\\alpha = 1\\) ficamos exatamente com o modelo ingênuo.\nA forma de componentes Em suavização exponencial é comum escrever os modelos em forma de componentes. Isso facilita extensões como adicionar tendências e sazonalidades. Para fazer suavização exponencial simples só precisamos do componente \\(l_t\\) (de level). Em outros métodos também usam-se os componentes \\(b_t\\) (tendência) e \\(s_t\\) (sazonalidade).\nNa forma de componente escrevemos o modelo da seguinte forma:\nEquação de previsão: \\(\\hat{y_{t+1|t}} = l_t\\). Equação de suavização: \\(l_t = \\alpha y_t + (1 - \\alpha )l_{t-1}\\) Note que essa forma é equivalente ao que escrevemos anteriormente - basta substituir \\(l_t\\) por \\(\\hat{y_{t+1|t}}\\) e \\(l_{t-1}\\) por \\(\\hat{y_{t|t-1}}\\) na equação de suavização.\nEscrevendo o modelo desta forma fica fácil de ver que precisamos escolher dois parâmetros: \\(\\alpha\\) e \\(l_0\\). Com os dois conseguimos prever qualquer valor da série.\nAntigamente (antes de computadores existirem) era comum definir \\(l_0\\) como o primeiro valor observado da série e \\(\\alpha\\) uma valor pequeno, por exemplo \\(0,2\\).\nHoje em dia temos jeitos melhores de encontrar bons valores para \\(\\alpha\\) e \\(l_0\\).\nPodemos minimizar a soma dos quadrados dos resíduos:\n\\[SSE = \\sum_{t=1}^{T}(y_t - \\hat{y}_{t|t-1})^2\\]\nOu assumir que \\(y_t \\sim Normal(l_t, \\sigma^2)\\) e estimar via máxima verossimilhança. É isso que a função ses do pacote forecast faz. As contas da verossimilhança são chatinhas, mas dá para provar que para suavização exponencial simples as duas abordagens são equivalentes.\nComo não é possível resolver algébricamente o problema de minimização, precisamos de um algoritmo numérico. A função ses também possui algumas heurísticas para decidir os valores iniciais dos parâmetros.\nTudo isso está escrito no livro Forecasting with Exponential Smoothing também do Hyndman et al.\nExemplo No exemplo, vamos usar esse banco de dados oil que vem no pacote fpp (do livro FPP). Esse banco de dados contém a produção anual de petróleo na Arábia Saudita.\nPara o exemplo pegamos dados desde 1996 até 2010.\nlibrary(forecast) ## Registered S3 method overwritten by \u0026#39;quantmod\u0026#39;: ## method from ## as.zoo.data.frame zoo library(fpp) ## Loading required package: fma ## Loading required package: expsmooth ## Loading required package: lmtest ## Loading required package: zoo ## ## Attaching package: \u0026#39;zoo\u0026#39; ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## as.Date, as.Date.numeric ## Loading required package: tseries data(oil) oildata \u0026lt;- window(oil, start=1996) plot(oildata) Para ajustar um modelo de suavização exponencial simples com o forecast podemos usar a função ses, mas ela já retorna os valores preditos para os próximos anos e não nos dá a oportunidade de ver os valores de \\(\\alpha\\) e \\(l_0\\) que ela escolheu.\nses(oildata) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 2011 469.2669 433.4186 505.1153 414.4416 524.0923 ## 2012 469.2669 423.6608 514.8730 399.5184 539.0155 ## 2013 469.2669 415.6505 522.8834 387.2676 551.2663 ## 2014 469.2669 408.6902 529.8437 376.6228 561.9110 ## 2015 469.2669 402.4512 536.0827 367.0810 571.4529 ## 2016 469.2669 396.7469 541.7870 358.3571 580.1768 ## 2017 469.2669 391.4597 547.0742 350.2710 588.2629 ## 2018 469.2669 386.5096 552.0243 342.7005 595.8334 ## 2019 469.2669 381.8393 556.6946 335.5579 602.9760 ## 2020 469.2669 377.4061 561.1277 328.7780 609.7559 Na nomenclatura do Hyndman para os modelos da classe de suavização exponencial, a suavização exponencial simples é chamada de ANN. Não vou entrar em detalhes mas, isto está descrito aqui.\nUsamos então a função ets com o argumento model = \"ANN\".\nmodelo \u0026lt;- ets(oildata, \u0026quot;ANN\u0026quot;) modelo ## ETS(A,N,N) ## ## Call: ## ets(y = oildata, model = \u0026quot;ANN\u0026quot;) ## ## Smoothing parameters: ## alpha = 0.7869 ## ## Initial states: ## l = 448.1262 ## ## sigma: 27.9726 ## ## AIC AICc BIC ## 144.4110 146.5929 146.5352 Agora sim, podemos ver os valores estimados para \\(\\alpha\\) e \\(l_0\\).\nplot(forecast(modelo, h=5)) Até aqui vimos como funciona o método de suavização exponencial simples, como o pacote forecast estima os seus parâmetros (método de máxima verossimilhança) e como fazer para ajustar esse modelo com o forecast, bem como obter os valores estimados para os parâmteros.\nAgora vamos tentar obter estimativas para os parâmetros \\(\\alpha\\) e \\(l_0\\) usando uma abordagem bayesiana.\nA abordagem bayesiana A partir do momento que sabemos escrever o modelo e sabemos todos os parâmetros que precisamos estimar fica fácil fazer com Stan.\nPara quem não conhece, Stan é uma plataforma para ajustar modelos probabilísticos e possui uma linguagem probabilística própria. Quando você escreve os modelos nessa linguagem, o Stan consegue compilar o seu modelo e criar um programa em C++ muito rápido para ajustar o seu modelo.\nAbaixo definimos o modelo de suavização exponencial simples usando a linguagem probabilística do Stan. Esse código é uma leve adaptação do código que está neste paper do Slawek Smyl (esse cara é muito bom, trabalha na Uber e ganhou recentemente competições de forecast).\nVeja que na linguagem definimos quais são os dados de entrada:\nn - o tamanho da série y - um vetor de tamanho n com valores positivos. mean_y - a média dos valores de y var_y - a variância dos valores de y Em seguida define quais são os parâmetros:\nsigma - variância do erro alpha - parâmetro da suavização exponencial l_0 - valor inicial do componente nível. Em seguida define os parâmteros transformados - aqueles que podem ser obtidos a partir dos valores definidos em parameters.\nPor fim, definimos o modelo.\nPrimeiro dizemos que a priori do sigma é a cauda positiva de uma cauchy. Depois que a priori para \\(l_0\\) é uma Normal com a mesma média e desvio padrão do que a própria série. Falamos que \\(y_t\\) tem distribuição normal com média \\(l_{t-1}\\) e desvio padrão \\(sigma\\). Não especificar uma priori para \\(\\alpha\\) indica que estamos assumindo uma priori uniforme no intervalo. data { int\u0026lt;lower=1\u0026gt; n; vector\u0026lt;lower=0\u0026gt;[n] y; real\u0026lt;lower=0\u0026gt; mean_y; real\u0026lt;lower=0\u0026gt; sd_y; } parameters { real\u0026lt;lower=0\u0026gt; sigma; real \u0026lt;lower=0,upper=1\u0026gt;alpha; real l_0; } transformed parameters { vector\u0026lt;lower=0\u0026gt;[n] l; l[1] = l_0; for (t in 2:n) { l[t] = alpha*y[t] + (1-alpha)*(l[t-1]) ; } } model { sigma ~ cauchy(0,2) T[0,]; // positive side of Cauchy distribution as prior l_0 ~ normal(mean_y, sd_y); for (t in 2:n) { y[t] ~ normal(l[t-1], sigma); } } Agora ajustamos o modelo e obtemos as estimativas dos parâmetros.\nlibrary(rstan) data \u0026lt;- list( n = length(oildata), y = as.numeric(oildata), mean_y = mean(oildata), sd_y = sd(oildata) ) model \u0026lt;- sampling(ann, data = data) print(model, pars = c(\u0026quot;sigma\u0026quot;, \u0026quot;l_0\u0026quot;, \u0026quot;alpha\u0026quot;)) ## Inference for Stan model: 7201dbd28b82b04c1f1a89c8607dc886. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## sigma 28.72 0.10 5.87 19.78 24.70 27.79 31.85 42.15 3184 1 ## l_0 460.02 0.36 20.76 419.79 446.52 459.43 473.98 501.41 3260 1 ## alpha 0.71 0.00 0.19 0.25 0.59 0.74 0.86 0.98 3852 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Jan 25 20:11:49 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). Pelo menos o Stan já nos dá a previsão para 2011 por meio do parâmetro l_15:\nprint(model, par = \u0026quot;l[15]\u0026quot;) ## Inference for Stan model: 7201dbd28b82b04c1f1a89c8607dc886. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## l[15] 471.88 0.07 4.46 467.73 468.31 470.13 474.18 483.14 4029 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Jan 25 20:11:49 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). Sabemos que é o l[15] pois sabemos que \\(y_{T+1|T} = l_t\\) e \\(T=15\\) na nossa série.\nBom, como era de se esperar, as estimativas dos parâmetros ficaram parecidas com as do pacote forecast. Então por que usar Stan? No forecast ele já me dá a previsão com intervalo de confiança - aqui vou ter que escrever tudo do zero.\nA vatangem é flexibilidade. O paper do Slawek comenta bastante sobre isso:\nFirst, the variations among consecutive time series points are sometimes too large to be fully captured by a light-tailed distribution like the Normal Distribution. Second, the time series sometimes have growth rates faster than linear but slower than exponential. Third, the randomness of the time series often grows with values. Fourth, from the business side, expert subjective information should be integrated into forecasting. Last but not least, from the implementation side, forecasts should be generated in a scalable and automated fashion when hundreds or even thousands curves to be forecasted every day. The forecasting methodology should require a minimum amount of data scientists’ attention when in production mode.\nEscrevendo o modelo probabilístico com Stan, podemos lidar com cada um desses pontos para definir um modelo mais adequado.\nConclusão Aprendemos:\nO que são e como funcionam os modelos de suavização exponencial simples. Como ajustá-los com o pacote forecast e como o forecast ajusta esses modelos. Como ajustar SES no Stan. ","permalink":"https://blog.curso-r.com/posts/2019-02-10-ses/","tags":["estatística"],"title":"Suavização exponencial simples com R"},{"author":["Bruna Wundervald"],"categories":["divulgação"],"contents":" Na semana passada, eu participei da minha primeira rstudio::conf, em Austin, no Texas. Essa conferência é, como o nome pode sugerir, organizada pelo pessoal do RStudio, e tem como objetivo reunir workshops e apresentações sobre tudo de R \u0026amp; RStudio (“All things R and RStudio”). Eu fui para a conferência com uma das bolsas de diversidade das 6 oferecidas para pessoas de fora dos Estados Unidos. O oferecimento das bolsa já é uma tradição, visto que o RStudio é umas das empresas que mais apoia e promove a diversidade na nossa comunidade.\nPrimeiramente, eu preciso dizer que aproveitei bastante.\nTudo foi excepcional: os workshops, as apresentações, as pessoas extremamente receptivas, a sessão de e-posters, os grupos temáticos de conversa, a localização. Algo muito importante é que, em todos os momentos, é muito claro o quanto qualquer tipo de julgamento ou assédio não é bem vindo ali. Os participantes são respeitados e acolhidos uns pelos outros, independente de qualquer característica externa, como raça ou gênero. Pode parecer óbvio que em todas as conferências e congressos essa deva ser uma constante, mas isso não é o que acontece na realidade. Neste post, eu vou falar brevemente sobre minha experiência na conferência deste ano.\nWorkshop Eu participei do workshop de aprendizado de máquina na prática, ministrado majoritariamente pelo Max Kuhn. Em resumo, o workshop consistiu em apresentar as principais etapas de um projeto envolvendo aprendizado de máquina (principios basicos - engenharia de covariaveis - modelagem (regressão/classificação)), mas seguindo o framework do tidymodels, o descendente (e complementar) do caret que tem como objetivo ser uma interface uniforme de modelagem usando R.\nA razão pela qual se justifica a existência do tidymodels é que existe uma grande variância na forma de usar os diferentes pacotes de modelagem. A versão tidy, por sua vez, faz o reuso das estruturas de dados, tornando mais fácil o encadeamento de funções através do pipe e é compatível com programação funcional (purrr). A sequência do workshop foi:\nPrincípios básicos: manipulação de dados com dplyr, visualização com ggplot2, um pouco de purrr e quasiquotation e modelagem com parsnip.\nEngenharia de covariáveis e pré-processamento com recipes rsample,parsnip\nModelagem, para regressão e classificação, usando o recipes, caret, broom e tidyposterior.\nE com isso, o framework foi apresentado do começo ao fim com apliação em alguns estudos de caso. Os materiais do workshop, claro, estão disponíveis aqui: https://github.com/topepo/rstudio-conf-2019\nApresentações É realmente meio difícil escolher quais palestras assistir durante a rstudio::conf, geralmente são 3 ocorrendo ao mesmo tempo e todas são interessantes. Eu preferi escolher que mais se aproximaram dos meis interesses pessoais ou que pareciam algo complicado de entender por conta própria. Aqui eu vou dar só um overview sobre as minhas 3 apresentações favoritas, mas os slides e vídeos de todas elas podem ser encontrados aqui e aqui:\n\"The unreasonable effectiveness of public work’, David Robinson: a palestra foi basicamente sobre a efetividade do compartilhamento de conhecimento online. David mostrou como ele mesmo foi convidado a trabalhar no StackOverFlow após ter dado uma resposta muito boa à uma das perguntas de lá. A partir disso, ele desenvolve e exemplifica porque é tão importante a existência de trabalhos públicos (e reprodutíveis!), seja no Twitter, blogs, GitHub, etc. A dica fundamental do David é: se você já precisou repetir uma mesma coisa várias vezes, torne-a “automática”, o que nesse contexto significa\nse você já precisou escrever a mesma função diversas vezes, faça um pacote; se você já precisou dar o mesmo conselho diversas vezes, escreva um post num blog. Fenomenal. Essa palestra vale muito a pena e eu com certeza vou usar ela como referência futura.\n“A guide to modern reproducible data science with R”, Karthik Ram: Ok, eu sei, mais uma palestra sobre ciência reproduzível, eu realmente quero enfatizar a importância dela aqui. Essa palestra complementa a do David, mas nesse caso, Karthik nós mostra quais são as ferramentas atuais para ciência reproduzível usando R e fornece os links para todas elas depois. Ou seja, agora não há mais desculpas: temos a motivação e as ferramentas para compartilhar nossos trabalhos e fortalecer a comunidade cada vez mais :)\n“Tidy eval in context”, Jenny Brian: non-standard evaluation é algo que eu estou achando extremamente útil ultimamente. A maioria dos meus códigos mais recentes usam em algum momento, mas até antes dessa palestra, eu não poderia dizer que entendia 100% o que é non-standard evaluation. Jenny apresentou casos sobre por que, quando e como usar NSE de forma correta, o que clarificou bastante a ideia pra mim.\nBonus: tidyverse developer day A rstudio::conf tem um evento satélite chamado tidyverse developer day, que acontece no dia posterior ao do encerramento do evento. Nesse dia, os participantes se reunem para trabalhar em algo relacionado ao tidyverse ou algum outro projeto de interesse (cada um escolhe o seu). Pode ser resolver uma issue no GitHub, escrever um post ou um livro, criar novos projetos com as pessoas que estão ali, trabalhar em seus próprios pacotes, qualquer coisa. O que isso tem de especial é: você tem a oportunidade de estar trabalhando em conjunto com todos os membros do RStudio e outros desenvolvedores importantes. Eles tiram dúvidas, dão dicas/sugestões, ideias novas. Eu foquei em resolver issues do broom e pude conversar com os próprios autores do pacote, especialmente o Alex Hayes, sobre o funcionamento do mesmo e como ele pode ser aprimorado. Essa é uma forma bem efetiva de estimular a participação e criatividade de todos e demonstrar que nós podemos contar uns com os outros.\nA pergunta que não quer calar: vale a pena? Resposta: sim. Essa foi a conferência mais cara que eu já participei (tudo bem, eu tive bolsa, mas quero falar do custo em geral) considerando evento + hotel + passagens + visto + outros gastos. Com certeza não é um valor que todos os acadêmicos podem pagar, mas o RStudio está trabalhando neste problema. Porém, apesar do preço eu considero que vale a pena principalmente pela parte do networking e de ser uma experiência de conferência totalmente diferente das outras. A maioria das pessoas que nós conhecemos, seja porque é autor de um bom pacote ou pelos contatos via Twitter, estão lá. Isso já configura uma ótima oportunidade de estender a rede profissional e acadêmica e o participantes são excepcionalmente receptivos. As palestras são absolutamente excelentes. Tirando a parte dos minicursos, essa não é uma conferência para se aprender a programar em R, e sim aprender sobre tudo que é relacionado mas vai além disso. Eu tive a impressão de que as palestras são muito cuidadosamente selecionadas e apresentadas de forma excepcional, tornando experiência de estar ali assistindo realmente única. Ano que vem estarei lá novamente na rstudio::conf 2020, que vai ser em São Francisco :)\nPS O Hadley gostou de cachaça e voltei com muitos stickers:\n@bwundervald thanks for the cachaça — it’s delicious!\n\u0026mdash; Hadley Wickham (@hadleywickham) January 22, 2019 stiiiickers 👩‍💻🎉 #rstats pic.twitter.com/J7N4NZXbmE\n\u0026mdash; Bruna Wundervald (@bwundervald) January 16, 2019 ","permalink":"https://blog.curso-r.com/posts/2019-01-30-rconf/","tags":["rstudio"],"title":"Resumo da rstudio::conf 2019"},{"author":["Julio"],"categories":["tutoriais"],"contents":" Nesse post vamos discutir um pouco sobre regressão logística, tensorflow e Modelos Lineares Generalizados (Generalized Linear Models, GLMs). Não vou economizar nas matemáticas nem nos códigos.\nSe você não conhece GLMs, recomendo dar uma lida, pelo menos na introdução, do livro do professor Gilberto A. Paula. Se você não conhece o Tensorflow, recomendo ver a página do RStudio sobre Tensorflow. Se você curte a parte computacional da estatística, esse livro do LEG-UFPR é obrigatório. Eles são os melhores. Introdução: o tensorglm Um de meus interesses no momento é implementar GLMs usando Tensorflow. O Tensorflow é uma biblioteca computacional mantida pela Google que utiliza paralelização e o poder das GPUs (Graphical Processing Units) para fazer contas. O Tensorflow foi especialmente desenhado para facilitar o ajuste de redes neurais profundas e outros modelos sofisticados.\nGLMs são casos particulares de redes neurais. Uma rede neural com apenas uma camada e com funções de perda / verossimilhanças baseadas na Divergência de Kullback-Leibler são exatamente iguais aos GLMs. Por exemplo, essa divergência equivale ao erro quadrático médio para a distribuição gaussiana e binary-crossentropy para logística.\nPor isso, não é de se surpreender que já existam soluções prontas para modelos específicos, como regressão linear normal, logística, e até Poisson. No entanto, essas soluções têm duas limitações:\nNão são extensivas. Por exemplo, não achei códigos para as distribuições normal inversa, gama e binomial negativa.\nAs soluções atuais utilizam o algoritmo descida de gradiente para otimização, que é muito legal, mas não se aproveita de alguns resultados que temos na área de GLMs, como o IWLS (Iterated Weighted Least Squares), que é uma derivação do algoritmo Fisher-scoring, que reduz o problema do ajuste ao cálculo iterado de inversas e multiplicações de matrizes.\nMeu intuito é, então, montar uma solução alternativa que funcione igual à função glm() do R, mas usando Tensorflow no backend ao invés do algoritmo atual, que é em Fortran. Com isso, espero que o ajuste seja mais eficiente quando os dados são grandes e permita trabalhar com dados que não cabem na memória.\nA regressão logística Meu primeiro experimento com o tensorglm foi implementar a regressão logística usando tensorflow, com descida de gradiente. Considere o problema\n\\[P(Y=1\\;|\\;\\mu, x) = \\mu = \\sigma(\\alpha + \\beta x),\\]\nem que \\(Y\\) é nossa variável resposta, \\(x\\) é nossa variável explicativa, \\(\\alpha\\) e \\(\\beta\\) são os parâmetros que queremos estimar e \\(\\sigma(\\cdot)\\) é a função sigmoide, cuja inversa é a função de ligação logística.\n\\[\\sigma(\\eta) = \\frac{1}{1 + e^{-\\eta}}\\]\nConsiderando que temos observações \\(Y_1, \\dots, Y_n\\) condicionalmente independentes, já temos o suficiente para especificar nosso modelo de regressão logística. O próximo passo é definir, com base nisso, a função que queremos otimizar.\nA partir de uma amostra \\(y_1, \\dots, y_n\\) e observando que \\(\\mu_i = \\sigma(\\alpha + \\beta x_i)\\), a verossimilhança do modelo é dada por\n\\[ \\mathcal L((\\alpha, \\beta)|\\mathbf y) = \\prod_{i=1}^n f(y_i|(\\alpha, \\beta), x_i) = \\prod_{i=1}^n\\mu_i^{y_i}(1-\\mu_i)^{1-y_i} \\]\nO logaritmo da verossimilhança é dado por\n\\[ \\begin{aligned} l((\\alpha, \\beta)|\\mathbf y) \u0026amp;= \\sum_{i=1}^n y_i\\log(\\mu_i) + (1-y_i)\\log(1-\\mu_i)\\\\ \u0026amp;= \\sum_{i=1}^n y_i\\log(\\sigma(\\alpha + \\beta x_i)) + (1-y_i)\\log(1 - \\sigma(\\alpha + \\beta x_i)) \\end{aligned} \\]\nNosso objetivo é maximizar \\(l\\) com relação à \\(\\alpha\\) e \\(\\beta\\).\nDetalhe: essa soma, se multiplicada por -1, também é chamada de função de perda binary cross-entropy. Por isso que tanto faz você definir GLMs a partir de \\(P(Y|x)\\) ou a partir da função de perda!\nOK, problema dado! vamos implementar usando tensorflow!\nlibrary(tensorflow) # gerando X usando uma distribuição qualquer set.seed(123) N \u0026lt;- 10000 x \u0026lt;- rnorm(N) # gerando y usando distribuição binomial (com n = 1 para ser bernoulli) # aqui alpha_gerador = 1 e beta_gerador = 2 sigmoide \u0026lt;- function(eta) 1 / (1 + exp(-eta)) y \u0026lt;- rbinom(N, 1, sigmoide(1 + 2 * x)) # transformando esses vetores objetos do tensorflow x \u0026lt;- tf$to_float(x) y \u0026lt;- tf$to_float(y) # inicialização das variáveis # o parâmetro seed é para permitir reprodutibilidade alfa \u0026lt;- tf$Variable(tf$random_normal(shape(1L), seed = 1)) beta \u0026lt;- tf$Variable(tf$random_normal(shape(1L), seed = 1)) # cálculo da perda mu \u0026lt;- sigmoide(alfa + beta * x) perda \u0026lt;- -tf$reduce_sum(y*log(mu) + (1-y)*log(1-mu)) Feito! Agora podemos usar a magia do tensorflow, que é esperto o suficiente para otimizar essa perda sem a gente se preocupar em calcular derivadas na mão. Para quem não conhece o algoritmo de descida de gradiente, ele funciona assim:\n\\[ (\\alpha, \\beta)_{\\text{novo}} = (\\alpha, \\beta)_{\\text{velho}} + k \\nabla_{(\\alpha, \\beta)} l((\\alpha, \\beta)_{\\text{velho}}), \\]\nonde\n\\(\\nabla_{(\\alpha, \\beta)} l((\\alpha, \\beta)_{\\text{velho}})\\) é o gradiente da verossimilhança em relação ao vetor \\((\\alpha, \\beta)\\), ou seja, são as derivadas parciais de \\(l\\) em relação à \\(\\alpha\\) e \\(\\beta\\). Isso dá a direção e intensidade em que os valores devem ser atualizados.\n\\(k\\) é chamado de learning rate, é um fator usado para controlar o tamanho do passo dado pelo gradiente. Esse valor normalmente é definido à mão. No caso dos GLMs, \\(k\\) é substituído pelo inverso da segunda derivada da \\(l\\) em relação aos parâmetros, gerando assim os algoritmos de Newton-Raphson e Fisher-scoring.\nDetalhe: se você procurar esse algoritmo na internet, você vai encontrar um \\(-\\) e não um \\(+\\). Isso acontece porque estamos usando a verossimilhança e não a perda.\n# definindo o otimizador k \u0026lt;- 0.001 # essa é a taxa de aprendizado k: learning rate otimizador \u0026lt;- tf$train$GradientDescentOptimizer(k) # calcular os gradientes e aplicar gradientes \u0026lt;- otimizador$compute_gradients(perda) atualizar \u0026lt;- otimizador$apply_gradients(gradientes) # agora realmente iniciamos o tensorflow para fazer as contas sess \u0026lt;- tf$Session() sess$run(tf$global_variables_initializer()) # agora aplicamos a atualização iterativamente # normalmente o número de iterações é escolhido dinamicamente # a partir da diferença entre os valores velhos e novos calculados. # se a diferença é muito pequena, então pode parar. iteracoes \u0026lt;- 10 for (step in seq_len(iteracoes)) { sess$run(atualizar) s \u0026lt;- \u0026#39;Iter: %02d, alpha=%s, beta=%s\\n\u0026#39; cat(sprintf(s, step, round(sess$run(alfa), 3), round(sess$run(beta), 3))) } Iter: 01, alpha=2.32, beta=3.593 Iter: 02, alpha=1.56, beta=3.409 Iter: 03, alpha=1.411, beta=2.989 Iter: 04, alpha=1.261, beta=2.665 Iter: 05, alpha=1.153, beta=2.422 Iter: 06, alpha=1.078, beta=2.257 Iter: 07, alpha=1.033, beta=2.154 Iter: 08, alpha=1.006, beta=2.095 Iter: 09, alpha=0.992, beta=2.062 Iter: 10, alpha=0.984, beta=2.045 Parece que funcionou! Agora sabemos ajustar uma regressão logística na mão, com o algoritmo de descida de gradiente… ou será que não?\nO Problema Vamos considerar o mesmo problema, mas agora com duas explicativas. temos\n\\[P(Y=1\\;|\\;\\mu, x) = \\mu = \\sigma(\\alpha + \\beta_1 x_2+ \\beta_2 x_2),\\]\nAs contas são exatamente as mesmas e vou omitir, mostrando apenas o código novo.\n# gerando X usando uma distribuição qualquer set.seed(1) N \u0026lt;- 10000 x1 \u0026lt;- rnorm(N) x2 \u0026lt;- rnorm(N) y \u0026lt;- rbinom(N, 1, sigmoide(1 + 2 * x1 + 3 * x2)) x1 \u0026lt;- tf$to_float(x1) x2 \u0026lt;- tf$to_float(x2) y \u0026lt;- tf$to_float(y) alfa \u0026lt;- tf$Variable(tf$random_normal(shape(1L), seed = 1)) beta1 \u0026lt;- tf$Variable(tf$random_normal(shape(1L), seed = 1)) beta2 \u0026lt;- tf$Variable(tf$random_normal(shape(1L), seed = 1)) mu \u0026lt;- sigmoide(alfa + beta1 * x1 + beta2 * x2) perda \u0026lt;- -tf$reduce_sum(y*log(mu) + (1-y)*log(1-mu)) k \u0026lt;- 0.001 otimizador \u0026lt;- tf$train$GradientDescentOptimizer(k) gradientes \u0026lt;- otimizador$compute_gradients(perda) atualizar \u0026lt;- otimizador$apply_gradients(gradientes) sess \u0026lt;- tf$Session() sess$run(tf$global_variables_initializer()) iteracoes \u0026lt;- 10 for (step in seq_len(iteracoes)) { sess$run(atualizar) s \u0026lt;- \u0026#39;Iter: %02d, alpha=%s, beta1=%s, beta2=%s\\n\u0026#39; cat(sprintf(s, step, round(sess$run(alfa), 3), round(sess$run(beta1), 3), round(sess$run(beta2), 3))) } Iter: 01, alpha=1.674, beta1=2.703, beta2=3.461 Iter: 02, alpha=NaN, beta1=NaN, beta2=NaN Iter: 03, alpha=NaN, beta1=NaN, beta2=NaN Iter: 04, alpha=NaN, beta1=NaN, beta2=NaN Iter: 05, alpha=NaN, beta1=NaN, beta2=NaN Iter: 06, alpha=NaN, beta1=NaN, beta2=NaN Iter: 07, alpha=NaN, beta1=NaN, beta2=NaN Iter: 08, alpha=NaN, beta1=NaN, beta2=NaN Iter: 09, alpha=NaN, beta1=NaN, beta2=NaN Iter: 10, alpha=NaN, beta1=NaN, beta2=NaN Oops! Explodiu! Por que será???\nUma forma de corrigir esse problema é considerando uma taxa de aprendizado k um pouco menor. Com os mesmos dados e modelo acima, ao fazer\nk \u0026lt;- 0.00094 e rodar novamente, já conseguimos chegar nos resultados abaixo.\nIter: 01, alpha=1.525, beta1=2.492, beta2=3.205 Iter: 02, alpha=1.183, beta1=2.32, beta2=3.36 Iter: 03, alpha=1.122, beta1=2.248, beta2=3.34 Iter: 04, alpha=1.101, beta1=2.208, beta2=3.296 Iter: 05, alpha=1.085, beta1=2.178, beta2=3.254 Iter: 06, alpha=1.073, beta1=2.152, beta2=3.216 Iter: 07, alpha=1.062, beta1=2.13, beta2=3.183 Iter: 08, alpha=1.053, beta1=2.112, beta2=3.154 Iter: 09, alpha=1.044, beta1=2.095, beta2=3.13 Iter: 10, alpha=1.037, beta1=2.082, beta2=3.109 Mais algumas iterações e o modelo converge.\nMas nós não queremos ficar fazendo um ajuste tão fino no valor de k, certo? Afinal, queremos resolver problemas do mundo real, não ficar escolhendo valores de k… Outra forma de resolver isso é evitando problemas numéricos nas contas. O cálculo da função de perda, por exemplo, pode ser melhorado. Mas como?\nBom, problemas numéricos não são minha especialidade, então agora é hora de seguir os mestres. Vamos olhar como o R e como o Tensorflow implementam as funções de perda para regressão logística.\nOs objetos de classe family no R No R, os GLMs buscam informações de objetos da classe family() para realizar os ajustes. No caso da logística, o objeto é retornado por uma função chamada binomial().\nfam \u0026lt;- binomial() O resultado disso é uma lista com vários métodos implementados. Por exemplo, a variância da binomial é dada por:\nfam$variance function (mu) mu * (1 - mu) \u0026lt;bytecode: 0x55fc8e220a18\u0026gt; \u0026lt;environment: 0x55fca4eb5040\u0026gt; A função de perda é dada pelo método fam$dev.resids() (resíduos deviance), e o código fonte é:\nfam$dev.resid function (y, mu, wt) .Call(C_binomial_dev_resids, y, mu, wt) \u0026lt;bytecode: 0x55fc8e2253a0\u0026gt; \u0026lt;environment: 0x55fca4eb5040\u0026gt; Hmm, parece que é uma função feita em C. Como as contas da nossa perda (soma, logaritmo, multiplicação e divisão) já são todas implementadas em C, provavelmente a conta foi implementada em C para garantir estabilidade numérica.\nOlhando o código-fonte do pacote stats, encontramos a definição da função. A função é um pouco longa, então eu mantive apenas as partes importantes:\nstatic R_INLINE double y_log_y(double y, double mu) { return (y != 0.) ? (y * log(y/mu)) : 0; } SEXP binomial_dev_resids(SEXP y, SEXP mu, SEXP wt) { /* inicialização de variáveis e verificações */ /* rmu e ry são os valores de mu e y transformados para reais */ /* rmu e ry são os valores de mu e y transformados para reais */ for (i = 0; i \u0026lt; n; i++) { mui = rmu[i]; yi = ry[i]; rans[i] = 2 * rwt[lwt \u0026gt; 1 ? i : 0] * (y_log_y(yi, mui) + y_log_y(1 - yi, 1 - mui)); } /* outros códigos não muito importantes */ UNPROTECT(nprot); return ans; } Eu não programo muito em C, mas desse código dá para ver duas coisas importantes: i) a função y_log_y só faz a conta se o valor de \\(y\\) for diferente de zero, se não, ela já retorna zero; ii) a função y_log_y faz a conta \\(y\\log({y}/{\\mu})\\), ao invés de apenas \\(y\\log({\\mu})\\). Isso acontece pois no R estamos minimizando o Desvio do modelo, dado por\n\\[ \\begin{aligned} \u0026amp;D(\\mathbf y, \\mu) = 2[l(\\mathbf y|\\mathbf y) - l(\\mathbf y|(\\alpha, \\beta))]\\\\ \u0026amp;=2\\left[\\sum_{i=1}^n y_i\\log(y_i) + (1-y_i)\\log(1-y_i)\\right. - \\\\ \u0026amp;\\left. -\\sum_{i=1}^n y_i\\log(\\mu_i) + (1-y_i)\\log(1-\\mu_i)\\right] \\\\ \u0026amp;=2\\left[\\sum_{i=1}^n y_i\\log\\left(\\frac{y_i}{\\mu_i}\\right) + (1-y_i)\\log\\left(\\frac{1-y_i}{1-\\mu_i}\\right)\\right]. \\end{aligned} \\]\nEssa é a formulação usual na literatura de GLMs, que apresenta uma série de propriedades estatísticas. Minimizar o desvio equivale a maximizar a verossimilhança. Será que isso ajuda nos problemas numéricos? Vamos ver:\n## mesmos códigos de antes... ## só substitua a perda por essas duas linhas y_log_y \u0026lt;- function(y, mu) y * log(y / mu) perda \u0026lt;- tf$reduce_sum(tf$where(y == 0, y_log_y(1-y, 1-mu), y_log_y(y, mu))) ## mesmos códigos de antes... Iter: 01, alpha=NaN, beta1=NaN, beta2=NaN Iter: 02, alpha=NaN, beta1=NaN, beta2=NaN Iter: 03, alpha=NaN, beta1=NaN, beta2=NaN Iter: 04, alpha=NaN, beta1=NaN, beta2=NaN Iter: 05, alpha=NaN, beta1=NaN, beta2=NaN Iter: 06, alpha=NaN, beta1=NaN, beta2=NaN Iter: 07, alpha=NaN, beta1=NaN, beta2=NaN Iter: 08, alpha=NaN, beta1=NaN, beta2=NaN Iter: 09, alpha=NaN, beta1=NaN, beta2=NaN Iter: 10, alpha=NaN, beta1=NaN, beta2=NaN Hmm, parece que não. Se olharmos mais atentamente para a função desvio, como \\(y\\) pode assumir apenas os valores zero ou um, é possível observar que a conta é equivalente à perda calculada anteriormente. Possivelmente o problema aqui é que o tensorflow não trabalha muito bem com essas condições (tf$where) na perda, e isso dá problemas na hora de calcular o gradiente.\nEssa função do R simplesmente não resolve o problema inicial. Melhor olhar o que o tensorflow faz!\nA binary cross-entropy no Tensorflow Eu escondi de vocês, mas o tensorflow já tem a função de perda implementada: tf$nn$sigmoid_cross_entropy_with_logits. Ela já assume que a função de ligação é logística, por isso o sigmoid_ no início. Traduzindo livremente o help da função, temos o seguinte (z=\\(y\\) e x=\\(\\eta = \\alpha + \\beta x\\))\nz * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x)) = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x))) = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x))) = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x)) = (1 - z) * x + log(1 + exp(-x)) = x - x * z + log(1 + exp(-x)) Para \\(\\eta \u0026lt; 0\\) para evitar problemas numéricos com \\(\\exp(-\\eta)\\), reformulamos para\nx - x * z + log(1 + exp(-x)) = log(exp(x)) - x * z + log(1 + exp(-x)) = - x * z + log(1 + exp(x)) Então, para garantir estabilidade e evitar problemas numéricos, a implementação usa essa formulação equivalente\nmax(x, 0) - x * z + log(1 + exp(-abs(x))) Beleza, vamos tentar!\n## mesmos códigos de antes... ## só substitua a perda por essas duas linhas ## agora não precisa calcular o sigmoide # mu \u0026lt;- sigmoide(alfa + beta1 * x1 + beta2 * x2) perda \u0026lt;- tf$nn$sigmoid_cross_entropy_with_logits( labels = y, logits = alfa + beta1 * x1 + beta2 * x2 ) ## mesmos códigos de antes... Iter: 01, alpha=1.674, beta1=2.703, beta2=3.461 Iter: 02, alpha=1.276, beta1=2.495, beta2=3.608 Iter: 03, alpha=1.197, beta1=2.396, beta2=3.562 Iter: 04, alpha=1.164, beta1=2.335, beta2=3.489 Iter: 05, alpha=1.14, beta1=2.287, beta2=3.42 Iter: 06, alpha=1.12, beta1=2.245, beta2=3.358 Iter: 07, alpha=1.102, beta1=2.21, beta2=3.303 Iter: 08, alpha=1.086, beta1=2.178, beta2=3.256 Iter: 09, alpha=1.073, beta1=2.152, beta2=3.215 Iter: 10, alpha=1.061, beta1=2.129, beta2=3.18 Funcionou! :)\nWrap-up Tensorflow é uma biblioteca interessante a ser explorada. É possível implementar uma regressão logística do zero em poucos passos. Precisamos tomar cuidado com problemas numéricos! No futuro, brincaremos também com o algoritmo IWLS. Será que ele roda mais rápido que a descida de gradiente?\nÉ isso pessoal. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2018-11-18-logistica-comp/","tags":["estatística"],"title":"Regressão logística: aspectos computacionais"},{"author":["Daniel"],"categories":["tutoriais"],"contents":" O Caio já escreveu aqui no blog sobre o CronR, um pacote do R que permite agendar scripts do R para rodar de tempos em tempos. O problema do CronR é que ele necessita que o computador que hospeda o processo fique ligado o tempo todo.\nEm geral, não colocamos processos agendados no nosso próprio computador de trabalho, uma vez que nem sempre o deixamos ligado. Por isso, o mais comum é agendar scripts em um servidor que fique sempre ligado, porém deixar um servidor ligado todos os dias o dia inteiro para rodar um script a cada 12h parece desperdício, certo?\nÉ aí que entra o Hyper.sh! Para quem não conhece o Hyper é um serviço cloud de hospedagem de containers. Basicamente ele funciona da seguinte maneira: você passa a imagem de um container Docker e como você quer executá-la lá dentro. O Hyper pode servir tanto para hospedar apps Shiny, API’s em plumber e outros serviços que ficam online o tempo inteiro quanto para executar scripts agendados, e é sobre essa parte que vamos falar nesse post.\nObjetivo Neste post vamos criar e colocar em produção um script em R que envia para você mesmo no Telegram a cada 2 horas uma mensagem do pacote fortunes. Vamos fazer isso simples, mas nada impede de você criar um bot que avisa que o Bitcoin tá barato.\nCriando o bot Antes de mais nada, vamos criar um bot no Telegram. Para isso clique neste link e comece a conversar com o BotFather.\nVocê vai ter que dar um nome e usuário para o seu bot e em troca, o BotFather vai retornar um token, parecido com 6768271902:AAGKFXUz7P_Qg2X_pE0ZjKSLPZ45TEX9Z5QL4.\nEsse token é secreto, então não é legal colocá-lo direto no seu script. A forma usual de deixá-lo disponível para o R é colocar no .Renviron.\nColocando o token no .Renviron O .Renviron é um arquivo de configuração do R que permite que você especifique variáveis de ambiente para que fiquem disponíveis para o R. Ele é muito usado para disponibilizar senhas, chaves de API, … - coisas que são secretas e por isso não é boa prática colocá-las no código.\nO jeito mais fácil de abrir o .Renviron é usando o pacote usethis. O usethis é um pacote do R que tem uma série de funções úteis no dia a dia de um desenvolvedor R.\nInstale o pacote e depois rode:\nusethis::edit_r_environ() Se você estiver no RStudio, isso vai abrir o arquivo .Renviron. Adicione a linha:\nR_TELEGRAM_BOT_RBot=6768271902:AAGKFXUz7P_Qg2X_pE0ZjKSLPZ45TEX9Z5QL4 Não esqueça de trocar o token que está aí pelo seu token, gerado pelo BotFather.\nAgora vamos testar se está tudo certo. Instale o pacote telegram (install.packages(\"telegram\")) e rode:\nlibrary(telegram) bot \u0026lt;- TGBot$new(token = bot_token(\u0026#39;RBot\u0026#39;)) bot$getMe() Isso deve retornar o nome e username do seu bot.\nComece um chat com o seu bot Para começar um chat com o seu bot no Telegram volte para a conversa com o BotFather e clique no link que ele te mandou na última mensagem (a mensagem que veio o token)\nClique no link e depois comece uma conversa com o seu bot. Agora volte para o R e rode:\nmsgs \u0026lt;- bot$getUpdates() msgs$message$chat$id[1] Assim você vai ter o o id do seu chat com o seu bot. Vamos setar esse id como o chat padrão do bot.\nbot$set_default_chat_id(137007207) Podemos então começar a enviar mensagens direto do R com:\nbot$sendMessage(\u0026quot;Oioioioi\u0026quot;) Criando o script No nosso caso, queremos enviar uma mensagem no telegram com mensagens do pacote fortunes. A função fortunes solta aleatóriamente frases famosas que apareceram na lista de e-mails do R ao longo do tempo.\nlibrary(fortunes) fortune() ## ## I recently read the small print on the academic license our site has for SAS. ## You have to: ## ## 1 inform SAS of any taught courses that use SAS, ## 2 inform SAS of any research projects using SAS, ## 3 allow SAS to refer to your institution as a SAS user, ## 4 allow SAS to review your taught courses, ## 5 ensure your courses are taught using qualified personnel, ## 6 give SAS your first-born male offspring. ## ## I spoke to our site\u0026#39;s licensing supremos and they say they\u0026#39;ve never heard of ## anyone complying with 1 or 2. Point 4 sounds like petty fiddling in our ## educational business, and point 5 left \u0026#39;qualified\u0026#39; undefined. Point 6 doesn\u0026#39;t ## bother me since I don\u0026#39;t have kids. ## -- Barry Rowlingson ## R-help (February 2008) Vamos criar uma função que envia uma mensagem aleatoriamente pelo Telegram. A função poderia ser algo assim:\nmain \u0026lt;- function() { bot \u0026lt;- TGBot$new(token = bot_token(\u0026#39;RBot\u0026#39;)) bot$set_default_chat_id(137007207) bot$sendMessage(fortune()) } Ok? Quando rodamos a função main() recebemos uma nova mensagem no Telegram, certo? Por fim, o nosso script pode ser:\nlibrary(telegram) library(fortunes) main \u0026lt;- function() { bot \u0026lt;- TGBot$new(token = bot_token(\u0026#39;RBot\u0026#39;)) bot$set_default_chat_id(user_id(\u0026#39;me\u0026#39;)) bot$sendMessage(fortune()) } main() Salvei esse código em um arquivo chamado send_fortune.R.\nEu gosto de encapsular todo o código em uma função sem argumentos para ser mais fácil para parar a execução em caso de errros durante o código, mas isso vai da forma que você preferir programar.\nA estrutura do projeto O nosso projeto possui a seguinte estrutura:\n. ├── Dockerfile └── send_fortune.R O nosso script .R e o Dockerfile que vamos falar em seguida. Nada impede de termos outros arquivos, ok? O que é necessário é que tanto o Dockerfile quanto o script estejam na mesma pasta.\nCriando o Dockerfile O Hyper não sabe que o R existe. Ele só sabe executar imagens do Docker. O Docker é uma ferramenta muito poderosa que permite que a partir de um arquivo chamado Dockerfile você especifique exatamente o estado de um computador, simulando a instalação de todos os softwares necessários e etc. Não é objetivo deste post explicar exatamente como funciona o Docker, por isso vamos direto ao necessário.\nCrie um arquivo chamado Dockerfile (sem nenhuma extensão nem nada) e coloque o seguinte dentro dele:\nFROM rocker/tidyverse COPY . /service # Install the R libraries needed to run the scripts RUN R -e \u0026quot;install.packages(c(\u0026#39;telegram\u0026#39;, \u0026#39;fortunes\u0026#39;), repos = \u0026#39;http://cran.us.r-project.org\u0026#39;)\u0026quot; # Execute the target script CMD [\u0026quot;Rscript\u0026quot;, \u0026quot;/service/send_fortune.R\u0026quot;] Vou explicar rapidamente o que significa cada linha do Dcoker:\nFROM rocker/tidyverse indica que vamos usar como base para o nosso container a imagem do rocker/tidyverse. COPY . /service copia todos os arquivos da pasta (indicado pelo .) para a pasta /service dentro do container. RUN R -e \"install.packages(c('telegram', 'fortunes'), repos = 'http://cran.us.r-project.org')\" instala os pacotes necessários para a execução do script. CMD [\"Rscript\", \"/service/send_fortune.R\"] indica o comando que será executado. No nosso caso: execute o script send_fortune.R que está na pasta service/. Construindo e subindo o container Para usar o nosso container no Hyper, precisamos que ele esteja em um registry. O próprio Docker oferece um serviço gratuito de hospedagem de containers. Basta criar um login lá.\nPara subir um container para o registry você precisa ter buildado o container localmente. Vamos fazer isso agora. Se você não possui o Docker instalado, instale por aqui.\nAbra um terminal na pasta do em que estão os arquivos Dockerfile e send_fortune.R e rode:\ndocker build -t seu_user_docker/nome_img . Esse passo transforma o seu Dockerfile em uma imagem. O argumento -t dá um nome para sua imagem, no caso estamos usando seu_user_docker/nome_da_img (No meu caso usei dfalbel/hyper-cron-r).\nCom o container buildado você pode testá-lo fazendo o seguinte:\ndocker run --env R_TELEGRAM_BOT_RBot=6793020822:AAGKFXUz7P_Qg2X_pshau8432X9ZAKHQL4 dfalbel/hyper-cron-r Isso deveria enviar uma mensagem para você no telegram. Se não mandar tem algo errado. Não esqueça de trocar o valor do Token para o que você obteve no primeiro passo.\nPassamos o argumento --env para poder passar as variáveis de ambiente que são necessárias para executar o nosso código. No nosso caso, temos que passar o token do Telegram, fiz isso passando o --env R_TELEGRAM_BOT_RBot=6793020822:AAGKFXUz7P_Qg2X_pshau8432X9ZAKHQL4.\nAgora vamos subir a imagem para o Docker registry. Se você já criou a sua conta aqui, rode e siga as inscrições para efetuar o login no CLI do Docker.\ndocker login Se seu login deu tudo certo, você pode rodar:\ndocker push seu_user_docker/nome_da_img Isso vai fazer o upload da sua imagem para o * registry*. No meu caso usei docker push dfalbel/hyper-cron-r.\nSe tiver dado tudo certo, você vai conseguir acessar uma página com uma url parecida com essa:\nhttps://hub.docker.com/r/seu_user_docker/nome_da_img/ O Docker hub permite quantas imagens públicas você quiser, mas se não me engano apenas 1 privada. Se você precisar que sua imagem seja privada, tente procurar pelo GCR (Google Container Registry).\nFinalmente, o Hyper Se você ainda não criou sua conta no Hyper, crie aqui. Também faça o download do CLI deles, aqui. As instruções completas de instalação estão aqui.\nCom o hyper instalado e se a sua imagem do Docker for pública basta rodar. Está fora do escopo deste tutorial, mas é possível fazer isso com imagens privadas também.\nhyper pull seu_user_docker/nome_da_img/ Isso irá copiar a sua imagem - do registry para o Hyper. Agora estamos prontos para criar o processo agendado.\nFazemos isso também com apenas uma linha no Hyper:\nhyper cron create --minute=*/5 --hour=* --name test-cron-job1 --env R_TELEGRAM_BOT_RBot=6793020822:AAGKFXUz7P_Qg2X_pEAHDGJ8432X9Z5QL4 seu_user_docker/nome_da_img/ Mais uma vez, não esqueça de trocar o valor do token para o token do seu bot!\nO código acima especifica que desejo executar esse container a cada 5 minutos, mas tudo isso pode ser configurado pelas opções do hyper-cron. A documentação completa está aqui.\nPor exemplo, se quiséssemos rodar o script todo dia às 8h da poderíamos colocar:\nhyper cron create --minute=0 --hour=11 --name test-cron-job1 --env R_TELEGRAM_BOT_RBot=6793020822:AAGKFXUz7P_Qg2X_pEAHDGJ8432X9Z5QL4 seu_user_docker/nome_da_img/ Note que no lugar da hora colocamos 11 e não 8, isso se dá porque a hora dos servidores do Hyper é sempre UTC+0, que é 3 horas na frente do fuso de São Paulo.\nSe tudo estiver correto, você pode rodar o seguinte no terminal para ver todos os processos agendados:\nhyper cron ls #$ Name Schedule Image Command #$ test-cron-job1 */5 * * * * dfalbel/hyper-cron-r Você também pode olhar o histórico de execuções do seu processo.\nhyper cron history test-cron-job1 #$ Container Start End Status Message #$ test-cron-job1-1538685600 2018-10-04 20:40:00 +0000 UTC 2018-10-04 20:40:39 +0000 UTC done Job[test-cron-job1] is success to run Custos O Hyper implica em custos. Ele cobra por segundos de execução do seu script e por GB da sua imagem. Dê uma lida melhor na página de preços.\nQuando terminar, para remover tudo e evitar custos depois, rode:\nhyper cron rm test-cron-job1 hyper rmi dfalbel/hyper-cron-r Substituindo os devidos nomes.\nConclusão Neste post usamos o Hyper e Docker para agendar um script R para rodar de tempos em tempos. Sei que tem bastante coisa e não dá para descrever todos os passos com toda precisão necessária para quem está fora do contexto. Por favor, se tiverem problemas ou dúvidas entre em contato para que possa melhorar o post.\nSe você quiser ver o código completo desse tutorial em um lugar só dê uma olhada neste repositório.\n","permalink":"https://blog.curso-r.com/posts/2018-10-04-hyper-cron/","tags":["cron"],"title":"Agendando scripts do R"},{"author":["Daniel","José"],"categories":["tutoriais"],"contents":" Neste post vamos criar um Shiny App simples usando o Auth0 como servidor de autenticação. O Auth0 implementa o OAuth2, o principal protocolo de autorização utilizado atualmente na indústria de software. Ele permite que os aplicativos tenham acesso limitado às contas de usuário em serviços HTTP.\nComo funciona o oauth2.0? Antes de começarmos - nesta parte do post resumi um pouco o que está escrito aqui. Para se aprofundar, vale a pena ler essa postagem do Aaron Parecki.\nPapéis A autorização/autenticação feita por meio de OAuth2 envolve 4 papéis:\nCliente: É o aplicativo que está tentando acessar a conta do usuário. Ele precisa obter a permissão do usuário antes de poder acessar as informações. API: É o servidor de recursos que é usado para acessar as informações do usuário. Servidor de autorização: Esse é o servidor que vai aprovar ou recusar as requisições. Ele pode ser o mesmo servidor que o API. No caso do Auth0 que vamos ver mais tarde, eles são o mesmo servidor. Dono do recurso: O dono do recurso é a pessoa que está dando acesso de alguma parte das informações da sua conta. Criando um App Antes de começar o processo de autenticação com o OAuth, você deve registrar o seu app com o serviço - no nosso caso, vamos registrá-lo com o Auth0. Ao registrar em geral passamos informações básicas sobre o app e o principal é o redirect URI - que será chamado de callback URL pelo Auth0. O servidor de autenticação só pode redirecionar o usuário para essas URL’s cadastradas após completar a autorização - isso ajuda a evitar alguns tipos de ataques.\nDepois de registrar o seu app você receberá um client ID e um client secret. Em teoria o client ID é informação pública e o client secret (é segredo) e não pode ser divulgado.\nO fluxo da autenticação Existem 4 fluxos distintos de autenticação no OAuth2.0, cada um utilizado em circuntâncias diferentes. No nosso caso vamos usar um método chamado de * Authorization Code*. Esse método funciona da seguinte forma:\nCriamos um link de autenticação para o usuário com as seguintes informações:\nclient_id redirect_uri scope state Já falamos dos 2 primeiros. O scope vai indicar quais partes da conta do usuário que o cliente pode ver. O state, uma string aleatória que verificaremos depois. O link que criamos é algo da forma:\nhttps://authorization-server.com/auth?response_type=code\u0026amp; client_id=CLIENT_ID\u0026amp;redirect_uri=REDIRECT_URI\u0026amp;scope=photos\u0026amp;state=1234zyx Ao acessar este link o usuário vê o que chamamos de prompt de autorização. Algo do tipo:\nImg do post https://aaronparecki.com/oauth-2-simplified/\nSe o usuário clicar em Autorizar, ele será redirecionado para o seu app passando um código gerado pelo servidor de autorização e o state. A URL será algo do tipo:\nhttps://example-app.com/cb?code=AUTH_CODE_HERE\u0026amp;state=1234zyx Se o state for o mesmo que tiver sido gerado anteriormente, estamos prontos para requisitar o token de acesso. O token de acesso é solicitado pelo app ao servidor de autorização por meio de uma requisição do tipo POST que envia as seguintes informações:\ngrant_type: o nível de autorização requisitada code: o código de autenticação gerado no anteriormente redirect_uri: a URL de redirecionamente client_id: o código do cliente. Em posse do token, temos o que é necessário para fazer requisições para a API e assim obter as informações que o usuário nos permitiu acesso.\nConfigurando o Auth0 Vá para a página applications e crie um novo app clicando no botão Create Application no canto superior direito.\nNa próxima tela, dê um nome para o app (esse é o nome que aparecerá na página de login) e selecione o Regular Web Applications. Selecionamos esse tipo de app para não ter que se preocupar em esconder o client secret.\nEm seguida vá para a aba Settings para configurarmos alguns detalhes do app.\nAs coisas mais importantes aqui são:\nDomain: indica o endpoint do OAuth2. Vamos nos referir a ele posteriormente como base_url. Client Id: é a chave do cliente. No nosso código vamos nos referir a ele como key. Client Secret: é a chave secreta do app. Vamos nos referir a ele como secret. Mais para baixo, na mesma página, vamos configurar o campo Allowed Callback URLs, essas são as URL’s para as quais permitimos que o usuário seja redirecionado após a autenticação. Também vamos configurar o campo Allowed Web Origins para poder indicar quis URL’s podem redirecionar para a página de autenticação.\nNo nosso caso, vamos preencher os dois campos com as mesmas URL’s. A primeira: https://localhost:8100 - caminho e porta que vamos usar para testes locais. A segunda URL é aquela em que o Shiny ficará hospedado. Você não precisa preencher as duas agora, por enquanto pode deixar somente a URL local enquanto testamos o app localmente.\nNo Auth0 é só isso. Agora vamos para o R.\nConectando o R com o Auth0 O código que vamos desenvolver e uma versão adaptada do código que está disponível neste link. A diferença é que este faz a autenticação usando a conta do Github e o nosso fará a autenticação com o Auth0. A versão completa deste código está disponível aqui.\nVamos desenvolver um app simples em Shiny que mostra as informações disponibilizadas pelo usuário. Neste app usaremos duas bibliotecas:\nlibrary(shiny) library(httr) Setup do Oauth Os códigos nesta sessão podem ser incluidos no arquivo global.R ( se você estiver desenvolvendo o seu shiny separando por arquivos ui.R e server.R) se você estiver usando apenas um app.R, esses blocos podem ser incluídos no início do arquivo - antes da chamada por shinyApp(uiFunc, server).\nNeste primeiro bloco de código vamos definir qual é a URL que o app está sendo servido. Criamos um bloco que faz o seguinte - quando estivermos em uma sessão interativa (por exemplo, rodando pelo RStudio) use a url APP_URL \u0026lt;- \"https://localhost:8100/\". Já quando estiver em uma sessão não interativa, use https://shiny.curso-r.com.\nif (interactive()) { # testing url options(shiny.port = 8100) APP_URL \u0026lt;- \u0026quot;https://localhost:8100/\u0026quot; } else { # deployed URL APP_URL \u0026lt;- \u0026quot;https://shiny.curso-r.com\u0026quot; } Essas devem ser as URL’s que indicamos anteriormente para o Auth0 nos campos Allowed Web Origins e Allowed Callback URLs.\nAgora vamos criar um objeto chamado app que vai armazenar todas as informações relativas ao seu app configurado no Auth0. Colocamos entre {} as informações que você deve preencher com os seus dados.\nAqui você indicará a sua key e secret que obtivemos ao criar um App no Auth0.\napp \u0026lt;- oauth_app( appname = \u0026quot;{coloque um nome para o seu App. (esse nome é opcional)}\u0026quot;, key = \u0026quot;{coloque aqui o seu Client ID - também chamamos de key\u0026quot;, # não é uma boa ideia deixar o secret no código # use variáveis de ambiente ou o pacote keyring secret = \u0026quot;{coloque aqui o seu Client Secret - também chamamos de secret}\u0026quot;, redirect_uri = APP_URL ) Agora vamos criar um outro objeto chamado api que vai armazenar as informações necessárias para determinar os endpoints para as requisições necessárias para fazer a autenticação com o Auth0. Esse objeto nos ajuda a fazer rapidamente um wrapper para a API de auenticação do Auth0 - documentada aqui.\napi \u0026lt;- oauth_endpoint( base_url = \u0026quot;{coloque aqui o seu Domain - também chamamos de base_url}\u0026quot;, request = \u0026quot;oauth/token\u0026quot;, authorize = \u0026quot;authorize\u0026quot;, access = \u0026quot;oauth/token\u0026quot; ) Basicamente, aqui estamos dizendo o seguinte. Para requisitar um token use o endpoint oauth/token, para solicitar autorização, use o endpoint authorize e para solicitar acesso use o endpoint oauth/token.\nNão é recomendado, mas você poderia alterar esses endpoints na sessão Endpoints das configurações avançadas do seu aplicativo no Auth0 - como mostra a imagem abaixo.\nOutro objeto importante que temos que definir é o scope. Essa string indica quais informações vamos solicitar do usuário para acesso. No nosso caso vamos usar:\nscope \u0026lt;- \u0026quot;openid profile\u0026quot; Indicando assim que queremos acesso ao openid e ao perfil do usuário. Mais informações sobre esse scope podem ser encontradas aqui.\nPor fim, definimos um objeto state uma string aleatória que vai servir para validar que falamos mesmo com o Auth0. Ele deve nos redirecionar para uma página com o mesmo state.\nstate \u0026lt;- paste(sample(c(letters, 0:9), size = 10, replace = TRUE), collapse = \u0026quot;\u0026quot;) Também vamos definir uma função chamada has_auth_code. Ela retorna TRUE quando baseando-se nos parâmetros da URL, parece que existe código do OAuth e quando o state é o mesmo que foi enviado para o Auth0, caso contrário ela retorna FALSE.\nhas_auth_code \u0026lt;- function(params, state) { if (is.null(params$code)) { return(FALSE) } else if (params$state != state) { return(FALSE) } else { return(TRUE) } } Construindo o UI Agora vamos construir a UI do nosso app. Se você estiver usando apenas um app.R pode definir os objetos da mesma forma que vamos fazer aqui. Se você estiver usando arquivos separados é só fazer a função que vamos definir a seguir uiFunc seja a última expressao do seu arquivo ui.R.\nPrimeiro definimos um objeto chamado ui que contém a UI normalmente, como se não estivéssemos criando a autenticação:\nui \u0026lt;- fluidPage( verbatimTextOutput(\u0026quot;code\u0026quot;) ) Agora vem a parte importante. Uma feature que é pouco conhecida no Shiny, é que o UI pode ser uma função e não somente um objeto. Essa função pode ser usada para modificar a interface baseando-se na requisição.\nNo nosso caso, a nossa função uiFunc irá identificar se o parâmetro code foi enviado na * query string. Query string* é um nome que damos aos parâmetros que vem pela URL. No exemplo www.curso-r.com?abb=1\u0026amp;x=2 temos os parâmetros abb=1 e x=2. No nosso caso precisamos receber um parâmetro code que é usado para obter o token de autenticação e um parâmetro state que usamos para evitar ataques do tipo CSRF.\nSe esses parêmetros não estiverem na URL, retornamos uma resposta de redirecionamento - para redirecionar o usuário para a url de autenticação.\nuiFunc \u0026lt;- function(req) { if (!has_auth_code(parseQueryString(req$QUERY_STRING), state)) { url \u0026lt;- oauth2.0_authorize_url(api, app, scope = scope, state = state) redirect \u0026lt;- sprintf(\u0026quot;location.replace(\\\u0026quot;%s\\\u0026quot;);\u0026quot;, url) tags$script(HTML(redirect)) } else { ui } } Com isso, quando o usuário tentar acessar o seu app pela URL direta, como ela não terá o parâmetro code, então o uiFunc vai redirecioná-lo para a página de autenticação. Quando o usuário fizer a autenticação, ele será novamente redirecionado para a mesma URL, no entanto, o Auth0 irá incluir os parâmetros code e state na requisição, fazendo que a função uiFunc retorne a UI regular do Shiny, e não o redirecionamento.\nConstruindo o server Vamos definir o server da forma a seguir. Lembre-se que se você estiver usando arquivos separados para ui e server, não precisará colocar a atribuição server \u0026lt;-.\nserver \u0026lt;- function(input, output, session) { params \u0026lt;- parseQueryString(isolate(session$clientData$url_search)) if (!has_auth_code(params, state)) { return() } token \u0026lt;- oauth2.0_token( app = app, endpoint = api, credentials = oauth2.0_access_token(api, app, params$code), cache = FALSE, user_params = list(grant_type = \u0026quot;authorization_code\u0026quot;) ) resp \u0026lt;- GET(\u0026quot;https://dfalbel.auth0.com/userinfo\u0026quot;, config(token = token)) output$code \u0026lt;- renderPrint(content(resp, \u0026quot;text\u0026quot;)) } Agora vamos explicar passo a passo da função server. Nas linhas abaixo estamos obtendo os parâmetros da Query String e em seguida verificamos se existe um parâmetro code e se o state retornado é o mesmo que enviamos para o Auth0 (usando a função has_auth_code). Se esses parâmetros não estiverem corretos fazemos o server parar retornando vazio.\nparams \u0026lt;- parseQueryString(isolate(session$clientData$url_search)) if (!has_auth_code(params, state)) { return() } Em seguida, dado que existe um parâmetro code, vamos requisitar o token de autorização do OAuth. Isso verifica que o code que temos é válido. No caso de o parâmetro code não ser válido, a função oauth2.0_token retorna um erro parando a execução do server. Fazemos isso da seguinte forma:\ntoken \u0026lt;- oauth2.0_token( app = app, endpoint = api, credentials = oauth2.0_access_token(api, app, params$code), cache = FALSE, user_params = list(grant_type = \u0026quot;authorization_code\u0026quot;) ) Nesse momento, se a autenticação tiver ocorrido corretamente você terá um objeto token que deve ser enviado junto com as suas próximas requisições para obter informações sobre o usuário.\nUm exemplo do que podemos obter é usar o endpoint userinfo para obter informações sobre o perfil do usuário logado. Você poderia usar essas informações para personalizar o app.\nresp \u0026lt;- GET(\u0026quot;https://dfalbel.auth0.com/userinfo\u0026quot;, config(token = token)) print(content(resp, \u0026quot;parsed\u0026quot;)) # $sub # [1] \u0026quot;google-oauth2|ashjkdhique92392\u0026quot; # # $given_name # [1] \u0026quot;Daniel\u0026quot; # # $family_name # [1] \u0026quot;Falbel\u0026quot; # # $nickname # [1] \u0026quot;dfalbel\u0026quot; # # $name # [1] \u0026quot;Daniel Falbel\u0026quot; # # $picture # [1] \u0026quot;https://lh6.googleusercontent.com/-KAr2tY871g4/AAAAAAAAAAI/AAAAAAAANMw/ZK4kajskakvs_5ftmk/photo.jpg\u0026quot; # # $gender # [1] \u0026quot;male\u0026quot; # # $locale # [1] \u0026quot;pt-BR\u0026quot; # # $updated_at # [1] \u0026quot;2018-09-24T17:44:49.768Z\u0026quot; Mais opções do Auth0 No Auth0 existem diversas opções de login que podem ser configuradas na aba Connections.\nVocê também pode impedir que as pessoas se cadastrem no seu app (somente o administrador pode cadastrar usuários) - assim você restringe as pessoas que podem acessar o seu app. Isso pode ser feito na aba Connections \u0026gt; Database e em seguida desabilitando o Sign up\nVocê poderia ainda conectar o Auth0 com diretórios de autenticação como LDAP, comuns no mundo corporativo.\nDisclaimer Não somos especialistas em segurança portanto usem esse código com desconfiança e sempre perguntem a um especialista antes de usar em ambiente de produção.\nAgradecimentos Esse post não teria sido possível sem a ajuda do José de Jesus Filho que nos apresentou a solução depois de muito tempo explorando diversas formar de criar autenticação para seus Shiny’s.\n","permalink":"https://blog.curso-r.com/posts/2018-09-24-shiny-auth0/","tags":["shiny"],"title":"Autenticação no Shiny com Auth0"},{"author":["Julio"],"categories":["conceitos"],"contents":" Você está num jogo na TV e o apresentador pede para escolher uma entre 3 portas. Atrás de uma dessas portas tem uma Ferrari e nas outras duas temos cabras. Você escolhe uma porta. Depois, o apresentador retira uma porta que tem uma cabra e pergunta: você quer trocar de porta?\nA princípio, você pode achar que sua probabilidade de ganhar é 1/2, já que uma das portas foi retirada, então não importa se você troca ou não. Mas a resposta é que sim, vale à pena trocar de porta! A probabilidade de vencer o jogo trocando a porta é de 2/3.\nFigura 1: Brincadeira do XKCD. O problema de Monty Hall é talvez o mais eloquente exemplo de como a probabilidade pode confundir a mente humana. Esse problema desafiou a comunidade científica no final do século XX e chegou até a ser considerado um paradoxo. Recomendo ler o livro O Andar do Bêbado, de Leonard Mlodinow, que conta essa e muitas outras histórias interessantes sobre a probabilidade.\nExistem várias formas de explicar por quê trocar a porta é a melhor estratégia. A que eu mais gosto é a do próprio Andar do Bêbado, que mostra que, quando você escolhe a primeira porta, você está apostando se acertou ou não a Ferrari. Se você apostar que acertou a Ferrari, não deve trocar a porta e, se você apostar que errou a Ferrari, deve trocar. A aposta de errar a Ferrari de primeira tem probabilidade 2/3, logo, vale à pena trocar.\nNesse post, mostramos uma solução alternativa, simples e elegante para o problema usando diagramas de influência e o pacote bnlearn.\nRedes bayesianas As redes Bayesianas são o resultado da combinação de conceitos probabilísticos e conceitos da teoria dos grafos. Segundo Pearl, tal união tem como consequências três benefícios: i) prover formas convenientes para expressar suposições do modelo; ii) facilitar a representação de funções de probabilidade conjuntas; e iii) facilitar o cálculo eficiente de inferências a partir de observações.\nDa teoria de probabilidades precisamos apenas de alguns resultados básicos sobre probabilidade condicional. Primeiramente, pela definição de probabilidade condicional, sabemos que\n\\[ p(x_1, x_2) = p(x_1)p(x_2|x_1). \\]\nAplicando essa regra iterativamente para \\(n\\) variáveis, temos\n\\[ p(x_1, \\dots, x_p) = \\prod_j p(x_j|x_1,\\dots, x_{j-1}). \\]\nAgora, imagine que, no seu problema, a variável aleatória \\(X_j\\) não dependa probabilisticamente de todas as variáveis \\(X_1,\\dots, X_{j-1}\\), e sim apenas de um subconjunto \\(\\Pi_j\\) dessas variáveis. Fazendo isso, a equação pode ser escrita como\n\\[ p(x_1, \\dots, x_p) = \\prod_j p(x_j|\\pi_j). \\]\nChamamos \\(\\Pi_j\\) de pais de \\(X_j\\). Esse conjunto pode ser pensado como as variáveis que são suficientes para determinar as probabilidades de \\(X_j\\).\nA parte mais legal das redes Bayesianas é que elas podem ser representadas a partir de DAGs (grafos direcionados acíclicos). No grafo, se \\(X_1\\) aponta para \\(X_2\\), então \\(X_1\\) é pai de \\(X_2\\). Por exemplo, esse grafo aqui\nrepresenta a distribuição de probabilidades \\(p(x_1, \\dots, x_5)\\) com\n\\[ p(x_1, \\dots, x_5) = p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_3,x_2)p(x_5|x_4). \\]\nDiagrama de influências Um diagrama e influências é uma rede Bayesiana com nós de decisão e utilidade (ganhos). Ou seja, é uma junção de três conceitos:\n\\[ \\underbrace{\\text{prob. condicional} + \\text{grafos}}_{\\text{rede Bayesiana}} + \\text{teoria da decisão} = \\text{diagrama de influências} \\]\nNa teoria da decisão, usualmente estamos interessados em maximizar a utilidade esperada. No diagrama, considerando a estrutura de probabilidades dada pela rede Bayesiana e as informações disponíveis, queremos escolher a decisão que faz com que, em média, nosso retorno seja mais alto.\nCom diagramas de influências, é possível organizar sistemas complexos com múltiplas decisões, considerando diferentes conjuntos de informações disponíveis. É uma ferramenta realmente muito poderosa.\nVoltando ao Monty Hall Agora que sabemos um pouquinho de diagramas de influência, podemos desenhar o do Monty Hall:\nO jogador tem duas decisões a tomar:\n\\(D_1\\) (escolha_inicial): A escolha da porta inicial (1, 2, 3). \\(D_2\\) (trocar): Trocar a porta ou não (s, n). Também temos duas fontes de incerteza:\n\\(X_1\\) (ferrari): Em qual porta está a Ferrari (1, 2, 3). \\(X_2\\) (porta_retirada): Qual porta foi retirada (1, 2, 3). Essa variável não é sempre aleatória: se eu escolho a porta 1 e a Ferrari está em 2, o apresentador é obrigado a retirar a porta 3. Se o apresentador tiver a opção de escolher (que acontece no caso da escolha inicial ser a Ferrari), o apresentador escolhe uma porta para retirar aleatoriamente. Finalmente, temos um nó de utilidade:\n\\(U_1\\) (result): Ganhei a Ferrari (ganhei, perdi). Em R, podemos construir a rede Bayesiana do problema utilizando o pacote bnlearn:\n# nós do grafo nodes \u0026lt;- c(\u0026quot;escolha_inicial\u0026quot;, \u0026quot;ferrari\u0026quot;, \u0026quot;porta_retirada\u0026quot;, \u0026quot;trocar\u0026quot;, \u0026quot;result\u0026quot;) # matriz de adjacências edges \u0026lt;- matrix( c(\u0026quot;escolha_inicial\u0026quot;, \u0026quot;porta_retirada\u0026quot;, \u0026quot;ferrari\u0026quot;, \u0026quot;porta_retirada\u0026quot;, \u0026quot;porta_retirada\u0026quot;, \u0026quot;trocar\u0026quot;, \u0026quot;trocar\u0026quot;, \u0026quot;result\u0026quot;, \u0026quot;ferrari\u0026quot;, \u0026quot;result\u0026quot;, \u0026quot;escolha_inicial\u0026quot;, \u0026quot;result\u0026quot;), ncol = 2, byrow = TRUE) edges ## [,1] [,2] ## [1,] \u0026quot;escolha_inicial\u0026quot; \u0026quot;porta_retirada\u0026quot; ## [2,] \u0026quot;ferrari\u0026quot; \u0026quot;porta_retirada\u0026quot; ## [3,] \u0026quot;porta_retirada\u0026quot; \u0026quot;trocar\u0026quot; ## [4,] \u0026quot;trocar\u0026quot; \u0026quot;result\u0026quot; ## [5,] \u0026quot;ferrari\u0026quot; \u0026quot;result\u0026quot; ## [6,] \u0026quot;escolha_inicial\u0026quot; \u0026quot;result\u0026quot; # criando o grafo a partir de um grafo vazio g \u0026lt;- bnlearn::empty.graph(nodes) bnlearn::arcs(g) \u0026lt;- edges O output desse conjunto de operações é um objeto do tipo bn com várias propriedades pré calculadas pelo pacote bnlearn:\ng Random/Generated Bayesian network model: [escolha_inicial][ferrari][porta_retirada|escolha_inicial:ferrari][trocar|porta_retirada] [result|escolha_inicial:ferrari:trocar] nodes: 5 arcs: 6 undirected arcs: 0 directed arcs: 6 average markov blanket size: 3.60 average neighbourhood size: 2.40 average branching factor: 1.20 generation algorithm: Empty Com as especificação do problema dada, se gerarmos aleatoriamente todos os cenários, chegamos à essa combinação de casos equiprováveis (ver Extra 2)\nAgora, vamos escrever todas as combinações possíveis de cenários e guardar num data.frame chamado dados:\nescolha_inicial ferrari porta_retirada trocar result 1 1 2 n ganhei 1 1 2 s perdi 1 1 3 n ganhei 1 1 3 s perdi 1 2 3 n perdi 1 2 3 s ganhei 1 3 2 n perdi 1 3 2 s ganhei 2 1 3 n perdi 2 1 3 s ganhei 2 2 1 n ganhei 2 2 1 s perdi 2 2 3 n ganhei 2 2 3 s perdi 2 3 1 n perdi 2 3 1 s ganhei 3 1 2 n perdi 3 1 2 s ganhei 3 2 1 n perdi 3 2 1 s ganhei 3 3 2 n ganhei 3 3 2 s perdi 3 3 1 n ganhei 3 3 1 s perdi Finalmente, ajustamos nossa rede Bayesiana, usando a função bnlearn::bn.fit().\nfit \u0026lt;- bnlearn::bn.fit(g, dados) A função bnlearn::cpquery() (conditional probability query) serve para realizar uma consulta de probabilidades dada a rede ajustada. No nosso caso, a partir de uma escolha inicial qualquer \\(d_1\\), queremos saber o ganho ao trocar é maior que o ganho ao não trocar.\n\\[ \\mathbb E(U_1\\; |\\; D_2 = \\text{s}, D_1 = d_1) \u0026gt; \\mathbb E(U_1\\; |\\; D_2 = \\text{n}, D_1 = d_1). \\]\nFazendo contas, isso equivale matematicamente a consultar se\n\\[ \\mathbb P(U_1=\\text{ganhei}\\; |\\; D_2 = \\text{s}) \u0026gt; \\mathbb P(U_1=\\text{ganhei}\\; |\\; D_2 = \\text{n}) \\]\nAgora, podemos consultar \\(\\mathbb P(U_1=\\text{ganhei}\\; |\\; D_2 = \\text{s})\\) com nosso modelo!\nset.seed(13) # reprodutibilidade bnlearn::cpquery( fitted = fit, event = (result == \u0026quot;ganhei\u0026quot;), # o que queremos saber? evidence = (trocar == \u0026quot;s\u0026quot;), # qual informação adicionar? n = 5e6) # n grande para aumentar a precisão [1] 0.6666704 E não é que dá 2/3 mesmo? Da mesma forma, temos\nbnlearn::cpquery(fit, (result == \u0026quot;ganhei\u0026quot;), (trocar == \u0026quot;n\u0026quot;), n = 5e6) [1] 0.3333187 Resolvido!\nWrap-up Vale à pena trocar a porta! Redes Bayesianas juntam grafos e probabilidades condicionais Diagramas de influência juntam redes Bayesianas e teoria da decisão Essas ferramentas podem ser utilizadas tanto para resolver Monty Hall quanto para ajudar em sistemas complexos. É isso pessoal. Happy coding ;)\nExtra Se você ficou interessada em como eu fiz o diagrama, utilizei o pacote DiagrammeR. O código está aqui:\ndiagrama \u0026lt;- \u0026quot; graph LR; A{D1Escolha inicial}--\u0026gt;B(X2Porta retirada); C(X1Ferrari)--\u0026gt;B; B--\u0026gt;D{D2Trocar porta}; D--\u0026gt;E[U1Ganhar] C--\u0026gt;E A--\u0026gt;E \u0026quot; # tweak para centralizar e grifar as variáveis diagrama \u0026lt;- stringr::str_replace_all( diagrama, pattern = \u0026quot;([XDU][0-9])\u0026quot;, replacement = \u0026quot;\u0026lt;center\u0026gt;\u0026lt;b\u0026gt;\\\\1\u0026lt;/b\u0026gt;\u0026lt;/center\u0026gt;\u0026lt;br/\u0026gt;\u0026quot;) DiagrammeR::DiagrammeR(diagrama) Extra 2 É possível simular os dados que coloquei no post com uma função simples, que adicionei abaixo. Na verdade, o fato de eu ter considerado somente as combinações únicas de cenários e não os dados simulados abaixo é um pouco roubado, e só funciona porque os cenários calham de ser, de fato, equiprováveis.\nset.seed(13) simular_monty_hall \u0026lt;- function(z = 0) { v \u0026lt;- 1:3 # opcoes escolha_inicial \u0026lt;- sample(v, 1) # escolha inicial aleatoria ferrari \u0026lt;- sample(v, 1) # ferrari aleatoria # qual porta retirar? if (escolha_inicial == ferrari) { porta_retirada \u0026lt;- sample(setdiff(v, ferrari), 1) } else { porta_retirada \u0026lt;- setdiff(v, c(escolha_inicial, ferrari)) } # trocar porta? trocar \u0026lt;- sample(c(\u0026quot;s\u0026quot;, \u0026quot;n\u0026quot;), 1) # calculando resultado if (trocar == \u0026quot;s\u0026quot;) { escolha_final \u0026lt;- setdiff(v, c(escolha_inicial, porta_retirada)) } else { escolha_final \u0026lt;- escolha_inicial } result \u0026lt;- ifelse(escolha_final == ferrari, \u0026quot;ganhei\u0026quot;, \u0026quot;perdi\u0026quot;) # guardando no BD tibble::tibble(escolha_inicial, ferrari, porta_retirada, trocar, result) } dados_simulados \u0026lt;- purrr::map_dfr(seq_len(1e4), simular_monty_hall) %\u0026gt;% dplyr::mutate_all(as.factor) dplyr::glimpse(dados_simulados) Observations: 10,000 Variables: 5 $ escolha_inicial \u0026lt;fct\u0026gt; 3, 1, 2, 1, 1, 1, 3, 1, 2, 3, 3, 1, 3, 1, 2, 2, 2,... $ ferrari \u0026lt;fct\u0026gt; 1, 1, 2, 1, 1, 2, 3, 3, 1, 2, 3, 3, 2, 1, 1, 3, 1,... $ porta_retirada \u0026lt;fct\u0026gt; 2, 3, 1, 3, 2, 3, 2, 2, 3, 1, 1, 2, 1, 2, 3, 1, 3,... $ trocar \u0026lt;fct\u0026gt; n, s, s, n, s, n, n, n, n, s, s, s, s, n, n, s, n,... $ result \u0026lt;fct\u0026gt; perdi, perdi, perdi, ganhei, perdi, perdi, ganhei,... Os dados do post podem ser obtidos fazendo isso aqui:\ndados_simulados %\u0026gt;% dplyr::distinct() %\u0026gt;% dplyr::arrange(escolha_inicial, ferrari) Agradecimentos: Rafael Stern, que me convenceu de que vale à pena mostrar os dados simulados 😉\n","permalink":"https://blog.curso-r.com/posts/2018-09-03-monty-hall/","tags":["modelagem"],"title":"Monty hall e diagramas de influência"},{"author":["Bruna Wundervald"],"categories":["tutoriais"],"contents":" Com uma frequência diária, eu me deparo com pessoas tendo dúvidas sobre a realização de tarefas no R, em grupos de Facebook, Telegram, Twitter, e assim por diante. Estas dúvidas tem duas principais fontes:\ndificuldades em construir o algoritmo para alcançar o resultado desejado. dificuldades em entender como se dá a utilização de alguma função. O foco deste post é dar uma noção sobre como podemos contornar o segundo problema: a utilização das funções. Para dar um pouco de contexto, deve-se comentar que os pacotes do R são construídos pela comunidade, de forma voluntária. Logo, nem todas as documentações são escritas de forma tão clara quanto necessário, o que pode gerar confusão em relação a à estrutura dos objetos que devem sem passados a uma função, por exemplo. Atualmente, o CRAN exige um certo rigor no que diz respeito ao bom funcionamento dos códigos dos pacotes, mas isso não se aplica à documentação das funções. Felizmente, essa situação está melhorando, o que pode ser observado com a existência de recomendações como a que está a seguir, retirada do livro do Hadley sobre construção de pacotes:\nEnquanto as documentações não são perfeitas, os usuários precisam buscar entender os erros inesperados em suas tarefas de outras formas. O método que eu vou descrever agora consiste basicamente em olhar o código da função e procurar nele aonde está a fonte causadora de problemas.\nExistem funções que podem ser diretamente visualizadas no console, apenas imprimindo seu nome sem os parênteses () finais, por exemplo:\nsoma \u0026lt;- function(x, y){ z \u0026lt;- x + y z } soma ## function(x, y){ ## z \u0026lt;- x + y ## z ## } Assim, só de rodar o nome da função podemos saber qual é o código que a compõe. Se eu tentar fazer, por exemplo:\nsoma(2, \u0026quot;1\u0026quot;) Temos o erro:\nError in x + y : argumento não-numérico para operador binário\nCausado pelo “1” ser um caractere e não um número, o que não parece estar tão óbvio na mensagem. Copiamos o código da função e rodamos linha por linha dele, fornecendo os devidos argumentos, até encontrar o erro:\nx \u0026lt;- 2 y \u0026lt;- \u0026quot;1\u0026quot; # Código da função soma z \u0026lt;- x + y ## Error in x + y: non-numeric argument to binary operator Encontrando exatamente aonde o erro está, fica muito mais fácil entender qual é o próvavel motivo do que apenas tentando interpretar a mensagem quando a função não roda. Muitas vezes, a questão é exatamente sobre objetos com a estrutura incorreta sendo usados, é isso por acaso também é o que gera os erros mais estranhos.\nO exemplo acima é com uma função simples e curta. Comumente você vai se deparar com funções grandes ou que, quando impressas no console, não mostram o código, e sim o seu qual método ela utiliza:\nmean ## function (x, ...) ## UseMethod(\u0026quot;mean\u0026quot;) ## \u0026lt;bytecode: 0x7fea5b839a88\u0026gt; ## \u0026lt;environment: namespace:base\u0026gt; E o que isso significa? que essa é uma função genérica da classe S3, que tem métodos para diferentes classes de objetos. Mas como assim?\nVamos usar o exemplo da função mean, que é usada para o calculo de médias. Quais são os tipos de objeto que podem ser usados nessa função? Em geral, utilizamos vetores, mas ela consegue lidar com outros tipos, como datas. O que queremos dizer aqui é que a mesma função vai conseguir fazer sua tarefa com objetos diferentes, através dos “métodos” da função mean. Um método é uma função associada com um tipo particular de objeto. Podemos verificar quais são os métodos disponíveis com:\nmethods(mean) ## [1] mean.Date mean.default mean.difftime mean.POSIXct mean.POSIXlt ## [6] mean.quosure* ## see \u0026#39;?methods\u0026#39; for accessing help and source code (dependendo do pacote, os métodos não estão exportados. Se encontrar problemas com isso, experimente usar o operador :::. Por exemplo, dplyr:::filter.tbl_df)\nNesse caso, é possível ver o código de um método específico imprimindo não só o nome da função, mas a sua extensão com o método desejado:\nmean.default ## function (x, trim = 0, na.rm = FALSE, ...) ## { ## if (!is.numeric(x) \u0026amp;\u0026amp; !is.complex(x) \u0026amp;\u0026amp; !is.logical(x)) { ## warning(\u0026quot;argument is not numeric or logical: returning NA\u0026quot;) ## return(NA_real_) ## } ## if (na.rm) ## x \u0026lt;- x[!is.na(x)] ## if (!is.numeric(trim) || length(trim) != 1L) ## stop(\u0026quot;\u0026#39;trim\u0026#39; must be numeric of length one\u0026quot;) ## n \u0026lt;- length(x) ## if (trim \u0026gt; 0 \u0026amp;\u0026amp; n) { ## if (is.complex(x)) ## stop(\u0026quot;trimmed means are not defined for complex data\u0026quot;) ## if (anyNA(x)) ## return(NA_real_) ## if (trim \u0026gt;= 0.5) ## return(stats::median(x, na.rm = FALSE)) ## lo \u0026lt;- floor(n * trim) + 1 ## hi \u0026lt;- n + 1 - lo ## x \u0026lt;- sort.int(x, partial = unique(c(lo, hi)))[lo:hi] ## } ## .Internal(mean(x)) ## } ## \u0026lt;bytecode: 0x7fea596dfc10\u0026gt; ## \u0026lt;environment: namespace:base\u0026gt; Voltando ao descobrimento dos erros. Digamos que nós rodamos o seguinte pedaço de código, que vai dar um erro:\nmean(c(\u0026quot;1\u0026quot;, 3)) ## Warning in mean.default(c(\u0026quot;1\u0026quot;, 3)): argument is not numeric or logical: ## returning NA ## [1] NA Note que isso não é um erro e sim um warning. Mas certamente não é esse o resultado que gostaríamos, e o valor numérico 2, que é a média entre 1 e 3.\nAgora, a mensagem é mais explicíta, mas podemos encontrar o problema diretamente na função, usando o código da mean.default. Como vimos antes, a função precisa de um vetor de entrada “x”:\n# Definindo o objeto de entrada da função x \u0026lt;- c(\u0026quot;1\u0026quot;, 3) # Código da mean.default if (!is.numeric(x) \u0026amp;\u0026amp; !is.complex(x) \u0026amp;\u0026amp; !is.logical(x)) { warning(\u0026quot;argument is not numeric or logical: returning NA\u0026quot;) return(NA_real_) } ## Warning: argument is not numeric or logical: returning NA ## [1] NA # Paramos aqui porque o problema já foi encontrado Logo na primeira linha do código da função, já temos a indicação do problema: o vetor passado não é do tipo numérico, complexo ou lógico.\nPor quê eu devo procurar entender os erros? Em linhas gerais, na minha experiência com a procura pelos erros\nde funções, eu sempre acabo aprendendo algo novo, como por exemplo justamente a correção do erro. A máxima do “é errando que se aprende” é altamente aplicável nestes casos. Os erros gerados pelas funções nos levam a procurar entendê-los melhor, o que consequentemente leva a uma compreensão aprimorada sobre lógica de programação e R em geral.\nAlém disso, é bem mais eficiente desenvolver técnicas para resolver seu próprio problema. Não é raro que uma pergunta sobre R em grupos da internet demore pra ter resposta. Com uma busca mais aprofundada pela fonte geradora do erro e sua consequente solução, essa espera pode ser evitada (não que seja errado fazer perguntas, claro).\nMais particularmente, eu posso comentar que, já que os pacotes do R são feitos pela comunidade, existe uma grande diversidade de formas de escrita de código presente neles. Assim, o contato com essa diversidade me leva tanto a aprender mais sobre R como formas de refinar o escrita de código.\nWrap-up Neste post, eu expliquei como faço para procurar erros em funções do R. Em geral, falamos sobre:\ncomos mostrar o código fonte de funções simples na tela; como mostrar o código fonte de funções da classe S3. como usar estes códigos para identificar o erro; como podemos aprender com nossos próprios erros; como este método pode ser útil na economia de tempo com a resolução de erros; ","permalink":"https://blog.curso-r.com/posts/2018-09-03-funcoes/","tags":["debug"],"title":"Uma forma de encontrar erros no R"},{"author":["Julio"],"categories":["discussões"],"contents":" A pesquisa eleitoral é a forma mais comum de conectar a população geral com conceitos básicos de estatística, como amostragem e inferência. Por isso, a descrença nas pesquisas eleitorais pode levar à descrença na estatística como um todo. Isso é prejudicial para nossos profissionais e, consequentemente, para a sociedade.\nUma discussão frequente em debates, redes sociais e conversas de bar é a validade de pesquisas eleitorais. No Brasil, existe uma sensação de descrença nas pesquisas, já que, historicamente, elas erraram os resultados de várias eleições.\nMas será que a comparação de resultados de pesquisas eleitorais e resultados de eleições é correta? Nesse artigo, argumentarei que não e darei uma ideia de como isso poderia funcionar na prática.\nPesquisas eleitorais: duas culturas Leo Breiman, em seu famoso artigo de 2001, discorre sobre a existência de duas culturas entre estatísticos: a voltada para modelagem de dados e a voltada para modelagem algorítmica. A primeira está interessada em compreender o mecanismo gerador dos dados observados, enquanto a segunda está interessada em predizer um conjunto de outputs a partir de um conjunto de inputs.\nDo ponto de vista matemático, essas culturas estão intimamente relacionadas e diferem somente com relação à função de perda que se quer minimizar. Na prática, no entanto, isso gera muito atrito na comunidade científica e é o que está por trás da maior parte dos debates sobre estatística vs machine learning.\nCuriosamente, o principal problema na interpretação de pequisas eleitorais está relacionado com as duas culturas. Considere as seguintes perguntas:\nSe a eleição fosse hoje, qual seria a proporção de votos do candidato \\(A\\)? Qual a probabilidade do candidato \\(A\\) ganhar a eleição? A principal afirmação desse artigo é a seguinte:\nPesquisas eleitorais têm como objetivo responder (1), mas a sociedade as usa para responder (2).\nPesquisas eleitorais são levantamentos de intenção de voto. Seu objetivo é entender as opiniões da sociedade sobre os candidatos, hoje. O objetivo não é predizer qual será o resultado da eleição.\nIsso significa que utilizar resultados de eleições para avaliar a qualidade das pesquisas é uma comparação, no mínimo, injusta. É como avaliar a qualidade de um livro pela sua capacidade de servir como peso de papel, uma vez que esta não é sua finalidade.\nPesquisas eleitorais podem ser utilizadas para predizer eleições? Claro que sim! Modelos estatísticos sempre têm propriedades preditivas. Um modelo preditivo para as eleições, no entanto, deve utilizar muito mais informações do que apenas as intenções de voto dos eleitores. Alguns exemplos de dados adicionais que poderiam ser utilizados:\nPodemos tratar indecisos como dados faltantes. Utilizando pesquisas anteriores, podemos estimar a proporção de indecisos que votarão em cada candidato, ao invés de simplesmente descartar os dados dos indecisos. Historicamente, sabemos que a proporção de brancos e nulos observada nas urnas é diferente do que as pessoas afirmam nas pesquisas. Podemos utilizar esse dado para predizer a proporção de votos brancos e nulos. Em determinadas regiões, as pessoas mudam o voto de última hora, possivelmente por ações ilegais como propaganda em boca de urna. Podemos utilizar essa informação para ponderar as estimativas obtidas nas pesquisas eleitorais. Podemos utilizar os dados de diversas pesquisas eleitorais ao mesmo tempo, dando pesos diferentes para elas, para predizer os resultados da eleição. Esses exemplos não são exaustivos. Existem muitas outras formas de considerar todo o conhecimento que temos em nossas predições sobre as eleições.\nComo podemos avaliar pesquisas eleitorais? Pesquisas de intenção de voto só podem ser avaliadas através da metodologia e da execução. Existem diversos parâmetros para levar em conta, como:\nMetodologia de listagem dos indivíduos da amostra: discussões como amostragem probabilística ou amostragem por cotas. Metodologia de acesso aos respondentes: discussões como ponto de fluxo, pesquisa em domicílio ou pesquisa por telefone. Tratamento da não resposta: discussões sobre auto seleção. Verificação das respostas: técnicas para evitar manipulação de resultados e fraudes na coleta de dados. O problema é que a avaliação das pesquisas sempre acaba sendo subjetiva. Por exemplo, sempre é possível argumentar que uma pesquisa é ruim por não ter utilizado amostragem aleatória simples (AAS) para realização do estudo, o que gera um debate incessante.\nAssim, me parece que a forma mais adequada de avaliar pesquisas de intenção de voto é o atendimento a requisitos formais, como o clarificação sobre o método de condução do estudo. Isso pode ser atingindo com a disponibilização de:\nDescrição detalhada da metodologia. Apresentação do questionário do estudo. Base de dados final utilizada e script ou lista de passos utilizados para reprodução dos resultados. O único critério estatístico coerente para avaliar uma pesquisa eleitoral desse tipo seria a comparação dos resultados da pesquisa com outras pesquisas que têm o mesmo objetivo. Por exemplo, se no dia 03 desse mês foram divulgadas dez pesquisas para presidente e uma das pesquisas coloca o candidato \\(A\\) com 15% de vantagem em relação à média das dez pesquisas, existem evidências de que algo pode estar errado nesse levantamento.\nHoje em dia temos alguns agregadores de pesquisas eleitorais que podem ser utilizados para monitorar os resultados das pesquisas e avaliar a qualidade dos resultados. Os meus preferidos são o do Agregador do JOTA, elaborado pelo Guilherme Jardim Duarte e o do Poder360, elaborado pelo Volt Data Lab.\nComo poderíamos mudar essa realidade? Se o interesse da sociedade está em (2), por quê não divulgar exatamente isso? Nos Estados Unidos, é o que o famoso portal FiveThirtyEight se dispôs a fazer. Eles soltam predições periódicas dos resultados de eleições e de eventos esportivos. No Brasil, iniciativas como o PollingData do Neale El-Dash fazem o mesmo, com diferentes metodologias.\nAvaliar a qualidade de estudos preditivos para responder (2) é muito mais fácil. Bastaria comparar o resultado do estudo com os resultados das eleições. O único cuidado seria o de, eventualmente, não dar o mesmo peso para pesquisas mais antigas e mais recentes. Uma forma de fazer isso é utilizando uma adaptação dos scores de Brier:\n\\[ B = \\frac{1}{N}\\sum_{t=1}^N\\sum_{i=1}^C p_t(f_{ti} - r_i)^2, \\]\nonde\n\\(C\\) é o número de candidatos. \\(N\\) é a quantidade de pesquisas realizadas. \\(f_{ti}\\) é a probabilidade de vitória do candidato \\(i\\) na data \\(t\\). \\(p_t\\) é um peso para a predição realizada na data \\(t\\), com a propriedade \\(\\sum_{t=1}^N p_t = 1\\). Idealmente, se \\(t=N\\) é a data mais próxima das eleições, então \\(p_N \\geq p_{N-1} \\geq \\dots \\geq p_1\\), ou seja, é dado maior peso para predições feitas perto das eleições. \\(r_i\\) é zero se o candidato \\(i\\) perdeu e um se o candidato \\(i\\) ganhou. Minha recomendação é que o Tribunal Superior Eleitoral (TSE) defina algumas datas para divulgação de estudos preditivos. Os institutos de pesquisa passariam a divulgar esses estudos nas datas especificadas e, com isso, seria possível elaborar um ranking dos melhores institutos. Essa metodologia já é utilizada pelo BACEN, por exemplo, para avaliar predições sobre a inflação.\nWrap-up Não avalie pesquisas de intenção de voto usando os resultados das eleições. A avaliação de qualidade de pesquisas de intenção de voto deve se limitar a i) atendimento a critérios formais e ii) comparação com resultados de outras pesquisas realizadas no mesmo período. Estudos preditivos de resultados de eleições é o que realmente responde ao que a população quer saber. Proponho que o TSE determine algumas datas para divulgação de modelos preditivos e, posteriormente, faça um ranking dos melhores institutos. Eu sei que o artigo é polêmico, então por favor sejam gentis nos comentários.\nÉ isso pessoal. Happy Coding e não esqueçam de acessar o aplicativo pesqEle para monitorar as pesquisas eleitorais!\nErrata Depois de divulgar o texto, recebi algumas sugestões de melhorias. Agradeço pelas contribuições! Preferi colocar comentários adicionais no lugar de substituir o texto original para não causar confusão nos leitores. Segue abaixo.\n“elas erraram os resultados de várias eleições.” Essa frase está um pouco exagerada. Na verdade, muitas vezes o resultado de eleições acompanha a tendência ajustada com o histórico de levantamentos das pesquisas eleitorais.\n“uma adaptação dos scores de Brier” A fórmula está ruim! Como os \\(p_t\\) somam 1, o \\(1/N\\) está aí de bobeira. Então a fórmula que eu sugiro é essa aqui:\n\\[ B = \\sum_{t=1}^N\\sum_{i=1}^C p_t(f_{ti} - r_i)^2, \\]\n“uma pesquisa é ruim por não ter utilizado amostragem aleatória simples (AAS)” Quando mencionei AAS, eu quis dizer amostragem probabilística. Acho que quase todos concordam que AAS é péssimo nesses casos por conta da abrangência territorial do Brasil, que torna essa metodologia inexequível.\n“se no dia 03 desse mês foram divulgadas dez pesquisas” Aqui é melhor colocar “se no período X foram conduzidas dez pesquisas”. As pesquisas devem ser pareadas pelo período de condução do levantamento das opiniões, não pela data de divulgação.\nVocê quer substituir pesquisas eleitorais por modelos preditivos? Não! Pesquisas eleitorais devem continuar existindo, até porquê elas são o input mais importante para modelos preditivos. A ideia do ranking é que as empresas se voluntariem para competir. A única restrição é que elas devem enviar suas predições para o TSE em datas pré-definidas pelo órgão. Se as datas de submissão fossem diferentes, isso geraria uma competição injusta, pois o instituto A poderia usar a submissão de B para melhorar sua predição.\n","permalink":"https://blog.curso-r.com/posts/2018-08-31-eleitorais/","tags":["eleições"],"title":"Não use resultados de eleições para avaliar pesquisas eleitorais"},{"author":["Athos","Daniel","Bruna Wundervald"],"categories":["divulgação"],"contents":" Arquivos com extensão .jpg guardam 5 coordenadas, que são suficientes para o computador entender e desenhar a imagem:\nx e y: são as coordenadas cartesianas da imagem; e r, g e b: red, green e blue, respectivamente, que juntas formam cores. Todas as cores que conhecemos podem ser compostas pela combinação dessas três cores. A intensidade de cada cor varia de 0 a 1. Para cada ponto no plano (x,y) existe uma cor associada. Assim, uma imagem pode ser representada por um banco de dados com 5 colunas (x, y, r, g e b) e cujo número de linhas é número de pixels da imagem.\nObjetivos Verificar qual modelo entre regressão linear e árvores de decisão é o mais adequado para recuperar a o componente azul da imagem purple_wave.jpg. Verificar qual modelo entre regressão linear e árvores de decisão é o mais adequado para recuperar a o componente azul da imagem xadrez_colorido.jpg. Como começar? Com auxílio da função readJPEG() do pacote jpeg, carregue uma das imagens no R e transforme para data.frame:\nlibrary(tidyverse) library(jpeg) # carrega uma imagem jpeg no R download.file(\u0026quot;http://curso-r.comimages/posts/conteudo/desafio-recuperacao-img/purple_wave.webp\u0026quot;, destfile = \u0026quot;purple_wave.jpg\u0026quot;) img \u0026lt;- readJPEG(\u0026quot;purple_wave.jpg\u0026quot;) # transforma o array da imagem em data.frame com infos de posicao (x,y) # e cor (r,g,b) dimensões da imagem img_dim \u0026lt;- dim(img) # RGB para data.frame img_df \u0026lt;- data.frame( x = rep(1:img_dim[2], each = img_dim[1]), y = rep(img_dim[1]:1, img_dim[2]), r = as.vector(img[,,1]), g = as.vector(img[,,2]), b = as.vector(img[,,3]) ) %\u0026gt;% mutate(cor = rgb(r, g, b), id = 1:n()) Divida o data.frame em duas partes: uma com o azul (coluna b) e outra sem. - Parte 1) x, y, r, g - Parte 2) x, y, r, g, b\n# para reprodução set.seed(2018) # Parte 1) x, y, r, g img_df_parte1 \u0026lt;- img_df %\u0026gt;% sample_frac(3/5) %\u0026gt;% # separando 3/5 do banco mutate(b_backup = b, # backup do azul original b = 0, # retirando o azul da imagem cor = rgb(r, g, b)) # cor da imagem sem o azul # Parte 2) x, y, r, g, b img_df_parte2 \u0026lt;- img_df %\u0026gt;% filter(!id%in%img_df_parte1$id) Visualizando:\n# Imagem sem o azul ggplot(data = img_df_parte1, aes(x = x, y = y)) + geom_point(colour = img_df_parte1$cor) + labs(x = \u0026quot;x\u0026quot;, y = \u0026quot;y\u0026quot;, title = \u0026quot;Imagem sem B (azul)\u0026quot;) + coord_fixed(ratio = 1) + theme_bw() A sua tarefa é recuperar o azul (que nós apagamos da imagem) da Parte 1, utilizando modelos preditivos construídos com a Parte 2 (que ainda tem o azul!) para as duas imagens (xadrez_colorido e purple_wave).\nResultado O resultado desse desafio de dados será publicado no blog no dia 24 de agosto! Boa sorte!\n","permalink":"https://blog.curso-r.com/posts/2018-08-09-desafio-recuperacao-imagem/","tags":["app","pacotes"],"title":"Desafio de Dados - Recuperação de imagens"},{"author":["Daniel"],"categories":["divulgação"],"contents":" O radix é um formato do RMarkdown para publicações técnicas e ciêntíficas. Esse template tem entre suas vantagens:\nUma fonte boa para leitura. Aceita LaTeX, citações e footnotes. Permite posicionar as figuras de forma bastante flexível. etc. O template baseado no formato usado no Distill Research Journal que é uma revista científica dedicada a artigos científicos que explicam de forma clara e dinâmica conceitos relacionados à Machine Learning.\nPara começar a usar o radix você precisa instalar o pacote radix que por enquanto só está disponível pelo Github:\ndevtools::install_github(\u0026quot;rstudio/radix\u0026quot;) Em seguida basta criar um novo RMarkdown e adicionar o seguinte no cabeçalho de metadados:\n--- title: \u0026quot;Radix for R Markdown\u0026quot; description: | Scientific and technical writing, native to the web date: May 4, 2018 author: - name: \u0026quot;JJ Allaire\u0026quot; url: https://github.com/jjallaire affiliation: RStudio affiliation_url: https://www.rstudio.com - name: \u0026quot;Rich Iannone\u0026quot; url: https://github.com/rich-iannone affiliation: RStudio affiliation_url: https://www.rstudio.com - name: \u0026quot;Yihui Xie\u0026quot; url: https://github.com/yihui affiliation: RStudio affiliation_url: https://www.rstudio.com bibliography: biblio.bib output: radix::radix_article --- Além disso, e talvez o mais legal do radix é que ele permite que você crie sites e blogs com conjuntos de artigos. Não vamos falar sobre isso nesse post mas, você pode ler mais na documentação do pacote.\nAlguns exemplos de sites que já estão feitos nesse formato são:\nBlog do Tensorflow para R Reproducible Finance R-Music E aí, o que acharam?\n","permalink":"https://blog.curso-r.com/posts/2018-07-27-radix/","tags":["r markdown"],"title":"radix: um template RMarkdown"},{"author":["Daniel","Bruna Wundervald"],"categories":["divulgação"],"contents":" Bem vindos à primeira edição do Desafio de Dados.\nO pesqEle é o repositório oficial de pesquisas eleitorais disponibilizado pelo Tribunal Superior Eleitoral (TSE). Lá podemos encontrar diversas informações sobre as pesquisas registradas. Dá até para ver os questionários aplicados e a metodologia de coleta.\nO CONRE3 fez um pacote para o R que permite acessar esses dados de forma mais organizada. Leia mais sobre o pacote neste post do blog.\nJunto com o pacote, o CONRE3 fez também um aplicativo em shiny, disponível neste link.\nA ideia dessa primeira edição do Desafio de Dados é replicar o mapa da página inicial do aplicativo que mostra a distribuição de pesquisas eleitorais por estado do Brasil.\nOs dados podem ser obtidos rodando o trecho de código abaixo:\n# devtools::install_github(\u0026quot;conre3/pesqEle\u0026quot;) # instalação do pacote; library(pesqEle) pe \u0026lt;- pesqEle_2018 dplyr::glimpse(pe) O shapefile do mapa está disponível no pacote ufshapes:\n# devtools::install_github(\u0026#39;jtrecenti/ufshape\u0026#39;) # instalação do pacote; df_uf \u0026lt;- ufshape::df_uf Links sugeridos:\nGráficos Miojo: Mapas temáticos do Brasil em 3 minutos ou menos Material Curso-R O post com a solução será publicado no dia 09-08-18.\n","permalink":"https://blog.curso-r.com/posts/2018-07-26-desafio-pesqele/","tags":["app","pacotes"],"title":"Desafio de Dados - PesqEle (parte I)"},{"author":["Julio","Bruna Wundervald"],"categories":["Tutoriais"],"contents":" Hoje nosso amigo Marcio Chiara nos procurou pedindo ajuda pois não estava conseguindo fazer o pacote Rfacebook funcionar. O problema estava logo no começo: a autenticação com a API do Facebook.\nO procedimento O procedimento básico para autenticar no Facebook usando o pacote Rfacebook consiste em três passos:\nCrie um aplicativo no facebook developers: O aplicativo deve ser do tipo “Login do Facebook” Depois de clicar em “Configurar”, seleccione a plataforma “web” No menu da esquerda, clique em Configurações e obtenha o ID do Aplicativo e a Chave Secreta do Aplicativo. Guarde nos objetos client_id e client_secret. Exemplo: Rodar Rfacebook::fbOAuth(). Exemplo (não rode):\nclient_id \u0026lt;- \u0026quot;274017323361669\u0026quot; client_secret \u0026lt;- \u0026quot;6ced33b11f41944e5a960f74c475a5fe\u0026quot; Rfacebook::fbOAuth(client_id, client_secret) O erro O erro aparece ao rodar\nRfacebook::fbOAuth(client_id, client_secret, extended_permissions = TRUE) A opção extended_permissions serve para dar acesso à algumas opções adicionais como aniversário, cidade de nascimento, etc. O erro que aparece é esse aqui:\nComo somos pessoas espertinhas, o que fizemos? Abrimos os links que o Facebook sugeriu? Re-estudamos toda a API do Facebook? Não! Apenas lemos e interpretamos o erro. A API estava recusando acesso às opções user_relationships e publish_actions, que são requeridas por meio da função Rfacebook::fbOAuth(). Aha!\nDepois de estudar um pouquinho, notamos que a API do Facebook mudou recentemente. Isso ocorreu por conta de alterações na política de privacidade do Facebook, que é sempre algo polêmico. Hoje em dia não é mais possível autorizar aplicativos para terem acesso à rede de relacionamentos e ações do usuário.\nO problema é que infelizmente o mantenedor do Rfacebook parou de atualizar o pacote:\nEntão, o que fazer? Quando temos uma ferramenta que não funciona mais ou que está obsoleta, podemos i) jogar fora e fazer outra do zero ou ii) tentar arrumá-la para ficar top novamente.\nPor sorte, o R é open-source e permite que (ii) seja feito sem maiores esforços. De fato, basta mudar uma linha de código para que tudo volte a funcionar como antes. Basta mudar o objeto scope de\nscope \u0026lt;- c(\u0026quot;user_birthday\u0026quot;, \u0026quot;user_hometown\u0026quot;, \u0026quot;user_location\u0026quot;, \u0026quot;user_relationships\u0026quot;, \u0026quot;publish_actions\u0026quot;, \u0026quot;user_status\u0026quot;, \u0026quot;user_likes\u0026quot;) para\nscope \u0026lt;- c(\u0026quot;user_birthday\u0026quot;, \u0026quot;user_hometown\u0026quot;, \u0026quot;user_location\u0026quot;, \u0026quot;user_status\u0026quot;, \u0026quot;user_likes\u0026quot;) A solução Com base nessas alterações, a Bruna sugere a utilização da função modificada abaixo:\nnew_fbOAuth \u0026lt;- function (app_id, app_secret, extended_permissions = FALSE, legacy_permissions = FALSE, scope = NULL) { facebook \u0026lt;- oauth_endpoint(authorize = \u0026quot;https://www.facebook.com/dialog/oauth\u0026quot;, access = \u0026quot;https://graph.facebook.com/oauth/access_token\u0026quot;) myapp \u0026lt;- oauth_app(\u0026quot;facebook\u0026quot;, app_id, app_secret) if (is.null(scope)) { if (extended_permissions == TRUE) { # as alterações significativas estão aqui \u0026gt;\u0026gt;\u0026gt; scope \u0026lt;- c(\u0026quot;user_birthday\u0026quot;, \u0026quot;user_hometown\u0026quot;, \u0026quot;user_location\u0026quot;, # \u0026quot;user_relationships\u0026quot;, \u0026quot;publish_actions\u0026quot;, \u0026quot;user_status\u0026quot;, \u0026quot;user_likes\u0026quot;) } else { scope \u0026lt;- c(\u0026quot;public_profile\u0026quot;, \u0026quot;user_friends\u0026quot;) } if (legacy_permissions == TRUE) { scope \u0026lt;- c(scope, \u0026quot;read_stream\u0026quot;) } } if (packageVersion(\u0026quot;httr\u0026quot;) \u0026lt; \u0026quot;1.2\u0026quot;) { stop(\u0026quot;Rfacebook requires httr version 1.2.0 or greater\u0026quot;) } if (packageVersion(\u0026quot;httr\u0026quot;) \u0026lt;= \u0026quot;0.2\u0026quot;) { facebook_token \u0026lt;- oauth2.0_token(facebook, myapp, scope = scope) fb_oauth \u0026lt;- sign_oauth2.0(facebook_token$access_token) if (GET(\u0026quot;https://graph.facebook.com/me\u0026quot;, config = fb_oauth)$status == 200) { message(\u0026quot;Authentication successful.\u0026quot;) } } if (packageVersion(\u0026quot;httr\u0026quot;) \u0026gt; \u0026quot;0.2\u0026quot; \u0026amp; packageVersion(\u0026quot;httr\u0026quot;) \u0026lt;= \u0026quot;0.6.1\u0026quot;) { fb_oauth \u0026lt;- oauth2.0_token(facebook, myapp, scope = scope, cache = FALSE) if (GET(\u0026quot;https://graph.facebook.com/me\u0026quot;, config(token = fb_oauth))$status == 200) { message(\u0026quot;Authentication successful.\u0026quot;) } } if (packageVersion(\u0026quot;httr\u0026quot;) \u0026gt; \u0026quot;0.6.1\u0026quot; \u0026amp; packageVersion(\u0026quot;httr\u0026quot;) \u0026lt; \u0026quot;1.2\u0026quot;) { Sys.setenv(HTTR_SERVER_PORT = \u0026quot;1410/\u0026quot;) fb_oauth \u0026lt;- oauth2.0_token(facebook, myapp, scope = scope, cache = FALSE) if (GET(\u0026quot;https://graph.facebook.com/me\u0026quot;, config(token = fb_oauth))$status == 200) { message(\u0026quot;Authentication successful.\u0026quot;) } } if (packageVersion(\u0026quot;httr\u0026quot;) \u0026gt;= \u0026quot;1.2\u0026quot;) { fb_oauth \u0026lt;- oauth2.0_token(facebook, myapp, scope = scope, cache = FALSE) if (GET(\u0026quot;https://graph.facebook.com/me\u0026quot;, config(token = fb_oauth))$status == 200) { message(\u0026quot;Authentication successful.\u0026quot;) } } error \u0026lt;- tryCatch(callAPI(\u0026quot;https://graph.facebook.com/pablobarbera\u0026quot;, fb_oauth), error = function(e) e) if (inherits(error, \u0026quot;error\u0026quot;)) { class(fb_oauth)[4] \u0026lt;- \u0026quot;v2\u0026quot; } if (!inherits(error, \u0026quot;error\u0026quot;)) { class(fb_oauth)[4] \u0026lt;- \u0026quot;v1\u0026quot; } return(fb_oauth) } É isso. Happy coding ;)\nNota: Se você já tinha um app criado no Facebook antes da mudança de política de privacidade, o ideal é que ele passe pelo App Review, como sugerido no próprio pacote do Rfacebook.\n","permalink":"https://blog.curso-r.com/posts/2018-07-22-rfacebook/","tags":["app","pacotes"],"title":"RFacebook: como conectar?"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" Provavelmente você já ouviu falar do operador pipe (%\u0026gt;%). Muita gente acha que ele é uma sequência mágica de símbolos que muda completamente o visual do seu código, mas na verdade ele não passa de uma função como outra qualquer.\nNesse post vou explorar um pouco da história do pipe, como ele funciona e por que utilizá-lo.\nOrigem O conceito de pipe existe pelo menos desde os anos 1970. De acordo com seu criador, o operador foi concebido em “uma noite febril” e tinha o objetivo de simplificar comandos cujos resultados deveriam ser passados para outros comandos.\nls | cat #\u0026gt; Desktop #\u0026gt; Documents #\u0026gt; Downloads #\u0026gt; Music #\u0026gt; Pictures #\u0026gt; Public #\u0026gt; Templates #\u0026gt; Videos Por essa descrição já conseguimos ter uma ideia de onde vem o seu nome: pipe em inglês significa “cano”, referindo-se ao transporte das saídas dos comandos. Em portugês o termo é traduzido como “canalização” ou “encadeamento”, mas no dia-a-dia é mais comum usar o termo em inglês.\nA partir daí o pipe tem aparecido nas mais diversas aplicações, desde HTML até o nosso tão querido R. Ele pode ter múltiplos disfarces, mas o seu objetivo é sempre o mesmo: transportar resultados.\nComo funciona Em R o pipe tem uma cara meio estranha (%\u0026gt;%), mas no fundo ele não passa de uma função infixa, ou seja, uma função que aparece entre os seus argumentos (como a + b ou a %in% b). Na verdade é por isso mesmo que ele tem porcentagens antes e depois: porque no R uma função infixa só pode ser declarada assim.\nVamos começar demonstrando sua funcionalidade básica. Carregue o pacote magrittr e declare o pipe usando Ctrl + Shift + M.\nlibrary(magrittr) `%\u0026gt;%`(\u0026quot;oi\u0026quot;, print) #\u0026gt; [1] \u0026quot;oi\u0026quot; Não ligue para os acentos graves em volta do pipe, o comando acima só serve para demonstrar que ele não é nada mais que uma função; perceba que o seu primeiro argumento (\"oi\") virou a entrada do seu segundo argumento (print).\n\u0026quot;oi\u0026quot; %\u0026gt;% print() #\u0026gt; [1] \u0026quot;oi\u0026quot; Observe agora o comando abaixo. Queremos primeiro somar 3 a uma sequência de números e depois dividí-los por 2:\nmais_tres \u0026lt;- function(x) { x + 3 } sobre_dois \u0026lt;- function(x) { x / 2 } x \u0026lt;- 1:3 sobre_dois(mais_tres(x)) #\u0026gt; [1] 2.0 2.5 3.0 Perceba como fica difícil de entender o que está acontecendo primeiro? A linha relevante começa com a divisão por 2, depois vem a soma com 3 e, por fim, os valores de entrada.\nNesse tipo de situação é mais legível usar a notação de composição de funções, com as funções sendo exibidas na ordem em que serão aplicadas: \\(f \\circ g\\).\nIsso pode ser realizado se tivermos uma função que passa o resultado do que está à sua esquerda para a função que está à sua direita…\nx %\u0026gt;% mais_tres() %\u0026gt;% sobre_dois() #\u0026gt; [1] 2.0 2.5 3.0 No comando acima fica evidente que pegamos o objeto x, somamos 3 e dividimos por 2.\nVocê pode já ter notado isso, mas a entrada (esquerda) de um pipe sempre é passada como o primeiro argumento agumento da sua saída (direita). Isso não impede que as funções utilizadas em uma sequência de pipes tenham outros argumentos.\nmais_n \u0026lt;- function(x, n) { x + n } x %\u0026gt;% mais_n(4) %\u0026gt;% sobre_dois() #\u0026gt; [1] 2.5 3.0 3.5 Vantagens A grande vantagem do pipe não é só enxergar quais funções são aplicadas primeiro, mas sim nos ajudar a programar pipelines (“encanamento” em inglês) de tratamentos de dados.\nlibrary(dplyr) starwars %\u0026gt;% mutate(bmi = mass/((height/100)^2)) %\u0026gt;% select(name, bmi, species) %\u0026gt;% group_by(species) %\u0026gt;% summarise(bmi = mean(bmi)) #\u0026gt; # A tibble: 38 x 2 #\u0026gt; species bmi #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Aleena 24.0 #\u0026gt; 2 Besalisk 26.0 #\u0026gt; 3 Cerean 20.9 #\u0026gt; 4 Chagrian NA #\u0026gt; 5 Clawdite 19.5 #\u0026gt; 6 Droid NA #\u0026gt; 7 Dug 31.9 #\u0026gt; 8 Ewok 25.8 #\u0026gt; 9 Geonosian 23.9 #\u0026gt; 10 Gungan NA #\u0026gt; # ... with 28 more rows Acima fica extremamente claro o que está acontecendo em cada passo da pipeline. Partindo da base starwars, primeiro transformamos, depois selecionamos, agrupamos e resumimos; em cada linha temos uma operação e elas são executadas em sequência.\nIsso não melhora só a legibilidade do código, mas também a sua debugabilidade! Se tivermos encontrado um bug na pipeline, basta executar linha a linha do encadeamento até que encontremos a linha problemática. Com o pipe podemos programar de forma mais compacta, legível e correta.\nTodos os exemplos acima envolvem passar a entrada do pipe como o primeiro argumento da função à direita, mas não é uma obrigatoriedade. Com um operador placeholder . podemos indicar exatamente onde deve ser colocado o valor que chega no pipe:\ny_menos_x \u0026lt;- function(x, y) { y - x } x %\u0026gt;% mais_tres() %\u0026gt;% purrr::map2(4:6, ., y_menos_x) # [[1]] # [1] 0 # # [[2]] # [1] 0 # # [[3]] # [1] 0 Bônus Agora que você já sabe dos usos mais comuns do pipe, aqui está uma outra funcionalidade interessante: funções unárias. Se você estiver familiarizado com o pacote purrr, esse é um jeito bastante simples de criar funções descartáveis.\nm3_s2 \u0026lt;- . %\u0026gt;% mais_tres() %\u0026gt;% sobre_dois() m3_s2(x) #\u0026gt; [1] 2.0 2.5 3.0 Usando novamente o . definimos uma função que recebe apenas um argumento com uma sequência de aplicações de outras funções.\nConclusão O pipe não é apenas algo que deve ser usado pelos fãs do tidyverse. Ele é uma função extremamente útil que ajuda na legibilidade e programação de código, independentemente de quais pacotes utilizamos.\nSe quiser saber mais sobre o mundo do pipe, leia este post do Daniel sobre o Manifesto Tidy e o nosso tutorial mais aprofundado sobre o próprio pipe.\n","permalink":"https://blog.curso-r.com/posts/2018-07-03-tutorial-pipe/","tags":["pipe","tidyverse"],"title":"Por que usar o %\u003e%"},{"author":["William"],"categories":["análises"],"contents":" Com a greve dos caminhoneiros no final do mês de maio, muitas cidades tiveram uma redução atípica no tráfego de caminhões e, posteriormente, com a falta de combustível, de veículos em geral.\nComo as emissões veiculares são a principal fonte de diversos poluentes em centros urbanos, faz todo o sentido analisarmos o impacto das paralisações nos níveis de poluição.\nNeste post, vamos analisar descritivamente as concentrações de alguns poluentes um pouco antes, durante e um pouco depois da greve dos caminhoneiros.\nComo aperitivo, veja a variação dos poluentes durante as paralisações:\npollutant Ibirapuera Osasco Parque D.Pedro II Pinheiros CO (manhã) -48.02% -32.78% -51.16% -65.9% CO (noite) -61.18% -50.81% -60.77% -62.96% MP10 NaN% -19.1% -19.06% -20.06% MP2.5 -10.26% -21.08% -20.98% -21.13% NO -89.66% -50.9% -75.38% -83.09% NO2 -42.49% -13.24% -38.73% -39.45% O3 53.7% NaN% 68.43% 126% Contextualizando A greve dos caminhoneiros foi como ficou conhecida a paralisação de caminhoneiros autônomos em todo o território nacional em maio de 2018. As manifestações começaram no dia 21 de maio e duraram até o início de junho. Nesse período, muitas cidades sofreram com desabastecimento, principalmente de combustível.\nNesta análise, eu foquei a observação entre o dias 23 e 30 de maio, período em que as consequências da greve foram mais intensos. Os poluentes considerados foram:\nMonóxido de carbono (CO), gás tóxico que pode causar dor de cabeça, tontura, náusea e, em altas concentrações, desorientação e morte por asfixia. Segundo a CETESB, 97% do CO liberado em São Paulo vem de emissões veiculares.\nOzônio (O3), na estratosfera, faz papel de herói, filtrando parte da radiação solar (camada de ozônio). Na troposfera, camada mais baixa da atmosfera, faz papel de vilão, sendo associado a diversas doenças respiratórias e cardiovasculares. Não é gerado diretamente pela queima de combustíveis, mas sim por reações químicas atmosféricas que envolvem diversos compostos e radiação solar.\nMonóxido de nitrogênio (NO) e dióxido de nitrogênio (NO2), gases tóxicos que contribuem para a formação de chuva ácida e do ozônio troposférico. Estão associados a diversos problemas respiratórios, como enfisema pulmonar e bronquite.\nMaterial particulado 2.5 (MP2.5) e material particulado 10 (MP10), partículas suspensas no ar com diâmetro menor que 2.5 \\(\\mu\\)m (MP2.5) ou entre 2.5 e 10 \\(\\mu\\)m (MP10). Estão associadas a diversas doenças respiratórias e câncer de pulmão.\nAs concentrações de cada poluente foram medidas em estações de monitoramento da CETESB. Considerei quatro estações na Grande São Paulo: Osasco, Pinheiros, Parque Dom Pedro II e Ibirapuera. O critério para a escolha foi a disponibilidade de dados para os poluentes escolhidos e o perfil do tráfego de veículos na região das estações. As estações Parque Dom Pedro II e Pinheiros ficam em regiões de tráfego intenso, a primeira no centro da cidade e a segunda próxima à marginal Pinheiros. A estação de Osasco também fica numa região de tráfego intenso e relativamente próxima a duas rodovias. A estação Ibirapuera não é muito afetada pelo tráfego pois fica dentro do Parque Ibirapuera e será utilizada como comparação.\nA localização de cada uma delas está representada no mapa a seguir:\nComo são muitos gráficos e tabelas, vou apresentar apenas alguns resultados aqui no post. O restante está disponível nesse flexdashboard.\nDados Os dados desta análise foram extraídos do sistema Qualar, como discutido neste post. O período considerado foi 01/05/2018 a 14/06/2018. Também considerei esse mesmo período nos anos de 2016 e 2017 para pode comparar o comportamento dos poluentes com e sem a greve de caminhoneiros.\nO código utilizado pode ser acessado aqui. Repare que, para facilitar o acompanhamento do processo, as requisições foram feitas separadamente para cada estação/ano, pois requisições com um volume muito grande de dados demoravam muito para serem concluídas. Após todos os arquivos serem baixados, os dados foram consolidados em uma única base, que pode ser acessada neste link.\nFunções usadas para gerar os gráficos Dado um poluente, um ano e um conjunto de horas, a função abaixo devolve um plot com os gráficos da série temporal das quatro estações, utilizando a média diária do poluente nas horas especificadas.\nmake_series_plot \u0026lt;- function(df, pollutant, year_, hours) { if(year_ == 2018) { p_lab \u0026lt;- labs(x = \u0026quot;Dia\u0026quot;, y = pollutant) } else { p_lab \u0026lt;- labs(x = \u0026quot;\u0026quot;, y = pollutant) } df %\u0026gt;% filter( hour %in% hours ) %\u0026gt;% group_by(date, stationname) %\u0026gt;% select(y = pollutant, everything()) %\u0026gt;% summarise(conc = mean(y, na.rm = TRUE)) %\u0026gt;% ungroup() %\u0026gt;% mutate(year = lubridate::year(date)) %\u0026gt;% filter(year == year_) %\u0026gt;% ggplot(aes(x = date, y = conc)) + geom_line() + geom_vline( xintercept = lubridate::dmy(paste0(\u0026quot;23-05-\u0026quot;, year_)), linetype = 2, color = \u0026quot;red\u0026quot; ) + geom_vline( xintercept = lubridate::dmy(paste0(\u0026quot;30-05-\u0026quot;, year_)), linetype = 2, color = \u0026quot;red\u0026quot; ) + facet_grid(year ~ stationname, scales = \u0026quot;free_y\u0026quot;) + theme_bw() + scale_x_date( labels = scales::date_format(\u0026quot;%d-%m\u0026quot;), breaks = c( lubridate::dmy(paste0(\u0026quot;01-05-\u0026quot;, year_)), lubridate::dmy(paste0(\u0026quot;01-06-\u0026quot;, year_)) ), date_breaks = \u0026quot;1 month\u0026quot; ) + p_lab } A função make_grid_plot() aplica a função acima e junta os gráficos dos três anos considerados.\nmake_grid_plot \u0026lt;- function(df, pollutant, hours) { map( 2016:2018, make_series_plot, df = df, pollutant = pollutant, hours = hours ) %\u0026gt;% patchwork::wrap_plots(nrow = 3) %\u0026gt;% print() } A função make_bar_plot() faz um gráfico de barras para a média dos poluentes para cada estação em diferentes períodos (antes, durante e após a greve).\nmake_bar_plot \u0026lt;- function(df, pollutant_, hours) { df %\u0026gt;% mutate(period = case_when( date \u0026gt; dmy(\u0026quot;09-05-2018\u0026quot;) \u0026amp; date \u0026lt; dmy(\u0026quot;16-05-2018\u0026quot;) ~ 1, date \u0026gt; dmy(\u0026quot;23-05-2018\u0026quot;) \u0026amp; date \u0026lt; dmy(\u0026quot;30-05-2018\u0026quot;) ~ 2, date \u0026gt; dmy(\u0026quot;06-06-2018\u0026quot;) \u0026amp; date \u0026lt; dmy(\u0026quot;14-06-2018\u0026quot;) ~ 3, TRUE ~ 0 )) %\u0026gt;% filter(period != 0) %\u0026gt;% gather(pollutant, conc, CO:O3) %\u0026gt;% group_by(pollutant, stationname, period) %\u0026gt;% filter(hour %in% hours, pollutant == pollutant_) %\u0026gt;% summarise(conc = mean(conc, na.rm = TRUE)) %\u0026gt;% ggplot(aes(x = stationname, y = conc, fill = as.factor(period))) + geom_bar(stat = \u0026quot;identity\u0026quot;, position = \u0026quot;dodge\u0026quot;) + labs(y = pollutant_, fill = \u0026quot;Período\u0026quot;, x = \u0026quot;Estação\u0026quot;) + scale_fill_discrete( labels = c(\u0026quot;09/05 a 16/05\u0026quot;, \u0026quot;23/05 a 30/05\u0026quot;, \u0026quot;06/06 a 14/06\u0026quot;) ) + theme(legend.position = \u0026quot;bottom\u0026quot;) } Resultados Para construir as séries, eu usei o gráfico abaixo para avaliar a média horária de cada poluente em cada dia da semana. Assim, em vez de usar as séries horárias, que, em geral, apresentam sazonalidade diária, eu usei contruí as séries da média diária nos horários de pico. O gráfico mostra, por exemplo, que os picos de CO acontecem de manhã e no começo da noite e que os níveis desse poluente são bem menores nos fins de semana.\ndf %\u0026gt;% group_by(dayofweek, hour) %\u0026gt;% summarise_at(vars(CO:O3), funs(mean), na.rm = TRUE) %\u0026gt;% gather(polluent, conc, CO:O3) %\u0026gt;% ggplot(aes(x = hour, y = conc)) + geom_line() + facet_grid(polluent ~ dayofweek, scales = \u0026quot;free_y\u0026quot;) + theme_bw() + labs(x = \u0026quot;Hora\u0026quot;, y = \u0026quot;Concentração\u0026quot;) Usando então a função make_grid_plot() e as informações do gráfico acima, podemos construir e analisar as séries de cada poluente. Nos gráficos abaixo, o intervalo entre as retas pontilhadas representa o período de greve.\nO gráfico abaixo mostra as séries do monóxido de carbono (CO), utilizando a média diária das 7 às 11 horas. Os fins de semana foram retirados da amostra.\ndf %\u0026gt;% filter(!dayofweek %in% c(\u0026quot;sáb\u0026quot;,\u0026quot;dom\u0026quot;)) %\u0026gt;% make_grid_plot(\u0026quot;CO\u0026quot;, 7:11) Agora, o gráfico do monóxido de nitrogênio (NO), também com média diária das 7 às 11 horas. Os domingos foram retirados da amostra.\ndf %\u0026gt;% filter(!dayofweek %in% c(\u0026quot;dom\u0026quot;)) %\u0026gt;% make_grid_plot(\u0026quot;NO\u0026quot;, 7:11) E o gráfico do ozônio, com média diária das 12 às 17 horas. Osasco foi retirada pois não há dados de ozônio nessa estação.\ndf %\u0026gt;% filter(!stationname == \u0026quot;Osasco\u0026quot;) %\u0026gt;% make_grid_plot(\u0026quot;O3\u0026quot;, 12:17) O gráfico abaixo apresenta a média das médias diárias de CO (das 7h às 11h) em um período anterior (09/05 a 16/05), durante (23/05 a 30/05) e em um período posterior à greve (06/06 a 14/06). Os três períodos têm o mesmo número de dias da semana e fins de semana.\nmake_bar_plot(df, \u0026quot;CO\u0026quot;, 7:11) Como comentei anteriormente, os gráficos restantes podem ser visualizados neste dashboard.\nPor fim, eu juntei os períodos anterior (09/05 a 16/05) e posterior (06/06 a 14/06), calculei a média dos poluentes nesses dias e comparei com a média durante o período de greve. A variação dos poluentes para cada estação está apresentada na tabela abaixo. O CO pela manhã, por exemplo, diminuiu 58.62% em Pinheiros durante a greve, enquanto o ozônio aumentou 111.75%.\ntab pollutant Ibirapuera Osasco Parque D.Pedro II Pinheiros CO (manhã) -48.02% -32.78% -51.16% -65.9% CO (noite) -61.18% -50.81% -60.77% -62.96% MP10 NaN% -19.1% -19.06% -20.06% MP2.5 -10.26% -21.08% -20.98% -21.13% NO -89.66% -50.9% -75.38% -83.09% NO2 -42.49% -13.24% -38.73% -39.45% O3 53.7% NaN% 68.43% 126% Comentários Com exceção do ozônio, a concentração média dos poluentes durante o período de paralisação diminuiu. A maior redução foi a de NO, que é diretamente produzido pela queima de combustíveis, principalmente gasolina e diesel. A greve não parece ter influenciado muito nos níveis de MP2.5.\nO ozônio não é produzido diretamente pela combustão de combustíveis. Ele é produto de um complexo processo químico que ocorre ao longo do dia, envolvendo diversos compostos e a radiação solar. O aumento da concentração de ozônio durante a greve era esperada, pois o NO emitido pela queima de diesel e gasolina reage com o O3 troposférico, diminuindo a sua concentração ao longo da tarde. Como o NO diminuiu por causa do menor tráfego de veículos, o nível de ozônio foi maior durante a greve. Para mais detalhes sobre esse fenômeno, veja este post.\nEsta análise só considera o período de greve e variáveis de calendário para explicar a variação da concentração dos poluentes. Uma análise mais completa deveria considerar também os efeitos climáticos (temperatura, precipitação, vento, radiação, entre outros). As conclusões aqui supõem que esses fatores se mantiveram homogêneos durante o período analisado, o que pode não ser razoável.\nTem sugestões? Alguma crítica? Está carente? Deixe seu comentário abaixo. :)\n","permalink":"https://blog.curso-r.com/posts/2018-06-18-poluicao-greve-caminhoneiros/","tags":["web scraping","dados abertos"],"title":"A poluição do ar durante a greve dos caminhoneiros"},{"author":["Julio"],"categories":["análises"],"contents":" Além de ser sócio-fundador da R6 consultoria, a holding que controla a Curso-R.com, também estou presidente do Conselho Regional de Estatística - 3a Região. Uma de nossas incumbências como conselheiros é encontrar formas de monitorar e fiscalizar as pesquisas eleitorais.\nNesse ano eleitoral, por conta da pluralidade de candidatos e grande fragmentação do cenário político, a preocupação com as pesquisas eleitorais é ainda maior. O cenário incerto gera pesquisas com resultados incertos. Isso é um prato cheio para oportunistas que desejam influenciar a opinião pública através de pesquisas forjadas.\nPor isso, é nossa obrigação como cidadãos e cidadãs acompanhar e monitorar as pesquisas eleitorais registradas, detectando e apontando o dedo para possíveis fraudes. Esse post busca ajudar a população nesse sentido.\nO sistema pesqEle O pesqEle é o repositório oficial de pesquisas eleitorais disponibilizado pelo Tribunal Superior Eleitoral (TSE). Lá podemos encontrar diversas informações sobre as pesquisas registradas. Dá até para ver os questionários aplicados e a metodologia de coleta.\nInfelizmente, o formulário de cadastro do pesqEle é problemático, permitindo a entrada de dados sem padronização. Isso atrapalha bastante na hora de organizar os dados e fazer análises.\nOutro problema do pesqEle é que ele é pouco divulgado. Essa base de dados tem informações importantíssimas que deveriam ser usadas pelas associações, conselhos e mídia para monitorar as pesquisas, mas o foco é sempre nos resultado que são apresentados, e não na metodologia empregada para realização dos levantamentos.\nComo bom estatístico que sou e faxineiro de dados, me senti obrigado a divulgar esses dados de uma forma razoável. Veja como abaixos\nO pacote pesqEle O site do pesqEle não disponibiliza API para acesso aos dados e seria muito complicado pedir atualizações de dados via LAI. Por isso, foi criamos um raspador para obter os dados automaticamente. O pacote pesqEle é o resultado desse esforço.\nO pacote exporta apenas uma função, pe_2018(), usada para download e arrumação dos dados do pesqEle. Ela faz o download de todas as pesquisas de 2018 por raspagem de dados. Por padrão, os arquivos HTML são armazenados na pasta data-raw/html_2018, mas é possível alterar essa pasta pelo parâmetro path=. Exemplo:\npe \u0026lt;- pe_2018(\u0026quot;pasta\u0026quot;) dplyr::glimpse(pe) Observations: 186 Variables: 24 $ id_seq \u0026lt;int\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... $ id_pesq \u0026lt;chr\u0026gt; \u0026quot;MA-05302/2018\u0026quot;, \u0026quot;DF-03958/2018\u0026quot;, \u0026quot;DF-06553/2... $ info_uf \u0026lt;chr\u0026gt; \u0026quot;MA\u0026quot;, \u0026quot;DF\u0026quot;, \u0026quot;DF\u0026quot;, \u0026quot;PI\u0026quot;, \u0026quot;PI\u0026quot;, \u0026quot;SP\u0026quot;, \u0026quot;SP\u0026quot;, \u0026quot;PI... $ info_election \u0026lt;chr\u0026gt; \u0026quot;Eleições Gerais 2018\u0026quot;, \u0026quot;Eleições Gerais 2018... $ info_position \u0026lt;chr\u0026gt; \u0026quot;\u0026quot;, \u0026quot;Deputado Distrital\u0026quot;, \u0026quot;Deputado Federal\u0026quot;,... $ comp_nm \u0026lt;chr\u0026gt; \u0026quot;M R BORGES SERVICOS / MBO PUBLICIDADE, MARKE... $ comp_cnpj \u0026lt;chr\u0026gt; \u0026quot;00905916000190\u0026quot;, \u0026quot;00850844000121\u0026quot;, \u0026quot;00850844... $ comp_contract_same \u0026lt;chr\u0026gt; \u0026quot;Sim\u0026quot;, \u0026quot;Sim\u0026quot;, \u0026quot;Sim\u0026quot;, \u0026quot;Não\u0026quot;, \u0026quot;Não\u0026quot;, \u0026quot;Não\u0026quot;, \u0026quot;Nã... $ stat_id \u0026lt;chr\u0026gt; \u0026quot;1791\u0026quot;, \u0026quot;CONRE 1a. Região No. 9403\u0026quot;, \u0026quot;CONRE 1... $ stat_nm \u0026lt;chr\u0026gt; \u0026quot;ANTONIO CARLOS RODRIGUES BARBOSA\u0026quot;, \u0026quot;LUCIANA ... $ pesq_n \u0026lt;dbl\u0026gt; 10973, 3200, 3200, 320, 320, 601, 601, 320, 8... $ pesq_val \u0026lt;dbl\u0026gt; 15700, 32000, 32000, 3000, 3000, 6000, 0, 300... $ pesq_contractors \u0026lt;chr\u0026gt; \u0026quot;CNPJ: 00905916000190 - M R BORGES SERVICOS\u0026quot;,... $ pesq_origin \u0026lt;chr\u0026gt; \u0026quot;Vazio\u0026quot;, \u0026quot;Vazio\u0026quot;, \u0026quot;Vazio\u0026quot;, \u0026quot;Recursos proprios... $ pesq_same \u0026lt;lgl\u0026gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL... $ dt_reg \u0026lt;date\u0026gt; 2018-05-12, 2018-03-26, 2018-03-26, 2018-02-... $ dt_pub \u0026lt;date\u0026gt; 2018-05-18, 2018-04-01, 2018-04-01, 2018-02-... $ dt_start \u0026lt;date\u0026gt; 2018-03-26, 2018-03-21, 2018-03-21, 2018-02-... $ dt_end \u0026lt;date\u0026gt; 2018-05-04, 2018-03-30, 2018-03-30, 2018-02-... $ txt_verif \u0026lt;chr\u0026gt; \u0026quot;Sistema interno de controle e verificação, c... $ txt_method \u0026lt;chr\u0026gt; \u0026quot;METODOLOGIA: A pesquisa foi realizada median... $ txt_about \u0026lt;chr\u0026gt; \u0026quot;Trata-se de uma amostra não aleatória por co... $ txt_plan \u0026lt;chr\u0026gt; \u0026quot;. PLANO AMOSTRAL E INTERVALOS DE CONFIANÇA: ... $ stat_unique \u0026lt;chr\u0026gt; \u0026quot;1791_A535\u0026quot;, \u0026quot;9403_L252\u0026quot;, \u0026quot;9403_L252\u0026quot;, \u0026quot;5102_... Ao rodar pela primeira vez, provavelmente o download demorará por volta de 15-20 minutos, dependendo da conexão com a internet. Nas próximas vezes o download é mais rápido pois o programa não baixa pesquisas já armazenadas em arquivos HTML.\nSe não quiser rodar a função pe_2018(), armazenamos os dados no objeto pesqEle::pesqEle_2018. Esse objeto é uma data.frame() com as mesmas colunas resultantes de pe_2018(). Essas variáveis são\nIdentificadores: id_seq: ID sequencial id_pesq: ID de registro da pesquisa Informações da eleição info_uf: Unidade Federativa indicando abrangência da pesquisa (“BR” = Brasil) info_election: Eleição (no caso, sempre 2018) info_position: Cargo eleitoral Informações da empresa comp_nm: Nome da empresa que realizou a pesquisa comp_cnpj: CNPJ da empresa que realizou a pesquisa comp_contract_same: Contratante é a própria empresa? Informações do estatístico responsável stat_id: ID do estatístico stat_nm stat_unique: Código único do estatístico. Os nomes dos estatísticos estavam escritos de formas diferentes, então arrumamos utilizando o pacote SoundexBR e outras heurísticas. Nada garante que o pacote contenha mais erros nos nomes. Informações sobre a pesquisa pesq_n: Tamanho da amostra. pesq_val: Custo da pesquisa informado no registro. pesq_contractors: Empresas que contrataram a pesquisa. pesq_origin: Origem dos recursos da pesquisa. pesq_same: Indica se o contratante da pesquisa é a própria empresa e a pesquisa foi realizada com recursos próprios (em 2018 não tem nenhuma). Datas dt_reg: Data de registro da pesquisa. dt_pub: Data de publicação da pesquisa. dt_start: Data de início da pesquisa. dt_end: Data de término da pesquisa. Informações adicionais (texto) txt_verif, txt_method, txt_about, txt_plan: informações adicionais da pesquisa, geralmente em texto, sobre metodologia, aplicação e outros detalhes. A base passou por uma série de limpezas para ficar minimamente utilizável. Essas faxinas incluem arrumação de nomes dos estatísticos, empresas e UFs.\nShiny App Não podemos ficar só nas tabelinhas, né?\nTambém fizemos um app para visualizar os dados e levantar algumas estatísticas básicas. O William é o ser maravilhoso e iluminado por trás desse app, que ficou realmente lindo.\nApós instalar o pacote, você pode abrir o app do pesqEle localmente rodando\nshiny::runApp(system.file(\u0026quot;app\u0026quot;, package = \u0026quot;pesqEle\u0026quot;)) Para rodar o app, além dos pacotes do tidyverse e shiny, você precisará instalar os pacotes shinydashboard, highcharter, shinyBS e shinyjs. Todos eles estão no CRAN e podem ser instalados via install.packages().\nTambém disponibilizamos o app nos servidores do CONRE-3. Você pode acessar diretamente aqui:\nhttp://pesqele.conre3.org.br/app\nColoquei o app dentro de um iframe para acesso direto:\nDesenvolvendo o pesqEle Se você tem interesse em baixar os dados de outras formas e aplicar outras regras para arrumação, você pode! No entanto, para isso terá de trabalhar com as funções não documentadas do pesqEle. O pacote está longe de seu estado ideal.\nCaso essa parte do pacote se torne realmente útil no futuro, pretendo deixar essas funções exportadas e documentadas. Para isso, no entanto, precisarei de muito apoio da comunidade.\nPor enquanto, recomendo que você dê uma fuçada no repositório do pacote no github e adicione issues.\nLinks Siga o CONRE-3 no Facebook e acesse a página do site sobre pesquisas eleitorais. Contribua com o pesqEle!! ","permalink":"https://blog.curso-r.com/posts/2018-05-30-pesqele/","tags":["app","pacotes"],"title":"pesqEle: dados de pesquisas eleitorais"},{"author":["Athos","Julio"],"categories":["conceitos"],"contents":" Modelos baseados em árvores como árvores de decisão, random forest, ligthGBM e xgboost são conhecidos, dentre outras qualidades, pela sua robustês diante do problema de multicolinearidade. É sabido que seu poder preditivo não se abala na presença de variáveis extremamente correlacionadas.\nPorém, quem nunca usou um Random Forest pra fazer seleção de variáveis? Pegar, por exemplo, as top 10 mais importantes e descartar o resto?\nOu até mesmo arriscou uma interpretação e concluiu sobre a ordem das variáveis mais importantes?\nAbaixo mostraremos o porquê não devemos ignorar a questão da multicolinearidade completamente!\nUm modelo bonitinho Primeiro vamos ajustar um modelo bonitinho, livre de multicolinearidade. Suponha que queiramos prever Petal.Length utilizando as medidas das sépalas (Sepal.Width e Sepal.Length) da nossa boa e velha base iris.\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.4 ✓ dplyr 1.0.2 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() iris2 \u0026lt;- iris %\u0026gt;% select(Sepal.Length, Sepal.Width, Petal.Length) iris2 %\u0026gt;% cor %\u0026gt;% corrplot::corrplot() O gráfico acima mostra que as variáveis explicativas não são fortemente correlacionadas. Ajustando uma random fores, temos a seguinte ordem de importância das variáveis:\nlibrary(randomForest) iris2_rf \u0026lt;- randomForest(Petal.Length ~ ., data = iris2) varImpPlot(iris2_rf) Sem surpresas. Agora vamos para o problema!\nUm modelo com feinho Vamos forjar uma situação extrema em que muitas variáveis sejam multicolineares. Vou fazer isso repetindo a coluna Sepal.Length várias vezes.\niris3 \u0026lt;- accumulate(1:20, ~{ .x[[paste0(\u0026quot;Sepal.Length\u0026quot;, .y)]] \u0026lt;- iris2$Sepal.Length .x }, .init = iris2) iris3[[20]] %\u0026gt;% cor %\u0026gt;% corrplot::corrplot(order = \u0026quot;alphabet\u0026quot;) Agora a coisa tá feia! Temos 20 variáveis perfeitamente colineares. Mesmo assim um random forest nessa nova base não perderia poder preditivo.\nMas como ficou a importância das variáveis?\niris3_rf \u0026lt;- randomForest(Petal.Length ~ ., data = iris3[[20]]) varImpPlot(iris3_rf) Aqui o jogo já se inverteu: concluiríamos que Sepal.Width é mais importante de todas as variáveis!\nSeleção de variáveis furado O gráfico abaixo mostra que quanto mais variáveis correlacionadas tivermos, menor a importância de TODAS ELAS SIMULTANEAMENTE! É como se as variáveis colineares repartissem a importância entre elas.\n# ajusta random forest para bases com 1 a 20 repetições de `Sepal.Length` rfs \u0026lt;- map(iris3, ~ randomForest(Petal.Length ~ ., data = .x) %\u0026gt;% importance) # extrai as importâncias das repetições de `Sepal.Length` importancia \u0026lt;- map_dfr(rfs, ~{ .x %\u0026gt;% as.data.frame() %\u0026gt;% tibble::rownames_to_column() %\u0026gt;% dplyr::filter(stringr::str_detect(rowname, \u0026quot;^Sepal.Length\u0026quot;)) }, .id = \u0026quot;n_repeticoes\u0026quot;) %\u0026gt;% mutate(n_repeticoes = as.numeric(n_repeticoes)) # Gráfico do número de variáveis multicolineares vs importância importancia %\u0026gt;% ggplot(aes(x = n_repeticoes, y = IncNodePurity)) + geom_point() + geom_hline(yintercept = 40, size = 1, linetype = \u0026quot;dashed\u0026quot;, colour = \u0026quot;red\u0026quot;) + labs(x = \u0026quot;Qtd de repetições da coluna `Sepal.Length`\u0026quot;, y = \u0026quot;Importância\u0026quot;, title = \u0026quot;Gráfico da relação entre o número de variáveis multicolineares vs importância\u0026quot;) Na prática, se estabelecessemos um corte no valor de importância pra descartar variáveis (como ilustrado pela linha vermelha), teríamos um problema em potencial: poderíamos estar jogando fora informação muito importante.\nComo tratar multicolinearidade, então? Algumas maneiras de lidar com multicolinearidade são:\nObservar a matriz de correlação VIF Recursive feature elimination Conclusão Cuidado ao jogar tudo no caldeirão! Devemos sempre nos preocupar com multicolinearidade, mesmo ajustando modelos baseados em árvores.\nAbs!\n","permalink":"https://blog.curso-r.com/posts/2018-05-22-arvore-e-multicolinearidade/","tags":["machine learning","random forest","multicolinearidade"],"title":"Modelos beseados em árvores são imunes à multicolinearidade?"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Durante nosso último curso de introdução a programação em R, o Reinaldo Chaves me pediu ajuda para carregar os dados de CNPJs da Receita Federal. Eu me animei tanto com essa base que decidi montar um pacotinho para baixar, ler e organizar esses dados.\nPacote O pacote rfbCNPJ baixa e lê os arquivos contendo a lista de todas as empresas do Brasil, disponibilizado pela Receita Federal em 15 de dezembro de 2017.\nSão duas tabelas, separadas por UF: i) empresas, contendo informações de CNPJ e nome da empresa e ii) socios, contendo quadro de sócios. Essa base de dados é atualizada pela própria RFB e eu não faço ideia de quando será atualizada novamente.\nComo os arquivos são do tipo fixed width, algumas pessoas podem ter dificuldade para ler e empilhar os arquivos no R. Esse pacote facilita as operações de download e leitura.\nInstalação Você pode instalar o pacote rfbCNPJ do CRAN rodando\n# ainda não foi aprovado no CRAN... install.packages(\u0026quot;rfbCNPJ\u0026quot;) Você também pode baixar diretamente do GitHub rodando:\n# install.packages(\u0026quot;devtools\u0026quot;) devtools::install_github(\u0026quot;jtrecenti/rfbCNPJ\u0026quot;) Download de arquivos brutos Você pode baixar o arquivo .txt bruto de cada UF usando o comando rfb_download(). Por padrão, temos ufs = NULL, que baixará os arquivos de todas as UFs. Esses arquivos somam aproximadamente 4.8 GB em disco.\nrfb_download(ufs = c(\u0026quot;AC\u0026quot;, \u0026quot;RR\u0026quot;), path = \u0026quot;caminho/da/pasta\u0026quot;) Parse A partir de uma pasta contendo os arquivos txt, você pode carregar as bases de dados rodando rfb_read() com os caminhos dos arquivos ou rfb_read_dir() diretamente para ler todos os arquivos da pasta. Certifique-se de que a pasta que contém os arquivos a serem lidos contém apenas os arquivos baixados em .txt.\npath \u0026lt;- \u0026quot;caminho/da/pasta\u0026quot; all_files \u0026lt;- fs::dir_ls(path) dados \u0026lt;- rfb_read(all_files) dados \u0026lt;- rfb_read_dir(path) # equivalente Carregando dados Os dados são carregados numa tabela complexa com duas list-columns.\nlibrary(tibble) print(dados, n = 27) # A tibble: 27 x 3 file empresa socio \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; 1 D71214AC.txt \u0026lt;tibble [15,690 × 3]\u0026gt; \u0026lt;tibble [26,268 × 6]\u0026gt; 2 D71214AL.txt \u0026lt;tibble [60,067 × 3]\u0026gt; \u0026lt;tibble [109,762 × 6]\u0026gt; 3 D71214AM.txt \u0026lt;tibble [64,306 × 3]\u0026gt; \u0026lt;tibble [121,095 × 6]\u0026gt; 4 D71214AP.txt \u0026lt;tibble [15,941 × 3]\u0026gt; \u0026lt;tibble [28,063 × 6]\u0026gt; 5 D71214BA.txt \u0026lt;tibble [422,396 × 3]\u0026gt; \u0026lt;tibble [787,637 × 6]\u0026gt; 6 D71214CE.txt \u0026lt;tibble [193,654 × 3]\u0026gt; \u0026lt;tibble [352,841 × 6]\u0026gt; 7 D71214DF.txt \u0026lt;tibble [194,734 × 3]\u0026gt; \u0026lt;tibble [368,607 × 6]\u0026gt; 8 D71214ES.txt \u0026lt;tibble [179,150 × 3]\u0026gt; \u0026lt;tibble [354,358 × 6]\u0026gt; 9 D71214GO.txt \u0026lt;tibble [328,524 × 3]\u0026gt; \u0026lt;tibble [619,810 × 6]\u0026gt; 10 D71214MA.txt \u0026lt;tibble [123,736 × 3]\u0026gt; \u0026lt;tibble [201,854 × 6]\u0026gt; 11 D71214MG.txt \u0026lt;tibble [962,930 × 3]\u0026gt; \u0026lt;tibble [1,916,405 × 6]\u0026gt; 12 D71214MS.txt \u0026lt;tibble [102,208 × 3]\u0026gt; \u0026lt;tibble [189,673 × 6]\u0026gt; 13 D71214MT.txt \u0026lt;tibble [141,464 × 3]\u0026gt; \u0026lt;tibble [262,358 × 6]\u0026gt; 14 D71214PA.txt \u0026lt;tibble [159,079 × 3]\u0026gt; \u0026lt;tibble [274,004 × 6]\u0026gt; 15 D71214PB.txt \u0026lt;tibble [79,275 × 3]\u0026gt; \u0026lt;tibble [138,596 × 6]\u0026gt; 16 D71214PE.txt \u0026lt;tibble [224,184 × 3]\u0026gt; \u0026lt;tibble [426,520 × 6]\u0026gt; 17 D71214PI.txt \u0026lt;tibble [61,627 × 3]\u0026gt; \u0026lt;tibble [105,008 × 6]\u0026gt; 18 D71214PR.txt \u0026lt;tibble [708,109 × 3]\u0026gt; \u0026lt;tibble [1,392,658 × 6]\u0026gt; 19 D71214RJ.txt \u0026lt;tibble [843,040 × 3]\u0026gt; \u0026lt;tibble [1,708,931 × 6]\u0026gt; 20 D71214RN.txt \u0026lt;tibble [80,562 × 3]\u0026gt; \u0026lt;tibble [150,411 × 6]\u0026gt; 21 D71214RO.txt \u0026lt;tibble [62,385 × 3]\u0026gt; \u0026lt;tibble [109,774 × 6]\u0026gt; 22 D71214RR.txt \u0026lt;tibble [11,908 × 3]\u0026gt; \u0026lt;tibble [21,737 × 6]\u0026gt; 23 D71214RS.txt \u0026lt;tibble [670,093 × 3]\u0026gt; \u0026lt;tibble [1,350,159 × 6]\u0026gt; 24 D71214SC.txt \u0026lt;tibble [498,511 × 3]\u0026gt; \u0026lt;tibble [974,351 × 6]\u0026gt; 25 D71214SE.txt \u0026lt;tibble [63,303 × 3]\u0026gt; \u0026lt;tibble [114,081 × 6]\u0026gt; 26 D71214SP.txt \u0026lt;tibble [2,730,412 × 3]\u0026gt; \u0026lt;tibble [5,585,988 × 6]\u0026gt; 27 D71214TO.txt \u0026lt;tibble [51,629 × 3]\u0026gt; \u0026lt;tibble [89,911 × 6]\u0026gt; A primeira coluna complexa mostra dados das empresas, e a segunda mostra dados dos sócios. Para carregar uma dessas listas, use tidyr::unnest(), como veremos abaixo.\nEmpresas:\nlibrary(magrittr) empresas \u0026lt;- dados %\u0026gt;% dplyr::select(file, empresa) %\u0026gt;% tidyr::unnest(empresa) empresas # A tibble: 9,048,917 x 4 file tipo cnpj nome_empresarial \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; 1 D71214AC.txt 01 07398403000180 BOI GORDO AGROPECUARIA COMERCIO E R… 2 D71214AC.txt 01 03173169000131 CONSELHO ESCOLAR BOM JESUS 3 D71214AC.txt 01 07399184000153 D \u0026amp; A SOLUCOES INFORMATICA LTDA - ME 4 D71214AC.txt 01 07399188000131 SOCIEDADE AGRICOLA POERINHA 5 D71214AC.txt 01 03300047000169 ASSOCIACAO MAO AMIGA DE PRODUTORES … 6 D71214AC.txt 01 04940648000107 ASSOCIACAO DOS PRODUTORES RURAIS E … 7 D71214AC.txt 01 04940653000101 CONSELHO ESCOLAR POLO HORTIGRANJEIRO 8 D71214AC.txt 01 04940654000156 CONSELHO ESCOLAR CENTRO EDUCACIONAL… 9 D71214AC.txt 01 03301098000105 ASSOCIACAO AGROEXTRATIVISTA SANTOS … 10 D71214AC.txt 01 01653480000152 DENEVS - TERCEIRIZACAO LTDA # ... with 9,048,907 more rows Sócios:\nsocios \u0026lt;- dados %\u0026gt;% dplyr::select(file, socio) %\u0026gt;% tidyr::unnest(socio) socios # A tibble: 17,780,860 x 7 file tipo cnpj indicador_cpf_c… cpf_cnpj_socio qualificacao nome \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; 1 D7121… 02 0739… 2 NA 49 SELMA… 2 D7121… 02 0739… 2 NA 22 MARCE… 3 D7121… 02 0317… 2 NA 16 MARIA… 4 D7121… 02 0739… 2 NA 49 DILSO… 5 D7121… 02 0739… 2 NA 22 ANGEL… 6 D7121… 02 0739… 2 NA 16 RAIMU… 7 D7121… 02 0330… 2 NA 16 MOISE… 8 D7121… 02 0494… 2 NA 16 RAIMU… 9 D7121… 02 0494… 2 NA 16 MARIA… 10 D7121… 02 0494… 2 NA 16 EUCLI… # ... with 17,780,850 more rows A descrição das qualificações pode ser encontrada na base qualificacao, disponível dentro do pacote:\ndata(qualificacao, package = \u0026quot;rfbCNPJ\u0026quot;) qualificacao # A tibble: 40 x 2 qualificacao qualificacao_nm \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; 1 05 Administrador 2 08 Conselheiro de Administração 3 10 Diretor 4 16 Presidente 5 17 Procurador 6 20 Sociedade Consorciada 7 21 Sociedade Filiada 8 22 Sócio 9 23 Sócio Capitalista 10 24 Sócio Comanditado # ... with 30 more rows Para juntar com a base de dados dos sócios, basta dar dplyr::inner_join()!\nDownload de arquivos binários Você pode carregar os arquivos binários (em .rds) diretamente da web usando a função rfb_import(). Essa função baixa os arquivos binários diretamente do Dropbox. Você pode baixar usando o parâmetro type=, com as opções “all” (tibble complexa com list-columns), “empresas” (tibble retangular) e “socios” (tibble retangular).\nempresas \u0026lt;- rfb_import(\u0026quot;empresas\u0026quot;, path = \u0026quot;.\u0026quot;) Você também pode baixar os dados dos arquivos binários em .rds desses links com arquivos armazenados diretamente no dropbox:\nrfb (list-column) empresas socios Para ler um desses arquivos, basta rodar\ndados \u0026lt;- readRDS(\u0026quot;caminho/para/dados.rds\u0026quot;) Análises Vamos brincar um pouco com esses dados?\nEmpresas por UF Minha hipótese inicial era que o estado com maior quantidade de CNPJs por pessoa era SP. Mas isso não é verdade! Os estados do Sul estão dominando, junto com DF. São Paulo aparece apenas na quarta posição.\n# você pode baixar o `abjData` do GitHub rodando # devtools::install_github(\u0026quot;abjur/abjData\u0026quot;) # relação de códigos de UF e UFs cods \u0026lt;- abjData::cadmun %\u0026gt;% dplyr::select(uf_sigla = uf, uf_cod = UFCOD) %\u0026gt;% dplyr::distinct() # População de cada UF pop \u0026lt;- abjData::pnud_uf %\u0026gt;% dplyr::filter(ano == 2010) %\u0026gt;% dplyr::inner_join(cods, c(\u0026quot;uf\u0026quot; = \u0026quot;uf_cod\u0026quot;)) %\u0026gt;% dplyr::select(uf_sigla, popt) # fazendo o join das bases! empresas %\u0026gt;% dplyr::mutate(uf_sigla = stringr::str_extract( file, \u0026quot;([A-Z]{2})(?=\\\\.txt)\u0026quot;)) %\u0026gt;% dplyr::inner_join(pop, \u0026quot;uf_sigla\u0026quot;) %\u0026gt;% # calculando a razão entre população e quantidade de empresas dplyr::group_by(uf_sigla) %\u0026gt;% dplyr::summarise(n = n(), pop = dplyr::first(popt), prop = n / pop) %\u0026gt;% dplyr::arrange(dplyr::desc(prop)) %\u0026gt;% dplyr::mutate(prop = scales::percent(prop)) %\u0026gt;% knitr::kable() UF Qtd. Empresas População Razão(%) SC 498511 6199948 8.04% DF 194734 2541714 7.66% PR 708109 10348243 6.84% SP 2730412 40915382 6.67% RS 670093 10593366 6.33% GO 328524 5934767 5.54% RJ 843040 15871448 5.31% ES 179150 3477472 5.15% MG 962930 19383604 4.97% MT 141464 2961977 4.78% MS 102208 2404628 4.25% RO 62385 1515025 4.12% TO 51629 1349770 3.83% SE 63303 2038459 3.11% BA 422396 13755200 3.07% RR 11908 421159 2.83% PE 224184 8646409 2.59% RN 80562 3127820 2.58% AP 15941 652768 2.44% CE 193654 8317604 2.33% AC 15690 690774 2.27% PA 159079 7247979 2.19% PB 79275 3706991 2.14% PI 61627 3050838 2.02% AL 60067 3045851 1.97% MA 123736 6317987 1.96% AM 64306 3301221 1.95% Quantidade de sócios Qual a distribuição de quantidade de sócios?\nPara responder à essa pergunta, considerei apenas sócios pessoa física. Não fiz filtros de qualificação. Você tem alguma ideia de como poderíamos selecionar as qualificações corretas sobre a base de qualificacao?\nQtd. Sócios Empresas Proporção 2 5911971 65.4% 1 2265100 25.0% 3 551072 6.1% 4 174558 1.9% [5, 15] 134898 1.5% [16, 584] 5714 0.1% Dois terços das empresas são formadas por dois sócios e 90% das empresas são formadas por um ou dois sócios.\nFiquei interessado em saber quais empresas estão na última faixa de quantidade de sócios. A tabela abaixo mostra as 10 empresas com mais sócios nessa base. Além da EY, não conheço nenhuma das empresas abaixo. O que dá para notar é que boa parte delas está ligada ao ramo médico.\nnome_empresarial n COMED - CORPO MEDICO LTDA 584 QUIRON PRONTO SOCORRO LTDA - ME 481 SIMEA - SOCIEDADE INTEGRADA MEDICA DO AMAZONAS LTDA 391 NOVA FAMILIA LTDA - EPP 371 SOCIEDADE BAURUENSE DE PRESTACAO DE SERVICOS MEDICOS SOBAME 347 ERNST \u0026amp; YOUNG ASSESSORIA EMPRESARIAL LTDA 332 UNIAO CONSULTORIA LTDA 285 INSTITUTO MEDICO DE CLINICA E PEDIATRIA DO ESTADO DO AMAZONAS S/S LTDA 284 UROLOGISTAS DO ESTADO DO RIO DE JANEIRO LTDA 281 SNC \u0026amp; CIA ADMINISTRACAO DE BENS 277 Pessoas com mais empresas Quem são os sócios com maior número de empresas? Temos abaixo uma lista dos 20 mais. Claramente, muitos desses nomes são homônimos. O primeiro nome da lista que me parece pouco provável de ter homônimos é o Renato Gamba Rocha Diniz. Também fiquei interessado nesse “Não Consta na Base”…\nnome n_empresas JOSE CARLOS DA SILVA 2084 JOAO BATISTA DA SILVA 1518 LUIZ CARLOS DA SILVA 1488 MARIA APARECIDA DA SILVA 1487 ANTONIO CARLOS DA SILVA 1372 JOSE CARLOS DOS SANTOS 1339 MARIA JOSE DA SILVA 1119 CARLOS ALBERTO DA SILVA 1091 JOSE CARLOS DE OLIVEIRA 1055 MARCOS ANTONIO DA SILVA 1038 NAO CONSTA NA BASE 1035 JOSE ANTONIO DA SILVA 1017 RENATO GAMBA ROCHA DINIZ 946 JOSE ROBERTO DA SILVA 907 LUIZ CARLOS DOS SANTOS 882 PAULO ROBERTO DA SILVA 839 JOSE PEREIRA DA SILVA 804 JOAO BATISTA DE OLIVEIRA 770 RENATA ROSSI CUPPOLONI RODRIGUES 766 ANTONIO CARLOS DOS SANTOS 762 Sexo dos sócios Agora vamos usar o pacote genderBR para pegar o sexo mais provável de todas as pessoas. No total, temos 11.388.120 nomes únicos na base (isso subestima a quantidade de pessoas, por conta de homônimos).\nPara essa análise, tivemos de retirar 392.533 nomes em que o genderBR não foi capaz de estimar o sexo. Isso corresponde a 2,2% da base. Creio que não seja absurdo afirmar que os valores omitidos influenciam no resultado.\nNessa análise também não desconsiderei pessoas que fazem parte de mais de uma empresa. Ou seja, se uma pessoa tem 2 ou mais empresas, ela é contada 2 ou mais vezes.\nMinha hipótese inicial era a de que teríamos mais homens como sócios de empresas, por razões históricas/culturais (e queremos mudar isso!). Aparentemente a hipótese foi confirmada: 3/5 dos sócios são homens. Sugiro refinar essa análise pela qualificação e pela repetição de pessoas em empresas distintas.\nsexo_socios \u0026lt;- socios %\u0026gt;% # apenas pessoas físicas dplyr::filter(indicador_cpf_cnpj == \u0026quot;2\u0026quot;) %\u0026gt;% # extrai primeiro nome dplyr::mutate(primeiro_nome = stringr::str_extract(nome, \u0026quot;[^ ]+\u0026quot;)) %\u0026gt;% # conta primeiro nome dplyr::count(primeiro_nome) %\u0026gt;% # estima a probabilidade de ser do sexo feminino dplyr::mutate(sexo = genderBR::get_gender(primeiro_nome, prob = TRUE)) %\u0026gt;% # retira estimações NA (cuidado!) dplyr::filter(!is.na(sexo)) library(ggplot2) grafico \u0026lt;- sexo_socios %\u0026gt;% # É feminino se sexo \u0026gt; 0.5 (mais de 50% feminino observado no IBGE) dplyr::mutate(sexo = dplyr::if_else(sexo \u0026gt; .5, \u0026quot;Fem\u0026quot;, \u0026quot;Masc\u0026quot;)) %\u0026gt;% dplyr::group_by(sexo) %\u0026gt;% dplyr::summarise(total = sum(n)) %\u0026gt;% dplyr::arrange(dplyr::desc(total)) %\u0026gt;% # adiciona proporção em % dplyr::mutate(prop = scales::percent(total / sum(total))) %\u0026gt;% # aqui começa o gráfico ggplot(aes(x = sexo, y = total/1000, fill = sexo)) + geom_col() + # daqui pra baixo é só estética coord_flip() + geom_text(aes(label = prop), size = 6, hjust = 1, vjust = -1) + theme_minimal(16) + ggtitle(\u0026quot;Quantidade de empresas por sexo\u0026quot;) + labs(x = \u0026quot;Sexo\u0026quot;, y = \u0026quot;Total (milhares)\u0026quot;) grafico Análise Extra O Sillas sugeriu (e executou!) uma análise adicional. A ideia era montar um grafo das empresas que compartilham sócios da Comissão de Valores Mobiliários (CVM). A CVM guarda informações de todas as empresas de capital aberto do Brasil. Uma investigação interessante é estudar se os sócios fazem parte de várias empresas. A visualização adequada para isso é um grafo das companhias, ligados pela presença de sócios com o mesmo nome.\nBaixando dados da CVM É bem tranquilo baixar as informações das empresas cadastradas na CVM:\n# download do arquivo \\ipado link \u0026lt;- \u0026quot;http://sistemas.cvm.gov.br/cadastro/SPW_CIA_ABERTA.ZIP\u0026quot; file_name \u0026lt;- \u0026quot;~/Downloads/companhias.zip\u0026quot; httr::GET(link, httr::write_disk(file_name)) # dezipando arquivo unzip(\u0026quot;companhias.zip\u0026quot;, exdir = \u0026quot;.\u0026quot;) cvm \u0026lt;- readr::read_delim( file = \u0026quot;SPW_CIA_ABERTA.txt\u0026quot;, delim = \u0026quot;\\t\u0026quot;, col_types = stringr::str_dup(\u0026quot;c\u0026quot;, 42), locale = readr::locale(encoding = \u0026quot;latin1\u0026quot;) ) %\u0026gt;% # limpa os nomes zoados janitor::clean_names() # pega apenas empresas da CVM socios_cvm \u0026lt;- socios %\u0026gt;% dplyr::inner_join(cvm, \u0026quot;cnpj\u0026quot;) %\u0026gt;% dplyr::filter(indicador_cpf_cnpj == \u0026quot;2\u0026quot;) %\u0026gt;% dplyr::distinct(cnpj, nome, .keep_all = TRUE) Montando o grafo Cada nó de nosso grafo é uma empresa Uma aresta existe entre dois nós se as empresas têm pelo menos uma pessoa em comum no quadro de sócios. Para fazer o grafo, eu fiz uma pequena alteração no código que o Sillas me mandou, para usar o visNetwork (que é dinâmico) no lugar do ggraph (que é lindo, porém estático).\n# pequena limpeza na base empresas \u0026lt;- socios_cvm %\u0026gt;% dplyr::select(empresa = denom_comerc, pessoa = nome) %\u0026gt;% dplyr::mutate(empresa = abjutils::rm_accent(empresa)) %\u0026gt;% dplyr::distinct(empresa, pessoa) # arestas do grafo edges \u0026lt;- dplyr::inner_join(empresas, empresas, by = \u0026quot;pessoa\u0026quot;) %\u0026gt;% dplyr::filter(empresa.x != empresa.y) %\u0026gt;% dplyr::distinct(empresa.x, empresa.y) %\u0026gt;% dplyr::rowwise() %\u0026gt;% # concatena as empresas de forma ordenada dplyr::mutate(concat = paste(sort(c(empresa.x, empresa.y)), collapse = \u0026quot;_\u0026quot;)) %\u0026gt;% dplyr::ungroup() %\u0026gt;% # tira arestas duplicadas dplyr::distinct(concat, .keep_all = TRUE) %\u0026gt;% dplyr::select(from = empresa.x, to = empresa.y) # nós do grafo nodes \u0026lt;- empresas %\u0026gt;% dplyr::distinct(empresa, .keep_all = TRUE) %\u0026gt;% dplyr::transmute(id = empresa, label = empresa) %\u0026gt;% # apenas empresas com alguma conexão dplyr::filter(label %in% c(edges$from, edges$to)) # guarda numa lista g \u0026lt;- list(nodes = nodes, edges = edges) library(visNetwork) v \u0026lt;- visNetwork::visNetwork(g$nodes, g$edges) v \u0026lt;- visNetwork::visOptions(v, highlightNearest = TRUE, nodesIdSelection = TRUE) v Use seu mouse para navegar no grafo e descobrir os principais clusters de empresas!\nWrap-up O pacote rfbCNPJ serve para organizar os dados de CNPJ da Receita Federal. Use a função rfb_import() para ler os binários prontinhos para leitura diretamente do dropbox. Além de descobrir todos os 9.048.917 CNPJs e 11.388.120 nomes únicos de pessoas, podemos fazer análises exploratórias com esses dados. Links e agradecimentos Valeu, amigos da curso-r.com e agregados, que me orientaram a criar a função rfb_import(). Veja também o Brasil.io do Turicas, que tem os mesmos arquivos para download. Ele me informou também que a base dele é mais confiável, pois apresenta correções de dados incorretos da Receita. Acompanhe o blog do Sillas que é muito mara! É isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2018-05-13-rfbcnpj/","tags":["banco de dados","pacotes","rfb"],"title":"rfbCNPJ: Repositório de CNPJs da Receita"},{"author":["Bruna Wundervald"],"categories":["análises"],"contents":" Olá! Tudo bem? Eu sou a Bruna, e talvez você já me conheça de algum grupo de R no Telegram ou Facebook, ou mesmo pelo meu outro pacote, o\nvagalumeR. Eu estou prestes a me formar em Estatística na UFPR, e esse post vai ter muito a ver com o tema do meu trabalho de conclusão de curso, que está sendo feito sob orientação do Professor Walmes Zeviani.\nRecentemente, eu fiz a primeira versão do pacote do meu TCC, o chorrrds. Ainda tem muito a ser feito nele, mas os primeiros resultados já são bem legais. O pacote chorrrds pode ser utilizado para a obtenção acordes de músicas, através da raspagem do site CifraClub. Junto com o pacote já vêm diversos bancos de dados relativos à música brasileira. Ele já está disponível no CRAN, mas as próximas atualizações devem sair em primeira mão nesse repositório: https://github.com/brunaw/chorrrds\n# Instalação devtools::install_github(\u0026quot;brunaw/chorrrds\u0026quot;) # ou install.packages(\u0026quot;chorrrds\u0026quot;) # do CRAN data(package = \u0026quot;chorrrds\u0026quot;) A base de dados chamada all, presente no pacote, contém dados referentes a 106 artistas nacionais, dos genêros: “rock”, “pop”, “sertanejo”, “MPB”, “bossa nova”, “forró”, “reggae” e “samba”. Ela já contém, além das variáveis extraídas com o pacote, as datas de lançamento e a popularidade das músicas, obtidas através da API do Spotify. Os detalhes sobre a seleção dos artistas e combinação dos dados com os do Spotify serão omitidos por enquanto, mas quem quiser falar sobre isso comigo, pode ficar a vontade :)\nEntão vamos lá. Eu vou começar acertando alguns pontos sobre os dados, já que eles não estão perfeitos, como:\nDeixar apenas os anos de lançamento das músicas, e não a data completa; Encontrar as formas mais simples dos acordes (sem acidentes ou extensões); Conectar a base original com a dos genêros dos artistas; Consertar enarmonias, ou seja, transformar as diferentes versões de um acorde com as mesmas notas em uma coisa só (por exemplo, Gb passa a ser F#, já que na prática eles são iguais). library(tidyverse) # Base de gêneros genre \u0026lt;- chorrrds::genre da \u0026lt;- chorrrds::all %\u0026gt;% dplyr::mutate(date = stringr::str_extract(date, pattern = \u0026quot;[0-9]{4,}\u0026quot;)) %\u0026gt;% # Extrai apenas os anos dplyr::mutate(date = as.numeric(date), # Deixa as datas como valores numéricos acorde = stringr::str_extract(chord, # Extrai as partes fundamentais dos pattern = \u0026quot;^([A-G]#?b?)\u0026quot;)) %\u0026gt;% # acordes dplyr::filter(date \u0026gt; 1900) %\u0026gt;% # Mantém apenas os anos que fazem sentido dplyr::left_join(genre, by = \u0026quot;artist\u0026quot;) %\u0026gt;% # Traz os gêneros dos artistas dplyr::mutate(acorde = case_when( # Contribuição do Julio acorde == \u0026quot;Gb\u0026quot; ~ \u0026quot;F#\u0026quot;, acorde == \u0026quot;C#\u0026quot; ~ \u0026quot;Db\u0026quot;, acorde == \u0026quot;G#\u0026quot; ~ \u0026quot;Ab\u0026quot;, acorde == \u0026quot;A#\u0026quot; ~ \u0026quot;Bb\u0026quot;, acorde == \u0026quot;D#\u0026quot; ~ \u0026quot;Eb\u0026quot;, acorde == \u0026quot;E#\u0026quot; ~ \u0026quot;F\u0026quot;, acorde == \u0026quot;B#\u0026quot; ~ \u0026quot;C\u0026quot;, TRUE ~ acorde)) # Conversão de enarmonias head(da) date music popul chord key artist acorde genre 1992 adriana calcanhotto a fabrica do poema 51 Cm D# adriana calcanhotto C MPB 1992 adriana calcanhotto a fabrica do poema 51 Ab D# adriana calcanhotto Ab MPB 1992 adriana calcanhotto a fabrica do poema 51 Db7 D# adriana calcanhotto Db MPB 1992 adriana calcanhotto a fabrica do poema 51 Db7/9 D# adriana calcanhotto Db MPB 1992 adriana calcanhotto a fabrica do poema 51 Cm D# adriana calcanhotto C MPB 1992 adriana calcanhotto a fabrica do poema 51 Fm D# adriana calcanhotto F MPB A base está no formato longo, ou seja, temos uma linha para cada acorde da música, mantendo a sequência na qual eles aparecem no site.\nMuito se fala sobre o quanto as músicas no Brasil andam ficando mais “simples”, ou que alguns genêros musicais são mais ricos que outros. Com os dados que temos, será que é possível concluir algo sobre isso olhando simplesmente para a quantidade média de acordes por música ao longo dos anos? Vejamos o gráfico a seguir.\nda_g \u0026lt;- da %\u0026gt;% # 2018 ainda não é um ano completo dplyr::mutate(date \u0026lt; 2018) %\u0026gt;% # Agrupamento por data + acorde + musica dplyr::group_by(date, genre, music, chord) %\u0026gt;% # Mantém os acordes distintos/ano dplyr::summarise(distintos = n_distinct(chord)) %\u0026gt;% dplyr::summarise(cont = n()) %\u0026gt;% # Média de acordes distintos nas músicas/ano dplyr::summarise(media = mean(cont), contagem = n()) # grafico p \u0026lt;- da_g %\u0026gt;% ggplot(aes(x = date, y = media)) + geom_point(colour = \u0026quot;skyblue3\u0026quot;) + facet_wrap(\u0026quot;genre\u0026quot;) + scale_fill_hue(c = 55, l = 75) + geom_smooth(aes(group = genre), span = 0.65, colour = \u0026quot;white\u0026quot;, fill = \u0026quot;tan\u0026quot;, method = \u0026quot;loess\u0026quot;) + labs(x = \u0026quot;Anos\u0026quot;, y = \u0026quot;Média de acordes/música\u0026quot;) # tema para deixar o gráfico bonitinho tema \u0026lt;- theme( legend.position=\u0026#39;none\u0026#39;, axis.line = element_line(size = 0.5, colour = \u0026quot;tan\u0026quot;), panel.grid.major = element_line( colour = \u0026quot;black\u0026quot;, size = 0.08, linetype = \u0026quot;dotted\u0026quot;), panel.border = element_blank(), panel.background = element_blank(), strip.background = element_rect(colour = \u0026quot;tan\u0026quot;, fill = \u0026quot;white\u0026quot;, size = 0.6), strip.text = element_text(size = 14), axis.title = element_text(size = 14), axis.text = element_text(size = 12)) p + tema Claramente, gêneros como o samba, MPB e bossa nova têm, em geral, uma quantidade média de acordes distintos por música muito maior do que os outros. E eles têm mais variação ao longo dos anos também, o que pode ser um reflexo da maior criatividade envolvida nestes genêros. Os menores valores, como é esperado, estão principalmente no sertanejo, que é um genêro conhecidamente mais uniforme na questão harmônica.\nNós podemos avançar um pouco e olhar para os próprios artistas. Quem será que usa as maiores quantidades de acordes distintos em suas composições? Vamos ver o próximo gráfico, que mostra apenas os artistas com mediana maior do que 8 acordes diferentes por música.\nda_g \u0026lt;- da %\u0026gt;% dplyr::mutate(artist = stringr::str_to_title(artist)) %\u0026gt;% # Agrupamento por artista + música dplyr::group_by(artist, genre, music) %\u0026gt;% # Mantém os acordes distintos dplyr::summarise(distintos = n_distinct(chord)) %\u0026gt;% # Obtém a mediana e quantis de acordes distintos por música/artistas dplyr::summarise(med = median(distintos), contagem = n(), inf = quantile(distintos)[2], sup = quantile(distintos)[4]) # grafico p \u0026lt;- da_g %\u0026gt;% dplyr::filter(med \u0026gt; 8) %\u0026gt;% ggplot(aes(x = reorder(artist, med), y = med)) + geom_pointrange(aes(ymin = inf, ymax = sup, colour = genre), size = 0.7) + scale_colour_hue(c = 55, l = 75) + coord_flip() + labs(colour = \u0026quot;Gênero\u0026quot;, x = \u0026quot;Artistas\u0026quot;, y = \u0026quot;Primeiro quartil, mediana e terceiro quartil\u0026quot;) # tema tema \u0026lt;- theme( axis.line = element_line(size = 0.5, colour = \u0026quot;tan\u0026quot;), panel.grid.major = element_line(colour = \u0026quot;black\u0026quot;, size = 0.08, linetype = \u0026quot;dotted\u0026quot;), panel.border = element_blank(), panel.background = element_blank(), axis.text = element_text(size = 12), axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16)) p + tema E voilá. A primeira posição é ocupada por um dos maiores musicistas brasileiros, que é referência internacional em questões de harmonia: Chico Buarque. Logo em seguida, temos o Reinaldo, um dos maiores sambistas que o Brasil já conheceu, e o Pixinguinha, um gênio do choro. E assim a lista segue, com artistas principalmente da bossa nova, samba e MPB. Demora até que apareça o primeiro dos rockeiros nesta lista, que é a Rita Lee. Dos membros do sertanejo, nenhum chega a aparecer no gráfico, mostrando que a “variedade” harmônica deste gênero musical é mesmo bem baixa.\nAté agora está legal mas, com música, sempre pode ficar ainda mais. A ideia mais recente que o meu caro orientador Walmes Zeviani teve para o nosso trabalho é de encadear as transições entre os acordes em um diagrama de cordas. Vocês já ouviram falar desse diagrama? Eu mal o conheço e já considero pacas.\nO diagrama de cordas é um método gráfico (e lindo) de explicitar relações entre grupos ou indivíduos. Os grupos ficam arranjados de forma radial/circular, e as cordas que aparecem dentro do círculo demonstram as conexões entre eles e suas forças. Pra quem sabe um pouquinho sobre harmonia, vai ser sensacional ver o quanto isso faz sentido (mas também não vou entrar nesse mérito agora).\nO exemplo que eu vou mostrar aqui é extremamente simples. Antes eu separei, dos acordes “crus” retirados do CifraClub, apenas a parte fundamental deles. Isto é, desconsiderei se um acorde tem notas extras, acidentes e maior/menor. Ou seja, aqui nós só veremos acordes como C, D, B, A#, o miolo da coisa. Podemos perder informação fazendo isso? Sim, e bastante. Mas como eu disse, esse caso é pra ser bem simples mesmo.\nEnfim, vamos voltar ao exemplo. A seguir, eu considero como uma “transição” quando um acorde aparece em sequência do outro (exemplo de transição muito comum: dó-sol). O código abaixo constrói o diagrama de cordas através do pacote chordiag:\ndevtools::install_github(\u0026quot;mattflor/chorddiag\u0026quot;) # Ordenando por círculo das quintas ordem \u0026lt;- c(\u0026quot;G\u0026quot;, \u0026quot;D\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;E\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;F#\u0026quot;, \u0026quot;Db\u0026quot;, \u0026quot;Ab\u0026quot;, \u0026quot;Eb\u0026quot;, \u0026quot;Bb\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;C\u0026quot;) da$acorde \u0026lt;- factor(da$acorde, levels = ordem) comp \u0026lt;- data.frame( acorde = da$acorde, seq = dplyr::lead(da$acorde)) %\u0026gt;% # Pega o acorde \u0026quot;seguinte\u0026quot; dplyr::group_by(acorde, seq) %\u0026gt;% # Agrupa por cada transição dplyr::summarise(contagem = n()) # Conta quantas são as transições mat_comp \u0026lt;- reshape2::dcast(comp, # Arranja em do tipo matriz quadrada acorde ~ seq, value.var = \u0026quot;contagem\u0026quot;) mm \u0026lt;- as.matrix(mat_comp[ ,-1]) # Converte o df em matriz (exigência do pacote) mm[is.na(mm)] \u0026lt;- 0 # Substitui na por 0 (exigência do pacote) dimnames(mm) \u0026lt;- list(acorde = unique(mat_comp$acorde), seq = unique(mat_comp$acorde)) # Constrói o diagrama interativo chorddiag::chorddiag(mm, showTicks = FALSE, palette = \u0026quot;Set2\u0026quot;, palette2 = \u0026quot;#Set3\u0026quot;) Vejam que interessante. Como eu disse antes, uma das relações mais fortes do diagrama é o C-G (ou dó-sol), que é justificada teoricamente, já que o G é a quinta do C. O mesmo acontece com D-A, A-E, F-C e assim por diante. Quem quiser saber mais sobre esse comportamento, pode dar uma olhada aqui. Transições meio malucas, como B-Bb, também acontecem. “Maluca” porque um acorde bemol, indicado pelo “b”, é aquele cuja raíz esta meio tom abaixo do indicado pela letra anterior, que neste caso é o B (si), então esse acontecimento não faz muito sentido.\nConsiderações Finais O que vimos aqui é um pedaço da análise inicial do meu TCC, que ainda não está nem um pouco pronto. São exemplos simples das informações podemos extrair com o pacote chorrrds, e o universo de possibilidades é infinito. Isso que nós nem começamos a falar sobre as conexões que podem ser feitas com a API do Spotify, o pacote music21, que é do próprio Julio Trecenti, com as letras das músicas,…\nAlém disso, om certeza, meu objetivo com os gráficos apresentados não é fazer nenhum tipo de juízo de valor sobre os genêros por conta de “complexidade harmônica”. Diga-se de passagem, eu mesma sou bem fã de todos esses genêros, desde a MPB até o sertanejo :D\nAgradecimentos Ao meu orientador, Walmes Zeviani, que fez eu me apaixonar pelo R, e ao Julio Trecenti, que é tão entusiasta do meu TCC quanto eu, e já fez diversas contribuições valiosas.\n","permalink":"https://blog.curso-r.com/posts/2018-04-25-chorrrds/","tags":["música"],"title":"chorrrds: Analisando acordes de músicas brasileiras"},{"author":["Athos"],"categories":["divulgação"],"contents":" Alô alôww Comunidade\nLançamos uma competição Kaggle e agora é a hora de você mostrar que é Jedi em DATA SCIENCE! =D\nLink: https://www.kaggle.com/c/guess-the-correlation/\nA gente fez isso por esporte, favor não tentar achar utilidade nessa aplicação =P.\nO Jogo O site Guess The Correlation coloca o ser humano frente a frente com um gráfico de dispersão em que é em seguida desafiado a adivinhar a respectiva a correlação linear.\nNo nosso desafio Kaggle, desafio similar foi construído. Por exemplo: quanto você chutaria que é a correlação entre os dados da figura abaixo?\nForam geradas 200 mil imagens em png como a figura acima e cada uma delas tem a sua correleção anotada para treinarmos um modelinho.\nObjetivo O objetivo é simples e direto: construir um robô que calcula a correlação (linear) apenas vendo o gráfico de dispersão.\nEm machine lârnês, queremos\nem que essa f seja digna de ser f de F@!#.\nChute inicial O Julião já andou trabalhando nesse problema e deu um chute inicial nos códigos pra vocês se inspirarem. Aliás, “inicial” numas, porque ele já saiu fazendo um CNN com a ajuda do pacote decryptr:\nlibrary(keras) library(tidyverse) library(decryptr) path \u0026lt;- \u0026quot;.\u0026quot; arqs \u0026lt;- dir(paste0(path, \u0026quot;/train_imgs\u0026quot;), full.names = TRUE) resp \u0026lt;- readr::read_csv(paste0(path, \u0026quot;/train_responses.csv\u0026quot;)) i_train \u0026lt;- sample(1:nrow(resp), 10000) arqs_train \u0026lt;- arqs[i_train] arqs_test \u0026lt;- arqs[-i_train] gen \u0026lt;- function(batch_size = 100, arqs = arqs_train) { f \u0026lt;- function() { a \u0026lt;- sort(sample(arqs, batch_size)) imgs \u0026lt;- decryptr::read_captcha(a) captchas_t \u0026lt;- purrr::transpose(imgs) xs \u0026lt;- captchas_t$x xs \u0026lt;- abind::abind(xs, along = 0.1) a_clean \u0026lt;- a %\u0026gt;% basename() %\u0026gt;% tools::file_path_sans_ext() %\u0026gt;% tolower() corr \u0026lt;- resp %\u0026gt;% filter(id %in% a_clean) %\u0026gt;% arrange(id) %\u0026gt;% pull(corr) data \u0026lt;- list(xs, corr) data } f } # Create model model \u0026lt;- keras_model_sequential() model %\u0026gt;% layer_conv_2d( input_shape = c(150, 150, 1), filters = 16, kernel_size = c(3, 3), padding = \u0026quot;same\u0026quot;, activation = \u0026quot;relu\u0026quot;) %\u0026gt;% layer_max_pooling_2d() %\u0026gt;% layer_conv_2d( filters = 32, kernel_size = c(3, 3), padding = \u0026quot;same\u0026quot;, activation = \u0026quot;relu\u0026quot;) %\u0026gt;% layer_max_pooling_2d() %\u0026gt;% layer_conv_2d( filters = 64, kernel_size = c(3, 3), padding = \u0026quot;same\u0026quot;, activation = \u0026quot;relu\u0026quot;) %\u0026gt;% layer_max_pooling_2d() %\u0026gt;% layer_flatten() %\u0026gt;% layer_dense(units = 300) %\u0026gt;% layer_dropout(.1) %\u0026gt;% layer_dense(units = 1, activation = \u0026quot;tanh\u0026quot;) # Compile and fit model model %\u0026gt;% compile( optimizer = \u0026quot;sgd\u0026quot;, loss = \u0026quot;mean_squared_error\u0026quot;, metrics = \u0026quot;mean_squared_error\u0026quot;) model %\u0026gt;% fit_generator( gen(100, arqs_train), steps_per_epoch = 100, validation_data = gen(100, arqs_test), validation_steps = 10 ) E aí? Será que dá pra acertar 100%? Ou será impossível?\nBoa R’ada!\n","permalink":"https://blog.curso-r.com/posts/2018-03-29-guess-the-correlation/","tags":["kaggle","deep learning"],"title":"Chamada pra briga - Competição Kaggle Guess The Correlation"},{"author":["William"],"categories":["Tutoriais"],"contents":" A minha tese de doutorado envolve a análise de dados de poluição do ar. Parte do trabalho está em encontrar bases de dados que exemplifiquem temas que eu precise discutir. Informações sobre poluição do ar geralmente são disponibilizados por órgãos públicos de monitoramento ambiental, o que deveria, por princípio, garantir a fácil acesso e a coleta eficiente dos dados. Como sabemos, nem sempre é isso que acontece.\nNeste post, vamos exercitar a arte do web scraping construindo um scraper para coletar dados de poluição do ar da CETESB. O texto original está no blog que criei para divulgar resultados parciais da minha tese.\nQualar O Qualar é o sistema de informações de qualidade do ar da CETESB. Por meio dele, podemos acessar os dados de todas as estações de monitoramento do estado de São Paulo. O sistema exige a criação de um login e então o envio de um pequeno formulário com quais informações você gostaria de visualizar.\nO Qualar funciona muito bem para pequenas consultas, mas na extração de uma massa grande de dados existem duas dificuldades:\nSe você precisa de dados de várias estações e de vários poluente, você precisará repetir esse processo para cada combinação de estação/poluente. Pegar todos os dados de ozônio da Grande São Paulo, por exemplo, exigiria repetir a solicitação 23 vezes.\nComo a planilha é impressa na tela, se você precisar de uma série muito longa, você vai demorar bastante para carregar a página e seu computador corre um grande risco de travar no meio do caminho por falta de RAM.\nPara contornar este problema, vamos usar o R para construir um código que simule uma requisição de dados ao Qualar. Em seguida, vamos transformar o código numa função para replicar o processo para diversos parâmetros rodando apenas algumas linhas de códigos.\nObservação: o sistema também tem uma opção “Exportar dados Avançado”. Nela, é possível escolher até 3 parâmetros para cada estação e os dados não são impressos na tela, sendo gerado diretamente um arquivo csv para download. Porém, com a desculpa de praticar a construção do scraper, não vamos usar essa opção.\nConstruindo o scraper Para construir o scraper, vamos seguir os passos definidos neste post da Curso-R escrito pelo Fernando Corrêa. São eles:\nDefinir a página que você quer raspar. Identificar exatamente as requisições que produzem o que você quer. Construir um programa que imite as requisições que você faria manualmente. Repetir o passo (3) quantas vezes quiser. Um fluxo mais estruturado do web scraping é discutido neste post do Caio Lente.\nPASSO 1 Para chegar na página que queremos raspar, precisamos passar pelas seguintes etapas dentro do Qualar: login, pesquisa e janela com os dados. Veja abaixo como prosseguir.\nLogin: fazer o login na página inicial.\nPesquisa: na próxima página, acessar “Consultas/Exportar Dados” no menu da esquerda e preencher a pesquisa com os dados do parâmetro que você quer acessar.\nDados: na nova janela aberta pelo site estão os dados que queremos raspar.\nPASSO 2 Para descobrir qual requisição é feita em cada momento, podemos utilizar o “Inspect element” do navegador. Eu estou usando o Firefox neste post, mas o procedimento é semelhante em outros navegadores.\nO login é uma submissão de formulário. Inspecionando o html da página, podemos ver que os itens que o formulário precisa enviar são o “cetesb_login” e o “cetesb_passoword”.\nPara descobrir que tipo de requisição o login faz, basta abrir o Inspect element antes de fazer o login, logar no site e olhar a aba “Network”. A requisição que queremos é a “autenticador”. Ela faz uma requisição POST para a url “http://qualar.cetesb.sp.gov.br/qualar/autenticador”.\nNão vou mostrar aqui, mas a requisição que a pesquisa faz para acessar os dados também é a submissão de um formulário e pode ser encontrada de forma equivalente.\nAssim, para acessar os dados, precisaremos enviar outra requisição POST, agora para a url “http://qualar.cetesb.sp.gov.br/qualar/exportaDados.do”, contendo os itens desse novo formulário, que são as informações que preencheríamos na pesquisa. No próximo passo, vai ficar mais claro o que estamos fazendo nessa etapa.\nPASSO 3 Vamos criar um código que imite essas requisições.\nPrimeiro, como o sistema Qualar exige login, precisamos capturar o cookie do site para manter a sessão logada nas requisições seguintes. O cookie da sessão é guardado no objeto my_cookie, que será passado em todas as requisições.\nlibrary(magrittr) library(httr) res \u0026lt;- GET(\u0026quot;http://qualar.cetesb.sp.gov.br/qualar/home.do\u0026quot;) my_cookie \u0026lt;- cookies(res)$value %\u0026gt;% purrr::set_names(cookies(res)$name) Agora, precisamos enviar a requisição POST para fazer o login e acessar o sistema.\nOs parâmetros do formulário são colocados dentro do argumento body= da função POST(). Se você estiver replicando, basta substituir os valores seu_login e sua_senha pelo login e senha que você obteve ao se cadastrar no Qualar.\nO argumento encode = \"form\" especifica que a requisição é uma submissão de formulário.\nO parâmetro enviar = \"OK\" indica que estamos submetendo o formulário.\nO cookie é passado usando a função set_cookies().\nurl \u0026lt;- \u0026quot;http://qualar.cetesb.sp.gov.br/qualar/autenticador\u0026quot; res \u0026lt;- POST( url, body = list( cetesb_login = \u0026quot;seu_login\u0026quot;, cetesb_password = \u0026quot;sua_senha\u0026quot;, enviar = \u0026quot;OK\u0026quot; ), encode = \u0026quot;form\u0026quot;, set_cookies(my_cookie) ) Então fazemos uma requisição POST para acessar os dados. Essa requisição imita a pesquisa dentro da página “Exportar dados”. Nela, precisamos definir quais dados queremos acessar.\nurl \u0026lt;- \u0026quot;http://qualar.cetesb.sp.gov.br/qualar/exportaDados.do\u0026quot; res \u0026lt;- POST( url, query = list(method = \u0026quot;pesquisar\u0026quot;), body = list( irede = \u0026quot;A\u0026quot;, dataInicialStr = \u0026quot;04/03/2018\u0026quot;, dataFinalStr = \u0026quot;05/03/2018\u0026quot;, cDadosInvalidos = \u0026quot;on\u0026quot;, iTipoDado = \u0026quot;P\u0026quot;, estacaoVO.nestcaMonto = \u0026quot;72\u0026quot;, parametroVO.nparmt = \u0026quot;63\u0026quot; ), encode = \u0026quot;form\u0026quot;, set_cookies(my_cookie) ) Observe que, apesar de na pesquisa conseguirmos selecionar o nome da estação e do parâmetro, a requisição envia ids numéricos. No Qualar, eu encontrei apenas os ids das estações, mas os valores de ambos os parâmetros podem ser encontrados inspecionando o html da página. Para facilitar a nossa vida, eu salvei esses valores nos arquivos station_ids.csv e param_ids.csv, que podem ser baixados pelo repositório do blog no Github.\nPara finalizar, precisamos ler o resultado da nossa requisição e transformar a tabela existe no html em um data frame.\ncontent(res) %\u0026gt;% rvest::html_table(fill = TRUE) %\u0026gt;% extract2(2) Passo 4 Agora, vamos transformar nosso código numa função para poder repetir o processo várias vezes para diversos parâmetros.\nscraper_CETESB \u0026lt;- function(station, parameter, start, end, type = \u0026quot;P\u0026quot;, login, password, invalidData = \u0026quot;on\u0026quot;, network = \u0026quot;A\u0026quot;) { res \u0026lt;- GET(\u0026quot;http://qualar.cetesb.sp.gov.br/qualar/home.do\u0026quot;) my_cookie \u0026lt;- cookies(res)$value %\u0026gt;% purrr::set_names(cookies(res)$name) url \u0026lt;- \u0026quot;http://qualar.cetesb.sp.gov.br/qualar/autenticador\u0026quot; res \u0026lt;- POST( url, body = list( cetesb_login = login, cetesb_password = password, enviar = \u0026quot;OK\u0026quot; ), encode = \u0026quot;form\u0026quot;, set_cookies(my_cookie) ) url \u0026lt;- \u0026quot;http://qualar.cetesb.sp.gov.br/qualar/exportaDados.do\u0026quot; res \u0026lt;- POST( url, query = list(method = \u0026quot;pesquisar\u0026quot;), body = list( irede = network, dataInicialStr = start, dataFinalStr = end, cDadosInvalidos = invalidData, iTipoDado = type, estacaoVO.nestcaMonto = station, parametroVO.nparmt = parameter ), encode = \u0026quot;form\u0026quot;, set_cookies(my_cookie) ) content(res) %\u0026gt;% rvest::html_table(fill = TRUE) %\u0026gt;% extract2(2) } Assim, basta rodar a função abaixo com o seu login e senha para obter em alguns segundos uma tabela com os dados solicitados. Veja um exemplo:\ndados_cetesb \u0026lt;- scraper_CETESB(station = \u0026quot;72\u0026quot;, parameter = \u0026quot;63\u0026quot;, start = \u0026quot;04/03/2018\u0026quot;, end = \u0026quot;05/03/2018\u0026quot;, type = \u0026quot;P\u0026quot;, login = \u0026quot;seu_login\u0026quot;, password = \u0026quot;sua_senha\u0026quot;, invalidData = \u0026quot;on\u0026quot;, network = \u0026quot;A\u0026quot;) Precisamos apenas tirar as colunas vazias e corrigir o nome das colunas.\ndados_cetesb \u0026lt;- dados_cetesb %\u0026gt;% janitor::remove_empty_cols() col_names \u0026lt;- as.character(dados_cetesb[1,]) dados_cetesb \u0026lt;- dados_cetesb %\u0026gt;% magrittr::set_colnames(col_names) %\u0026gt;% dplyr::slice(-1) Assim obtemos os dados que queríamos:\ndados_cetesb %\u0026gt;% dplyr::select(`Nome Estação`:`Média Horária`) %\u0026gt;% head %\u0026gt;% knitr::kable() %\u0026gt;% kableExtra::kable_styling() Nome Estação Nome Parâmetro Unidade de Medida Média Horária Parque D.Pedro II O3 (Ozônio) µg/m3 20 Parque D.Pedro II O3 (Ozônio) µg/m3 19 Parque D.Pedro II O3 (Ozônio) µg/m3 18 Parque D.Pedro II O3 (Ozônio) µg/m3 12 Parque D.Pedro II O3 (Ozônio) µg/m3 11 Parque D.Pedro II O3 (Ozônio) µg/m3 12 Iterando essa função fica fácil repetir o processo para diversas estações e parâmetros.\nÉ isso! Dúvidas, críticas e sugestões, deixe um comentário.\nAté a próxima!\n","permalink":"https://blog.curso-r.com/posts/2018-03-19-scraper-cetesb/","tags":["web-scraping","dados-abertos"],"title":"Web scraping do sistema de qualidade do ar da CETESB"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" RStudio é a melhor IDE para fazer ciência de dados. R e python são, no momento, as melhores linguagens para fazer ciência de dados. O RStudio foi criado para rodar R.\nMas será que o RStudio é bom para rodar python?\nNesse post curtinho eu mostro que sim e explico como.\nRodando R e python no RMarkdown Primeiro, certifique-se de que você tem a versão 1.1.x ou maior do RStudio instalado na sua máquina. Abra um arquivo RMarkdown em\nFile \u0026gt; New file \u0026gt; R Markdown... Talvez seja necessário instalar alguns pacotes (por exemplo, o próprio rmarkdown) para abrir a telinha.\nSe você não sabe o que é RMarkdown, dê uma olhada na documentação oficial do RStudio. Em poucas palavras, o RMarkdown é uma forma escrever textos e códigos dentro de um mesmo arquivo. Fazer análises com RMarkdown no RStudio é uma das coisas que torna essa IDE tão poderosa.\nCom o RMarkdown aberto, você pode inserir um chunk com Ctrl + Alt + I. Vai aparecer isso aqui:\n\u0026#39;\u0026#39;\u0026#39;{r} \u0026#39;\u0026#39;\u0026#39; Se você escrever um código em R aí dentro e mandar rodar, ele vai rodar.\n\u0026#39;\u0026#39;\u0026#39;{r} eu_gosto_de_R \u0026lt;- \u0026quot;nossa, como você é legal!\u0026quot; eu_gosto_de_R \u0026#39;\u0026#39;\u0026#39; [1] \u0026quot;nossa, como você é legal!\u0026quot; Se você trocar r por python, você poderá rodar python. Só isso!\n\u0026#39;\u0026#39;\u0026#39;{python} eu_gosto_de_python = \u0026quot;tudo bem, vida que segue\u0026quot; print eu_gosto_de_python \u0026#39;\u0026#39;\u0026#39; tudo bem, vida que segue É possível preservar a sessão entre chunks? Até o RStudio 1.1.x, não. Mas a partir da versão 1.2.x, que ainda não foi lançada oficialmente, isso já funciona. Então, isso aqui funciona:\n\u0026#39;\u0026#39;\u0026#39;{python} eu_gosto_de_python += \u0026quot;!!!\u0026quot; print eu_gosto_de_python \u0026#39;\u0026#39;\u0026#39; tudo bem, vida que segue!!! Com essa atualização, você também pode pegar os objetos rodados em python direto para o R, e vice-versa\nLer objetos do python no R, carregando o pacote reticulate:\n\u0026#39;\u0026#39;\u0026#39;{r} library(reticulate) veio_do_python \u0026lt;- py$eu_gosto_de_python paste(veio_do_python, \u0026quot; Segue no R.\u0026quot;) \u0026#39;\u0026#39;\u0026#39; [1] \u0026quot;tudo bem, vida que segue!!! Segue no R.\u0026quot; Ler objetos do R no python usando o objeto r.:\n\u0026#39;\u0026#39;\u0026#39;{python} print r.eu_gosto_de_R + \u0026quot; Também no python.\u0026quot; \u0026#39;\u0026#39;\u0026#39; nossa, como você é legal! Também no python As versões 1.2.x ainda não foram lançadas oficialmente. Ou seja, elas podem ter problemas de funcinamento. Muito cuidado ao usar as daily builds do RStudio!\nUm pouco de explicação Tudo isso é possível graças ao pacote reticulate do RStudio, que é responsável por criar uma sessão do python dentro do R. A versão mais atualizada desse pacote utiliza a função reticulate::eng_python, que é adicionada como uma opção knitr::knit_engines$set() do pacote knitr.\nNa versão 1.2.x do RStudio, a utilização dessa engine vem integrada ao editor do RMarkdown, permitindo que você rode os códigos em python e em R diretamente. Nas versões anteriores, você só conseguirá obter os resultados após compilar o documento para algum formato de saída (HTML, PDF, Word, etc).\nAtualmente, o reticulate já sabe transferir vários objetos entre as linguagens: - Objetos atômicos como números, strings etc funcionam sem nenhuma dor. - Com um pouquinho de esforço, também podemos transferir data.frames entre R e Pandas, matrizes e arrays entre R e NumPy. - Objetos complexos do python são tratados como environments no R. - Para mais detalhes, veja esse link.\nO pacote reticulate também permite que você abra uma sessão interativa do python dentro do console do R, usando a função py_repl(). Veja essa parte da documentação do reticulate para detalhes.\nWrap-up Você pode rodar R e python no RMarkdown dentro do RStudio. Para obter todas as funcionalidades, baixe a daily build, versão 1.2.x. Mas tome cuidado. O objeto reticulate::py guarda objetos carregados no python dentro do R. O objeto r. guarda objetos carregados no R dentro do python. Fique atualizada e obtenha mais informações no pacote reticulate. É isso pessoal. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2018-03-11-python/","tags":["python","rstudio","reticulate"],"title":"É possível rodar python no RStudio?"},{"author":["Daniel"],"categories":["Tutoriais"],"contents":" Existem diversas formas de hospedar o seu app feito em shiny. A maneira mais simples é usando o shinyapps.io, SaaS desenvolvido pela própria RStudio. No entanto, na versão Free o seu app fica com um logotipo do RStudio no canto e não é permitido configurar um domínio próprio, ou seja, a url do seu app sempre vai ser algo do tipo: dfalbel.shinyapps.io/meuapp.\nUma alternativa gratuita e que não possui essas contra-partidas é usar o serviço now.sh. O now, originalmente, foi desenvolvido para hospedar aplicativos feitos em nodejs e javascript, mas recentemente eles possibilitaram a hospedagem de aplicativos feitos com Docker.\nAlém dessas vantagens o now conta com infraestrutura, segundo os desenvolvedores “auto scaling ready”, isto é o número de instâncias servindo o seu aplicativo pode variar de acordo com tráfego que o seu app está recebendo. No plano free o número de instâncias pode ser 0 ou 1, ou seja, seu app fica desligado se ninguém estiver usando. Nos planos pagos você pode fazer esse número variar da forma que quiser. Tudo isso está na documentação.\nÉ difícil explicar o que é o Docker em poucas palavras, mas ele é uma plataforma de containers que facilita a configuração e preparação do ambiente para a hospedagem de um app. Em outras palavras, você escreve em um arquivo chamado Dockerfile especificando como você quer que seja o ambiente, o que você precisa instalar, que arquivos você quer colocar lá dentro. Para este tutorial você só precisará fazer pequenas modificações no Dockerfile e não precisará entender profundamente como ele funciona.\nInstalação: Para seguir o tutorial você precisa instalar o now e criar uma conta. Siga este link para instalação e configuração do now.\nEstrutura do projeto Esse repositório contém um exemplo simples funcionando do que iremos detalhar agora. Crie uma pasta no seu computador ou um projeto no RStudio com a seguinte estrutura:\n. ├── Dockerfile ├── app │ └── app.R ├── app.R └── shiny-now.Rproj Dockerfile é um arquivo de texto vazio por enquanto. Dentro da pasta app coloque o seu app shiny. No meu exemplo coloquei um shiny com apenas umarquivo app.R, mas nada impede você de colocar um shiny com a estrtura mais comum (um arquivo ui.R e um arquivo server.R).\nDockerfile Dockerfiles podem ser referenciados a outras imagens docker. Existe uma imagem chamada rocker/shiny que já deixa quase tudo preparado para hospedar um app shiny, por isso em nosso Dockerfile vamos incluir a linha:\nFROM rocker/shiny Em seguida vamos indicar as instruções para instalação dos pacotes do R que são usados no shiny. No meu caso, como o app não faz praticamente nada, só usei o pacote shinydashboard por isso a próxima linha do meu Dockerfile é apenas:\nRUN R -e \u0026quot;install.packages(c(\u0026#39;shinydashboard\u0026#39;), repos=\u0026#39;https://cran.rstudio.com/\u0026#39;)\u0026quot; Se você precisar instalar mais pacotes basta colocar o nome deles no comando acima, por exemplo:\nRUN R -e \u0026quot;install.packages(c(\u0026#39;shinydashboard\u0026#39;, \u0026#39;tidyverse\u0026#39;), repos=\u0026#39;https://cran.rstudio.com/\u0026#39;)\u0026quot; Feito isso, o próximo comando do Dockerfile é o COPY que serve para indicar para o Docker quais arquivos da pasta devem ser copiados para dentro do ambiente de hospedagem e para qual pasta dentro do novo ambiente eles devem copiados. No nosso caso, queremos copiar a pasta app para dentro da pasta /srv/shiny-server/ (que é por padrão a pasta em que o Shiny Server procura por apps). Para fazer isso, adicionamos a linha:\nCOPY /app/ /srv/shiny-server/ No final, o nosso Dockerfile contém apenas 3 linhas:\nFROM rocker/shiny RUN R -e \u0026quot;install.packages(c(\u0026#39;shinydashboard\u0026#39;), repos=\u0026#39;https://cran.rstudio.com/\u0026#39;)\u0026quot; COPY /app/ /srv/shiny-server/ App Dentro da pasta app, coloque o seu código shiny. A princípio, você não precisa fazer nenhuma modificação. No meu exemplo, coloquei o arquivo app.R contendo um aplicativo usando o shinydashboard (copiado da documentação do pacote).\nO conteúdo do arquivo do arquivo app.R está abaixo:\nlibrary(shinydashboard) ui \u0026lt;- dashboardPage( dashboardHeader(title = \u0026quot;Basic dashboard\u0026quot;), dashboardSidebar(), dashboardBody( # Boxes need to be put in a row (or column) fluidRow( box(plotOutput(\u0026quot;plot1\u0026quot;, height = 250)), box( title = \u0026quot;Controls\u0026quot;, sliderInput(\u0026quot;slider\u0026quot;, \u0026quot;Number of observations:\u0026quot;, 1, 100, 50) ) ) ) ) server \u0026lt;- function(input, output) { set.seed(122) histdata \u0026lt;- rnorm(500) output$plot1 \u0026lt;- renderPlot({ data \u0026lt;- histdata[seq_len(input$slider)] hist(data) }) } shinyApp(ui, server) Chamando o now Se você configurou o now corretamente essa será a parte mais fácil. Basta abrir um terminal e rodar now.\nEle vai soltar uma mensagem parecida com a abaixo:\nDaniels-MacBook-Air:shiny-now dfalbel$ now \u0026gt; Deploying ~/Documents/shiny-now under dfalbel@curso-r.com \u0026gt; Your deployment\u0026#39;s code and logs will be publicly accessible because you are subscribed to the OSS plan. \u0026gt; \u0026gt; NOTE: You can use `now --public` or upgrade your plan (https://zeit.co/account/plan) to skip this prompt \u0026gt; Ready! https://shiny-now-zoyrrtdqwv.now.sh (copied to clipboard) [5s] \u0026gt; Synced 1 file (131B) [5s] \u0026gt; Initializing… \u0026gt; Building \u0026gt; ▲ docker build Sending build context to Docker daemon 6.656 kBkB \u0026gt; ▲ Deploying image \u0026gt; ▲ Container started \u0026gt; [2018-03-05T20:48:12.696] [INFO] shiny-server - Shiny Server v1.5.7.883 (Node.js v6.10.3) \u0026gt; [2018-03-05T20:48:12.701] [INFO] shiny-server - Using config file \u0026quot;/etc/shiny-server/shiny-server.conf\u0026quot; \u0026gt; [2018-03-05T20:48:12.780] [WARN] shiny-server - Running as root unnecessarily is a security risk! You could be running more securely as non-root. \u0026gt; [2018-03-05T20:48:12.788] [INFO] shiny-server - Starting listener on 0.0.0.0:3838 \u0026gt; Deployment complete! Na sexta linha dessa mensagem, temos o endereço do nosso aplicativo. Basta abrir essa url no navegador e ver o aplicativo funcionando. Por enquanto, o now colocou um endereço para testes, mas tudo isso é configurável.\nPara mudar a url do seu app você vai precisar usar o comando now alias. A documentação desse comando é muito boa e vai ser melhor do que qualquer coisa que eu escrever aqui. Vá para este link!\nConclusão O now é um serviço gratuito que permite a hospedagem de containers Docker. Neste post configuramos um Dockerfile para instalar hospedar um app shiny e usando o comando now rapidamente enviamos o nosso aplicativo para o now e iniciamos a hospedagem.\n","permalink":"https://blog.curso-r.com/posts/2018-03-05-shiny-now/","tags":["shiny"],"title":"Hospedando seu Shiny app no now"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Tiago Belotti é o meu crítico de cinema favorito. Gasto uma quantidade de tempo que não me orgulho vendo os vídeos desse ser maravilhoso. Atualmente, muitas vezes eu prefiro ver a crítica dele do filme que o próprio filme, seguindo cegamente as indicações dele a partir das notas que ele coloca no final da crítica. Recomendo fortemente que acessem o canal do Belotti.\nO Tiago já mencionou várias vezes que as notas que ele dá aos filmes não representam toda a crítica realizada. Por exemplo, ele já montou listas de Top 10 colocando filmes com notas medianas à frente de filmes com notas altas. Mesmo assim, não dá pra não querer saber a nota que ele dá aos filmes, é um vício!\nA minha necessidade visceral de saber as notas dadas aos filmes começou quando a seguinte pergunta apareceu na minha mente:\nSerá que o Tiago está ficando bonzinho? A nota média está aumentando ao longo do tempo?\nE assim acabou o meu final de semana. Eu precisava obter esses dados de qualquer forma.\nComo extrair as notas dos vídeos O código para extrair as notas é pra lá de esotérico e usa um monte de funções auxiliares de pacotes do linux. Como o amigo Caio Lente adora falar, meus códigos costumam ter muitas partes móveis…\nA obtenção das notas dos filmes passa pelo seguinte fluxo:\nBaixar os vídeos do youtube Extrair os frames dos vídeos Encontrar a imagem que tem a nota Limpar a imagem que tem a nota Passar um algoritmo de OCR (Optical Character Recognition) na imagem Cruzar com uma base de metadados do YouTube, usando o pacote tuber Vou explicar brevemente como cada um desses passos foi executado. O código não é totalmente reprodutível pois o código completo com os loops ficaria pouco legível.\nPasso 1: Baixar os vídeos Os vídeos foram baixados usando um pacote chamado youtube-dl. Para instalá-lo num Ubuntu 16.04 podemos rodar\nsudo apt install youtube-dl O download da lista de reprodução do Tiago Belotti de 2015 pode ser realizada rodando, por exemplo:\nyoutube-dl -f 160 https://www.youtube.com/playlist?list=PLy-phHpv7EKaioTZ39lNd_nvitGNtQadl Realizei o download numa resolução bem ruim e sem áudio, para ocupar pouco espaço em disco. Faça isso para todos os anos (2013 até 2017) e pronto! Temos os vídeos.\nPasso 2: Extrair os frames dos vídeos Os arquivos de vídeo foram transformados utilizando ffmpeg, também baixado como pacote do linux. Para instalá-lo num Ubuntu 16.04 podemos rodar\nsudo apt install ffmpeg A função tirar_screenshots() recebe o nome de um arquivo de vídeo, cria uma pasta com esse nome e salva os screenshots na pasta.\ntirar_screenshots \u0026lt;- function(vfile) { # folder and file names file_name \u0026lt;- tools::file_path_sans_ext(basename(vfile)) folder_name \u0026lt;- glue(\u0026quot;imgs/{file_name}\u0026quot;) if (!file.exists(folder_name)) { dir.create(folder_name, showWarnings = FALSE, recursive = TRUE) out \u0026lt;- glue(\u0026quot;{folder_name}/out_\u0026quot;) # take screenshots take_shots \u0026lt;- glue(\u0026quot;ffmpeg -y -i \u0026#39;{vfile}\u0026#39; -vf fps=1 \u0026#39;{out}%04d.png\u0026#39;\u0026quot;) system(take_shots) } } Basta fazer um loop (com purrr, não for, rs) para processar todos os arquivos de vídeo. Resultado:\nPasso 3: Encontrar a imagem que tem a nota A nota geralmente aparece no canto superior direito da imagem:\nEntão, vamos recortar esse pedaço e pegar qual imagem tem maior concentração de vermelho. A função cortar_canto() utiliza o magick para pegar o canto da imagem e salva num arquivo temporário.\ncortar_canto \u0026lt;- function(img_file) { img_file %\u0026gt;% magick::image_read() %\u0026gt;% magick::image_crop(\u0026quot;x60+180\u0026quot;) %\u0026gt;% magick::image_write(tempfile()) } A função somar_vermelho() pega a imagem e calcula a quantidade de vermelhos, tirando os casos em que há vermelho conjuntamente com outras cores (como branco, por exemplo).\nsomar_vermelho \u0026lt;- function(img_file) { p \u0026lt;- png::readPNG(img_file) if (length(dim(png_img)) != 3) return(0) sum(p[,,1] == 1.0 \u0026amp; p[,,2] \u0026lt; 0.1 \u0026amp; p[,,3] \u0026lt; 0.1) } Assim, a soma de vermelhos de um arquivo fica\n\u0026quot;out_0277.png\u0026quot; %\u0026gt;% cortar_canto() %\u0026gt;% somar_vermelho() # [1] 758 Basta rodar isso para todos os arquivos e depois selecionar a imagem que tem a maior contagem.\nPasso 4: Limpar a imagem que tem a nota A função limpar_imagem() recebe a imagem cortada e faz a limpeza, soltando apenas os números, prontos para receber o algoritmo de OCR.\nlimpar_imagem \u0026lt;- function(img_cut) { tmp \u0026lt;- tempfile() # filtros a serem aplicados em cada cor parms \u0026lt;- c(red = .9, green = .2, blue = .2) png::readPNG(img_cut) %\u0026gt;% purrr::array_branch(3) %\u0026gt;% purrr::map2(parms, ~.x \u0026gt; .y) %\u0026gt;% purrr::map_at(2:3, magrittr::not) %\u0026gt;% purrr::reduce(magrittr::and) %\u0026gt;% magrittr::multiply_by(-1) %\u0026gt;% magrittr::add(1) %\u0026gt;% png::writePNG(tmp) tmp } \u0026quot;out_0277.png\u0026quot; %\u0026gt;% cortar_canto() %\u0026gt;% limpar_imagem() Passo 5: Passar um algoritmo de OCR Finalmente, a função pegar_nota() utiliza tesseract para transformar as imagens em números\npegar_nota \u0026lt;- function(img_filtered) { # whitelist de numeros engine \u0026lt;- tesseract::tesseract(options = list( tessedit_char_whitelist = \u0026quot;1234567890.\u0026quot;)) # apply tesseract img \u0026lt;- img_filtered %\u0026gt;% magick::image_read() %\u0026gt;% magick::image_trim() %\u0026gt;% tesseract::ocr(engine = engine) %\u0026gt;% readr::parse_number() } A utilização final do algoritmo fica assim:\n\u0026quot;out_0277.png\u0026quot; %\u0026gt;% cortar_canto() %\u0026gt;% limpar_imagem() %\u0026gt;% pegar_nota() # [1] 8.3 Pronto! Agora é só loopar em todos os arquivos e teremos as notas de todos os filmes.\nInfelizmente, o algoritmo de OCR do tesseract não é lá essas coisas, e precisei arrumar muitos desses dados na mão.\nPasso 6: Cruzar com uma base de metadados do YouTube Agora, vamos cruzar com os metadados do youtube. Os nomes dos arquivos baixados pelo youtube-dl vêm com um código no final, por exemplo com UVN6ljuRzp8. É possível utilizar esses códigos no tuber para obter metadados de um vídeo, dessa forma:\n## código para obter um token de acesso # tuber::yt_oauth( # \u0026quot;998136489867-5t3tq1g7hbovoj46dreqd6k5kd35ctjn.apps.googleusercontent.com\u0026quot;, # \u0026quot;MbOSt6cQhhFkwETXKur-L9rN\u0026quot;) detalhes \u0026lt;- tuber::get_video_details(\u0026quot;UVN6ljuRzp8\u0026quot;) ## pegar nome do filme purrr::pluck(detalhes, 4, 1, \u0026quot;snippet\u0026quot;, \u0026quot;title\u0026quot;) # [1] \u0026quot;JOGO PERIGOSO (Gerald\u0026#39;s Game, Netflix, 2017) - Crítica\u0026quot; ## pegar data de disponibilização purrr::pluck(detalhes, 4, 1, \u0026quot;snippet\u0026quot;, \u0026quot;publishedAt\u0026quot;) # [1] \u0026quot;2017-10-07T14:39:54.000Z\u0026quot; Usei esses metadados para extrair a data de disponibilização dos vídeos. No final, fiquei com a seguinte base de dados com 386 filmes classificados (mostrando apenas 20 linhas):\nid nm data nota j5udRStwytk 007 CONTRA SPECTRE (2015) 2015-11-05 6.4 HdYnkiiWwe8 120 BATIMENTOS POR MINUTO (2017) 2018-01-04 8.6 GXfe2CSKiHw 12 YEARS A SLAVE (12 Anos de Escravidão, 2013) 2014-01-04 9.0 j-kiltl6_uk 1922 (Netflix, 2017) 2017-10-24 7.0 zsNoXCMnTmo 300 - A ASCENSÃO DO IMPÉRIO (2014) 2014-03-07 7.1 60hUNlmK30A A AUTÓPSIA (The Autopsy of Jane Doe, 2016) 2017-05-01 5.7 1sNNzBDVlLk A BELA E A FERA (Beauty and the Beast, 2017) 2017-03-16 8.5 Zuk5QzVccWA ABOUT TIME (Questão de Tempo, 2013) 2013-11-28 8.1 aRjxN0ShsE8 A BRUXA (The Witch, 2016) 2016-03-05 8.2 n4xrIu4xKo8 A CABANA (The Shack, 2017) 2017-04-07 3.4 t8W-DbCbPVY A CHEGADA (Arrival, 2016) 2016-11-22 9.4 M5GBLtYgz14 A COLINA ESCARLATE (Crimson Peak, 2015) 2015-10-15 6.2 Sv-M7jfTGM0 A CULPA É DAS ESTRELAS (The Fault in Our Stars, 2014) 2014-06-05 7.2 EALV4nw-vx0 A ENTREVISTA (The Interview, 2014) 2014-12-27 6.7 4I44eE82Jk4 A FORMA DA ÁGUA (The Shape of Water, 2017) 2018-01-24 9.3 MnpZqlfZ_KE A GAROTA DINAMARQUESA (The Danish Girl, 2015) 2016-02-12 6.8 HjAxuKPzucY A GAROTA NO TREM (The Girl on the Train, 2016) 2016-10-27 5.1 UuyN4G9BQgI A GHOST STORY (2017) 2017-09-28 10.0 9irKAQoDN88 A GRANDE APOSTA (The Big Short, 2015) 2016-01-18 7.8 bNsUkl8hGvk ÁGUAS RASAS (The Shallows, 2016) 2016-08-07 7.0 Análises Agora que temos uma base de dados, podemos fazer algumas análises!\nPrimeiro, é claro, vamos fazer os top 20 e os top -20\nnotas_2cents %\u0026gt;% dplyr::select(nm, nota) %\u0026gt;% dplyr::arrange(desc(nota)) %\u0026gt;% utils::head(20) %\u0026gt;% knitr::kable() nm nota A GHOST STORY (2017) 10.0 A QUALQUER CUSTO (Hell or High Water, 2016) 10.0 BINGO - O REI DAS MANHÃS (2017) 10.0 BLADE RUNNER 2049 (2017) 10.0 ELLE (2016) 10.0 MAD MAX - ESTRADA DA FÚRIA (2015) 10.0 MOONLIGHT (2016) 10.0 RELATOS SELVAGENS (Relatos salvajes, 2014) 10.0 SPOTLIGHT (Segredos Revelados, 2015) 10.0 THE HANDMAIDEN (Ah-ga-ssi, 2016) 10.0 O QUARTO DE JACK (Room, 2015) 9.9 TRÊS ANÚNCIOS PARA UM CRIME (2017) 9.9 BOYHOOD (2014) 9.7 DEADPOOL (2016) 9.5 WHIPLASH - EM BUSCA DA PERFEIÇÃO (2014) 9.5 A CHEGADA (Arrival, 2016) 9.4 AMOR \u0026amp; AMIZADE (Love \u0026amp; Friendship, 2016) 9.4 CORRA! (Get Out, 2017) 9.4 LA LA LAND (2016) 9.4 O REGRESSO (The Revenant, 2015) 9.4 Não assistiu a algum desses filmes? Tá na hora de ver!\nnotas_2cents %\u0026gt;% dplyr::select(nm, nota) %\u0026gt;% dplyr::arrange(nota) %\u0026gt;% utils::head(20) %\u0026gt;% knitr::kable() nm nota DEUSES DO EGITO (Gods of Egypt, 2016) 0.0 PORTA DOS FUNDOS - CONTRATO VITALÍCIO (2016) 2.1 MULHERES AO ATAQUE (The Other Woman, 2014) 2.3 CINQUENTA TONS DE CINZA (2015) 2.5 TRANSFORMERS - A ERA DA EXTINÇÃO (2014) 2.5 INFERNO (2016) 3.3 A CABANA (The Shack, 2017) 3.4 ASSASSINS CREED (2016) 3.5 ALICE ATRAVÉS DO ESPELHO (2016) 3.6 FRANKENSTEIN - ENTRE ANJOS E DEMÔNIOS (2014) 3.6 PETER PAN (Pan, 2015) 3.6 A MÚMIA (The Mummy, 2017) 3.8 ANTES QUE EU VÁ (Before I Fall, 2017) 3.8 QUARTETO FANTÁSTICO (2015) 3.8 CONVERGENTE (Allegiant, 2016) 3.9 DEBI \u0026amp; LÓIDE 2 (Dumb and Dumber To, 2014) 3.9 JOGOS MORTAIS - JIGSAW (2017) 4.0 JUNTOS E MISTURADOS (Blended, 2014) 4.0 The Mortal Instruments - City of Bones (2013) 4.0 ANNABELLE (2014) 4.1 Já assistiu a algum desses filmes? Meus pêsames!\nTestando a minha hipótese Finalmente, pude testar minha hipótese!\np \u0026lt;- notas_2cents %\u0026gt;% ggplot(aes(x = data, y = nota, group = nm)) + geom_point() + geom_smooth(aes(group = 1), method = \u0026quot;lm\u0026quot;, se = FALSE) + theme_minimal(16) plotly::ggplotly(p) ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; ## Warning: `group_by_()` is deprecated as of dplyr 0.7.0. ## Please use `group_by()` instead. ## See vignette(\u0026#39;programming\u0026#39;) for more help ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. Parece que a nota média do Tiago Belotti subiu um pouquinho ao longo dos anos, mas nada muito significativo. Ou seja, minha hipótese de que o Tiago Belotti ficou mais bonzinho não se verifica. Ele é super consistente!\nOutras investigações que eu gostaria de fazer:\nQual a relação das notas do Tiago com o Rotten Tomatoes ou o IMDb? Será que é possível prever a nota do Tiago com base em outras notas e características do filme? O Tiago avalia diferente filmes de gêneros diferentes? Para essas investigações, seria necessário cruzar essa base com a base do Rotten Tomatoes e do IMDb. Quem sabe num próximo post!\nÉ isso pessoal. Um abraço, e até a próxima.\nAgradecimentos Sillas pelas ideias de visualização, que acabei não postando. E também para os amigos que brincaram comigo na datathon sobre isso: Bruna Wunderwald, Luciana Keiko, William, Caio Lente, Guilherme e quem mais eu tiver esquecido.\nCursos R básico e Web Scraping Não perca a oportunidade de se inscrever nos nossos cursos!\nWeb Scraping - 10 de março de 2018 R básico - 17 e 24 de março de 2018 ","permalink":"https://blog.curso-r.com/posts/2018-02-21-2cents/","tags":["sf","tidyverse"],"title":"Meus 2 Centavos: análise das notas"},{"author":["Caio"],"categories":["conceitos"],"contents":" Web scraping (ou raspagem web) não é nada mais que o ato de coletar dados da internet. Hoje em dia é muito comum termos acesso rápido e fácil a qualquer conjunto de informações pela web, mas raramente esses dados estão estruturados e em uma forma de fácil obtenção pelo usuário.\nIsso faz com que precisemos aprender a coletar esses dados por conta própria. Neste post vou descrever o fluxo do web scraping, um passo a passo para explicar aos iniciantes como funciona a criação de um raspador.\nO fluxo Caso você já tenha visto o fluxo da ciência de dados descrito por Hadley Wickham, o fluxo do web scraping vai ser bastante simples de entender. Todos os itens a seguir vão se basear neste diagrama:\nCada verbo indica um fase do processo de raspar dados da internet. A caixa azulada no meio do diagrama denominada reprodução indica um procedimento iterativo que devemos repetir até que a coleta funcione, mas, de resto, o fluxo é um processo linear.\nNas próximas seções, vamos explorar um exemplo bem simples para entender como esses passos se dariam no mundo real: extrair os títulos de artigos da Wikipédia.\nIdentificar O primeiro passo do fluxo se chama identificar porque nele identificamos a informação que vamos coletar. Aqui precisamos entender bem qual é a estrutura das páginas que queremos raspar e traçar um plano para extrair tudo que precisamos.\nNo nosso exemplo, precisaríamos entrar em algumas páginas da Wikipédia para entender se os títulos se comportam da mesma forma em todas. Como a Wikipédia é um site organizado, todos os títulos são criados da mesma forma em absolutamente todos os artigos.\nNavegar Agora precisamos entender de onde vem o dado que queremos extrair. Esse passo pode ser extremamente simples, mas de vez em quando ele se tornara algo bastante complexo.\nUsando as ferramentas de desenvolvedor do nosso navegador, vamos navegar para encontrar a fonte dos dados. Sem entrar em muitos detalhes, poderíamos analisar o networking do navegador para entender as chamadas HTTP que são feitas, poderíamos estudar os resultados das funções JavaScript invocadas pela página e assim por diante.\nNo nosso caso, como escolhi um exemplo simples, precisamos apenas inspecionar o elemento do título e ver qual é o seu XPath (basicamente o endereço do elemento no HTML da página): //*[@id=\"firstHeading\"].\nReplicar Se tivéssemos que fazer várias requests HTTP para chegar até a informação que queremos, seria aqui em que tentaríamos replicar essas chamadas. Neste passo é importante compreender absolutamente tudo que a página está fazendo para trazer o conteúdo até você, então é necessário analisar o seu networking a fim de entender tais requests e seus respectivos queries.\nNo nosso caso, basta fazer uma chamada GET para obter a página do artigo desejado. Também se faz necessário salvar a página localmente para que possamos dar continuidade ao fluxo.\nurl \u0026lt;- \u0026quot;https://en.wikipedia.org/wiki/R_(programming_language)\u0026quot; httr::GET(url, httr::write_disk(\u0026quot;~/Desktop/wiki.html\u0026quot;)) Parsear O anglicismo parsear vem do verbo to parse, que quer dizer algo como analisar ou estudar, mas que, no contexto do web scraping, significa extrair os dados desejados de um arquivo HTML.\nAqui vamos usar a informação obtida no passo 2 para retirar do arquivo que chamei de wiki.html o título do artigo.\n\u0026quot;~/Desktop/wiki.html\u0026quot; %\u0026gt;% xml2::read_html() %\u0026gt;% rvest::html_node(xpath = \u0026quot;//*[@id=\u0026#39;firstHeading\u0026#39;]\u0026quot;) %\u0026gt;% rvest::html_text() #\u0026gt; [1] \u0026quot;R (programming language)\u0026quot; Validar Se tivermos feito tudo certo até agora, validar os resultados será uma tarefa simples. Precisamos apenas reproduzir o procedimento descrito até agora para algumas outras páginas de modo verificar se estamos de fato extraindo corretamente tudo o que queremos.\nCaso encontremos algo de errado precisamos voltar ao passo 3, tentar replicar corretamente o comportamento do site e parsear os dados certos nas páginas.\nIterar O último passo consiste em colocar o nosso scraper em produção. Aqui, ele já deve estar funcionando corretamente para todos os casos desejados e estar pronto para raspar todos os dados dos quais precisamos.\nNa maior parte dos casos isso consiste em encapsular o scraper em uma função que recebe uma série de links e aplica o mesmo procedimento em cada um. Se quisermos aumentar a eficiência desse processo, podemos paralelizar ou distribuir o nosso raspador.\nscraper \u0026lt;- function(url, path) { httr::GET(url, httr::write_disk(path)) path %\u0026gt;% xml2::read_html() %\u0026gt;% rvest::html_node(xpath = \u0026quot;//*[@id=\u0026#39;firstHeading\u0026#39;]\u0026quot;) %\u0026gt;% rvest::html_text() } purrr::map2_chr(links, paths, scraper) Conclusão Fazer um scraper não é uma tarefa fácil, mas, se toda vez seguirmos um método consistente e robusto, podemos melhorar um pouco o nosso trabalho. O fluxo do web scraping tenta ser este método, englobando em passos simples e razoavelmente bem definidos essa arte que é fazer raspadores web.\n","permalink":"https://blog.curso-r.com/posts/2018-02-18-fluxo-scraping/","tags":["web-scraping"],"title":"O Fluxo do Web Scraping"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" Caso você já tenha se aventurado no mundo do web scraping, é provável que tenha se deparado com um grande problema: volume. Muitas vezes, antes fazer uma análise, precisamos scrapear um número colossal de páginas até que tenhamos dados o suficiente para a nossa tarefa, número esse que chega a ser proibitivo a ponto de não conseguirmos fazer aquilo que queremos.\nNeste post vou explicar duas técnicas para aumentar em dezenas de vezes a velocidade dos seus scrapers de forma que você nunca mais precise de preocupar com a quantidade de dados necessária para uma análise.\nScrapers sequenciais Um scraper sequencial é qualquer scraper que baixe uma página por vez, ou seja, que varra as páginas em sequência baixando uma a uma. Como veremos na seção a seguir isso não é muito eficiente, mas é mesmo assim o que a maioria de nós faz.\nNota: Nos exemplos que darei daqui em diante estarei baixando uma lista de 1441 artigos da Wikipédia obtida com o pacote WikipediR. Se você quiser reproduzir os meus achados, disponibilizei um arquivo com o código completo em um Gist\nVeja mais ou menos como funcionaria para baixar um link da Wikipédia por vez:\n# Função para baixar uma página da Wikipédia download_wiki \u0026lt;- function(url, path) { # Converter um URL em um nome de arquivo file \u0026lt;- url %\u0026gt;% utils::URLdecode() %\u0026gt;% stringr::str_extract(\u0026quot;(?\u0026lt;=/)[^/]+$\u0026quot;) %\u0026gt;% stringr::str_replace_all(\u0026quot;[:punct:]\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;% stringr::str_to_lower() %\u0026gt;% stringr::str_c(normalizePath(path), \u0026quot;/\u0026quot;, ., \u0026quot;.html\u0026quot;) # Salvar a página no disco httr::GET(url, httr::write_disk(file, TRUE)) return(file) } # Baixar arquivos sequencialmente files \u0026lt;- purrr::map_chr(links, download_wiki, \u0026quot;~/Desktop/Wiki\u0026quot;) Nada muito complexo até aí. Com a purrr::map_chr() itero com facilidade nos links e os baixo sequencialmente (se você quiser saber mais sobre a função map() veja este post). O código acima demorou mais ou menos 5 minutos para executar na minha máquina.\nScrapers em paralelo Uma das formas mais simples de aumentar a eficiência de um web scraper é através de paralelização. Um fato que nem todos sabem é que praticamente qualquer scraper passa a maior parte do tempo esperando respostas do servidor; seja para carregar uma nova página ou seja para baixar a página em questão, ficar esperando é o que o seu scraper provavelmente mais faz.\nIsso quer dizer que seu computador poderia ter, em qualquer dado momento, múltiplos scrapers rodando simultaneamente sem perder eficiência! Enquanto o processador está salvando no disco os resultados de um scraper, é perfeitamente possível ter muitos outros ativos e esperando uma resposta do servidor.\nNo exemplo de código abaixo uso uma função muito simples para paralelizar a execução do scraper. parallel::mcmapply() (multicore mapply()) é análoga a map(), com a diferença de que ela instancia as chamadas para a função download_wiki() em múltiplos threads de execução, tirando vantagem do fato de que cada instância fica parada a maior parte do tempo.\n# Criar uma versão empacotada de download_wiki() download_wiki_ \u0026lt;- purrr::partial( download_wiki, path = \u0026quot;~/Desktop/Wiki\u0026quot;, .first = FALSE) # Baixar arquivos em paralelo files \u0026lt;- parallel::mcmapply( download_wiki_, links, SIMPLIFY = TRUE, mc.cores = 4) No código acima, crio uma versão pré-preenchida de download_wiki() para não precisar lidar com argumentos constantes na chamada para parallel::mcmapply(), mas depois disso a única coisa que preciso fazer é especificar o número de cores disponíveis no meu computador para que o pacote parallel faça a sua magia. Desta forma, com uma chamada marginalmente mais complexa, consegui baixar os mesmos arquivos em meros 1.2 minutos.\nScrapers distribuídos Para o nosso grand finale temos um pequeno salto de dificuldade. Agora que somos capazes de usar todo o potencial do nosso computador, a única forma de fazer scraping mais rápido é usando mais computadores.\nIsso parece loucura, mas usando máquinas virtuais da Amazon ou da Google essa é na verdade uma tarefa bastante simples! Podemos criar algumas instâncias virtuais e enviar os links para que elas os baixem, distribuindo o download entre várias máquinas.\nPara permitir que uma máquina virtual receba o comando de download, criei um pequeno servidor HTTP em cada uma, assim elas ficarão esperando por uma chamada POST contendo os URLs a serem baixados.\n# Trecho do código em python do servidor que lida com POSTs def do_POST(self): content_length = int(self.headers[\u0026#39;Content-Length\u0026#39;]) post_data = self.rfile.read(content_length) call([\u0026quot;Rscript\u0026quot;, \u0026quot;~/script.R\u0026quot;, post_data]) Como pode-se ver no código acima, a única coisa que o servidor faz é coletar os dados enviados pelo post e redirecioná-los para o script script.R. Lá o R coleta os links vindos de post_data e os baixa (usando, é claro, parallel::mcmapply).\n#!/usr/bin/env Rscript args = commandArgs(trailingOnly=TRUE) # Tratar o pacote de dados enviado no POST links \u0026lt;- stringr::str_split(args[1], \u0026quot; \u0026quot;)[[1]] Acima temos a única diferença no código em R (que agora se encontra nas máquinas virtuais): o tratamento necessário em script.R dos dados trazidos pela chamada POST.\nO último passo é, em nossa máquina local, quebrar a lista de links em um pacote para cada máquina serva; assim que as máquinas receberem esses links via HTTP elas começarão, distribuidamente, a baixá-los em paralelo.\n# Quebrar os links de acordo com o número de servos num_workers \u0026lt;- 3 links_split \u0026lt;- links %\u0026gt;% split(., ceiling(seq_along(.)/(length(.)/num_workers))) %\u0026gt;% purrr::map(stringr::str_c, collapse = \u0026quot; \u0026quot;) # Dados do endpoint workers \u0026lt;- \u0026quot;localhost\u0026quot; # AQUI VÃO OS IPS DOS SERVOS endpoints \u0026lt;- stringr::str_c(\u0026quot;http://\u0026quot;, workers, \u0026quot;:8000\u0026quot;) # Chamar todos os servos mas não esperar por eles for (i in seq_len(num_workers)) { command \u0026lt;- paste0(\u0026quot;curl -d \u0026#39;\u0026quot;, links_split[[i]], \u0026quot;\u0026#39; \u0026quot;, endpoints[i]) system(command, wait = FALSE) } Usando 3 máquinas virtuais de 4 cores cada no Google Cloud Platform, o download das 1400 páginas demorou meros 34 segundos. Isso é uma melhora de aproximadamente 10 vezes na performance em relação à execução sequencial!\nConclusão Como vimos nos exemplos acima, scrapers são por padrão processos lentos e ineficientes. Usando uma arquitetura razoavelmente simples distribuída e paralela podemos aumentar em até uma ordem de grandeza a eficiência de um scraper sem nem pensar sobre o seu código! Na prática, podemos aumentar e diminuir o quanto quisermos o número de servos ou de cores em cada servo, permitindo que qualquer scraper possa virar uma máquina incrível de coleta de dados.\nCaso você tenha se interessado pelo conteúdo abordado nesse post, eu e o pessoal da Curso-R vamos dar no dia 10/03/2018 um workshop em São Paulos sobre web scraping com R. Lá você vai ter a oportunidade de aprender, do zero, como fazer bons web scrapers em R além de muitas dicas como a desse post para tornar seus scrapers ainda melhores.\n","permalink":"https://blog.curso-r.com/posts/2018-02-17-scraper-distribuido/","tags":["web-scraping"],"title":"Web Scraper Distribuído"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Eu precisava escrever esse post porque estou morrendo de tesão por essa linguagem maravilhosa. O R é incrível. O tidyverse é incrível. Estatística é incrível. Não me aguento!!\nHoje mais uma vez fui salvo por uma feature pensada no universo tidy. Dessa vez, o grande herói foi o pacote sf, um pacote ainda em estágio de desenvolvimento, mas que já considero pacas.\nO sf, a.k.a. Simple Features, é um pacote para trabalhar com mapas. Com ele é possível fazer projeções, gráficos, leitura/gravação de diversos formatos de mapas, entre muitas outras coisas. Esse é um dos pacotes patrocinados pelo R Consortium, uma iniciativa criada por várias empresas e pela R Foundation para injetar dinheiro em projetos de R que tenham alta relevância.\nO pacote sf é tão sensacional que merece um post só para ele. Hoje, eu falarei de apenas de uma de suas vantagens, que é sua integração perfeita com o tidyverse.\nContexto Eu faço parte da Associação Brasileira de Jurimetria (ABJ), uma entidade sem fins lucrativos que faz pesquisa aplicada na área do Direito. Na ABJ produzimos diversas soluções tecnológicas para problemas do Direito, especialmente em questões relacionadas à elaboração de políticas públicas e administração dos tribunais.\nNum projeto recente, eu precisava levantar uma tabela de municípios, comarcas, circunscrições e regiões administrativas do Tribunal de Justiça de São Paulo. Minha tarefa era visualizar essas informações em mapas. Fazendo curta uma história longa, podemos dizer que:\numa comarca é o conjunto de um ou mais municípios contíguos (contíguos = municípios que se tocam); uma circunscrição é um conjunto de uma ou mais comarcas contíguas; e uma região é… adivinha? No Estado de São Paulo, temos no total:\n645 municípios 319 comarcas 57 circunscrições 10 regiões administrativas O problema é que não existe uma base de dados pública que relacione todos os 645 municípios com as respectivas regiões que as contêm. Então partimos para a obtenção via robizinhos.\nP.S.: Não se esqueça do ciclo da ciência de dados! Vou falar disso sem parar para explicar. Na dúvida, leia o http://r4ds.had.co.nz/\nAs partes de import e tidy desse projeto processo foram sofríveis. Precisei fazer vários web scrapers e diversos códigos para lidar com nomes zoados dos municípios de São Paulo, como Moji/Mogi, Estrela D’Oeste/Doeste, Brodowski/Brodosqui etc. Quem quiser pode dar uma olhada nos códigos do github da ABJ.\nO problema Após passar o dia todo pegando esses dados, finalmente cheguei na parte de transform e visualize. Nossos objetivos nessa parte são:\nCarregar um mapa dos municípios de São Paulo no R. Agrupar as formas dos municípios em comarcas, circunscrições e regiões. Fazer mapas dos resultados obtidos. Em experiências anteriores já passei por maus bocados tentando completar a parte (2). Antigamente, a única função conhecida que fazia a união de polígonos era a maptools::unionSpatialPolygons(), que é contraintuitiva, mal documentada e difícil de usar na prática. Na verdade, tudo era difícil: desde ler o arquivo .shp baixado da internet até a parte de fazer o gráfico. Era R raiz mesmo. Mas hoje, para a felicidade de todos, temos a solução nutella ;)\nCuriosamente, o Edzer Pebesma, autor do sf, também é contributor do maptools e mantenedor do sp, os pacotes mais importantes de análise espacial pré-tidyverse. Ou seja, ele não só manja do assunto como sabia que os pacotes dele precisavam melhorar. E, olha, ele mandou bem dessa vez.\nSolução Vamos utilizar o tidyverse e o pacote sf. Para instalar o tidyverse, rode\ninstall.packages(\u0026quot;tidyverse\u0026quot;) Para quem não sabe: o tidyverse é um pacote que instala muitos outros pacotes por trás, como httr, ggplot2 e dplyr. Isso significa que pode demorar bastante tempo para instalar!\nA instalação do sf pode ser um tanto trabalhosa. Se você usa Windows, basta instalar o Rtools e depois rodar\ninstall.packages(\u0026quot;sf\u0026quot;) Se você usa Mac ou Linux, recomendo ler a primeira página da documentação oficial do pacote. Lá você pode checar todos os requerimentos do pacote em detalhe. Eu recomendo que você instale logo a versão de desenvolvimento:\ndevtools::install_github(\u0026quot;r-spatial/sf\u0026quot;) Para rodar as funções gráficas, também recomendo que você instale a versão de desenvolvimento do ggplot2, rodando\ndevtools::install_github(\u0026quot;tidyverse/ggplot2\u0026quot;) Outros pacotes que usaremos no meio do código são\ndevtools::install_github(\u0026quot;abjur/abjutils\u0026quot;) # principalmente para remover acentos installed.packages(\u0026quot;janitor\u0026quot;) # para arrumar nomes das colunas da base de dados Parte 1: baixando os dados Primeiro, vamos baixar da internet! Obviamente, o melhor lugar para baixar esses arquivos é no FTP do Instituto Brasileiro de Geografia e Estatística.\n# Cria uma pasta onde os arquivos serão salvos dir.create(\u0026quot;shp\u0026quot;, showWarnings = FALSE) # URL de download u_ibge \u0026lt;- paste0( \u0026quot;ftp://geoftp.ibge.gov.br/organizacao_do_territorio/\u0026quot;, \u0026quot;malhas_territoriais/malhas_municipais/\u0026quot;, \u0026quot;municipio_2015/UFs/SP/sp_municipios.zip\u0026quot;, collapse = \u0026quot;\u0026quot;) Utilizamos o pacote httr para baixar o arquivo zipado.\n# Salva o arquivo em disco. httr::progress() # serve para mostrar o andamento do download httr::GET(u_ibge, httr::write_disk(\u0026quot;shp/sp.zip\u0026quot;), httr::progress()) E usamos unzip() para dezipar os dados.\n# dezipa os arquivoe unzip(\u0026quot;shp/sp.zip\u0026quot;, exdir = \u0026quot;shp/\u0026quot;) No final, você terá esses arquivos na pasta:\ndir(\u0026quot;shp\u0026quot;) # \u0026quot;35MUE250GC_SIR.cpg\u0026quot; # \u0026quot;35MUE250GC_SIR.dbf\u0026quot; # \u0026quot;35MUE250GC_SIR.prj\u0026quot; # \u0026quot;35MUE250GC_SIR.shp\u0026quot; # \u0026quot;35MUE250GC_SIR.shx\u0026quot; # \u0026quot;sp.zip\u0026quot; Para ler esses arquivos estranhos num objeto do R, utilizamos a função sf::st_read(), simples assim:\nlibrary(tidyverse) d_sf_municipio \u0026lt;- sf::st_read(\u0026quot;shp/35MUE250GC_SIR.shp\u0026quot;, quiet = TRUE) dplyr::glimpse(d_sf_municipio) # Observations: 645 # Variables: 3 # $ NM_MUNICIP \u0026lt;fctr\u0026gt; CAIUÁ, CASTILHO, DRACENA, ESTRELA DO NORTE, EUCLIDES D... # $ CD_GEOCMU \u0026lt;fctr\u0026gt; 3509106, 3511003, 3514403, 3515301, 3515350, 3528700, ... # $ geometry \u0026lt;simple_feature\u0026gt; MULTIPOLYGON (((-51.8600105..., MULTIPOLYGON... Observe que temos três colunas na base de dados, nome do município, código do município e geometry. Essa terceira é do tipo simple_feature e carrega objetos do tipo MULTIPOLYGON. Ou seja, um objeto lido pelo sf nada mais é do que um data.frame que tem uma coluna especial, capaz de guardar objetos mais complexos, como polígonos.\nChamamos esse tipo de coluna especial de list-column. Quem faz nossos cursos avançados acaba aprendendo esses conceitos a partir do aninhamento de objetos e utilização de algoritmos mais complexos usando o pacote purrr.\nAgora, queremos juntar essa base com nossos dados de comarcas, circunscrições e regiões.\nmuni_comarcas_completo \u0026lt;- readRDS(\u0026quot;muni_comarcas_completo.rds\u0026quot;) dplyr::glimpse(muni_comarcas_completo) # Observations: 645 # Variables: 9 # $ cod_municipio \u0026lt;int\u0026gt; 6504, 6829, 6506, 6508, 6808, 6511, 6915, 6515... # $ comarca \u0026lt;chr\u0026gt; \u0026quot;ADAMANTINA\u0026quot;, \u0026quot;ADAMANTINA\u0026quot;, \u0026quot;AGUAI\u0026quot;, \u0026quot;AGUAS DE... # $ municipio \u0026lt;chr\u0026gt; \u0026quot;ADAMANTINA\u0026quot;, \u0026quot;MARIAPOLIS\u0026quot;, \u0026quot;AGUAI\u0026quot;, \u0026quot;AGUAS DE... # $ tipo \u0026lt;chr\u0026gt; \u0026quot;comarca\u0026quot;, \u0026quot;municipio\u0026quot;, \u0026quot;comarca\u0026quot;, \u0026quot;comarca\u0026quot;, ... # $ circunscricao \u0026lt;chr\u0026gt; \u0026quot;Tupã\u0026quot;, \u0026quot;Tupã\u0026quot;, \u0026quot;São João da Boa Vista\u0026quot;, \u0026quot;Ampa... # $ entrancia \u0026lt;chr\u0026gt; \u0026quot;Entrância Inicial\u0026quot;, \u0026quot;Entrância Inicial\u0026quot;, \u0026quot;Ent... # $ num_circunscricao \u0026lt;chr\u0026gt; \u0026quot;30ª CJ\u0026quot;, \u0026quot;30ª CJ\u0026quot;, \u0026quot;50ª CJ\u0026quot;, \u0026quot;54ª CJ\u0026quot;, \u0026quot;54ª C... # $ num_regiao \u0026lt;chr\u0026gt; \u0026quot;5ª RAJ\u0026quot;, \u0026quot;5ª RAJ\u0026quot;, \u0026quot;4ª RAJ\u0026quot;, \u0026quot;4ª RAJ\u0026quot;, \u0026quot;4ª RA... # $ regiao \u0026lt;chr\u0026gt; \u0026quot;Presidente Prudente\u0026quot;, \u0026quot;Presidente Prudente\u0026quot;, ... Para isso, vamos primeiro arrumar os nomes de d_sf_municipio e depois usar dplyr::inner_join(), assim:\nd_sf_municipio \u0026lt;- d_sf_municipio %\u0026gt;% # deixa os nomes das colunas minusculos janitor::clean_names() %\u0026gt;% # tira os acentos dplyr::mutate(municipio = abjutils::rm_accent(nm_municip)) %\u0026gt;% # bases unidas jamais serão vencidas! dplyr::inner_join(muni_comarcas_completo, \u0026quot;municipio\u0026quot;) dplyr::glimpse(d_sf_municipio) # Observations: 645 # Variables: 12 # $ nm_municip \u0026lt;fctr\u0026gt; CAIUÁ, CASTILHO, DRACENA, ESTRELA DO NORTE, E... # $ cd_geocmu \u0026lt;fctr\u0026gt; 3509106, 3511003, 3514403, 3515301, 3515350, ... # $ municipio \u0026lt;chr\u0026gt; \u0026quot;CAIUA\u0026quot;, \u0026quot;CASTILHO\u0026quot;, \u0026quot;DRACENA\u0026quot;, \u0026quot;ESTRELA DO NO... # $ cod_municipio \u0026lt;int\u0026gt; 6606, 6629, 6663, 6678, 6679, 6826, 6843, 6857... # $ comarca \u0026lt;chr\u0026gt; \u0026quot;PRESIDENTE EPITACIO\u0026quot;, \u0026quot;ANDRADINA\u0026quot;, \u0026quot;DRACENA\u0026quot;,... # $ tipo \u0026lt;chr\u0026gt; \u0026quot;municipio\u0026quot;, \u0026quot;municipio\u0026quot;, \u0026quot;comarca\u0026quot;, \u0026quot;municipi... # $ circunscricao \u0026lt;chr\u0026gt; \u0026quot;Presidente Venceslau\u0026quot;, \u0026quot;Andradina\u0026quot;, \u0026quot;Dracena\u0026quot;... # $ entrancia \u0026lt;chr\u0026gt; \u0026quot;Entrância Inicial\u0026quot;, \u0026quot;Entrância Final\u0026quot;, \u0026quot;Entrâ... # $ num_circunscricao \u0026lt;chr\u0026gt; \u0026quot;28ª CJ\u0026quot;, \u0026quot;37ª CJ\u0026quot;, \u0026quot;29ª CJ\u0026quot;, \u0026quot;27ª CJ\u0026quot;, \u0026quot;28ª C... # $ num_regiao \u0026lt;chr\u0026gt; \u0026quot;5ª RAJ\u0026quot;, \u0026quot;2ª RAJ\u0026quot;, \u0026quot;5ª RAJ\u0026quot;, \u0026quot;5ª RAJ\u0026quot;, \u0026quot;5ª RA... # $ regiao \u0026lt;chr\u0026gt; \u0026quot;Presidente Prudente\u0026quot;, \u0026quot;Araçatuba\u0026quot;, \u0026quot;President... # $ geometry \u0026lt;simple_feature\u0026gt; MULTIPOLYGON (((-51.8600105..., MUL... Se você quiser fazer um gráfico feinho só pra saber o que está rolando, use plot():\nplot(d_sf_municipio[, c(\u0026quot;num_regiao\u0026quot;, \u0026quot;geometry\u0026quot;)]) Parte 2: agrupando os municípios Aqui é onde a mágica acontece! Para unir os polígonos do mapa, por incrível que pareça, utilizaremos o pacote dplyr. O autor do pacote sf, Edzer Pebesma, teve a incrível ideia de criar um método S3 para objetos do tipo sf (como é nosso caso), já fazendo algumas operações para nós. Na prática, o pacote estende e reimplementa mais de 20 funções provenientes do dplyr. Veja ?sf::dplyr para detalhes.\nNo nosso caso, vamos usar group_by() (ou sf::group_by.sf()) e summarise() (ousf::summarise.sf()):\nd_sf_comarca \u0026lt;- d_sf_municipio %\u0026gt;% dplyr::group_by(comarca) %\u0026gt;% dplyr::summarise() %\u0026gt;% dplyr::ungroup() dplyr::glimpse(d_sf_comarca) # Observations: 319 # Variables: 2 # $ comarca \u0026lt;chr\u0026gt; \u0026quot;ADAMANTINA\u0026quot;, \u0026quot;AGUAI\u0026quot;, \u0026quot;AGUAS DE LINDOIA\u0026quot;, \u0026quot;AGUDOS\u0026quot;, \u0026quot;... # $ geometry \u0026lt;simple_feature\u0026gt; MULTIPOLYGON (((-50.9930145..., MULTIPOLYGO... Simples assim! Note que agora temos 319 objetos, que é exatamente o número de comarcas. Se quiser adicionar mais variáveis, basta incluí-las no summarise():\nd_sf_comarca \u0026lt;- d_sf_municipio %\u0026gt;% dplyr::group_by(comarca) %\u0026gt;% # a entrancia identifica o quão grande/relevante é uma comarca dplyr::summarise(entrancia = dplyr::first(entrancia)) %\u0026gt;% dplyr::ungroup() Para criar d_sf_circunscricao e d_sf_regiao utilizamos o mesmo procedimento.\nParte 3: montando os gráficos Agora digamos que você tenha essa lista de mapas\nmapas \u0026lt;- list( d_sf_municipio, d_sf_comarca, d_sf_circunscricao, d_sf_regiao) E você quer desenhar mapas com esses títulos\ntitulos \u0026lt;- c( \u0026quot;Municípios\u0026quot;, \u0026quot;Comarcas\u0026quot;, \u0026quot;Circunscrições judiciárias\u0026quot;, \u0026quot;Regiões Admnistrativas\u0026quot;) Vamos usar o purrr::map2() para montar nossos gráficos em ggplot2 e guardar numa lista. Internamente, utilizaremos o geom_sf(), uma extensão do ggplot2 criada para tratar objetos do pacote sf. Um código minimalista seria\ngraficos \u0026lt;- purrr::map2(mapas, titulos, ~{ ggplot(.x) + # cria o ggplot geom_sf() + # desenha o mapa ggtitle(.y) + # adiciona o título theme_minimal() # deixa o fundo mais bonitinho }) Você pode usar a função gridExtra::grid.arrange() para juntar essa lista de gráficos em um gráfico só, rodando\ngridExtra::grid.arrange(grobs = graficos) O resultado final é o gráfico abaixo.\nCoisa mais linda!\nWrap-up O pacote sf coloca a análise espacial no tidyverse Use st_read() para ler shapefiles (arquivos SHP) O objeto resultante é um data.frame com uma coluna geometry especial que guarda os polígonos. Várias funções do dplyr foram reimplementadas para funcionar com o sf. summarise, por exemplo, faz a união de polígonos, o que resolve nosso problema. Você pode usar geom_sf() para fazer gráficos com sf usando o poderoso ggplot2. É isso pessoal. Espero que seja tão life saver para vocês como foi para mim. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2017-11-23-union-sf/","tags":["sf","tidyverse"],"title":"Unindo mapas: the tidy way"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" O que fazer quando precisamos que o nosso script rode mais rápido? Geralmente a primeira ideia que temos é otimizar o código: reduzir a quantidade de laços, diminuir o tamanho das estruturas, utilizar programação paralela, etc… Mas quando se trata de R, temos a possibilidade de aumentar a velocidade do código sem alterar praticamente nada da sua estrutura.\nNeste post darei uma introdução básica ao pacote Rcpp, uma ferramenta que nos permite rodar código em C++ de dentro do R.\nO básico C++ é uma linguagem de programação muito famosa e versátil. Ela têm elementos de programação genérica, imperativa e orientada a objeto e também fornece uma interface para manipulação de memória de baixo nível.\nUma característica interessante do C++ é que ele é extremamente veloz. Diferentemente do R, ela é uma linguagem compilada, com tipagem estática e que não fornece tantas abstrações de operações, permitindo que seu código execute com eficiência incrível.\nPara explorar os benefícios que o C++ pode trazer para o seu código R, instale e carregue o pacote Rcpp com os comandos abaixo:\ninstall.packages(\u0026quot;Rcpp\u0026quot;) library(Rcpp) Vejamos agora um exemplo simples de como chamar código C++ do R. O jeito mais fácil de fazer isso é através da função cppFunction(): ela recebe uma string que será interpretada como uma função em C++.\nadicao_r \u0026lt;- function(x, y, z) { sum = x + y + z return(sum) } cppFunction( \u0026quot;int adicao_c(int x, int y, int z) { int sum = x + y + z; return sum; }\u0026quot;) adicao_r(1, 2, 3) #\u0026gt; [1] 6 adicao_c(1, 2, 3) #\u0026gt; [1] 6 Como podemos observar no exemplo acima, ambas as funções têm o mesmo comportamento apesar de algumas diferenças superficiais. Note como temos sempre que declarar os tipos das variáveis em C++! Usando a palavra-chave int deixamos claro para o compilador que uma variável terá o tipo inteiro e até mesmo que uma função deve retornar um valor de tipo inteiro. Outra coisa que é importante lembrar é que precisamos colocar um ponto-e-vírgula após cada comando C++.\nVetores Normalmente o C++ teria diferenças enormes em relação ao R no seu tratamento de vetores, mas para a nossa sorte o Rcpp nos disponibiliza uma biblioteca de estruturas que abstraem o comportamento do R. No exemplo a seguir temos uma função que recebe um número e vetor numérico, computa a distância euclidiana entre o valor e o vetor e retorna um vetor numérico como saída.\ndist_r \u0026lt;- function(x, ys) { sqrt((x - ys) ^ 2) } cppFunction( \u0026quot;NumericVector dist_c(double x, NumericVector ys) { int n = ys.size(); NumericVector out(n); for(int i = 0; i \u0026lt; n; i++) { out[i] = sqrt(pow(ys[i] - x, 2.0)); } return out; }\u0026quot;) dist_r(10, 20:25) #\u0026gt; [1] 10 11 12 13 14 15 dist_c(10, 20:25) #\u0026gt; [1] 10 11 12 13 14 15 A estrutura NumericVector abstrai um vetor numérico do R, nos permitindo trabalhar com ele de uma maneira mais familiar. Com o método .size() obtemos o seu comprimento (equivalente a length()) e podemos declarar um novo com o construtor NumericVector nome(comprimento);. O único ponto de diferença fundamental entre o C++ e o R é que o primeiro não possui operações vetorizadas propriamente ditas, fazendo com que precisemos usar laços para toda e qualquer iteração.\nVelocidade máxima Certos aspectos da filosofia do R o tornam uma linguagem extremamente versátil, mas isso vem com certas desvantagens. Alguns pontos em que a performance do R deixa a desejar são laço não vetorizáveis (por uma iteração depender da anterior), funções recursivas e estruturas de dados complexas.\nNestas e em muitas outras situações, usar C++ pode ser extremamente vantajoso. No exemplo a seguir veremos a diferença entre a performance de um laço em C++ e um em R; note que esta nem é uma das 3 situações listadas no parágrafo anterior e que mesmo assim o código em C++ é 6 vezes mais rápido.\nsoma_r \u0026lt;- function(v) { total \u0026lt;- 0 for (e in v) { if (e \u0026lt; 0) { total = total - e } else if (e \u0026gt; 0.75) { total = total + e/2 } else { total = total + e } } return(total) } cppFunction( \u0026quot;double soma_c(NumericVector v) { double total = 0; for (int i = 0; i \u0026lt; v.size(); i++) { if (v[i] \u0026lt; 0) { total -= v[i]; } else if (v[i] \u0026gt; 0.75) { total += v[i]/2; } else { total += v[i]; } } return(total); }\u0026quot;) v \u0026lt;- runif(100000, -1, 1) microbenchmark::microbenchmark(soma_r(v), soma_c(v)) #\u0026gt; Unit: milliseconds #\u0026gt; expr min lq mean median uq max neval #\u0026gt; soma_r(v) 6.105048 6.436608 6.911819 6.718456 7.183266 11.610624 100 #\u0026gt; soma_c(v) 1.045805 1.063956 1.161585 1.097920 1.210052 1.955702 100 Obs.: Os símbolos += e -= são equivalentes a a = a +/- b, já o símbolo ++ é equivalente a a = a + 1.\nConclusão Com o pacote Rcpp, podemos rodar código em C++ de dentro do próprio R. Através dessa técnica conseguimos otimizar nosso código ou mesmo ter acesso a estruturas de dados complexas disponibilizadas pelo C++.\nPara saber mais sobre o assunto, dê uma olhada no tutorial escrito por Hadley Wickham no livro Advanced R. Também recomendo a própria página do Rcpp e sua extensa galeria de exemplos.\nP.S.: Se você quiser o código completo deste tutorial, disponibilizei ele em um Gist. Além disso, também escrevi uma versão em inglês deste post no meu blog pessoal. Abraços!\n","permalink":"https://blog.curso-r.com/posts/2017-11-23-introducao-rcpp/","tags":["rcpp"],"title":"Fazendo o R Voar: uma Introdução ao Rcpp"},{"author":["Gabriela Lima Borges"],"categories":["Tutoriais"],"contents":" Se você também quiser escrever um post como convidada, entre em contato a gente em contato@curso-r.com!\nPacote Reticulate Imagine se você pudesse aproveitar da quantidade e da variedade enorme de módulos e bibliotecas do Python no ambiente amigável do RStudio? Graças ao pacote reticulate isso é possível de uma maneira familiar para quem quem já é usuário do R.\nO reticulate é um pacote que proporciona a integração Python \u0026amp; R via R e pode ser bastante útil se você quiser fazer todas as análises no ambiente do Rstudio. Os módulos, classes e funções do Python importados podem ser utilizados como se fossem funções nativas do R.\nPara utilizar o pacote são necessárias a instalação do Python com versão superior a 2.7, a instalação dos módulos do Python que serão utilizados durante a análise e a instalação do pacote via install.packages().\n# install.packages(\u0026quot;reticulate\u0026quot;) library(reticulate) Quando for utilizada uma função do Python em um objeto do R, ele será convertido para seu formato equivalente do Python e vice-versa. Os tipos de conversões de objetos são explicitadas neste link.\n## Data frame do R é convertido em Dict do Python a \u0026lt;- r_to_py(mtcars) class(a) ## Dict do Python é convertido em List do R class(py_to_r(a)) Importando módulos A importação de módulos do Python para o R é bem simples. Basta usar a função import() do pacote reticulate e o nome do módulo que você quer importar, em seguida guardar isso em um objeto no R.\nnp \u0026lt;- import(\u0026quot;numpy\u0026quot;) pandas \u0026lt;- import(\u0026quot;pandas\u0026quot;) os \u0026lt;- import(\u0026quot;os\u0026quot;) Use a função py_module_available() pra checar se um módulo do Python está disponível no seu computador.\npy_module_available(\u0026quot;matplotlib\u0026quot;) As funções que estão dentro de módulos ou classes do Python podem ser acessadas utilizando o operador $:\nos$getcwd() np$abs(-1) E podemos misturar funções do R e do python:\nlibrary(magrittr) rnorm(10) %\u0026gt;% np$abs() %\u0026gt;% np$cumproduct() %\u0026gt;% plot() Exemplo Para ilustrar a integração Python \u0026amp; R via R, vamos usar o Astropy, que é uma biblioteca do Python desenvolvida para a astronomia. Vamos tentar reproduzir esse exemplo que foi implementado no python. Primeiro, precisamos instalar o módulo:\npip install astropy No ambiente do RStudio, vamos importar os módulos do Astropy que usaremos.\ndown \u0026lt;- import(\u0026quot;astropy.utils.data\u0026quot;) fits \u0026lt;- import(\u0026quot;astropy.io.fits\u0026quot;) Em seguida, vamos baixar a imagem em arquivo FITS e verificar se está tudo ok usando a função info do Python que está no pacote Astropy.io.fits.\nurl \u0026lt;- \u0026quot;https://astropy.stsci.edu/data/tutorials/FITS-images/HorseHead.fits\u0026quot; im.file \u0026lt;- down$download_file(url, cache=TRUE) fits$info(im.file) Filename: ~/.astropy/cache/download/py2/2c9202ae878ecfcb60878ceb63837f5f No. Name Ver Type Cards Dimensions Format 0 PRIMARY 1 PrimaryHDU 161 (891, 893) int16 1 er.mask 1 TableHDU 25 1600R x 4C [F6.2, F6.2, F6.2, F6.2] Agora vamos transformar nossa imagem em uma matriz de pixels e plotar com a função image do R.\nim.data \u0026lt;- fits$getdata(im.file) im.data %\u0026gt;% t() %\u0026gt;% image(col = gray(seq(0, 1, length = 256))) ","permalink":"https://blog.curso-r.com/posts/2017-11-20-reticulate/","tags":["reticulate","python"],"title":"Pacote reticulate"},{"author":["Daniel"],"categories":["Tutoriais"],"contents":" Matrizes esparsas são matrizes em que a maior parte dos elementos é igual a zero. Matrizes dessa forma surgem em diversos problemas relacionados a Machine Learning e análise de dados.\nPor exemplo, é comum em text mining representar os documentos usando o chamado Bag of Words. Bag of Words nada mais é do que listar as palavras que aparecem em todos os documentos e em seguida criar uma matriz em que cada linha é um documento e cada coluna é uma palavra que foi listada anteriormente. Cada elemento \\((i,j)\\) desssa matriz é 1 se a palavra \\(j\\) aparace no documento \\(i\\) e 0 caso contrário. Naturalmente, o número de palavras que podem aparecer é muito maior do que o número de palavras que de fato aparecem em um documento, por isso a maioria dos elementos dessa matriz será 0.\nMatrizes esparsas também aparecem muito em problemas de recomendação. Nesse tipo de aplciação representamos as transações em uma matriz em que cada linha é um cliente e cada coluna um produto que ele poderia ter comprado. Para recomendar filmes no Netflix, por exemplo, cada linha seria um cliente e cada coluna um filme que está no catálogo do Netflix. Em seguida marcaríamos cada elemento \\((i,j)\\) dessa matriz com 1 se o cliente \\(i\\) assistiu o filme \\(j\\) e 0 caso contrário. Como o catálogo de filmes é muito grande, a mairoia dos elementos dessa matriz será 0.\nEssa pergunta do Quora tem mais algumas aplicações importantes de matrizes esparsas.\nNote que nos problemas que eu mencionei, encontramos dimensões muito altas. O número de palavras distintas em um conjunto de documentos pode facilmente passar de 20.000. O número de filmes no catálogo do netflix pode passar de 100.000. Agora vamos definir uma matriz como esta no R da forma usual. Vou preenchê-la aleatoriamente com 0’s e 1’s, sendo 1’s aproximadamente 1%. Considere que essa matriz seria utilizada em um problema de classificação de textos com 1 milhão de documentos com apenas 500 palavras distintas. Veja que aqui estou reduzindo bastante o número de palavras possíveis, na prática esse número é muito maior.\nnrow \u0026lt;- 1e6 ncol \u0026lt;- 500 x \u0026lt;- matrix(sample(c(0,1), size = nrow*ncol, replace = TRUE,prob = c(0.99, 0.1)), nrow = nrow, ncol = ncol) Se você tiver um computador com bastante RAM, talvez consiga rodar isso, mas provavelmente você terá um erro do tipo Error: cannot allocate vector of size 74.5 Gb.\nDe fato, essa matriz ocupa bastante memória:\npryr::object_size(x) #\u0026gt; 4 GB Será que existe uma forma mais eficiente de representar essa matriz na memória do computador? A resposta é sim! E no R vamos usar o pacote Matrix.\nExistem diversas formas de transformar a matriz x em uma matriz esparsa, a forma mais simples é:\nlibrary(Matrix) x_s \u0026lt;- Matrix(x) pryr::object_size(x_s) #\u0026gt; 550 MB Ou seja, a matriz esparsa ocupa quase 1/8 menos memória do que a matriz densa. A maioria dos métodos para matrizes no R estão também implementados para matrizes esparsas. Isso quer dizer que você pode fazer x*y, x+y, x/y, x%*%y, x[1,1], etc. como se fossem matrizes normais. Na prática o pacote Matrix representa as matrizes esparsas internamente de uma forma muito mais inteligente, sem gastar memória com os valores nulos.\nUma outra grande vantagem é que muitos pacotes possuem implementações mais eficientes (tanto em tempo de execução quanto em memória utilizada) para matrizes esparsas, por exemplo o glmnet muito usado para fazer regressão do tipo LASSO. O recommenderlab que implementa alguns algoritmos de recomendação também é inteiramente baseado em matrizes esparsas. O pacote text2vec que implementa algoritmos como GloVe também usa muito esse tipo de matrizes.\nVale lembrar que na maioria das vezes você possui uma base transacional que precisa ser representada como uma matriz. Algo mais ou menos assim:\nbd \u0026lt;- data.frame( cliente = c(1,1,1,2,2,3,3,4,5,6,7,7,8,8,8,8,9,9,9,9), itens = sample(1:50, 20) ) cliente itens 1 17 1 30 1 7 2 14 2 42 3 49 3 4 4 24 5 16 6 10 7 9 7 41 8 6 8 47 8 38 8 21 9 37 9 35 9 11 9 31 Nesse caso, faz mais sentido criar a matriz esparsa usando a função sparseMatrix. Assim, você só especifica as coordenadas da matriz que têm algum 1.\nlibrary(Matrix) sparseMatrix(bd$cliente, bd$itens) ## 9 x 49 sparse Matrix of class \u0026quot;ngCMatrix\u0026quot; ## ## [1,] . . . . . . | . . . . . . . . . | . . . . . . . . . . . . | . . . . . . . ## [2,] . . . . . . . . . . . . . | . . . . . . . . . . . . . . . . . . . . . . . ## [3,] . . . | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [4,] . . . . . . . . . . . . . . . . . . . . . . . | . . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . . . . | . . . . . . . . . . . . . . . . . . . . . ## [6,] . . . . . . . . . | . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [7,] . . . . . . . . | . . . . . . . . . . . . . . . . . . . . . . . . . . . . ## [8,] . . . . . | . . . . . . . . . . . . . . | . . . . . . . . . . . . . . . . ## [9,] . . . . . . . . . . | . . . . . . . . . . . . . . . . . . . | . . . | . | ## ## [1,] . . . . . . . . . . . . ## [2,] . . . . | . . . . . . . ## [3,] . . . . . . . . . . . | ## [4,] . . . . . . . . . . . . ## [5,] . . . . . . . . . . . . ## [6,] . . . . . . . . . . . . ## [7,] . . . | . . . . . . . . ## [8,] | . . . . . . . . | . . ## [9,] . . . . . . . . . . . . Outra função importante é a sparse.model.matrix. Ela é equivalente à função model.matrix mas cria uma matriz de modelo esparsa o que pode ser útil quando você tem um fator que possui muitos níveis no seu modelo. A vignette Sparse Model Matrices fala sobre isso.\nTambém é possível programar em Rcpp usando matrizes esparsas usando o RcppArmadillo, veja esse exemplo para mais detalhes.\nPara saber mais leia as vignettes do pacote Matrix. Em especial, vale a pena ler as seguintes 2nd Introduction to the Matrix Package.\nPS: a inspiração para esse texto foi esse post de 2011 do John Myles White\n","permalink":"https://blog.curso-r.com/posts/2017-11-16-matrizes-esparsas/","tags":["estatística"],"title":"Matrizes Esparsas no R"},{"author":["Caio"],"categories":["Tutoriais"],"contents":" Nem sempre os dados que precisamos para uma análise podem ser encontrados em uma base consolidada. Muitas vezes as informações que queremos não estão imediatamente disponíveis e precisam ser coletadas com o tempo através de um processo lento e monótono.\nImagine, por exemplo, que quiséssemos baixar os dados meteorológicos das maiores cidades do mundo a cada 12 horas para uma análise sobre previsões do tempo. Um programador desavisado talvez criasse alarmes em seu relógio e para baixar as tabelas necessárias quando eles tocassem.\nMas isso não parece uma boa estratégia, certo?\nDarkSky Para demonstrar uma alternativa a este método, vamos usar um serviço de previsões do tempo chamado DarkSky. Esta plataforma ficou conhecida recentemente pela sua precisão incrível e pelo seu aplicativo extremamente bem feito, mas uma coisa que poucos sabem é que a DarkSky também disponibiliza uma API para qualquer um interessado em dados meteorológicos.\nPara a nossa sorte, o hrbrmstr já criou uma interface em R para essa API que pode ser instalada facilmente com o comando abaixo:\n# install.packages(\u0026quot;devtools\u0026quot;) devtools::install_github(\u0026quot;hrbrmstr/darksky\u0026quot;) Depois de instalado o pacote, vá para o portal do desenvolvedor da DarkSky, crie uma conta e obtenha uma chave secreta para acessar a API.\nSys.setenv(DARKSKY_API_KEY = \u0026quot;SUA CHAVE SECRETA\u0026quot;) Baixando os dados O primeiro passo da nossa análise é determinar as latitudes e longitudes das maiores cidades do mundo para que possamos pedir as previsões do tempo destas coordenadas.\nCom o pacote maps podemos fazer isso de uma maneira bastante simples:\nforecasts \u0026lt;- maps::world.cities %\u0026gt;% dplyr::as_tibble() %\u0026gt;% dplyr::filter(pop \u0026gt; 2000000) %\u0026gt;% dplyr::rename(country = country.etc) %\u0026gt;% dplyr::select(name, country, lat, long) %\u0026gt;% dplyr::mutate( currently = list(\u0026quot;\u0026quot;), hourly = list(\u0026quot;\u0026quot;), daily = list(\u0026quot;\u0026quot;)) No trecho de código acima pegamos todas as cidades com mais de 2 milhões de habitantes (juntamente com suas localizações) da base maps::world.cities. As últimas 4 linhas são uma preparação para a obtenção das previsões do tempo que faremos logo a seguir:\nfor (i in 1:nrow(forecasts)) { forecast \u0026lt;- darksky::get_current_forecast(forecasts$lat[i], forecasts$long[i]) forecasts$currently[i] \u0026lt;- forecast$currently %\u0026gt;% dplyr::as_tibble() %\u0026gt;% list() forecasts$hourly[i] \u0026lt;- forecast$hourly %\u0026gt;% dplyr::as_tibble() %\u0026gt;% list() forecasts$daily[i] \u0026lt;- forecast$daily %\u0026gt;% dplyr::as_tibble() %\u0026gt;% list() } Na coluna currently guardamos o estado meteorológico atual da cidade, enquanto em hourly e daily colocamos as previsões do tempo para as próximas 48 horas e para os próximos 7 dias respectivamente. Agora só resta salvar isso tudo em um arquivo RDS:\nfile \u0026lt;- lubridate::now() %\u0026gt;% lubridate::ymd_hms() %\u0026gt;% as.character() %\u0026gt;% stringr::str_replace_all(\u0026quot;[-: ]\u0026quot;, \u0026quot;_\u0026quot;) %\u0026gt;% stringr::str_c(\u0026quot;.rds\u0026quot;) readr::write_rds(forecasts, stringr::str_c(\u0026quot;DIRETÓRIO DOS ARQUIVOS\u0026quot;, file)) cronR Perceba que o script descrito na seção anterior não depende de nenhum input do programador e pode ser rodado automaticamente. Agora só nos resta automatizar essa execução, tarefa que realizaremos com o pacote cronR. Ele funciona nas plataformas Mac e Linux e pode ser instalado com o comando a seguir:\ninstall.packages(\u0026quot;cronR\u0026quot;) Esse pacote nos permite agendar a execução de qualquer comando para que ela ocorra a cada tantos minutos/horas/dias/… Certifique-se de que você está em uma máquina ou servidor que não será desligado, verifique se o cron daemon está ativo e agende a execução do nosso script:\ncmd \u0026lt;- cronR::cron_rscript(\u0026quot;CAMINHO PARA SCRIPT\u0026quot;) cronR::cron_add(cmd, \u0026quot;daily\u0026quot;, \u0026quot;12AM\u0026quot;) cronR::cron_add(cmd, \u0026quot;daily\u0026quot;, \u0026quot;12PM\u0026quot;) E isso é tudo! No meu caso, agendei o script para executar diariamente às 00:00 e às 12:00, mas a frequência das chamadas fica a seu critério (lembrando apenas que o plano gratuito da API do DarkSky só permite 1000 chamadas por dia). Para saber mais sobre como mudar a frequência das execuções, consulte a documentação do cronR.\nConclusão Como vimos, não é difícil agendar a execução de um script. A maior parte do nosso trabalho é criar um código que funcione independentemente do programador (por exemplo nomeando os arquivos gerados automaticamente), mas depois disso é só chamar cronR::cron_rscript() e cronR::cron_add().\nNo meu próximo post usarei os dados baixados com esse tutorial para uma análise sobre previsões meteorológicas, então fique ligado na parte dois!\nP.S.: Se você quiser o código completo do meu arquivo get_forecasts.R, disponibilizei ele como um Gist. Além disso, também disponibilizei uma versão em inglês deste post no meu blog pessoal. Abraços!\n","permalink":"https://blog.curso-r.com/posts/2017-11-13-tutorial-cronr/","tags":["cron"],"title":"Usando o cronR para Agendar Scripts"},{"author":["Julio"],"categories":["análises"],"contents":" Voltando aos saudosos captchas. Demorei para fazer esse post pois estava esperando o lançamento do curso de redes neurais convolucionais do Andrew Ng. O curso foi muito bom, valeu à pena! E, como prometido, vamos agora trabalhar com modelagem dos captchas.\nObjetivo Nosso objetivo é aprender a aplicar a operação da convolução em imagens, replicando o modelo já ajustado dos captchas. O jeito que fazemos para ajustar o modelo ficará para um próximo post.\nPré-requisitos Na nossa jornada, utilizaremos o pacote decryptr e teremos como o base o captcha da Receita Federal. Para baixar um captcha e plotar na sua tela, rode o código abaixo. Utilizaremos o caminho do arquivo em arq várias vezes no decorrer do post. Instale também o pacote decryptrModels para carregar o modelo ajustado do captcha da receita.\n# devtools::install_github(\u0026quot;decryptr/decryptr\u0026quot;) # devtools::install_github(\u0026quot;decryptr/decryptrModels\u0026quot;) library(decryptr) arq \u0026lt;- captcha_download_rfb(dest = \u0026quot;img\u0026quot;) arq %\u0026gt;% read_captcha() %\u0026gt;% purrr::pluck(1) %\u0026gt;% plot() Também precisaremos do keras, um pacote maravilhoso feito pela turma do RStudio, com contribuições do Daniel Falbel. Não esqueça de fazer o tensorflow funcionar na sua máquina! Esse post do Athos Damiani pode ajudar.\n# install.packages(\u0026quot;keras\u0026quot;) library(keras) Para os retoques finais nas imagens, vamos usar o pacote magick. Meu intuito inicial era usar a função image_convolve() desse pacote, mas infelizmente essa operação é limitada. Acabei usando apenas funções para juntar imagens e fazer gifs. Se quiser mais detalhes sobre o magick, veja o excelente post A kind of magick, feito pelo William Amorim.\n# install.packages(\u0026quot;magick\u0026quot;) library(magick) O que é convolução, afinal? Convolução é uma técnica usada há muito tempo na área de visão computacional para aplicar filtros em imagens e detectar padrões. Basicamente, o que ela faz é calcular um novo valor para um pixel da imagem com base nos pixels da vizinhança. Por exemplo, você pode fazer com que o pixel \\((i,j)\\) da sua imagem seja atualizado pela soma ponderada dos valores dos pixels na vizinhança.\nSe você não está entendendo nada, Veja o vídeo abaixo para entender o que são pixels. No nosso caso, teremos uma matriz com valores entre zero e um, sendo zero = preto e um = branco.\nUma forma esperta de fazer essa soma ponderada é criando uma matriz de pesos: dessa forma, você não precisa ficar procurando os pontos da vizinhança. Para cada ponto \\((i,j)\\), você pega o subset da matriz de vizinhança, multiplica pontualmente pela matriz de pesos e soma todos os valores. Isso é exatamente o que a convolução faz.\nDaqui em diante, chamaremos essa matriz de pesos de kernel. Considere esse exemplo 3x3:\nkern_horizontal \u0026lt;- rbind(c(-1,-1,-1), c( 0, 0, 0), c( 1, 1, 1)) kern_horizontal E considere essa imagem super complexa:\nNa prática, essa imagem é isso aqui (tirei algumas linhas e colunas):\nemoji \u0026lt;- load_image(\u0026quot;../../staticimages/posts/conteudo/captcha-conv/emoji3.webp\u0026quot;)[,,1] round(emoji, 1)[1:10, 1:12] Tome por exemplo o ponto \\((i,j) = (12,16)\\). A vizinhança 3x3 em torno desse ponto é dada por\nemoji[12 + (-1):1, 16 + (-1):1] A operação de convolução é feita da seguinte forma:\nsum(emoji[12 + (-1):1, 16 + (-1):1] * kern_horizontal) Pronto, esse é o valor a ser colocado no ponto \\((i,j)\\). Fazemos isso para todos os outros pontos. Algumas dúvidas que podem rolar nesse ponto:\nQ: Mas os números não devem variar de 0 a 1?\nR: Não! Para visualizar a imagem, você poderia normalizar essas quantidades (por exemplo, dividindo pelo máximo). Mas quem disse que o resultado da sua operação precisa ser visualizável? O resultado pode até ser negativo. Sem problemas.\nPara visualização, por padrão os valores menores que zero são substituídos por zero (preto) e valores maiores que um são substituídos por um (branco).\nQ: Mas e no canto da imagem, o que fazemos?\nR: Nos cantos, você tem duas opções: 1) considerar apenas os pixels válidos, ou seja, pixels em que você consegue encaixar a matriz kernel inteira, resultando numa matriz de tamanho menor; ou 2) criar uma borda na imagem, preenchendo com zeros, para que toda a imagem fique com pixels válidos. Por isso que o keras disponibiliza as opções valid (apenas os válidos) e same (mantém a mesma dimensão).\nQ: E se a imagem for colorida?\nR: Boa pergunta! Se a imagem for colorida, você pode considerar um kernel diferente para cada cor, e depois você soma todos os valores. Mais pra frente, chamaremos as cores de canais, pois teremos muito mais do que 3 kernels.\nCom base nisso, montei um algoritmo que faz a conta para todos os pixels, já criando uma borda na imagem:\nconvolve \u0026lt;- function(img, kern) { # monta a bordinha na imagem. A borda deve ter (tamanho kernel) / 2, # de tamanho, arredondando para baixo pad \u0026lt;- floor(dim(kern)[1] / 2) img_pad \u0026lt;- matrix(0, nrow = nrow(img) + 2 * pad, ncol = ncol(img) + 2 * pad) img_pad[pad + 1:nrow(img), pad + 1:ncol(img)] \u0026lt;- img[,,1] # aplica a convolução nos pontos da imagem for (i in seq_len(nrow(img))) { for (j in seq_len(ncol(img))) { img[i, j, 1] \u0026lt;- sum(img_pad[i + 0:(2 * pad), j + 0:(2 * pad)] * kern) } } img[,,2] \u0026lt;- img[,,3] \u0026lt;- img[,,1] img } (desculpe aos amigos por usar for. Shame on me…)\nVoltando para nossa imagem agora. No nosso caso, o resultado fica assim:\n\u0026quot;../../staticimages/posts/conteudo/captcha-conv/emoji3.webp\u0026quot; %\u0026gt;% load_image() %\u0026gt;% convolve(kern_horizontal) %\u0026gt;% image_read() %\u0026gt;% plot() Ficou um pouco assustador, não? Essa matriz não foi escolhida por acaso. Ela serve para destacar padrões horizontais da imagem. Como a primeira linha é formada -1s e a última é formada por 1s, a matriz fica com valor alto se a parte de cima do pixel for preta e a parte de baixo for branca (grande * 1 + pequeno * (-1)). A parte destacada da imagem acabou sendo os olhos (pois temos maior concentração de pixels pretos ali), além das extremidades superior e inferior do rosto.\nCom esse kernel aqui (vertical), a parte destacada do rosto são as extremidades dos lados:\nkern_vertical \u0026lt;- rbind(c(-1, 0, 1), c(-1, 0, 1), c(-1, 0, 1)) kern_vertical \u0026quot;../../staticimages/posts/conteudo/captcha-conv/emoji3.webp\u0026quot; %\u0026gt;% load_image() %\u0026gt;% convolve(kern_vertical) %\u0026gt;% image_read() %\u0026gt;% plot() Aplicando nos captchas Não tem segredo! Basta reaplicar o que já vimos. Vou apenas introduzir uma nova função chamada add_bias(), que simplesmente adiciona uma constante numérica para a matriz. Isso pode auxiliar na visualização, pois controlamos melhor os valores que ficam dentro do intervalo [0,1]. Lá na frente você entenderá o porquê do “bias”.\nadd_bias \u0026lt;- function (x, b) x + b Esse é o resultado de adicionar o kernel vertical e bias de 0.8.\narq %\u0026gt;% load_image() %\u0026gt;% convolve(kern_vertical) %\u0026gt;% add_bias(.8) %\u0026gt;% image_read() %\u0026gt;% plot() Agora o kernel na horizontal. Note que identificamos padrões das linhas horizontais que tentam atrapalhar a visão das letras.\narq %\u0026gt;% load_image() %\u0026gt;% convolve(kern_horizontal) %\u0026gt;% add_bias(.8) %\u0026gt;% image_read() %\u0026gt;% plot() Colocando um após o outro, temos um resultado bem esquisito:\narq %\u0026gt;% load_image() %\u0026gt;% convolve(kern_horizontal) %\u0026gt;% convolve(kern_vertical) %\u0026gt;% add_bias(.7) %\u0026gt;% image_read() %\u0026gt;% plot() Também vou introduzir uma função chamada relu() aqui. ReLu significa Restricted Linear Unit e é uma função bem simples que zera tudo aquilo que é negativo e mantém tudo aquilo que é positivo. Assim, temos:\nrelu \u0026lt;- function(x) (x + abs(x)) / 2 relu(-1) relu( 3) Para visualização, essa função não serve para muita coisa, pois já fazemos a substituição de valores negativos por zero. No entanto, podemos fazer combos com a aplicação de várias convoluções. O resultado dos combos não seria possível somente com somas e multiplicações. Na prática, o que estou afirmando é que com a aplicação de convoluções, bias e ReLu, podemos montar operações não lineares para extrair componentes da imagem.\nOlhe o exemplo abaixo. Parece que consegui identificar bem as coisas que são inúteis na imagem. Isso pode ser útil… ou não.\narq %\u0026gt;% load_image() %\u0026gt;% # primeira convolucao convolve(kern_horizontal) %\u0026gt;% add_bias(-.25) %\u0026gt;% relu() %\u0026gt;% # segunda convolucao convolve(kern_vertical) %\u0026gt;% add_bias(.1) %\u0026gt;% image_read() %\u0026gt;% plot() Isso tudo nos leva a pensar: será que eu consigo pensar em kernels que me ajudem a identificar as letras de uma forma razoável?\nE se pudermos usar kernels treinados? A revolução da convolução aparece quando conseguimos obter kernels úteis por métodos estatísticos. Podemos pensar na matriz abaixo\n\\[ W = \\left[\\begin{array}{ccccc} w_{11} \u0026amp; w_{12} \u0026amp; w_{13} \u0026amp; w_{14} \u0026amp; w_{15} \\\\ w_{21} \u0026amp; w_{22} \u0026amp; w_{23} \u0026amp; w_{24} \u0026amp; w_{25} \\\\ w_{31} \u0026amp; w_{32} \u0026amp; w_{33} \u0026amp; w_{34} \u0026amp; w_{35} \\\\ w_{41} \u0026amp; w_{42} \u0026amp; w_{43} \u0026amp; w_{44} \u0026amp; w_{45} \\\\ w_{51} \u0026amp; w_{52} \u0026amp; w_{53} \u0026amp; w_{54} \u0026amp; w_{55} \\end{array}\\right] \\]\ne tentar encontrar os valores de \\(W\\) que minimizem alguma função de interesse. Podemos pensar que esses são os \\(\\beta\\)’s de uma regressão logística, e queremos encontrar os valores que minimizam uma Loss ou maximizam uma verossimilhança. Para ver mais sobre isso, recomendo o excelente post do Athos sobre a menor deep learning do mundo. Nós também podemos fazer vários \\(W\\) como esse, sendo que cada um extrai alguma coisa de importante da imagem.\nNosso super modelo de magia negra nada mais é do que isso: a aplicação consecutiva de convolve(), add_bias() e relu(), mas com pesos escolhidos a dedo (ou por um moedor de carne super-poderoso como o keras).\nAgora podemos ver nosso modelo atual da Receita Federal:\nm \u0026lt;- decryptrModels::read_model(\u0026quot;rfb\u0026quot;) m$model Model ____________________________________________________________________________________________________ Layer (type) Output Shape Param # ==================================================================================================== conv2d_4 (Conv2D) (None, 50, 180, 12) 312 ____________________________________________________________________________________________________ max_pooling2d_4 (MaxPooling2D) (None, 25, 90, 12) 0 ____________________________________________________________________________________________________ conv2d_5 (Conv2D) (None, 25, 90, 48) 14448 ____________________________________________________________________________________________________ max_pooling2d_5 (MaxPooling2D) (None, 12, 45, 48) 0 ____________________________________________________________________________________________________ conv2d_6 (Conv2D) (None, 12, 45, 96) 115296 ____________________________________________________________________________________________________ max_pooling2d_6 (MaxPooling2D) (None, 6, 22, 96) 0 ____________________________________________________________________________________________________ flatten_2 (Flatten) (None, 12672) 0 ____________________________________________________________________________________________________ dense_3 (Dense) (None, 32) 405536 ____________________________________________________________________________________________________ dropout_2 (Dropout) (None, 32) 0 ____________________________________________________________________________________________________ dense_4 (Dense) (None, 210) 6930 ____________________________________________________________________________________________________ reshape_2 (Reshape) (None, 6, 35) 0 ____________________________________________________________________________________________________ activation_2 (Activation) (None, 6, 35) 0 ==================================================================================================== Total params: 542,522 Trainable params: 542,522 Non-trainable params: 0 ____________________________________________________________________________________________________ O modelo aplica convolução 3 vezes consecutivas e faz algumas contas que não entendemos. Explico agora:\nconv2d_: são as convoluções. As aplicações de add_bias() e relu() estão escondidas aí dentro. max_pooling2d_: serve para simplificar a imagem. Isso ajuda a fazer computações mais rápido e ajuda a pegar mais relações entre partes da imagem, sem precisar mudar o tamanho dos kernels. dropout_: é utilizado para regularização. Serve para evitar que seu modelo quebre apenas o captcha que você tem na base, e não novos captchas que chegam. Na prática, o dropout joga fora uma parte dos \\(W\\) obtidos. Se você consegue prever coisas bem sem esses \\(W\\), isso significa que eles não são tão úteis assim. flatten_ e reshape_: não fazem nada demais, só reorganizam os parâmetros de matriz para um vetor ou de vetor para matriz. Isso é útil pois i) depois de aplicar os kernels, nós misturamos todos os parâmetros resultantes e ii) no final, precisamos prever 6 letras, então precisamos deixar as probabilidades numa matriz, como vimos no post anterior sobre captchas. dense_: são camadas de redes neurais comuns como as do post do Athos. NÃO ME ABANDONE AQUI!!! Se você não estiver entendendo direito, saiba apenas que a execução de um modelo de deep learning envolve somente\nPegar o input (imagem). Multiplicar (convoluir) por alguns pesos \\(W\\). Adicionar um viés (ou bias, ou intercepto) \\(b\\). Aplicar uma função não linear, por exemplo ReLu. Voltar para 2 várias vezes (o deep vem daí). Pegar os pesos finais e normalizar (usando, por exemplo, softmax) para obter probabilidades dos resultados. No nosso caso, repetimos o passo 2 três vezes, aplicando três convoluções seguidas.\nPrimeira convolução Para obter os valores de kernels ajustados pelo modelo, podemos usar a função get_weights() do keras. Nessa primeira parte, utilizamos 12 kernels 5x5.\nw \u0026lt;- keras::get_weights(m$model$layers[[1]])[[1]] w_list \u0026lt;- purrr::map(seq_len(dim(w)[4]), ~w[,,1,.x]) bias \u0026lt;- keras::get_weights(m$model$layers[[1]])[[2]] w_list[[1]] [,1] [,2] [,3] [,4] [,5] [1,] -0.00889198 0.04569587 0.11906113 0.08591988 -0.09028889 [2,] -0.05898214 0.20692091 -0.13479255 -0.15641896 -0.10511240 [3,] 0.02517573 -0.63352644 -3.81658459 -4.39883375 -1.05918467 [4,] -0.22003661 -1.80763698 -3.13373542 -1.73096466 -0.01640752 [5,] -0.02915078 -0.11879896 -0.07475707 0.06014036 0.15733875 Os doze valores de bias estimados pelo modelo (um para cada matriz) são dados por\nround(bias, 3) [1] 0.150 0.013 0.181 -0.275 0.179 0.040 -0.128 -0.036 0.030 0.042 0.201 0.043 Para cada um dos doze kernels, calculamos uma matriz convoluída. Esses os resultados que o modelo entende serem úteis para prever o captcha.\nO código abaixo aplica convolve(), add_bias() e relu() para todos os kernels. Para isso usamos o purrr. Se você não entende purrr, leia este maravilhoso post do Caio Lente.\nconv1 \u0026lt;- purrr::map2(w_list, bias, ~{ arq %\u0026gt;% load_image() %\u0026gt;% convolve(.x) %\u0026gt;% add_bias(.y) %\u0026gt;% relu() }) E como será que ficam essas imagens? Abaixo, temos o resultado da aplicação dos doze kernels. A maioria parece estar extraindo partes das letras. A sétima (posição (2,3)) parece estar pegando o ruído e a quarta parece guardar a imagem original.\nO gif animado abaixo mostra a aplicação do oitavo kernel da nossa lista. Com esse kernel dá pra pegar bem o padrão das letras, não é?\nNo próximo nível, vamos convoluir mais 48 kernels. Essa operação será feita com todos os doze filtros atuais, ou seja, é uma contaiada que não acaba mais. Para simplificar as contas e para permitir a obtenção de padrões diferentes, faz sentido simplificar a imagem. Para isso, usamos o max pooling.\nAplicando max pooling O max pooling simplesmente pega o pixel de maior valor dentro de uma janela. No caso, estamos usando uma janela 2x2 e aplicamos ela igualzinho convolução, só que ao invés de pegar a soma ponderada dos pixels, pegamos o pixel máximo. Outra diferença é que ao invés de andar o pixel de 1 em 1, andamos de 2 em 2. Assim cada janelinha é considerada apenas uma vez (esse é o conceito de strides, que não vou discutir aqui).\nMontei esse algoritmo que faz max pooling:\nmax_pool \u0026lt;- function(img) { # monta a matriz com metade da resolução x_new \u0026lt;- matrix(0.0, nrow = floor(nrow(img) / 2), ncol = floor(ncol(img) / 2)) # adiciona uma bordinha para o caso da matriz ter um número ímpar de pixels # por exemplo, se ela é 51x181, daria bug se não adicionar a bordinha img \u0026lt;- cbind(rbind(img[,,1], 0), 0) # percorre a matrix pegando o máximo das janelinhas for (i in 1:nrow(x_new)) { for (j in 1:ncol(x_new)) { x_new[i, j] \u0026lt;- max(img[i * 2 - 1 + 0:1, j * 2 - 1 + 0:1]) } } array(x_new, c(dim(x_new), 3)) } A aplicação da primeira convolução com max pooling é feita igual anteriormente:\nresult_conv1 \u0026lt;- purrr::map2(w_list, bias, ~{ arq %\u0026gt;% load_image() %\u0026gt;% convolve(.x) %\u0026gt;% add_bias(.y) %\u0026gt;% relu() %\u0026gt;% max_pool() }) No final, temos essas imagens com resolução 25x90 (as originais são 50x180).\nFicou bem parecido com o anterior!\nAo final da convolução, é como se tivéssemos uma nova imagem, menor e alterada, mas com 12 cores. Como não faz muito sentido pensar em 12 cores primárias, vamos chamá-las de canais.\nSegunda convolução Os parâmetros da segunda convolução são obtidos novamente pelo keras. Sugiro que você dê uma olhada nesses índices para entender o que exatamente estamos pegando.\nw2 \u0026lt;- keras::get_weights(m$model$layers[[3]])[[1]] bias2 \u0026lt;- keras::get_weights(m$model$layers[[3]])[[2]] dim(w2) [1] 5 5 12 48 Agora temos 12 * 48 kernels 5x5 a serem aplicados. Precisamos seguir essas operações:\nPara cada uma das 48 matrizes: Faça a convolução das 12 matrizes obtidos na convolução anterior pelos 12 kernels atuais e some os valores obtidos. Adicione o bias. Faça a ativação com ReLu. Aplique o max pooling. Logo, temos 2 laços. O código para fazer isso fica assim:\nresult_conv2 \u0026lt;- purrr::map(1:dim(w2)[4], ~{ kern \u0026lt;- w2[,,,.x] %\u0026gt;% plyr::alply(3, identity) %\u0026gt;% purrr::map(as.matrix) actual_bias \u0026lt;- bias2[[.x]] purrr::map2(result_conv1, kern, convolve) %\u0026gt;% purrr::reduce(magrittr::add) %\u0026gt;% add_bias(actual_bias) %\u0026gt;% relu() %\u0026gt;% max_pool() }) Plotamos os 48 resultados abaixo. Alguns resultados foram completamente zerados (eles devem ser úteis em outros captchas mais esquisitos), enquanto os demais pegam pedaços da imagem anterior que mal lembram o captcha original. A imagem da posição (4,1) é uma das únicas que mostra o captcha claramente. Isso mostra uma coisa comum do deep learning: quanto mais profundo vamos, menos entendemos o que de fato o modelo está fazendo. Recomendo fortemente a leitura desse blog do distill, que discute o assunto detalhadamente\nAgora temos 48 canais de uma imagem com dimensões 12x45. Vamos em frente.\nTerceira convolução A terceira convolução é feita de forma idêntica à segunda. A única diferença é que teremos no final 96 canais, pois estamos ajustando essa quantidade de kernels para cada canal.\nw3 \u0026lt;- keras::get_weights(m$model$layers[[5]])[[1]] bias3 \u0026lt;- keras::get_weights(m$model$layers[[5]])[[2]] dim(w3) [1] 5 5 48 96 Revisando o algoritmo:\nresult_conv3 \u0026lt;- purrr::map(1:dim(w3)[4], ~{ kern \u0026lt;- w3[,,,.x] %\u0026gt;% plyr::alply(3, identity) %\u0026gt;% purrr::map(as.matrix) actual_bias \u0026lt;- bias3[[.x]] purrr::map2(result_conv2, kern, convolve) %\u0026gt;% purrr::reduce(magrittr::add) %\u0026gt;% add_bias(actual_bias) %\u0026gt;% relu() %\u0026gt;% max_pool() }) Plotando os resultados, temos\nNo final, ficamos com 96 imagens com resolução 6x22 cada. Várias imagens ficaram zeradas e as que não ficaram parecem apenas feixes de luz no meio do breu. Pode ser que a posição dessas luzes tenha a ver com a posição de pedaços importantes do captcha original, que por sua vez seriam importantes para determinar o valor do captcha.\nVerificando se funcionou mesmo Quando eu montei o post, não estava 100% seguro das contas que estava fazendo. Pode ser que tenha alguma coisa diferente dentro do keras, que é um canhão que usa tensorflow por trás, fazendo alguma otimização esquisita. Por isso, também aprendi a plotar os resultados parciais do modelo quebrador de captchas diretamente do keras.\nPodemos montar um modelo parcial do keras escolhendo qual o layer final do modelo.\nm2 \u0026lt;- keras::keras_model( inputs = m$model$input, outputs = keras::get_layer(m$model, \u0026quot;max_pooling2d_6\u0026quot;)$output ) m2 Model ____________________________________________________________________________________________________ Layer (type) Output Shape Param # ==================================================================================================== conv2d_4_input (InputLayer) (None, 50, 180, 1) 0 ____________________________________________________________________________________________________ conv2d_4 (Conv2D) (None, 50, 180, 12) 312 ____________________________________________________________________________________________________ max_pooling2d_4 (MaxPooling2D) (None, 25, 90, 12) 0 ____________________________________________________________________________________________________ conv2d_5 (Conv2D) (None, 25, 90, 48) 14448 ____________________________________________________________________________________________________ max_pooling2d_5 (MaxPooling2D) (None, 12, 45, 48) 0 ____________________________________________________________________________________________________ conv2d_6 (Conv2D) (None, 12, 45, 96) 115296 ____________________________________________________________________________________________________ max_pooling2d_6 (MaxPooling2D) (None, 6, 22, 96) 0 ==================================================================================================== Total params: 130,056 Trainable params: 130,056 Non-trainable params: 0 ____________________________________________________________________________________________________ Esse modelo é idêntico ao inicial, mas acaba no último max pooling. Para obter as imagens, utilizamos a função predict a partir da base de dados X montada com o arq, utilizando a função prepare do decryptr:\nX \u0026lt;- prepare(read_captcha(arq))$x res \u0026lt;- predict(m2, X) dim(res) [1] 1 6 22 96 O resultado pode ser plotado da seguinte forma:\npar(mfrow = c(12, 8), mar = c(.1,.1,.1,.1)) purrr::walk(seq_len(dim(res)[4]), ~{ array(res[,,,.x], c(dim(res)[2:3], 3)) %\u0026gt;% image_read() %\u0026gt;% plot() }) Ufa, ficou igualzinho!\nApenas um check final: quero ver se os pesos obtidos são todos iguais. Para isso, ordeno todos os pesos obtidos diretamente pelo keras ou aplicando as minhas funções feitas no braço:\nw_calculado \u0026lt;- unlist(purrr::map(result_conv3, ~.x[,,1])) all.equal(sort(res), sort(w_calculado), tolerance = 1e-6) [1] TRUE UHUL!\nVisualizando na imagem original Aqui fiz um esforço para tentar entender de qual parte da imagem original esses resultados estão pegando informações. Para isso, re-escalei essas imagens de resolução mais baixa na última convolução para o tamanho original do captcha (50x180) e depois multipliquei os valores das matrizes pelos valores da imagem original.\nO resultado foi esse gif. Cada imagem é um dos canais obtidos.\nParece que os filtros são capazes de pegar as curvas que as letras fazem no captcha. Mas não tenho opinião formada sobre isso. Digam aí nos comentários o que vocês acham!\nPassos finais Para acabar a predição do captcha, nós pegamos os resultados das imagens anteriores e juntamos todos os pixels num vetorzão que junta tudo. Em estatistiquês, esse vetor nada mais é do que uma linha de um data.frame, que podemos usar numa regressão logística, por exemplo. Ou seja, essas convolucionais funcionam como um grande gerador automático de features importantes para prever os resultados do captcha. A vantagem é que essas features são obtidas de forma automática e são otimizadas dentro do processo de estimação. Por essas e outras que deep learning é realmente sensacional!\nComo já estamos no framework do keras, acabamos fazendo tudo lá dentro. Após jogar tudo no vetorzão, aplicamos mais dois layers de redes neurais comuns, que funcionam como a regressão logística. O único detalhe sensível é que, como estamos prevendo 6 letras ao mesmo tempo, precisamos novamente transformar esse vetor em uma matriz 35x6, sendo 35 o total de letras possíveis por posição e 6 a quantidade de posições.\nAbaixo, montei uma tabelinha com as probabilidades de cada letra nas posições correspondentes. Substituí valores muito pequenos por . para ver melhor.\nres2 \u0026lt;- keras::predict_proba(m$model, X) probabilidades \u0026lt;- res2[1,,] %\u0026gt;% tibble::as_tibble() %\u0026gt;% purrr::set_names(m$labs) %\u0026gt;% tidyr::gather(letra, prob) %\u0026gt;% dplyr::group_by(letra) %\u0026gt;% dplyr::mutate(pos = 1:n()) %\u0026gt;% dplyr::ungroup() %\u0026gt;% dplyr::mutate(prob = dplyr::if_else(prob \u0026lt; 5e-5, \u0026quot;.\u0026quot;, as.character(round(prob, 6))) ) %\u0026gt;% tidyr::spread(pos, prob, sep = \u0026quot;\u0026quot;) Ficamos então com “o” na primeira posição, “g” na segunda posição, “7” na terceira posição, “v” na quarta posição, “5” na quinta posição e “q” na sexta posição. Ou seja, “og7v5q”.\nVamos ver a imagem novamente:\narq %\u0026gt;% read_captcha() %\u0026gt;% purrr::pluck(1) %\u0026gt;% plot() Parece que funcionou!\nWrap-up Essa jornada foi longa, mas acho que aprendemos bastante coisa. Resumindo:\nConvoluções são somas ponderadas dos valores da vizinhança de um pixel. Esses pesos são dados por uma matriz chamada kernel. Aplicar redes neurais convolucionais consiste em i) aplicar convolução; ii) adicionar um bias; iii) aplicar uma função não linear (geralmente ReLu). max pooling serve para simplificar a resolução da imagem. Na prática, aplicamos vários kernels. O número de kernels de uma convolucional igual ao número de canais da operação anterior (input), multiplicado pelo número de canais que queremos de output. Para a convolução inicial, os canais são as cores: partimos de 1 canal se a imagem for preto e branco ou 3 canais se for colorida. No nosso caso, aplicamos três convolucionais com kernels 5x5, sendo 1*12 no primeiro nível, 12*48 no segundo nível e 48*96 no terceiro nível. Depois, pegamos as imagens resultantes e aplicamos o flatten, para trabalhar com esses números como se fossem a matriz X de uma regressão logística usual. Como vimos, é possível implementar todas essas operações na mão sem muita dificuldade. Conseguimos! Mas resta uma dúvida…\nComo é que, de fato, conseguimos esses valores mágicos de \\(W\\)? Na próxima vez provavelmente resolverei esse problema, mostrando como se faz o back-propagation em um modelo de deep learning.\nÉ isso. Happy coding :)\n","permalink":"https://blog.curso-r.com/posts/2017-11-10-captcha-conv/","tags":["captcha"],"title":"Quebrando Captchas - Parte V: Fazendo redes convolucionais na mão"},{"author":["William"],"categories":["discussões"],"contents":" Volta e meia eu escuto as famosas frases\npaste(\u0026quot;Eu sou de\u0026quot;, c(\u0026quot;Humanas\u0026quot;, \u0026quot;Exatas\u0026quot;, \u0026quot;Biológicas\u0026quot;)) ## [1] \u0026quot;Eu sou de Humanas\u0026quot; \u0026quot;Eu sou de Exatas\u0026quot; \u0026quot;Eu sou de Biológicas\u0026quot; de alguém tentando justificar por que não vai fazer alguma coisa.\nMuitas vezes, não passa de uma brincadeira na hora de dividir a conta do bar. Muitas outras, me soa como uma desculpa pronta para não encarar problemas complicados. Para mim, todo aprendizado é difícil, não acho que existe conhecimento de graça, então realmente importa se ele é de Humanas, Exatas ou Biológicas?\nA divisão do conhecimento nessas três grandes áreas tem a sua importância organizacional, mas acaba motivando muita gente a criar limitações que não existem de verdade. Por que alguém de Exatas não conseguiria assimilar as ideias de um texto filosófico? Ou por que alguém de Biológicas não conseguiria aprender Cálculo?\nAcredito que cada um de nós tem afinidade por uma das áreas e maior facilidade em estudar um tópico ou outro. Normal. Mas fico triste quando vejo pessoas inteligentes se diminuindo ao se declararem incapazes de aprender outras competências que não a delas. Sei que essa incapacidade não existe e enxergo apenas como uma forma sofisticada de dizer “Estou com preguiça”.\nUma das belezas da Estatística é nos fazer perder esse preconceito. Por mais que tenhamos nossos gostos, descobrimos que não estamos presos ao domínio de apenas uma área. Nós trabalhamos com pessoas que pensam e aprendem de formas diferentes de nossa e construímos juntos pontes para trocarmos conhecimento. Ser estatístico é não ter medo de estudar, seja lá o que for.\nTrazendo a reflexão aqui para o nosso mundinho, já ouvi muitas vezes colegas dizendo, principalmente na Graduação, que não usam o R porque ele é difícil ou porque não gostam de programar. A minha opinião sobre a primeira desculpa está nos parágrafos acima. Sobre a segunda, vou discutir no próximo e último post desta série: a relação entre Estatística e programação.\nResumindo a ópera: sempre vamos apanhar aprendendo, e vamos apanhar mais ainda quando não gostamos do que estamos estudando, mas cedo ou tarde, com a quantidade certa de esforço, o conhecimento dá as caras.\nE no bar, na hora de dividir a conta, o problema não é você ser de Humanas. O problema é a sua preguiça. :D\n","permalink":"https://blog.curso-r.com/posts/2017-11-11-preconceitos-no-aprendizado/","tags":["estatística"],"title":"Aprendendo assuntos difíceis"},{"author":["Julio"],"categories":["análises"],"contents":" Para quem está esperando posts sobre captchas, esperem mais um pouquinho. Em novembro voltarei a postar sobre esse assunto.\nEm setembro de 2017, participei do 16th Symposium on Computer Music, organizado pelo prof. Marcelo Queiroz do IME-USP. O evento foi muito legal! Estou estudando música nos últimos meses e acho que farei meu doutorado em soluções que ligam estatística e música. É uma área realmente apaixonante.\nUma das formas que usarei para aprender os métodos e estudar os conceitos será esse blog! Assim vocês poderão acompanhar meus avanços e propor ideias para colaborar com meus trabalhos. Pretendo desenvolver tanto modelos estatísticos e programação quanto teoria musical.\nAté o momento, tenho três posts planejados ou prontos:\npacote music21: esse post! Veja abaixo. pacote chordgen: pacote que criei para gerar sequências aleatórias de acordes com o R artigo deepbach: um modelo de redes neurais capaz de imitar corais de Bach. Computação musical numa casca de noz Apesar da minha falta de conhecimento sobre o tema, acredito que a área de computação musical pode ser resumida em três frentes:\nLeitura: extrair dados estruturados dos áudios. Envolve Music Information Retrieval - MIR\nComposição: Criar ou completar músicas automaticamente\nEdição: Adicionar efeitos na música para produção\nMeu foco será na parte de composição, com foco em harmonização, que significa criar e completar harmonias das músicas dado algum estilo (e.g. Bach ou Beatles). Também tentarei criar soluções de improviso, apesar de não garantir nada.\nmusic21 O objetivo do pacote music21 é funcionar como um port pipeável (%\u0026gt;%-able) da biblioteca music21, em python. Ele usa o excelente pacote reticulate do RStudio como backend.\nAtualmente, o music21 é mais limitado que a bibloteca em python. No entanto, graças ao reticulate é fácil rodar qualquer função do music21 no R.\nInstalação Como o music21 utiliza bibliotecas python, é necessário instalar algumas dependências antes. No Ubuntu, basta rodar\nsudo apt-get install python-pip sudo pip install pip --upgrade sudo pip install music21 Em outros sistemas opetacionais, veja a documentação oficial da biblioteca.\nEu tenho o péssimo hábito de não subir meus pacotes para o CRAN. Mas prometo que esse vou subir e manter. Por enquanto, para instalar o pacote music21 é necessário rodar\ndevtools::install_github(\u0026quot;jtrecenti/music21\u0026quot;) Você pode carregar o pacote rodando\nlibrary(music21) Para usar a função de plotar uma partitura, você também precisara do lilypond:\nsudo apt-get install lilypond O objeto music21 library(music21) music21 O objeto music21 é um módulo do python. Com ele é possível rodar qualquer função do pacote usando $, tratando o objeto como uma Reference Class.\nnote \u0026lt;- music21$note$Note(\u0026quot;C#\u0026quot;) note note %\u0026gt;% plot() Exemplo Corais do Bach get_composer(\u0026quot;bach\u0026quot;) %\u0026gt;% head() Vamos pegar uma delas e guardar numa variável\nbach_music \u0026lt;- get_composer(\u0026quot;bach\u0026quot;)[61] %\u0026gt;% read_music() Plotando a música bwv165.6 Basta rodar plot()!!\nbach_music %\u0026gt;% plot() No RStudio, você pode usar a função view() para visualizar a música em melhor resolução no Viewer Pane.\nImprimindo bwv165.6 (só o soprano) Para visualizar as notas de uma música basta rodar o objeto no console. A música é organizada por Measures (compassos) e cada compasso vai mostrar seus componentes. No primeiro compasso, podemos ver a clave de sol (TrebleClef), as definições do tom (sol maior) e do tempo (4/4). Nos demais, observamos as notas e os temos associados a cada nota. Por exemplo, temos uma nota F# no primeiro tempo.\nbach_music[[1]] Você pode tratar esse objeto como uma lista. Se você entrar numa nota, você poderá ver algumas características dela\n# Nota F# bach_music[[1]][[2]][[3]] # duração da nota bach_music[[1]][[2]][[3]]$duration # som exato bach_music[[1]][[2]][[3]]$pitch Por enquanto vou acabar por aqui. Convido vocês a usarem o music21 e experimentar suas funcionalidades. Adicionem issues em https://github.com/jtrecenti/music21/issues e mandem pergundas nos comentários.\nÉ isso! Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2017-11-03-music01/","tags":["música"],"title":"Estatística e Música"},{"author":["Daniel"],"categories":["conceitos"],"contents":" O que teste t-pareado, modelos mistos e tidy data podem ter a ver? Veja neste post como a estrutura dos seus dados pode afetar a forma que você fará a sua análise.\nPara começar, vamos relemebrar o que é tidy data para depois seguir ao ponto do post.\nTidy Data Tidy data é um conceito introduzido pelo Hadley Wickham neste paper.\nEsse paper é, para mim, o melhor artigo do Hadley. A primeira frase da definição cita Tolstói e diz:\nLike families, tidy datasets are all alike but every messy dataset is messy in its own way.\nEssa frase resume a vida de qualquer um que trabalha ou já trabalhou com análise de dados. O ponto mais importante do que significa tidy data também está neste primeiro parágrafo: são datasets em que a estrutura dos dados está ligada padronizadamente com o seu significado. A forma padronizada é:\nCada variável é uma coluna de uma tabela Cada observação é uma linha de uma tabela Cada tipo de unidade observacional forma uma tabela O exemplo cássico é o seguinte. Primeiro vamos ver um banco de dados desarrumado.\nPais Idh2015 Idh2014 Brasil 0.754 0.755 Argentina 0.827 0.836 Chile 0.847 0.832 Esse dataset está desarrumado pois existem duas colunas Idh2015 e Idh2014 que representam a mesma variável: IDH e uma variável implícita ANO, que também aparece nesta duas colunas. A forma tidy de representar este dataset seria:\nPais ano idh Brasil 2015 0.754 Argentina 2015 0.827 Chile 2015 0.847 Brasil 2014 0.755 Argentina 2014 0.836 Chile 2014 0.832 O que isso tem a ver com … … teste t-pareado ou com modelos mistos?\nSuponha que queremos inferir se houve alguma mudança na média do IDH de um ano para o outro. Ou seja testar se a média do IDH de 2015 é diferente da média do IDH de 2014. Vamos considerar um banco de dados simulado:\nset.seed(10201) library(tidyverse) df \u0026lt;- data_frame( Pais = paste0(\u0026quot;pais\u0026quot;, 1:50), Idh2014 = runif(50), Idh2015 = Idh2014 + rnorm(50, mean = 0.1, sd = 0.025) ) Uma forma de fazer isso é usar o teste t pareado que é ensinado nos cursos introdutórios de estatística. Basicamente o que ele faz é testar se a média da diferença entre o IDH2015 e o IDH 2014 é diferente de zero. Isso é diferente de um teste T usual, pois o teste t pareado ajusta o seu cálculo da variância para considerar que existem duas fontes de incerteza.\nNo R a forma mais natural de fazer isso é:\nteste \u0026lt;- t.test(df$Idh2015, df$Idh2014, paired=TRUE) Note que o nosso banco de dados está desarrumado e mesmo assim foi muito simples fazer esse teste no R. Agora vamos arrumar o banco de dados.\ndf \u0026lt;- df %\u0026gt;% gather(ano, idh, -Pais) %\u0026gt;% mutate(ano = parse_number(ano)) Agora para fazer o mesmo teste, poderíamos filtrar o banco de dados duas vezes, por exemplo:\nt.test(df$idh[df$ano == 2015], df$idh[df$ano == 2014], paired = TRUE) Paired t-test data: df$idh[df$ano == 2015] and df$idh[df$ano == 2014] t = 27.355, df = 49, p-value \u0026lt; 2.2e-16 alternative hypothesis: true mean difference is not equal to 0 95 percent confidence interval: 0.09000554 0.10427822 sample estimates: mean difference 0.09714188 Mas aí estamos voltando para a forma desarrumada para fazer o teste. Outra forma de fazer é considerar essa comparação de médias como um problema de regressão em que a suposição independência das observações não é válida, uma vez que dado um pais, com certeza existe relação entre o idh de 2014 e de 2015.\nVamos ajustar um modelo com efeitos aleatórios para esse problema e comparar os resultados.\nlibrary(nlme) model \u0026lt;- lme(idh ~ as.factor(ano), random = ~1|Pais, data = df) summary(model) Linear mixed-effects model fit by REML Data: df AIC BIC logLik -184.7518 -174.4119 96.37588 Random effects: Formula: ~1 | Pais (Intercept) Residual StdDev: 0.3009017 0.01775584 Fixed effects: idh ~ as.factor(ano) Value Std.Error DF t-value p-value (Intercept) 0.4840132 0.04262795 49 11.35436 0 as.factor(ano)2015 0.0971419 0.00355117 49 27.35491 0 Correlation: (Intr) as.factor(ano)2015 -0.042 Standardized Within-Group Residuals: Min Q1 Med Q3 Max -1.88877771 -0.44544521 -0.01239249 0.39934207 1.84475543 Number of Observations: 100 Number of Groups: 50 Estamos interessados em comparar a significância do efeito fixo da variável ano nesse modelo com a do teste t-pareado. Veja que no caso a estatística T do testes é idêntica: 27.35.\nVimos que a forma como os dados estão estruturados no seu banco de dados pode influenciar a técnica utilizada para a sua análise. Se ele estivesse na forma desarrumada o mais natural seria aplicar um teste t pareado, se ele estivesse em formado tidy o natural seria usar um modelo misto. No seu paper, Hadley argumenta que a maioria dos softwares esperam que o seu banco de dados esteja arrumado no sentido de que cada variável é uma coluna e cada observação é uma linha.\n","permalink":"https://blog.curso-r.com/posts/2017-10-27-tidy-data-mixed-models/","tags":["estatística","tidy"],"title":"Tidy Data, Teste T Pareado e Modelos Mistos"},{"author":["Daniel"],"categories":["Tutoriais"],"contents":" Nas últimas semanas sumimos um pouco aqui do blog, pois estávamos ocupados na aMostra de Estatística no IME-USP.\nO evento teve a organização impecável graças aos alunos do bacharelado em estatística Lucas Hamaguchi, Beatriz Vianna, Gabriela Uhrigshardt, Milene Farhat, Luíza Baratojo, Rodrigo Marcel Araujo Oliveira e do Prof. Victor Fossaluza. Parabens a todos!\nA Curso-R participou do evento ministrando 3 oficinas abordando tópicos bem distintos sobre programação em R. Graças à fantástica organização, todas as palestras foram gravadas e agora podem ser revistas pelo YouTube da aMostra.\nNo dia 02/10, o Fernando ministrou a oficina WebScraping e Conexões! Veja o vídeo neste link.\nNo dia 04/10 foi a vez do Julio ministrar a oficina sobre API’s no R. O vídeo está disponível neste link.\nNo dia 05/10 o AThos e eu ministramos a oficina de Introdução ao Machine Learning. Veja o vídeo neste link.\nClaro que não fomos os únicos palestrantes e tem muita gente boa que participou da aMostra. Vale muito a pena ver os demais vídeos do canal da aMostra.\nAproveitem!\n","permalink":"https://blog.curso-r.com/posts/2017-10-19-cursor-amostra/","tags":["curso-r"],"title":"Curso-R na aMostra de Estatística 2017"},{"author":["Daniel"],"categories":["Tutoriais"],"contents":" Filtro de Bloom é um algoritmo muito interessante para testar se um elemento pertence a um conjunto. Ele é considerado uma estrutura de dados probabilística, ou seja, o resultado pode não estar correto com alguma probabilidade. Especificamente para o filtro de bloom, existe a possibilidade de falsos positivos mas não de falsos negativos: o algoritmo pode dizer que o elemento pertence ao conjunto, mas na verdade não pertencer, mas nunca dirá que ele não pertence sendo que ele pertence.\nBloom Filters são úteis em diversas situações, geralmente relacionadas ao ganho de velocidade e de espaço que o seu uso pode trazer. Muitos sistemas de bancos de dados usam bloom filters para reduzir o número de buscas no disco (ex. Cassandra). O Medium usa para evitar recomendar uma paǵina que você já leu. Recentemente, encontraram até aplicações para bloom filters em machine learning.\nNesse post vamos implementar uma versão simplificada, nada otimizada dos filtros de Bloom em R. Mas antes disso, vale a pena ler o verbete da Wikipedia sobre o assunto.\nEssencialmente, um filtro de bloom é um vetor de TRUEs e FALSES de tamanho \\(m\\). Inicializamos esse vetor com FALSES. Em seguida para cada elemento do conjunto que você deseja representar pelo filtro, repetimos o seguinte processo: Hasheamos o elemento usando \\(k\\) funções de hash diferentes. Cada uma dessas funções indicará um elemento do vetor que deve ser marcado como TRUE. Armazenamos então esse vetor de bits. São os valores de \\(m\\) e de \\(k\\) que controlam a probabilidade de falsos positivos.\nVeja como podemos criar uma função em R para fazer essas operações. Essa função inicializa o vetor de bits de tamanho \\(m\\) com FALSES e em seguida, para cada uma das \\(k\\) funções de hash (no caso apenas variamos a semente do hash MurMur32) e para cada elemento de x calculamos o elemento do vetor vec que deve se tornar TRUE. No final, ela retorna o vetor vec, onde armazenamos como atributos os parâmetros usados na sua construção.\nlibrary(digest) library(magrittr) criar_vetor_de_bits \u0026lt;- function(x, m = 1000, k = 7){ vec \u0026lt;- rep(FALSE, m) for (i in 1:k) { for (j in 1:length(x)) { hash \u0026lt;- digest(x[j], algo = \u0026quot;murmur32\u0026quot;, serialize = FALSE, seed = i) %\u0026gt;% Rmpfr::mpfr(base = 16) %% m %\u0026gt;% as.integer() vec[hash + 1] \u0026lt;- TRUE } } # armazenamos os parâmetros usados na construção attributes(vec) \u0026lt;- list(m = m, k= k) return(vec) } Dado um conjunto de strings, podemos criar o vetor de bits que o representa.\nvect \u0026lt;- criar_vetor_de_bits(c(\u0026quot;eu\u0026quot;, \u0026quot;pertenco\u0026quot;, \u0026quot;ao\u0026quot;, \u0026quot;conjunto\u0026quot;, \u0026quot;de\u0026quot;, \u0026quot;strings\u0026quot;), m = 1000, k = 7) Agora vamos definir uma função que verifica se uma string pertence ao conjunto, dada apenas a representação dos bits desse conjunto. Hasheamos o elemento que desejamos verificar a presença no conjunto com a primeira função de hash. Se ela indicar um elemento do vetor que já está marcado com TRUE então continuamos, se não, retorna FALSE indicando que o elemento não pertence ao conjunto. Continuamos até acabarem as funções de hash ou até 1 FALSE ter sido retornado.\nverificar_presenca \u0026lt;- function(x, vetor_de_bits){ k \u0026lt;- attr(vetor_de_bits, \u0026quot;k\u0026quot;) m \u0026lt;- attr(vetor_de_bits, \u0026quot;m\u0026quot;) for(i in 1:k){ hash \u0026lt;- digest(x, algo = \u0026quot;murmur32\u0026quot;, serialize = FALSE, seed = i) %\u0026gt;% Rmpfr::mpfr(base = 16) %% m %\u0026gt;% as.integer() if(!vetor_de_bits[hash + 1]) { return(FALSE) } } return(TRUE) } verificar_presenca(\u0026quot;nao\u0026quot;, vect) verificar_presenca(\u0026quot;eu\u0026quot;, vect) verificar_presenca(\u0026quot;abc\u0026quot;, vect) Com m = 1000 e k = 7 não consegui encontrar nenhum falso positivo, mas basta diminuir o tamanho de m e de k que encontraremos. No verbete da Wikipedia a conta está bonitinha mas de fato a probabilidade de falsos positivos pode ser estimada em função dos parâmetros \\(k\\) e \\(m\\) e \\(n\\) (tamanho do conjunto representado) é dada por\n\\[(1 - e^{-kn/m})^k\\]\nNo caso apresentado, a probabilidade de colisão é de 1.991256e-10.\n","permalink":"https://blog.curso-r.com/posts/2017-09-18-bloom-filter/","tags":["algoritmos"],"title":"Filtros de Bloom em R"},{"author":["Fernando"],"categories":["Tutoriais"],"contents":" Em maio deste ano, escrevi um post sobre web scraping dos dados da secretaria de segurança pública de São Paulo. Como o título indica, o foco do texto é mostrar como se raspa a página de estatísticas da SSP, mas, mais do que isso, o texto também sugere um roteiro de construção de web scrapers.\nNeste post, venho divulgar uma expansão do conteúdo do post passado. Agora é possível acessar os dados da SSP diretamente no R usando o pacote brcrimR, mas a ideia é que no futuro todas as informações divulgadas por alguma Secretaria de Segurança fiquem disponíveis diretamente no R!\ndevtools::install_github(\u0026quot;abjur/brcrimR\u0026quot;) O brmcrimR se propõe a resolver três problemas:\nObter informações criminais brasileiras diretamente no R - Muitas análises interessantes seriam viabilizadas se fosse fácil e rápido carregar informações criminais históricas num data_frame. Fazer isso é a motivação principal do brmcrimR. Consolidar tabelas em bases históricas - Assim como em São Paulo, muitas Secretarias de Segurança disponibilizam as informações filtradas por mês ou localidade. A segunda motivação principal do brcrimR é iterar por essas páginas. Padronização - O objetivo menos direto do brcrimR é padronizar as informações disponibilizadas. Esse não é um problema simplesmente computacional, mas algumas rotinas de pré-processamento podem ajudar no processo. Para ilustrar o funcionamento do brcrimR, vamos olhar o que já está implementado em São Paulo.\nInformações agregadas As tabelas de informações agregadas podem ser obtidas seguindo os passos que descrevi aqui, mas a função brmcrimR::get_summarized_table_sp faz todo o trabalho por nós.\nbrcrimR::get_summarized_table_sp(year = \u0026#39;2016\u0026#39;, city = \u0026#39;1\u0026#39;) %\u0026gt;% knitr::kable(caption = \u0026quot;Contagem de boletins de ocorrência na cidade de Amparo.\u0026quot;) %\u0026gt;% kableExtra::kable_styling(font_size = 8) Tabela 1: Contagem de boletins de ocorrência na cidade de Amparo. Natureza Jan Fev Mar Abr Mai Jun Jul Ago Set Out Nov Dez Total municipio ano HOMICÍDIO DOLOSO (2) 0 0 0 0 0 0 0 0 0 1 0 0 1 1 2016 Nº DE VÍTIMAS EM HOMICÍDIO DOLOSO (3) 0 0 0 0 0 0 0 0 0 1 0 0 1 1 2016 HOMICÍDIO DOLOSO POR ACIDENTE DE TRÂNSITO 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2016 Nº DE VÍTIMAS EM HOMICÍDIO DOLOSO POR ACIDENTE DE TRÂNSITO 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2016 HOMICÍDIO CULPOSO POR ACIDENTE DE TRÂNSITO 0 0 0 1 0 0 1 0 0 0 0 0 2 1 2016 HOMICÍDIO CULPOSO OUTROS 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2016 TENTATIVA DE HOMICÍDIO 0 0 1 0 0 0 0 0 0 1 0 3 5 1 2016 LESÃO CORPORAL SEGUIDA DE MORTE 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2016 LESÃO CORPORAL DOLOSA 4 19 13 13 12 15 15 8 16 23 23 10 171 1 2016 LESÃO CORPORAL CULPOSA POR ACIDENTE DE TRÂNSITO 10 18 21 22 13 15 18 10 17 15 15 17 191 1 2016 LESÃO CORPORAL CULPOSA - OUTRAS 1 1 2 1 0 3 0 0 0 0 0 1 9 1 2016 LATROCÍNIO 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2016 Nº DE VÍTIMAS EM LATROCÍNIO 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2016 TOTAL DE ESTUPRO (4) 2 0 0 1 0 1 0 1 0 1 0 0 6 1 2016 ESTUPRO DE VULNERÁVEL 0 0 0 0 0 0 0 0 0 1 0 0 1 1 2016 TOTAL DE ROUBO - OUTROS (1) 1 0 0 0 1 0 0 1 0 0 0 1 4 1 2016 ROUBO DE VEÍCULO 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2016 ROUBO A BANCO 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2016 ROUBO DE CARGA 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2016 FURTO - OUTROS 17 19 20 22 13 11 11 12 23 27 17 30 222 1 2016 FURTO DE VEÍCULO 0 1 1 1 5 1 2 1 0 1 2 1 16 1 2016 Para obter os dados históricos, basta usar a função brcrimR::get_historical_summarized_table_sp. Ela funciona da mesma maneira que brcrimR::get_summarized_table_sp, mas pode receber vetores como input. Nesse caso, ela organiza os parâmetros num grid e retorna uma tabela com todas as requisições empilhadas.\nbrcrimR::get_historical_summarized_table_sp( y = c(\u0026#39;2016\u0026#39;, \u0026#39;2017\u0026#39;), c = \u0026#39;1\u0026#39;, ty = \u0026#39;ctl00$conteudo$btnMensal\u0026#39;) %\u0026gt;% filter(Natureza == \u0026quot;LESÃO CORPORAL CULPOSA POR ACIDENTE DE TRÂNSITO\u0026quot;) %\u0026gt;% set_names(c(\u0026#39;Natureza\u0026#39;, 1:12, \u0026quot;Total\u0026quot;, \u0026quot;municipio\u0026quot;, \u0026quot;ano\u0026quot;)) %\u0026gt;% gather(mes, valor, -municipio, -ano, -Natureza, -Total) %\u0026gt;% filter(!is.na(valor)) %\u0026gt;% mutate(data_bo = lubridate::dmy(paste(\u0026quot;01\u0026quot;, mes, ano, sep = \u0026quot;-\u0026quot;))) %\u0026gt;% ggplot(aes(x = data_bo, y = valor)) + geom_bar(stat = \u0026#39;identity\u0026#39;, fill = \u0026#39;royalblue\u0026#39;) + theme_minimal(15) + labs(x = \u0026#39;Mês\u0026#39;, y = \u0026quot;Número de BO\u0026#39;s\u0026quot;) Informações desagregadas Além dos dados agregados, a partir do ano passado a SSP de São Paulo também passou a divulgar informações detalhadas sobre os BO’s da capital no portal da trasparência. O José de Jesus começou o scraping desses conteúdos e eu só encapsulei tudo dentro de um pacote. Vejam só:\nbrcrimR::get_detailed_table_sp(folder = \u0026#39;btnHomicicio\u0026#39;, year = \u0026#39;2017\u0026#39;, month = \u0026#39;1\u0026#39;, department = \u0026#39;0\u0026#39;) %\u0026gt;% select(NUM_BO, BO_INICIADO, DATAOCORRENCIA, BO_AUTORIA, FLAGRANTE, LATITUDE, LONGITUDE) %\u0026gt;% distinct(NUM_BO, .keep_all = T) %\u0026gt;% head(10) %\u0026gt;% knitr::kable(caption = \u0026quot;Algumas colunas de dez BO\u0026#39;s da tabela de boletins de ocorrência de homicídio de janeiro de 2017.\u0026quot;) %\u0026gt;% kableExtra::kable_styling(font_size = 8) Tabela 2: Algumas colunas de dez BO’s da tabela de boletins de ocorrência de homicídio de janeiro de 2017. NUM_BO BO_INICIADO DATAOCORRENCIA BO_AUTORIA FLAGRANTE LATITUDE LONGITUDE 11589 31/12/2016 22:02:52 31/12/2016 Conhecida Não -23,6027829499999 -46,516050348 1761 31/12/2016 21:02:50 31/12/2016 Conhecida Não -22,7974702349999 -45,1786399879999 2915 31/12/2016 23:33:16 31/12/2016 Conhecida Sim 3 01/01/2017 03:45:42 31/12/2016 Conhecida Não -23,4663377281984 -47,4644919148778 2 01/01/2017 03:40:13 01/01/2017 Desconhecida Não -23,7062770598301 -46,5997084663396 6 01/01/2017 04:25:12 01/01/2017 Desconhecida Não -23,435339469 -45,0776321386071 1 01/01/2017 02:30:11 31/12/2016 Conhecida Sim -23,838385412 -46,5778088709999 4 01/01/2017 04:53:21 01/01/2017 Desconhecida Não -23,6528390081698 -46,4471844195849 859 31/12/2016 21:35:30 31/12/2016 Conhecida Não -23,6027829499999 -46,516050348 13933 31/12/2016 21:09:59 31/12/2016 Conhecida Sim -23,6221113323333 -46,4778365256666 É fantástico que a gente tenha acesso a informações tão detalhadas como latitude e longitude de um crime, mas o formato não é lá essas coisas. Como os dados da SSP provavelmente estão armazenados em um banco de dados relacional, a tabela que baixamos parece ser um inner_join de várias tabelas do banco, pois existem repetições da chave primária NUM_BO. É claro que quem tem um pouco de experiência com esse tipo de coisa vai tirar de letra, mas esse certamente não é o melhor formato para a população em geral.\nDe toda forma, mesmo nesse formato, é interessante loopar por todas as páginas rapidamente. Por isso, também implementamos a função get_detailed_table_sp, que funciona mais ou menos da mesma forma que a get_historical_summarized_table_sp.\nbrcrimR::get_historical_detailed_table_sp(f = \u0026#39;btnHomicicio\u0026#39;, y = \u0026#39;2017\u0026#39;, m = 1:8, d = \u0026#39;0\u0026#39;) Próximos passos Encerro aqui a apresentação do pacote, com os seus objetivos e funcionalidades básicas. Ele está longe de ser ideal e tem uma listinha de coisas que queremos implementar no futuro, mas como se trata de dados abertos, não vejo porque não contar com a comunidade para isso! Seguem abaixo algumas ideias:\nHelper functions para os parâmetros das funções de get de São Paulo. Precisa existir uma função que pegue uma especificação de parâmetro do tipo “Homicídio” e transforme em “btnHomicicio”, que é o parâmetro que precisamos passar pro site do tribunal. Um “desacoplador” de tabelas do portal da transparência. Implementações de funções parecidas para outros estados. Por hoje é isso, pessoal. Happy coding!\n","permalink":"https://blog.curso-r.com/posts/2017-09-13-brcrimr/","tags":["web scraping","tidyverse","banco de dados","dados abertos","api"],"title":"brcrimR"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Nesse post vou mostrar como fazer um pacote em R muito, muito rápido. Tirei várias coisas que costumo fazer nos pacotes, com dor no coração, tudo pela velocidade, mantendo só o essencial.\nDuas restrições que usei são\nO pacote precisa ficar disponível no GitHub. O pacote precisa ter pelo menos uma função. Essa é a solução que eu acho mais segura e rápida. Você também pode usar o próprio RStudio para criar pacotes ou clonar coisas do github, mas isso pode dar alguns bugs.\nPasso 1: Crie um repositório no Github Clique no link: https://github.com/new\nEscreva o nome do seu pacote. O nome do pacote não pode ter espaços, underline (_) nem hífen (-) nem começar com números. Tique a opção Initialize this repository with a README. Clique em Create repository. Passo 2: Clonar o repositório Recomendo clonar repositórios abrindo um terminal e digitando\n$ git clone https://github.com/usuario/nomeDoPacote Você pode clonar de outras formas, inclusive dentro do RStudio. Se você usar o RStudio, saia do projeto e delete o arquivo .RProj criado automaticamente, pois ele terá metadados inapropriados para criar pacotes.\nPasso 3: usar devtools::setup() Abra uma nova sessão R em qualquer lugar. Rode devtools::setup(\"caminho/da/pasta/clonada/nomeDoPacote\"). Passo 4: Crie sua função Exemplo:\n#\u0026#39; Soma 2 #\u0026#39; #\u0026#39; Recebe um vetor de números e retorna um vetor de números somando dois #\u0026#39; #\u0026#39; @param x vetor de números. #\u0026#39; #\u0026#39; @export soma_2 \u0026lt;- function(x) { x + 2 } Crie a função dentro de um arquivo com extensão .R na pasta R As informações que começam com #' acima da função servem para documentar. Nesse caso, a primeira linha é o título a segunda linha é a descrição a parte que começa com @param descreve o que é o parâmetro de entrada a parte que começa com @export diz para o pacote que essa função deve estar disponível para o usuário quando ele rodar library(nomeDoPacote). Passo 5: document, commit e push! Rode devtools::document(). Commite suas alterações. Dê um push! Se não saba o que é commitar e pushar, veja o artigo do Athos sobre o uso do git e do GitHub.\nPasso 6: Instalar o pacote em outra máquina Mande o nome do seu usuário do GitHub e o nome do seu pacote para sua migue. Peça para ela rodar: devtools::install_github(\u0026#39;usuario/nomeDoPacote\u0026#39;) Agora ela poderá usar sua função! library(nomeDoPacote) soma_2(1:10) # [1] 3 4 5 6 7 8 9 10 11 12 Você também pode ver o help da função com ?soma_2:\nFIM!\nConclusões Agora você não tem desculpa para não empacotar suas soluções em R. Esse tutorial é incompleto! Para acessar mais detalhes, veja http://r-pkgs.had.co.nz, elaborado por você sabe quem. Outras pequenas dicas práticas Use sempre devtools::check() para checar se seu pacote está 100% bem construído. Use devtools::use_package() para usar funções de outros pacotes. Sempre use os :: para chamar as funções e nunca rode library() ou require() dentro de um pacote. Use devtools::use_mit_license() para adicionar um arquivo LICENSE ao seu pacote. Use abjutils::use_pipe() para poder usar o %\u0026gt;% nos seus pacotes. Use devtools::use_data() para adicionar dados ao seu pacote. Use devtools::use_vignettes() para escrever um tutorial sobre seu pacote, igual a esse do dplyr, por exemplo. É isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2017-09-07-pacote-miojo/","tags":["miojo"],"title":"Pacotes miojo - como fazer um pacote no R em 3 minutos"},{"author":["Fernando"],"categories":["conceitos"],"contents":" Manter-se atualizado na parte mais computacional é muito importante para um estatístico. Essa convicção é bastante razoável, mas costuma gerar polêmica em alguns ciclos acadêmicos. Quem nunca se questionou se “(eu) deveria implementar esse Newton-Raphson ou usar um pacote de procedência duvidosa?” ou “escrever a matriz de delineamento é mesmo importante? O R já solta pra mim!” que atire a primeira pedra.\nNa minha opinião, esse tipo de discussão nasce morta. Ao mesmo tempo em que é importante conhecer bem a teoria que se aplica, o mundo é vasto e grandioso demais pra gastar tempo estudando detalhes técnicos de implementação de todas as coisas. Isso é óbvio, mas por outro lado “um pouco” de conhecimento sobre a parte suja do trabalho também é essencial em algumas situações. Se a sua base ficou grande demais para o R, todo o seu conhecimento de estatística pode ser inútil, porque o que te separa do sucesso é um bom gerenciamento dos recursos computacionais.\nPrecisa existir um equilíbrio entre “apertar botões” e “reinventar a roda”. Isso é óbvio, mas a parte triste da discussão é que esse equilíbrio pode ser difícil de alcançar.\nExistem muitas versões da discussão entre estatísticos-computeiros e estatísticos-papel-e-caneta. Uma delas, que será tema deste post, parte da questão “Eu deveria implementar todas as minhas funções ou usar as guloseimas tecnológicas disponíveis no CRAN?”. De um lado, os apóstolos do tidyverse, como a própria Curso-R, defendem e divulgam os avanços proporcionados por ferramentas que computam em altíssimo nível. Os estatísticos mais caxias gostam de implementar todas as contas e funções que foram utilizar, o que provoca alguns hábitos estranhos. Os tipos mais radicais invertem matrizes em Fortran, não acreditam na função quantile e não raramente são afeitos à teorias da conspiração.\nComo sempre, nenhum dos dois lados está 100% correto, mas na última semana me deparei com um problema que me fez sentir mais próximo do mais radical dos xiitas.\nO problema Considere que você precisa carregar um vetor de datas que vieram do Excel. Pulando a parte de leitura e as soluções que poderiam vir do bom uso de pacotes como readxl e openxlsx, considere que o problema prático consiste em converter, já no R, o seguinte vetor de textos num vetor de datas.\nexemplo_1 \u0026lt;- c(\u0026quot;22/08/2016\u0026quot;,\u0026quot;29/08/2016\u0026quot;, \u0026quot;05/09/2016\u0026quot;,\u0026quot;12/09/2016\u0026quot;) Quem usa o tidyverse faria isso usando a função dmy do pacote lubridate:\nlubridate::dmy(exemplo_1) ## [1] \u0026quot;2016-08-22\u0026quot; \u0026quot;2016-08-29\u0026quot; \u0026quot;2016-09-05\u0026quot; \u0026quot;2016-09-12\u0026quot; e o resultado seria exatamente o que a gente quer. O que aconteceu comigo, entretanto, foi um comportamento inesperado num exemplo muito parecido com esse.\nPor conta da formatação irregular de um arquivo de Excel, me deparei com um vetor parecido com esse o_que_observei:\nrepeticoes \u0026lt;- 800 o_que_observei \u0026lt;- c(rep(\u0026quot;39419\u0026quot;, repeticoes), \u0026quot;22/08/2016\u0026quot;,\u0026quot;29/08/2016\u0026quot;, \u0026quot;05/09/2016\u0026quot;,\u0026quot;12/09/2016\u0026quot;) Eu imaginava que, quando repeticoes valesse 800, o resultado fosse o mesmo que acontece quando repeticoes vale 1, mas esse não é o caso.\nPrimeiro, vamos o que acontece quando repeticoes vale 1:\nexemplo_2 \u0026lt;- c(\u0026quot;39419\u0026quot;, \u0026quot;22/08/2016\u0026quot;,\u0026quot;29/08/2016\u0026quot;, \u0026quot;05/09/2016\u0026quot;,\u0026quot;12/09/2016\u0026quot;) O lubridate não é esperto o suficiente para perceber que a primeira entrada representa uma data com origem no dia “30/12/1899” (a data padrão em Excel’s de Windows), mas tudo aquilo que ele não sabe como converter vira um NA.\nlubridate::dmy(exemplo_2) ## Warning: 1 failed to parse. ## [1] NA \u0026quot;2016-08-22\u0026quot; \u0026quot;2016-08-29\u0026quot; \u0026quot;2016-09-05\u0026quot; \u0026quot;2016-09-12\u0026quot; Agora, veja só o que acontece quando fazemos a mesma coisa no meu exemplo:\nlubridate::dmy(o_que_observei) ## Warning: All formats failed to parse. No formats found. ## [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [101] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [126] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [151] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [176] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [201] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [226] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [251] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [276] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [301] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [326] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [351] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [376] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [401] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [426] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [451] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [476] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [501] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [526] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [551] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [576] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [601] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [626] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [651] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [676] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [701] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [726] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [751] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [776] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [801] NA NA NA NA O resultado é bastante estranho. Embora as últimas entradas do vetor sejam datas num formato conhecido, não é isso que a função retorna.\nA investigação Me perguntando sobre o que aconteceu, eu resolvi criar uma issue no pacote lubridate.\nConsider the following vectors:\ndates_vector_1 \u0026lt;- c(\u0026quot;22/08/2016\u0026quot;,\u0026quot;29/08/2016\u0026quot;,\u0026quot;05/09/2016\u0026quot;,\u0026quot;12/09/2016\u0026quot;) dates_vector_2 \u0026lt;- c(rep(\u0026quot;1\u0026quot;, 500), dates_vector_1) dates_vector_3 \u0026lt;- c(rep(\u0026quot;1\u0026quot;, 800), dates_vector_1) dmy fails to detect formats on the third, even though it’s capable of finding it on the second.\nlibrary(lubridate) dmy(dates_vector_1) dmy(dates_vector_2) dmy(dates_vector_3) Is this supposed to happen? If it is, why?\nA resposta do vspinu, pricipal contribuidor do lubridate hoje, me respondeu muito rapidamente!\nThis is a consequence of guessing formats based on a deterministic sub-sample of the original vector. This issue was aleviated somewhat recently, but it’s impossible to solve efficiently without dropping format guesser to C level. Not something which I would consider in the future.\nSee #307 #308. You can disable guesser with parse_date_time and train=FALSE.\nCase solved! O problema é a parse_date_time, que é usada por trás do dmy. Antes de converter, o lubridate adivinha quais são os formatos, que usa apenas um pedaço do vetor.\nA solução A solução proposta pelo vspinu não funcionou no meu caso.\nparse_date_time(o_que_observei, train = T, orders = \u0026#39;dmy\u0026#39;) O parse_date_time continua não reconhecendo as últimas entradas do meu vetor como datas, mesmo com o train = FALSE.\nPara não gastar muito tempo, terminei fazendo um gato pra resolver o meu problema.\nnovo \u0026lt;- c(\u0026quot;01/01/2001\u0026quot;, o_que_observei) solucao \u0026lt;- dmy(novo)[-1] ## Warning: 800 failed to parse. solucao ## [1] NA NA NA NA NA ## [6] NA NA NA NA NA ## [11] NA NA NA NA NA ## [16] NA NA NA NA NA ## [21] NA NA NA NA NA ## [26] NA NA NA NA NA ## [31] NA NA NA NA NA ## [36] NA NA NA NA NA ## [41] NA NA NA NA NA ## [46] NA NA NA NA NA ## [51] NA NA NA NA NA ## [56] NA NA NA NA NA ## [61] NA NA NA NA NA ## [66] NA NA NA NA NA ## [71] NA NA NA NA NA ## [76] NA NA NA NA NA ## [81] NA NA NA NA NA ## [86] NA NA NA NA NA ## [91] NA NA NA NA NA ## [96] NA NA NA NA NA ## [101] NA NA NA NA NA ## [106] NA NA NA NA NA ## [111] NA NA NA NA NA ## [116] NA NA NA NA NA ## [121] NA NA NA NA NA ## [126] NA NA NA NA NA ## [131] NA NA NA NA NA ## [136] NA NA NA NA NA ## [141] NA NA NA NA NA ## [146] NA NA NA NA NA ## [151] NA NA NA NA NA ## [156] NA NA NA NA NA ## [161] NA NA NA NA NA ## [166] NA NA NA NA NA ## [171] NA NA NA NA NA ## [176] NA NA NA NA NA ## [181] NA NA NA NA NA ## [186] NA NA NA NA NA ## [191] NA NA NA NA NA ## [196] NA NA NA NA NA ## [201] NA NA NA NA NA ## [206] NA NA NA NA NA ## [211] NA NA NA NA NA ## [216] NA NA NA NA NA ## [221] NA NA NA NA NA ## [226] NA NA NA NA NA ## [231] NA NA NA NA NA ## [236] NA NA NA NA NA ## [241] NA NA NA NA NA ## [246] NA NA NA NA NA ## [251] NA NA NA NA NA ## [256] NA NA NA NA NA ## [261] NA NA NA NA NA ## [266] NA NA NA NA NA ## [271] NA NA NA NA NA ## [276] NA NA NA NA NA ## [281] NA NA NA NA NA ## [286] NA NA NA NA NA ## [291] NA NA NA NA NA ## [296] NA NA NA NA NA ## [301] NA NA NA NA NA ## [306] NA NA NA NA NA ## [311] NA NA NA NA NA ## [316] NA NA NA NA NA ## [321] NA NA NA NA NA ## [326] NA NA NA NA NA ## [331] NA NA NA NA NA ## [336] NA NA NA NA NA ## [341] NA NA NA NA NA ## [346] NA NA NA NA NA ## [351] NA NA NA NA NA ## [356] NA NA NA NA NA ## [361] NA NA NA NA NA ## [366] NA NA NA NA NA ## [371] NA NA NA NA NA ## [376] NA NA NA NA NA ## [381] NA NA NA NA NA ## [386] NA NA NA NA NA ## [391] NA NA NA NA NA ## [396] NA NA NA NA NA ## [401] NA NA NA NA NA ## [406] NA NA NA NA NA ## [411] NA NA NA NA NA ## [416] NA NA NA NA NA ## [421] NA NA NA NA NA ## [426] NA NA NA NA NA ## [431] NA NA NA NA NA ## [436] NA NA NA NA NA ## [441] NA NA NA NA NA ## [446] NA NA NA NA NA ## [451] NA NA NA NA NA ## [456] NA NA NA NA NA ## [461] NA NA NA NA NA ## [466] NA NA NA NA NA ## [471] NA NA NA NA NA ## [476] NA NA NA NA NA ## [481] NA NA NA NA NA ## [486] NA NA NA NA NA ## [491] NA NA NA NA NA ## [496] NA NA NA NA NA ## [501] NA NA NA NA NA ## [506] NA NA NA NA NA ## [511] NA NA NA NA NA ## [516] NA NA NA NA NA ## [521] NA NA NA NA NA ## [526] NA NA NA NA NA ## [531] NA NA NA NA NA ## [536] NA NA NA NA NA ## [541] NA NA NA NA NA ## [546] NA NA NA NA NA ## [551] NA NA NA NA NA ## [556] NA NA NA NA NA ## [561] NA NA NA NA NA ## [566] NA NA NA NA NA ## [571] NA NA NA NA NA ## [576] NA NA NA NA NA ## [581] NA NA NA NA NA ## [586] NA NA NA NA NA ## [591] NA NA NA NA NA ## [596] NA NA NA NA NA ## [601] NA NA NA NA NA ## [606] NA NA NA NA NA ## [611] NA NA NA NA NA ## [616] NA NA NA NA NA ## [621] NA NA NA NA NA ## [626] NA NA NA NA NA ## [631] NA NA NA NA NA ## [636] NA NA NA NA NA ## [641] NA NA NA NA NA ## [646] NA NA NA NA NA ## [651] NA NA NA NA NA ## [656] NA NA NA NA NA ## [661] NA NA NA NA NA ## [666] NA NA NA NA NA ## [671] NA NA NA NA NA ## [676] NA NA NA NA NA ## [681] NA NA NA NA NA ## [686] NA NA NA NA NA ## [691] NA NA NA NA NA ## [696] NA NA NA NA NA ## [701] NA NA NA NA NA ## [706] NA NA NA NA NA ## [711] NA NA NA NA NA ## [716] NA NA NA NA NA ## [721] NA NA NA NA NA ## [726] NA NA NA NA NA ## [731] NA NA NA NA NA ## [736] NA NA NA NA NA ## [741] NA NA NA NA NA ## [746] NA NA NA NA NA ## [751] NA NA NA NA NA ## [756] NA NA NA NA NA ## [761] NA NA NA NA NA ## [766] NA NA NA NA NA ## [771] NA NA NA NA NA ## [776] NA NA NA NA NA ## [781] NA NA NA NA NA ## [786] NA NA NA NA NA ## [791] NA NA NA NA NA ## [796] NA NA NA NA NA ## [801] \u0026quot;2016-08-22\u0026quot; \u0026quot;2016-08-29\u0026quot; \u0026quot;2016-09-05\u0026quot; \u0026quot;2016-09-12\u0026quot; Conclusão A eterna disputa entre os que sabem exatamente o que acontece por trás dos programas e aqueles que não sabem vai continuar eternamente. Como disseram numa das issues que mencionei neste texto\nObviously there is no mathematical solution without inspecting the full vector that won’t run into edge cases.\nMas isso não deve ser motivo para se isolar numa caverna. Sempre podemos olhar com cuidado para as soluções que implementamos e entrar em contato com os desenvolvedores das nossas ferramentas.\nPor hoje, espero que esse problema específico sobre o lubridate esteja esclarecido e que ninguém fique com medo de usar a infinitude de pacotes de R. Como vimos aqui, mesmo que a princípios as coisas sejam obscuras, sempre podemos buscar mais informações sobre a implementação de uma função.\n","permalink":"https://blog.curso-r.com/posts/2017-07-29-comportamentos-estranhos-lubridate/","tags":["tidyverse","strings"],"title":"Comportamentos imprevisíveis do lubridate"},{"author":["William"],"categories":["discussões"],"contents":" Escolher uma profissão, para quem tem esse privilégio, é uma das decisões mais importantes das nossas vidas. Aos 17, 18 anos, a imaturidade, o pouco auto-conhecimento e a falta de informação sobre as alternativas podem nos desviar da opção que nos vestiria melhor, um erro que muitas vezes nunca será reparado.\nÀs vezes, eu me pergunto o que levou amigos e conhecido a escolherem suas profissões na hora do vestibular. No meu caso, eu quase segui um caminho da “profissões da moda”. O que me impediu de prestar Administração foi descobrir, na hora da inscrição, que era uma carreira da área de Humanas, não Exatas.\nSim, eu era bem perdido.\nNa época, a segunda fase da FUVEST era diferente para cada área, e eu não tinha perspectiva nenhuma de ir bem se tivesse que fazer uma prova dissertativa de História e Geografia em vez de Matemática e Física, disciplinas que eu dominava muito mais. Por isso, após uma (muito breve) pesquisa na internet, fui convencido a prestar Estatística, e o que me convenceu foi a frase “[…] envolve bastante matemática e o mercado de trabalho é muito bom”. Foi baseado nisso que eu tomei uma das decisões mais importantes da minha vida e era basicamente tudo o que eu sabia sobre a carreira quando comecei a graduação.\nPrestar vestibular para Estatística foi um tiro no escuro tão certeiro que às vezes me pego pensando em destino e esoterismos desse tipo. Durante a graduação, conheci pessoas que não tiveram a mesma sorte e acabaram desistindo nos primeiros semestres, que são bem pesados na matemática. A primeira parte da informação que eu tinha sobre realmente estava certa, e o curso de Estatística pode assustar quem não estiver na pegada de provar vários teoremas. Mas, neste post, não quero falar sobre as dificuldades da escalada, mas sim sobre a vista ao se chegar ao topo.\nConforme fui conhecendo a Estatística, eu descobri que ela é a profissão mais nerd que existe1. Eu sustento essa opinião porque a melhor definição de nerd que já escutei é “pessoa ama aprender” e, graças à Estatística, tenho a oportunidade de estudar muita coisa diferente.\nNesses dez anos como estatístico, já fiz análises na área de engenharia, finanças, educação, jornalismo, zoologia, farmácia, fisioterapia, medicina, psicologia, odontologia, educação física… e essas são apenas as que eu lembrei de cabeça. Estatística é parte essencial do método científico e está presente em todas as ciências. Pegar trabalhos novos para um estatístico nerd é extremamente motivante, porque não é apenas uma troca de tempo por dinheiro, é uma ótima chance para aprender coisas novas. A Estatística te estimula a ser curioso e criativo, e isso é o que eu mais amo nela.\nOutra coisa para se amar é o mercado de trabalho.\nA segunda parte da informação que eu tinha também estava correta: o mercado de trabalho para o estatístico é excelente! Não só pelo número de oportunidades, mas pela gama de lugares diferentes onde somos necessários. Não vou listar aqui porque é praticamente qualquer área. E sobre salários, como diria um professor do IME, dá para alimentar famílias.\nApesar de ter sido um dos poucos dos meus colegas a não mergulhar de cabeça no mercado, já tive duas experiências. A primeira foi como estagiário em um banco, onde aprendi bastante sobre o que eu não queria fazer na vida. Tudo o que eu fazia era rodar modelos pré-estabelecidos para gerar relatórios pré-formatados. Tinha aprendido tanta coisa legal na graduação e não podia usar nada, o que me fazia sentir como um pássaro engaiolado.\nA segunda foi no Instituto Butantan, onde eu era o único estatístico ao lado de vários biólogos, farmacêuticos e veterinários. Foi uma ótima experiência, na qual conheci muita gente bacana e aprendi muita coisa de biologia, farmâcia e controle de qualidade. Trabalhar com pessoas diferentes de você, com outras formas de pensar, é outra parte legal de ser estatístico. O pessoal do Butantan me ensinou bastante, principalmente sobre como a ciência e a pesquisa funcionam na prática. Além disso, foi lá que nasceu o meu interesse em ensinar Estatística.\nBom, essa foi uma parte da história de como eu me apaixonei pela Estatística. Talvez eu não tenha acrescentado nada se você já compartilha desse sentimento, mas espero que esse texto chegue a pessoas que ainda estejam escolhendo sua profissão e jogue luz sobre essa alternativa. Essa é a hora de mudarmos gráficos como esse.\nResumindo:\nEstatística é a profissão para quem gosta de aprender. Um bom estatístico no mercado é uma criança com cartão de crédito numa loja de brinquedos. No próximo post desta série, vou levantar um pouco de polêmica desabafando sobre alguns preconceitos de aprendizagem. Até breve!\nSe você ainda vê alguma conotação negativa na palavra nerd, mande as minha lembranças aos anos 90. :D↩︎\n","permalink":"https://blog.curso-r.com/posts/2017-08-31-por-que-amar-estatistica/","tags":["estatística"],"title":"Por que amar Estatística?"},{"author":["Julio"],"categories":["análises"],"contents":" Nesse final de semana decidi assistir a alguns vídeos do YouTube do Siraj Raval e ao curso do Andrew Ng sobre deep learning. Após assistir alguns vídeos, fiquei com uma vontade insana de implementar um modelo pra gerar músicas aleatórias do Wesley Safadão!\nNa minha opinião, o resultado ficou bem mais ou menos. Acho que tem muito o que melhorar ainda. Vejam o que acham!\nInstruções de uso:\nAperte o botão. ESPERE UM POUCO. Meu código é lento e botei num serviço gratuito do OpenCPU, então tenham paciência, por favor. Veja o texto que aparece. O texto até o | é original, e o resto é gerado automaticamente. Quando o tamanho do texto fica grande demais, adicionamos um \u0026lt;truncated\u0026gt; #comofas O trabalho foi feito em 3 passos: download, modelagem e implantação. Descrevemos cada um dos passos a seguir.\nDownload As letras foram baixadas do letras.mus.br. Primeiro, rodamos um script que lista os links de todas as músicas a partir da página do Wesley Safadão. O CSS path esquisito abaixo foi a forma mais compacta que encontrei de acessar os links diretamente.\nlibrary(magrittr) link_base \u0026lt;- \u0026#39;https://www.letras.mus.br\u0026#39; # listando os links ws_links \u0026lt;- paste0(link_base, \u0026#39;/wesley-safadao/\u0026#39;) %\u0026gt;% rvest::html_session() %\u0026gt;% rvest::html_nodes(\u0026#39;.cnt-list--alp \u0026gt; ul \u0026gt; li \u0026gt; a\u0026#39;) %\u0026gt;% rvest::html_attr(\u0026#39;href\u0026#39;) Em seguida, criamos uma função que pega a letra a partir de uma página.\npegar_letra \u0026lt;- function(link) { # do link até a parte que tem o conteúdo result \u0026lt;- paste0(link_base, link) %\u0026gt;% rvest::html_session() %\u0026gt;% rvest::html_nodes(\u0026#39;.cnt-letra \u0026gt; article \u0026gt; p\u0026#39;) %\u0026gt;% # Peguei o texto com as tags html para pegar os \\n as.character() %\u0026gt;% stringr::str_replace_all(\u0026#39;\u0026lt;[brp/]+\u0026gt;\u0026#39;, \u0026#39;\\n\u0026#39;) %\u0026gt;% paste(collapse = \u0026#39;\\n\\n\u0026#39;) %\u0026gt;% # Limpeza do texto limpar_musica() %\u0026gt;% tokenizers::tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE) c(result, \u0026#39;@\u0026#39;) # Adicionando @ no final } E usamos o maravilhoso combo purrr::map com progress::progress, que já tem um post dedicado no nosso blog.\n# baixando todas as listas p \u0026lt;- progress::progress_bar$new(total = length(ws_links)) ws_letras \u0026lt;- unlist(purrr::map(ws_links, ~{ p$tick() pegar_letra(.x) })) Note que eu escondi de vocês a função limpar_musica(). Essa função aplica uma série de expressões regulares para limpar os textos.\nlimpar_musica \u0026lt;- function(txt) { txt %\u0026gt;% stringr::str_trim() %\u0026gt;% stringr::str_to_lower() %\u0026gt;% stringr::str_replace_all(\u0026#39;[^a-z0-9êâôáéíóúãõàç;,!?: \\n-]\u0026#39;, \u0026#39;\u0026#39;) %\u0026gt;% stringr::str_replace_all(\u0026#39;[0-9]+x| bis\u0026#39;, \u0026#39;\u0026#39;) %\u0026gt;% stringr::str_replace_all(\u0026#39;([ ,?!])+\u0026#39;, \u0026#39;\\\\1\u0026#39;) %\u0026gt;% stringr::str_replace_all(\u0026#39; ([;,!?:-])\u0026#39;, \u0026#39;\\\\1\u0026#39;) %\u0026gt;% stringr::str_replace_all(\u0026#39;\\n{3,}\u0026#39;, \u0026#39;\\n\\n\u0026#39;) } O resultado é o objeto ws_letras: um vetor tamanho 557459 em que cada elemento é um caractere, que pode ser uma letra, número, espaço e até uma pulada de linha. Cada música é separada pelo caractere @. Aqui está a primeira delas:\ncat(head(ws_letras, which(ws_letras == \u0026#39;@\u0026#39;)[1] - 1), sep = \u0026#39;\u0026#39;) ## assim é o nosso amor ## io io io io io iooo ## 100 amor ## io io io io io iooo ## ## só a gente se olhar que coração dispara ## e as bocas calam ## e o desejo fala por nós dois ## ## canalisando o nosso amor, ## nada se compara ## é fogo é tara, ## no antes durante e depois ## ## coisa rara bonito de ver ## o mundo pára pra eu e você ## é um conto de fadas a nossa paixão ## duas vidas em um só coração Modelagem Não vou entrar em detalhes na parte estatística, mas basicamente utilizei uma rede LSTM (Long Short-Term Memory) e apenas uma camada oculta, copiada covardemente de um código feito pelo Daniel Falbel nos tutoriais do Keras para o R. O modelo serve para classificar caracteres (não palavras) e considera uma janela de passado máximo de 40 caracteres para realizar suas predições. Por esse motivo as letras geradas podem ter erros gramaticais feios (e.g. palavras iniciadas em ç).\nPor simplicidade, omiti o código que faz a preparação dos dados para ajustar no keras. Assim que eu tiver mais domínio sobre LSTM e Recurrent Neural Networks (RNNs) em geral farei um post dedicado.\nA especificação do modelo é simples: i) adicionamos apenas uma camada LSTM com 128 unidades, ii) adicionamos uma camada oculta com o número de unidades igual ao total de caracteres distintos presentes no texto e iii) aplicamos uma ativação softmax, que dá as probabilidades de cada candidato a próximo caractere.\nConsideramos como função de custo a Categorical Cross Entropy, a mesma da regressão logística. Como otimizador usamos o Adam, que faz basicamente uma descida de gradiente, mas aplica médias móveis com o passo anterior e com a derivada obtida via back propagation, realizando atualizações mais suaves.\nNo final, ajustamos o modelo com mini-batches de 256 observações e cinco épocas. Isso significa que fazemos 5 passos gigantes da descida de gradiente usando toda a base de dados, separados em diversos passinhos com 256 observações cada.\nNa prática, eu rodei o fit algumas vezes, reduzindo manualmente a taxa de aprendizado lr para fazer um ajuste mais fino. Cada época demorava aproximadamente 6 minutos no meu notebook, que não tem GPU.\nlibrary(keras) model \u0026lt;- keras_model_sequential() model %\u0026gt;% layer_lstm(128, input_shape = c(maxlen, length(chars))) %\u0026gt;% layer_dense(length(chars)) %\u0026gt;% layer_activation(\u0026quot;softmax\u0026quot;) # custo e otimizador model %\u0026gt;% compile( loss = \u0026quot;categorical_crossentropy\u0026quot;, optimizer = optimizer_adam(lr = 0.0001) ) # ajuste model %\u0026gt;% fit( keras_data$X, keras_data$y, batch_size = 256, epochs = 5 ) Também temos duas funções interessantes a serem discutidas. A primeira é a sample_mod(), uma função que recebe as probabilidades de cada letra e gera uma nova letra com essas probabilidades. O parâmetro diversity= aumenta ou diminui manualmente todas essas probabilidades, fazendo o modelo alterar um pouco seu comportamento. Quando maior esse parâmetro, maior a chance de saírem caracteres inesperados e, quanto menor, maior a chance de sair um texto completamente repetitivo.\nsample_mod \u0026lt;- function(preds, diversity = 1) { preds \u0026lt;- log(preds) / diversity exp_preds \u0026lt;- exp(preds) preds \u0026lt;- exp_preds / sum(exp_preds) which.max(as.integer(rmultinom(1, 1, preds))) } A outra função é gerar_txt(), nosso gerador de textos. Essa função recebe o modelo do Wesley Safadão e retorna um novo texto. O algoritmo funciona assim:\nPosicionamento. Escolhemos aleatoriamente uma posição do texto de entrada que tenha um @ (start_index). Lembre-se, o @ delimita o final ou início de uma letra. Inicialização. Pegamos os 40 caracteres seguintes, indicados por maxlen= e guardamos no vetor sentence. Geração de caracteres. Em seguida, entramos no seguinte laço: enquanto o modelo não gera um @ (final da canção), criamos um novo caractere com sample_mod() e adicionamos à nossa sentença final. Para garantir que o código termina de rodar num tempo finito, paramos o laço se criarmos mais de limit= sem aparecer um @. Impressão. Na hora de imprimir o texto, adicionamos um | como separador para indicar qual parte foi extraída da base real e qual parte é gerada automaticamente. Também adicionamos um \u0026lt;truncated\u0026gt; no final caso a fase anterior tenha passado do limit=. gerar_txt \u0026lt;- function(model, txt, diversity = 1.0, limit = 1000, maxlen = 40) { # parte 1 - posicionamento chars \u0026lt;- sort(unique(txt)) txt_index \u0026lt;- which(txt[-length(txt)] == \u0026#39;@\u0026#39;) start_index \u0026lt;- sample(txt_index, size = 1) + 1L id_txt \u0026lt;- which(txt_index == start_index) # parte 2 - inicialização sentence \u0026lt;- txt[start_index:(start_index + maxlen - 1)] generated \u0026lt;- paste0(c(sentence, \u0026#39;|\u0026#39;), collapse = \u0026quot;\u0026quot;) next_char \u0026lt;- \u0026quot;\u0026quot; total_chars \u0026lt;- 0 # parte 3 - geração de caracteres while (next_char != \u0026#39;@\u0026#39; \u0026amp;\u0026amp; total_chars \u0026lt; limit) { x \u0026lt;- sapply(chars, function(x) {as.integer(x == sentence)}) dim(x) \u0026lt;- c(1, dim(x)) next_index \u0026lt;- sample_mod(predict(model, x), diversity) next_char \u0026lt;- chars[next_index] generated \u0026lt;- paste0(generated, next_char, collapse = \u0026quot;\u0026quot;) sentence \u0026lt;- c(sentence[-1], next_char) total_chars \u0026lt;- total_chars + 1 } # parte 4 - impressão s_final \u0026lt;- stringr::str_sub(generated, 1, -2) if (total_chars == limit) s_final \u0026lt;- paste0(s_final, \u0026#39;\\n\u0026lt;truncated\u0026gt;\u0026#39;) s_final } Implantação Para deixar o modelo acessível pela internet, utilizei o maravilhoso OpenCPU. Trata-se de um pacote em R e também um software para transformar códigos R em API. Basicamente, o que fazemos é:\nCriar um pacote do R com as funções que temos interesse. No nosso caso, temos o pacote safadao, que foi criado para guardar o modelo ajustado e a função que gera as letras, definida acima. Instalar o OpenCPU em um servidor na nuvem. Informar ao OpenCPU que queremos servir um pacote específico. Felizmente, só precisei realizar de fato o primeiro passo dessa lista. O Jeroen Ooms, autor dessa solução, nos dá uma vantagem a mais: ele mantém um servidor na nuvem onde qualquer usuário pode subir seu próprio pacote, totalmente de graça. Ou seja, podemos criar APIs com nossos modelos preferidos, de graça e sem esforço. Acesse esse link para instruções mais detalhadas de como fazer a implantação.\nNo nosso caso, a API é acessível pelo link abaixo.\nhttp://jtrecenti.ocpu.io/safadao/R/gen/json Basta fazer uma requisição POST para esse link e ele retornará uma letra do Wesley Safadão.\nWrap-up Vimos aqui mais uma aplicação da estatística que parece um pouco fora da caixa mas que na verdade é bem pé no chão. Para trabalhar com esse tipo de dados, usualmente usamos redes neurais LSTM, adequada para dados em sequência. O modelo ainda tem muito a melhorar, tanto com ajustes na modelagem quanto na melhoria ao tratamento dos dados. Agora você pode criar o gerador de músicas do seu artista preferido. Tente replicar para outro artista! É isso. Happy coding ;)\nPS: Também montei um gerador de salmos (da bíblia) aleatório, usando a mesma técnica, mas ainda não estou feliz com o resultado. Quando estiver, posto aqui também :P\n","permalink":"https://blog.curso-r.com/posts/2017-08-27-safadao/","tags":["música"],"title":"Aquele 1% é Deep Learning - Gerando letras do Wesley Safadão"},{"author":["Athos"],"categories":["Tutoriais"],"contents":" O ponto máximo da história é o R mandando mensagem pro Telegram. Só para motivar, vou contar o que fiz de divertido usando telegram e preços de bitcoin.\nHerói do dia: Luca Biglia, autor do pacote telegram do R.\nO que faremos Como mencionei, Vou construir um Acompanhador de bitcoin pra ilustrar e ao fim do post teremos feito o R mandar um telegram quando o preço da bitcoin atingir um dado patamar.\nIngredientes Na arte de hoje vamos precisar de:\nUm bot do telegram O pacote do R library(telegram) A API da BlinkTrade Barbante e um tubo de cola Os passos que precisamos seguir para alcançar isso são:\nCriar um .Rproj (projeto do RStudio). instalar e carregar o pacote telegram do R. Criar um bot do telegram com a ajuda do BotFather. Conectar o R com o bot. Consultar e guardar os preços da bitcoin de 30 em 30 segundos. criar um loop infinito no R para acompanhar os preços sem parar. Fazer o R mandar mensagem pra gente quando o preço da bitcoin for maior que X, por exemplo. (extra) Pensar em ideias mirabolantes do que fazer com telegram + R. Introdução aos componentes Antes de mais nada, uma breve introdução às coisas que aparecerão por aqui:\ntelegram: é igual ao Whatsapp, mas melhorado. BotFather: um contato do seu celular (no telegram) que te ajuda a criar um bot do telegram. bitcoin: moeda digital de sucesso. BlinkTrade: dentre outras coisas, fornece API para valores da bitcoin em tempo real. Acompanhador de bitcoin Já crie o seu .Rproj, rode install.packages(\"telegram\");library(telegram) e vamos direto ao terceiro passo.\nPasso 3: Bot do telegram - Criar um bot Bot é como se fosse uma pessoa a mais na sua lista de contato do telegram, mas que são máquinas em vez de humanos e respondem a comandos específicos. A graça é que você pode customizar esses comandos do jeito que quiser! Basta ter um propósito e saber programá-lo.\nO README que tá no github do pacote é muito bom! Vou resumir com pequenas mudanças o que está lá:\nVá ao seu telegram e procure pelo BotFather como se estivesse procurando uma pessoa da sua lista de contato. Abra uma conversa com ele! Envie o texto “/start” e em seguida “/newbot”. Dê um nome ao seu bot (pode ser um nome fofo) e depois um nome de usuário para o seu bot que necessariamente termina em bot. Agora copie e cole o token que o BotFather te enviou no .Renviron. O meu bot tem o username AthosDamianiBot, então eu devo colocar o nome da variável assim: Se você usar essa convenção de nome você poderá usar a função bot_token() pra pegar o seu token. Caso contrário vai ter que apelar para a Sys.getenv().\nOBS: Reinicie o R para o .Renviron ficar configurado.\nPasso 4: Bot do telegram - Conectar ao R Carregue o pacote e crie um objeto TGBot para o seu bot criando anteriormente: library(telegram) bot \u0026lt;- TGBot$new(token = bot_token(\u0026#39;AthosDamianiBot\u0026#39;)) bot$getMe() Agora precisamos do chat_id. Para isso, no seu telegram, procure o seu bot como se fosse um contato (que nem você fez com o BotFather) e comece uma conversa com ele.\nNo R, chame o método bot$getUpdates() para pegar no R as mensagens que você enviou a ele e, finalmente, encontre o chat_id escondido no msgs.\nmsgs \u0026lt;- bot$getUpdates() msgs$message$chat$id[1] [1] 135717340 Com o chat_id em mãos, configure ele como chat_id padrão. bot$set_default_chat_id(135717340) Neste momento já estamos prontos para interagir com o nosso bot!\nPasso 5: Consultar preços da bitcoin Os preços da bitcoin são fornecidos pela API da BlinkTrade que é bem simples usar: basta pegar o json que a url do código abaixo solta. Aproveito e dou um tapinha para deixar em forma de data.frame e com a data de consulta junto.\nlibrary(jsonlite) library(tidyverse) safe_fromJSON \u0026lt;- safely(fromJSON, as.numeric(NA)) nova_consulta_list \u0026lt;- safe_fromJSON(\u0026quot;https://api.blinktrade.com/api/v1/BRL/ticker?crypto_currency=BTC\u0026quot;) nova_consulta \u0026lt;- nova_consulta_list$result %\u0026gt;% as.tibble %\u0026gt;% mutate(timestamp = lubridate::now()) nova_consulta # A tibble: 1 x 9 high vol buy last low pair sell vol_brl timestamp \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dttm\u0026gt; 1 14438.21 511.5911 14077.01 14200 13801.04 BTCBRL 14200 7257317 2017-08-18 17:09:44 OBS: Usei o advérbio safely() porque a API pode engasgar a qualquer momento, fazendo assim o R retornar um erro que interromperia o acompanhamento do preço.\nPasso 6: Loop infinito para acompanhar os preços O esqueleto do acompanhador é composto por um loop infinito (while(TRUE)), um data.frame historico.RData, um tempo entre uma consulta e outra (30 segundos por padrão) e a consulta propriamente dita.\n# inicializa o historico.RData # historico \u0026lt;- nova_consulta # save(historico, file = \u0026quot;historico.RData\u0026quot;) acompanhar_bitcoin \u0026lt;- function(frequencia = 30) { load(\u0026quot;historico.RData\u0026quot;) # loop infinito while(TRUE) { # pega a cotação do bitcoin brasil (BTCBRL) da API do blinktrade nova_consulta_list \u0026lt;- safe_fromJSON(\u0026quot;https://api.blinktrade.com/api/v1/BRL/ticker?crypto_currency=BTC\u0026quot;) # verifica se a API retornou uma lista if(\u0026quot;list\u0026quot; %in% class(nova_consulta_list$result)) { nova_consulta \u0026lt;- nova_consulta_list$result %\u0026gt;% as.tibble %\u0026gt;% mutate(timestamp = lubridate::now()) # --------------------- # # espaço reservado para as regras! # # --------------------- # guarda a consulta historico \u0026lt;- bind_rows(historico, nova_consulta) save(historico, file = \u0026quot;historico.RData\u0026quot;) } } Sys.sleep(frequencia) } Passo 7: Regras para mensagens de telegram Agora é a hora de decidir o que o bot deve nos avisar! Deixei dois exemplos simples abaixo usando o método bot$sendMessage(), que como o nome sugere faz o bot enviar mensagem pra gente. Agora, toda vez que o preço da bitcoin valer menos que R$13.600 ou valer mais que R$14.600 eu vou ficar sabendo na hora!\n# caso o valor da cotação atinja algum critério, envia uma mensagem via telegram. if(nova_consulta$buy \u0026lt; 13600 \u0026amp; nova_consulta$last \u0026gt; 13900) { bot$sendMessage(\u0026#39;baixa!\u0026#39;) bot$sendMessage(nova_consulta$buy) } # ... if(nova_consulta$buy \u0026gt; 14600 \u0026amp; nova_consulta$last \u0026lt; 14500) { bot$sendMessage(\u0026#39;alta!\u0026#39;) bot$sendMessage(nova_consulta$buy) } Passo Extra: Ideias mirabolantes do que fazer com telegram + R Aqui reina a criatividade. Dá pra mandar gráficos, sons, stickers e qualquer tipo de mídia. Eu fiz um ggplot para ficar vendo a tendência, daí poderia pedir para ver o gráfico ao meu bot a qualquer momento.\n# ... isso vai dentro da funcao acompanhar_bitcoin() p \u0026lt;- ggplot(historico %\u0026gt;% gather(indicador, valor, high, low, buy, sell, last)) + geom_line(aes(x = timestamp, y = valor, colour = indicador)) print(p) Também é possível programar comandos para serem passados ao Bot para que ele construa algo para a gente, por exemplo, vc poderia criar um comando para comprar ou vender bitcoins em tempo real!\nO Julio uma vez criou um bot que era praticamente o próprio R, vc enviava código como mensagem e ele retornava o output do R! Depois vou pedir pra ele postar algo sobre isso.\nEnfim, dá pra ser engenhoso aí. Basta ter aquela ideia de chuveiro genial.\nExercício para casa: - Enviar o gemidão do zap para 3 amigos via linha de código.\nabs!\n","permalink":"https://blog.curso-r.com/posts/2017-08-19-r-telegram-bitcoin/","tags":["telegram","bitcoin"],"title":"Como fazer o R avisar pelo telegram que bitcoin tá barato"},{"author":["William"],"categories":["discussões"],"contents":" — Não sabia que nessa cidade a cada 20 minutos atropelam um homem? — Nossa! E como está o coitado?\nO episódio “Estatísticas” do Chaves foi o meu primeiro contato com o conceito de Estatística (pelo menos que eu possa me lembrar). Claro que naquela época, com 5 ou 6 anos, eu nunca imaginaria que seria essa a minha profissão. Assim como o Quico e o Chaves, eu não fazia muita ideia do que as “senhoras estatísticas”” eram e continuei sem saber de fato até entrar na graduação, em 2007.\nReassistindo o episódio, depois de mais de dez anos estudando a disciplina, me identifiquei bastante com a dificuldade que a Dona Florinda e o Professor Girafales têm para explicar o que são estatísticas, o que antes via apenas como uma escada para as piadas que constroem a cena.\nQuando saio da minha bolha de colegas de faculdade e trabalho, percebo o quanto conceitos básicos de probabilidade e estatística são desconhecidos pela população, mesmo aqueles presentes no dia a dia.\nRecentemente, lendo um comentário de um radialista sobre a derrota do São Paulo para o Coritiba, na rodada 18 do Campeonato Brasileiro, uma frase me chamou atenção.\n“Ao iniciar o jogo dessa quinta à noite, o Tricolor, de acordo com as estatísticas, tinha 1,78% de chances de vencer o Coritiba.”\nA tese do radialista é que a probabilidade dos quatro grandes times de São Paulo vencerem na mesma rodada do campeonato é de 1,78%, a frequência relativa desse evento na era de pontos corridos do Campeonato Brasileiro. Como o São Paulo foi o último grande a jogar e os outros três já haviam vencido, o pobre tricolor paulista teve suas chances reduzidas pelas estatísticas e acabou perdendo o jogo.\nEssa interpretação com certeza pode ser refutada por vários motivos, mas o que mais me incomodou foi o desconhecimento de probabilidade condicional, ou simplesmente como novas informações modificam as probabilidades dos eventos.\nEncucado, eu deixei uma resposta, cuja parte central é essa:\nMesmo se considerássemos que a probabilidade dos 4 grandes de SP ganharem numa rodada não dependesse de fatores como a fase dos times, os adversários, o momento do campeonato etc., esse número, 1,78%, seria a probabilidade dos quatro ganharem antes da rodada começar. Dado que já sabemos que os outros três ganharam, e considerando que o resultado desses jogos não influenciam o jogo do SP, a probabilidade do evento em questão ocorrer passa a ser apenas a probabilidade do SP ganhar o jogo dele.\nEm seguida, recebi alguns comentários de outros torcedores dizendo (jocosamente) que não tinham entendido nada do que escrevi. Comecei então a refletir sobre o assunto, pensando no quanto a minha explicação poderia estar confusa e de que forma poderia ter explicado melhor, no quanto as pessoas não costumam se esforçar para entender temas que elas não dominam e no quanto a falta de uma base matemática adequada atrapalha nessas horas.\nEu acredito que a Probabilidade e a Estatística são vítimas da onda do “é legal odiar Matemática”, que muitas pessoas se orgulham de surfar. Crianças saem da escola com um conhecimento superficial dessas disciplinas (quando muito!), achando que é tudo uma questão de jogar dados, calcular médias e fazer gráficos. Comunicadores sofrem para interpretar os números de uma pesquisa e pesquisadores encaram a análise estatística como o grande vilão que os separa da publicação.\nFelizmente, esse comportamento vem mudando, mesmo que a passos lentos. Profissionais estão buscando cursos de data science e programação, empresas estão promovendo cursos para qualificarem seus funcionários e o mercado para estatísticos continua um céu estrelado, tanto para analistas e programadores quanto para educadores.\nEu vejo essa mudança, e as pessoas ao meu redor também a veem. Mas o exemplo que citei acima me faz acreditar que preciso espiar fora da minha bolha.\nPor isso, vou começar uma pequena série de posts, dando a minha opinião sobre algumas coisas que orbitam a educação estatística e a programação, com o objetivo de gerar reflexão e discussão sobre o assunto. A Estatística vem crescendo como carreira, o estatístico vem se tornando cada vez mais protagonista, e vejo esse momento como o ideal para melhorarmos a educação da nossa disciplina.\nA minha ideia inicial era fazer um único post, mas na era dos textões, não quero colocar mais um na fila de vocês. Assim, dividirei nos seguintes tópicos:\nPor que amar a Estatística? Preconceitos no aprendizado Estatística e programação Espero que esses posts possam contribuir para mostrarmos para mais gente a importância da Estatística e da Computação e por que amamos tanto trabalhar com essas ciências.\nAté breve!\n","permalink":"https://blog.curso-r.com/posts/2017-08-17-eu-a-estatistica-e-o-r/","tags":["estatística"],"title":"Eu, a Estatística e a programação"},{"author":["Julio"],"categories":["análises"],"contents":" São Paulo é a minha cidade preferida. Não só porque moro aqui, mas também porque é uma cidade cheia de diversidade, boa gastronomia e oportunidades. Para sentir um pouco dessa vibe, recomendo passear na avenida Paulista aos domingos. É sensacional!\nMas a cidade da diversidade só é o que é porque temos muita, muita gente nela. O município tem 12 milhões de habitantes. Esse número é tão grande que temos um paulistano para cada 17 brasileiros! Se São Paulo fosse um país, seria o 77 do mundo, ganhando de países como a Bélgica, Grécia, Portugal, Bolívia e muitas outras.\nOutro dia eu estava pensando na seguinte problemática: qual é a área do Brasil ocupada pela população de São Paulo? Ou seja, se pegarmos os municípios com grandes áreas, quanto do país conseguiríamos preencher com 12 milhões de habitantes?\nO interessante é que essa questão recai exatamente no problema da mochila, que é um famoso desafio de programação inteira. Depois de estudar profundamente no wikipedia (😄), vi que o problema não é tão trivial como parece.\nO problema da mochila Considere o seguinte contexto: você tem uma mochila com capacidade de 15kg e precisa carregar a combinação de itens com maior valor, com cada item possuindo valores e pesos diferentes.\nOutra forma de pensar nesse problema é com um cardápio de restaurante:\nEm linguagem matemática, o que temos é a task:\n\\[ \\begin{aligned} \u0026amp; \\text{maximizar } \\sum_{i=1}^n v_i x_i \\\\ \u0026amp; \\text{sujeito à } \\sum_{i=1}^n w_i x_i \\leq W, \\text{ com } x_i \\in\\{0,1\\}\\\\ \\end{aligned} \\]\nNo nosso caso essas letras significam isso aqui:\n\\(n\\) é o número de municípios no Brasil (5570). \\(v_i\\) é a área do município \\(i\\). \\(w_i\\) é a população do município \\(i\\). \\(W\\) é a população de São Paulo (12 milhões). \\(x=(x_1,\\dots,x_n)^\\top\\) é o vetor que seleciona os municípios. Se o município \\(i\\) faz parte da solução \\(x_i=1\\) e, caso contrário, \\(x_i=0\\). Ou seja, queremos escolher municípios para colocar na mochila tentando maximizar a área, mas o máximo de população que podemos contemplar é 12 milhões.\nO problema da mochila é muito interessante pois trata-se de um problema NP-difícil, ou seja, não existe um algoritmo de polinomial capaz de resolvê-lo. Se \\(w_i \u0026gt; 0, \\forall i\\in1,\\dots,n\\) então a solução pode ser encontrada com um algoritmo pseudo-polinomial.\nForma ad-hoc Se \\(x_i\\) pudesse assumir valores entre zero e um (ou seja, se pudéssemos selecionar apenas pedaços de municípios), a solução seria trivial. Bastaria colocar os municípios em ordem decrescente pela razão \\(v_i/w_i\\) e escolher os municípios ou parte deles até obter \\(W\\).\nIsso indica uma forma sub-ótima de resolver o problema. Chamamos essa solução de ad-hoc. A solução é encontrada assim:\nColocar os municípios em ordem decrescente pela razão \\(v_i/w_i\\), Escolher os municípios de maior razão até que a população do próximo município estoure \\(W\\). Escolher outros municípios com maior razão na ordem até não ser possível incluir mais nenhum município. Solução ótima A solução ótima pode ser encontrada usando a função mknapsack() do pacote adagio. Por exemplo, considere os vetores de pesos w, valores p e máximo cap abaixo.\np \u0026lt;- c(15, 100, 90, 60, 40, 15, 10, 1) w \u0026lt;- c( 2, 20, 20, 30, 40, 30, 60, 10) cap \u0026lt;- 102 O vetor-solução é dado por\nis \u0026lt;- adagio::mknapsack(p, w, cap) is$ksack [1] 1 1 1 1 0 1 0 0 Dados As áreas e estimativas das populações dos municípios do Brasil em 2010 foram obtidas dos pacotes {geobr} e {abjData}. A leitura é realizada usando pacotes do {tidyverse}.\nPacotes:\nlibrary(magrittr) library(sf) da_sf \u0026lt;- geobr::read_municipality(year = 2010) dados \u0026lt;- da_sf %\u0026gt;% dplyr::mutate(area = as.numeric(sf::st_area(geom)) / 1e6) %\u0026gt;% tibble::as_tibble() %\u0026gt;% dplyr::select(-geom) %\u0026gt;% dplyr::mutate(muni_id = as.character(code_muni)) %\u0026gt;% dplyr::inner_join( dplyr::filter(abjData::pnud_min, ano == \u0026quot;2010\u0026quot;), \u0026quot;muni_id\u0026quot; ) %\u0026gt;% dplyr::select(muni_id, area, pop) %\u0026gt;% dplyr::mutate(razao = area / pop) %\u0026gt;% dplyr::filter(area \u0026gt; 0) Resultados A solução ad-hoc e ótima são computadas com esse código:\nd_solucao \u0026lt;- dados %\u0026gt;% dplyr::arrange(dplyr::desc(razao)) %\u0026gt;% # ordena para solucao adhoc funcionar dplyr::mutate( area2 = as.integer(area * 1000), # necessario para mknapsack funcionar s_knapsack = adagio::mknapsack(area2, pop, max(pop))$ksack, acu = cumsum(pop), s_adhoc0 = dplyr::if_else(acu \u0026lt; max(pop), 1, 0), s_adhoc = s_adhoc0 ) Agora, vamos melhorar a solução ad-hoc incluindo os melhores municípios.\nid_melhor \u0026lt;- 0 pop_faltam \u0026lt;- with(d_solucao, max(pop) - sum(s_adhoc0 * pop)) while (is.na(id_melhor)) { # pega id do melhor municipio a ser incluido id_melhor \u0026lt;- with(d_solucao, which(pop \u0026lt;= pop_faltam \u0026amp; s_adhoc == 0)[1]) if (is.na(id_melhor)) { d_solucao$s_adhoc[id_melhor] \u0026lt;- 1 pop_faltam \u0026lt;- with(d_solucao, max(pop) - sum(s_adhoc * pop)) } } A Tabela abaixo mostra os municípios que foram classificados diferentemente nos dois métodos. Note que a solução ótima trocou apenas um município da solução adhoc (Nova Aurora - GO) pelo município de Monte Alegre de Minas - MG.\nd_solucao %\u0026gt;% dplyr::filter(s_adhoc != s_knapsack) %\u0026gt;% dplyr::inner_join(abjData::muni, \u0026quot;muni_id\u0026quot;) %\u0026gt;% select(uf_nm, muni_nm, area, pop, s_adhoc, s_knapsack) %\u0026gt;% knitr::kable(caption = \u0026#39;Municípios diferentes nas duas soluções.\u0026#39;) uf_nm muni_nm area pop s_adhoc s_knapsack Piauí Colônia Do Piauí 949.0047 7387 0 1 Tocantins Riachinho 517.6699 4030 0 1 Santa Catarina Palmeira 289.4767 2291 0 1 A Tabela abaixo mostra a diferença dos resultados dos dois métodos. A solução ótima fica com apenas 92 pessoas a menos que São Paulo.\nMétodo Área total População total Diferença para sp adhoc 5669732.52073351 11152637 13906 knapsack 5671488.67201654 11166345 198 São Paulo - 11166543 0 Mapa final Visualmente, a solução ótima e a solução adhoc são idênticas. Por isso vou mostrar apenas como fica o mapa para a solução ótima.\nO resultado aparece na Figura 1. É realmente impressionante ver que aquela regiãozinha vermelha tem a mesma população que toda a região azul do mapa.\nda_sf %\u0026gt;% dplyr::mutate(muni_id = as.character(code_muni)) %\u0026gt;% dplyr::inner_join(d_solucao, \u0026quot;muni_id\u0026quot;) %\u0026gt;% ggplot2::ggplot() + ggplot2::geom_sf( ggplot2::aes(fill = as.factor(s_knapsack)), size = 0 ) + ggplot2::geom_sf( fill = \u0026quot;red\u0026quot;, size = 0, colour = \u0026quot;black\u0026quot;, data = dplyr::filter(da_sf, code_muni == \u0026quot;3550308\u0026quot;) ) + ggplot2::scale_fill_manual( values = c(\u0026quot;gray90\u0026quot;, viridis::viridis(1, 1, .2, .8)) ) + ggplot2::theme_void() + ggplot2::theme(legend.position = \u0026quot;bottom\u0026quot;) + ggplot2::labs(fill = \u0026quot;Solução\u0026quot;) Figura 1: Resultado final da análise. A área em azul tem a mesma população da área em vermelho! É isso! Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2017-04-10-sao-paulo/","tags":["mapas"],"title":"São Paulo e o problema da mochila"},{"author":["Fernando"],"categories":["conceitos"],"contents":" A característica mais importante de um modelo estatístico é a sua flexibilidade. Esse termo pode ser entendido de várias formas, mas neste texto vou considerar que um modelo é flexível se ele explica coerentemente uma ampla gama de fenômenos reais.\nPensando assim, a regressão linear pode ser considerada um modelo flexível, já que muitas relações funcionais cotidianas são do tipo \\(y = \\beta x\\). É justamente por causa dessa flexibilidade que a boa e velha regressão de mínimos quadrados é tão usada, até mesmo aonde não deveria. O seu uso é tão indiscriminado que uma vez, em aula, um professor extraordinariamente admirável me disse que “90% dos problemas do mundo podem ser resolvidos com uma regressão linear”.\nSendo bastante honesto, é provável que o meu professor esteja certo, mas este post não é sobre isso. Este é um post sobre o que fazer quando a regressão linear simples não basta. No que segue, vamos discutir uma pequena (e poderosa) extensão do modelo de regressão linear simples, mas antes de prosseguir para o problema propriamente dito (e sua implementação em R), vamos discutir da teoria que existe por trás dele.\nRegressão linear é programação quadrática Embora seja pouco enfatizado nos bacharelados de estatística, uma regressão linear pode ser formulada como um problema de programação quadrática. Entrando nos detalhes, essa afirmação deve-se a dois fatos:\nExiste uma teoria, que chama-se programação quadrática, que soluciona problemas da forma \\[\\min_x \\frac{1}{2}x\u0026#39; Q x + c\u0026#39; x,\\]\nonde \\(x \\in \\mathbb{R}^p\\) e \\(Q\\) e \\(c\\) tem dimensões que fazem a conta acima ter sentido. A teoria ocupa-se desenvolvendo algoritmos exatos e aproximados para obter soluções desses problemas, inclusive com generalizações:\n\\[\\min_x \\frac{1}{2}x\u0026#39; Q x + c\u0026#39; x, \\text{ sujeito a }Ax \\geq 0.\\]\nUma regressão linear consiste em resolver \\[\\min_\\beta (Y - \\beta X)\u0026#39;(Y-\\beta X),\\]\nque, com um pouco de álgebra, é equivalente à\n\\[ \\min_\\beta -2Y\u0026#39;X\\beta + \\beta\u0026#39;X\u0026#39;X\\beta.\\]\nLogo, tomando \\(Q = 2X\u0026#39;X\\) e \\(c = \\frac{1}{2}X\u0026#39;Y\\) tem-se que esse é um problema de programação quadrática, que por sua vez é um problema convexo, que, segundo a teoria, tem uma única solução no ponto \\(\\beta = (X\u0026#39;X)^{-1}X\u0026#39;Y\\).\nUma regressão linear simples mais flexível Talvez o jeito mais simples de flexibilizar uma regressão linear no sentido mencionado no começo desse texto é restringir os seus parâmetros. Em muitos contextos, esse é o único jeito de colocar conhecimentos prévios na modelagem1.\nUm caso bastante emblemático aparece nas curvas de crédito divulgadas pela ANBIMA2. Lá, ajusta-se um conjunto de curvas que depende de 6 parâmetros e cada curva representa uma classificação de risco (que nem aquela em que o Brasil pode tomar downgrade3). Como os níveis de risco estão ordenados, é natural exigir que também exista uma ordenação entre as curvas. Sem entrar em detalhes, a ideia pode ser expressa assim:\n\\[\\beta_{AAA} \u0026lt; \\beta_{AA} \u0026lt; \\beta_{A} \u0026lt; \\beta_{BBB} \u0026lt; ...\\]\nO que é que isso tem a ver com programação quadrática? A resposta é que a inequação acima pode ser escrita como \\(A\\beta \\geq 0\\), de tal forma já existe uma teoria para resolver uma regressão linear simples com restrições desse tipo! Basta que ela seja vista como um problema de programação quadrática.\nO pacote quadprog Existe um pacote de R para quase tudo, então, como não poderia deixar de ser, existe um pacote em R para resolver problemas do tipo:\n\\[\\min_x \\frac{1}{2}x\u0026#39; Q x + c\u0026#39; x, \\text{ sujeito a }Ax \\geq 0.\\]\nPara ilustrar o seu uso, vamos considerar um exemplo. Vamos simular um conjunto de dados em que \\(\\beta_5 = 0.31, \\beta_4 = 0.43, \\beta_3 = 1.31, \\beta_2 = 2.19, \\beta_1 = 2.29\\) são os valores reais que precisamos estimar, considere que vale\n\\[Y \\approx \\beta_1X_1 + \\beta_2X_2+\\beta_3X_3+\\beta_4X_4+\\beta_5X_5\\]\ne que o erro de regressão tem distribuição normal.\nset.seed(11071995) N \u0026lt;- 30 betas \u0026lt;- c(2.29, 2.19, 1.31, 0.43, 0.31) X \u0026lt;- matrix(rnorm(5*N), byrow = T, ncol = length(betas), nrow = N) Y \u0026lt;- X %*% betas + rnorm(N, sd = 3) Se soubermos a priori que valem as seguintes afirmações\n\\[ \\beta_1,\\beta_2,\\beta_3,\\beta_4,\\beta_5 \u0026gt; 0 \\text{ e } \\beta_1 \u0026gt; \\beta_2 \u0026gt; \\beta_3 \u0026gt; \\beta_4 \u0026gt; \\beta_5,\\]\na minimização de \\((Y-\\beta X)\u0026#39;(Y-\\beta X)\\) pode ser resolvida usando a função solve.QP. Tudo que precisamos fazer é escrever o conjunto de inequações na forma \\(A\\beta \\geq 0\\). Mas isso é bem fácil! Basta notar que as restrições são equivalentes à\n\\[ \\left(\\begin{array}{cccc} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; -1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; -1 \\\\ \\end{array}\\right) \\times \\left(\\begin{array}{c}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5 \\end{array}\\right) \\geq 0.\\]\nDessa forma, o problema está prontinho pra passar no moedor de carne, com uma última ressalva. O problema resolvido no solve.QP é\n\\[\\min_x \\frac{1}{2}x\u0026#39; Q x + c\u0026#39; x, \\text{ sujeito a }A\u0026#39;x \\geq 0,\\]\nentão vamos ter que tomar o cuidado de passar as nossas restrições através do transposto da matriz que obtivemos acima. Isso resultará na matriz \\(A\\).\nlibrary(tidyverse) library(quadprog) Q \u0026lt;- t(X) %*% X c \u0026lt;- t(Y) %*% X A \u0026lt;- cbind(c(1,0,0,0,0),c(0,1,0,0,0),c(0,0,1,0,0), c(0,0,0,1,0),c(0,0,0,0,1),c(1,-1,0,0,0), c(0,1,-1,0,0),c(0,0,1,-1,0),c(0,0,0,1,-1)) solucao \u0026lt;- solve.QP(Q, # X\u0026#39;X c, # Y\u0026#39;X A, # A transposta c(0,0,0,0,0,0,0,0,0)) # vetor de comparação Para checar como valeu a pena todo esse esforço, dá uma olhada na diferença entre as estimativas! Os pontinhos vermelhos são as estimativas do modelo irrestrito, enquanto as barras são as estimativas do modelo com restrições.\nConclusões Regressão linear simples é um problema de programação quadrática. Algumas restrições interessantes podem ser escritas na forma \\(B\\beta \\geq 0\\). Programação quadrática resolve regressão linear simples com restrições lineares. Se em algum dia você topar com um bicho desses, o quadprog pode resolver o problema pra você. A menos que você seja uma pessoa razoável bayesiano.↩︎\nhttp://www.anbima.com.br/data/files/05/43/3E/84/E12D7510E7FCF875262C16A8/metodologia-curvas_20credito_20131104_v2_1_.pdf↩︎\nhttp://economia.estadao.com.br/noticias/geral,agravamento-da-crise-politica-eleva-risco-de-rebaixamento-do-brasil-diz-sep,70001824274↩︎\n","permalink":"https://blog.curso-r.com/posts/2017-08-07-minimos-quadrados-restrito/","tags":["conceitos","regressão"],"title":"Mínimos quadrados com restrições lineares"},{"author":["Julio"],"categories":["análises"],"contents":" Esse post assume uma versão mais antiga do decryptr para instalar a versão usada use: devtools::install_github(\"decryptr/decryptr@05bfd48\")\nNo último post sobre CAPTCHAs nós vimos que a segmentação das imagens (separar uma imagem em várias imagens, uma para cada caractere) é um problema complicado. Definir uma largura fixa ou utilizar outros métodos ad-hoc para segmentar as imagens pode dar bons frutos, mas não é o suficiente para quebrar CAPTCHAs mais complexos, como o da Receita Federal.\nAlguns meses atrás, tentamos resolver esse problema de várias formas. Uma delas foi utilizar algoritmos de agrupamento (\\(k\\)-médias) ou de identificação de conjuntos conectados. Esses algoritmos se mostraram instáveis e não aumentaram muito o poder preditivo. Outra ideia que tentamos foi criar vários critérios de corte fixos e incluir todas as colunas geradas na base de dados. Mas isso deixou nos deixou com uma dimensão muito grande pra tratar, e parecia que os modelos precisavam de muito mais dados pra começarem a funcionar.\nFoi aí que o Daniel nos disse que estava trabalhando no pacote do Keras e que existia uma forma de trabalhar com a imagem completa, sem segmentar. A tarefa de segmentação seria “parametrizada” num modelão complexo de deep learning e conseguiríamos resolver o problema sem pré-processamento.\nInicialmente, eu e o Athos ficamos perplexos com a ideia. Foi só quando o Daniel mostrou um modelo que acertava 100% dos CAPTCHAs do TJMG que fomos convencidos, e passamos a chamar esse modelo de “magia negra”.\nNesse post, vamos discutir como montar a base para fazer a magia negra.\nResposta Nossa resposta não é mais uma categoria, e sim uma matriz. A matriz tem \\(k\\) linhas (número de letras em um CAPTCHA) \\(p\\) colunas (número de valores possíveis de um caractere). O elemento \\((i,j)\\) vale 1 se na posição \\(i\\) aparece a letra relativa à posição \\(j\\).\nAssim,\na49f36 vira isso: (substituí 0 por '.' para ficar mais fácil de ver)\nIsso para uma imagem. Vamos precisar de uma terceira dimensão, que são as “linhas” de nossa resposta (uma para cada CAPTCHA).\nNosso y final é um array de dimensões \\(n \\times k \\times p\\). Achou estranho? Estamos só começando!\nExplicativas Uma imagem nada mais é do que uma matriz de pixels. Cada elemento da matriz é um número entre zero e um indicando o quanto de cor há nesse pixel. Assim, zero significa preto (ausência de cor), e um significa branco (todas as cores). Valores intermediários dão escala de cinza. Para representar imagens com cores, é necessária uma terceira dimensão de tamanho 3, indicando os pesos de R (red) G (green) e B (blue).\nAssim, nossa base de dados de explicativas é um array de dimensões \\(n \\times h \\times w \\times 3\\), em que \\(h\\) e \\(w\\) são a altura e a largura da imagem, respectivamente.\nFunção prepare Para facilitar a vida, criamos uma função prepare que prepara os arquivos de imagem num formato adequado para ajuste dos modelos usando o Keras.\nVeja um exemplo com 30 CAPTCHAs do TJMG. Aqui temos apenas cinco números por imagem, então \\(k=5\\) e \\(p=10\\).\nlibrary(decryptr) arqs \u0026lt;- dir(\u0026#39;../../static/data/captcha-dados/tjmg\u0026#39;, full.names = TRUE) d_captcha \u0026lt;- arqs %\u0026gt;% read_captcha(ans_in_path = TRUE) %\u0026gt;% join_captchas() str(d_captcha) Wrap-up Segmentar as imagens é complicado Vamos trabalhar com a imagem completa, mas precisamos de uma estrutura de dados adequada. Nossa resposta será um array de dimensão #captchas x #caracteres x #categorias, preenchidas sempre com zeros e uns. Nossa explicativa será um array de dimensão #captchas x altura x largura x 3, preenchidas com números entre zero e um, com os pesos de vermelho, verde e azul. Use a função decryptr::prepare() num vetor de caminhos de arquivos classificados para montar a base de forma adequada. Next No próximo post sobre esse tema, vamos falar um pouco das redes neurais profundas que vamos ajustar.\nÉ isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2017-06-29-captcha-dados/","tags":["captcha"],"title":"Quebrando CAPTCHAs - Parte IV: Trabalhando com a imagem completa"},{"author":["Athos"],"categories":["conceitos"],"contents":" Objetivos A finalidade do post é:\naprender a fazer uma regressão logística com o keras aprender a fazer um PCA com o keras aproximar o Deep Learning do que já havia de conhecido pela maioria dos analistas de dados. instigar a todos que vieram antes do deep learning a estudar e a ficar à vontade com as novidades em torno dela. mostrar que muitos profissionais inseridos na área de machine learning já conheciam grande parte do que o deep learning usa. levantar discussão sobre alguns mitos que não são construtivos para a comunidade dos analistas de dados. Motivação Li estatísticos, cientistas da computação, engenheiros de dados a afins questionando o futuro do Machine Learning e se tudo que conhecíamos antes sobre modelagem estatística havia ficado obsoleto (como essa pergunta no Quora: Should I Quit Machine Learning?).\nE em conversas com pessoas próximas percebia certa ufania pela novidade e frustração pela “obsolência” do que se havia investido tempo estudando antes.\nPara piorar, aproveitadores pegaram jacaré nessa onda para fazer marketing malicioso com o intuito de desvalorizar e dividir a comunidade dos analistas de dados. Algo bem similar com o que aconteceu com outras palavras da moda como data science, big data, Python versus R e a própria machine learning. Antes havia a clássica propaganda de que a empresa X utilizava MACHINE LEARNING em vez de modelos preditivos. Agora a coisa evoluiu e apelam para o uso da palavra Deep Learning.\nO que realmente importa:\nDeep Learning é uma grande novidade e colocou a Inteligência Artificial em evidência. Quem manjava Machine Learning antes vai conseguir aplicar 95% do seu conhecimento nas aplicações de Deep Learning (incluindo baysianismo, bootstrap, inferência, probabilidade e a boiada toda). Deep Learning tem que ser visto como uma ferramenta a mais na caixa do analista de dados e não um substituto. E para abordar essa questão resolvi ajustar uma regressão logística usando deep learning para que todos que já fizeram uma regressão logística antes possam dizer que já fizeram uma rede neural também! Confesso ter uma leve motivação provocativa, mas qual graça teria se assim não fosse? =P\nO que faremos Regressão logística para \\(Y_1\\) (com glm) Deep Learning para \\(Y_1\\) (com keras) Mostrar que regressão logística não é o melhor para \\(Y_2\\) e que Deep Learning vai além da limitação dos modelos lineares (com glm) Deep Learning para \\(Y_2\\) (com keras) Mãos à obra.\nPacotes library(keras) library(dplyr) library(tidyr) library(forcats) library(ggplot2) Regressão logística versus Deep Learning Hora de ajustar modelos para os mesmos dados de duas maneiras diferentes: regressão logística com glm e deep learning com o keras.\nDados simulados logit \u0026lt;- function(p) log(p) - log(1 - p) logistic \u0026lt;- function(x) 1/(1 + exp(-x)) n \u0026lt;- 100000 set.seed(19880923) df \u0026lt;- data_frame(x = runif(n, -2, 2.5)) %\u0026gt;% mutate(y_1 = rbinom(n, 1, prob = logistic(-1 + 2 * x)), # y_1 y_2 = rbinom(n, 1, prob = logistic(-1 + 2 * tanh(-1 + 2 * x)))) # y_2 O código acima criou duas variáveis respostas (targets). Em representação matemática, elas possuem as seguintes definições:\nResposta y_1\n\\[E[Y_1|x] = \\text{logistic}{(-1 + 2x)} = \\frac{1}{1 + e^{{-(-1 + 2x)}}}\\]\nResposta y_2\n\\[E[Y_2|x] = \\text{logistic}{(-1 + 2\\tanh(-1 + 2x))} = \\frac{1}{1 + e^{{-(-1 + 2\\tanh(-1 + 2x))}}}\\]\n\\(x\\) é linear no logito de y_1, então a regressão logística vai cair bem para descobrir os parâmetros \\(-1\\) e \\(2\\). Porém, \\(x\\) não é linhar no logito de y_2 e por isso a regressão logística não conseguirá representar fielmente o gerador de y_2.\nOBS 1: A forma \\(\\text{logistic}{(\\beta_0 + \\beta_1\\tanh(\\beta_2 + \\beta_3X))}\\) tem parâmetros dentro do função tanh, o que significa que a nossa hipótese para \\(E[Y_2|x]\\) não é mais linear nos parâmetros. Por isso que modelos lineares (como o nome sugere) não são mais indicados. E a não linearidade é uma das generalizações que as redes neurais nos fornece! (sim, isso é muito relevante)\nOBS 2: é claro que nesse caso bem simples de uma variável conseguiríamos inspecionar os dados para chegar em boas transformações de \\(x\\) de tal forma que o ajuste da logística ficasse tão bom quanto o de uma rede neural, mas se acrescentássemos muitas outras variáveis aí a coisa complicaria!\nEm representação de redes neurais, as fórmulas acima ficam assim:\nResposta y_1\nResposta y_2\nO que era função de ligação no GLM, em redes neurais virou função de ativação (no final eu falo mais sobre vocabulários que mudaram).\nOlhada nos dados # skimr::skim(df) %\u0026gt;% skim_print %\u0026gt;% with(numeric) %\u0026gt;% mutate_if(is.numeric, round, 2) %\u0026gt;% DT::datatable() df %\u0026gt;% gather(y_id, y_val, y_1, y_2) %\u0026gt;% mutate(x_cat = cut_number(x, n = 70)) %\u0026gt;% group_by(x_cat, y_id) %\u0026gt;% summarise(p = mean(y_val), n = n()) %\u0026gt;% mutate(logit_p = logit(p)) %\u0026gt;% gather(transformacao, p, p, logit_p) %\u0026gt;% mutate(transformacao = transformacao %\u0026gt;% fct_inorder %\u0026gt;% fct_recode(\u0026quot;logit(p)\u0026quot; = \u0026quot;logit_p\u0026quot;)) %\u0026gt;% ggplot() + geom_point(aes(x = x_cat, y = p, colour = y_id)) + theme_minimal(20) + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + facet_wrap(~forcats::fct_inorder(transformacao), nrow = 1, scales = \u0026quot;free_y\u0026quot;) + labs(x = \u0026quot;x\u0026quot;, colour = \u0026quot;resposta\u0026quot;) + theme(axis.text.x = element_blank(),panel.grid.major.x = element_blank(), panel.grid.minor.y = element_blank()) ## `summarise()` regrouping output by \u0026#39;x_cat\u0026#39; (override with `.groups` argument) O gráfico da direita mostra que x é proporcional ao logito das probabilidades de y_1 (em vermelho) como era pra ser por termos construído assim. Já com o y_2 (em azul) ainda ficou parecendo uma sigmoide mesmo depois da transformação.\nAjuste de modelos Regressão logística para \\(Y_1\\) (com glm) # modelo glm 1 ------------------------------------------------------ modelo_glm_1 \u0026lt;- glm(y_1 ~ x, data = df, family = binomial) # coefficients coef(modelo_glm_1) # (Intercept) x # -1.018434 2.011647 # accuracy conf_matrix_glm_1 \u0026lt;- table(modelo_glm_1$fitted.values \u0026gt; 0.5, df$y_1) sum(diag(conf_matrix_glm_1))/sum(conf_matrix_glm_1) # [1] 0.85061 As estimativas ficaram bem próximas dos verdadeiros valores \\(\\beta_0 = -1\\) e \\(\\beta_1 = 2\\).\nA acurácia foi de 85%.\nDeep Learning para \\(Y_1\\) (com keras) Vamos montar nossa hipótese para \\(E[Y_1|x]\\).\n# modelo keras 1 ------------------------------------------------------- # input: 1 variável: o x. input_keras_1 \u0026lt;- layer_input(1, name = \u0026quot;modelo_keras_1\u0026quot;) # output: não há camadas escondidas, apenas a função de ligação logit diretamente. output_keras_1 \u0026lt;- input_keras_1 %\u0026gt;% layer_dense(units = 1, name = \u0026quot;camada_unica\u0026quot;) %\u0026gt;% layer_activation(\u0026quot;sigmoid\u0026quot;, input_shape = 1, name = \u0026quot;link_logistic\u0026quot;) # sigmoid no tensorflow é a logistic # keras_model é o que constrói a nossa hipótese f(x) (da E[y] = f(x)) modelo_keras_1 \u0026lt;- keras_model(input_keras_1, output_keras_1) # summary(modelo_keras_1) Model _____________________________________________________________ Layer (type) Output Shape Param # ============================================================= modelo_keras_1 (InputLayer) (None, 1) 0 _____________________________________________________________ camada_unica (Dense) (None, 1) 2 _____________________________________________________________ link_logistic (Activation) (None, 1) 0 ============================================================= Total params: 2 Trainable params: 2 Non-trainable params: 0 _____________________________________________________________ A hipótese construída tem 2 parâmetros. Parece que está certo! \\(\\beta_0\\) e \\(\\beta_1\\).\nAgora é a vez da função de perda.\nComo nosso objetivo é construir uma regressão logística, nós vamos escolher a função de perda binary_crossentropy que é sinônimo de deviance da logística, termo mais comum no mundo da estatística.\nA métrica 'accuracy' não entra no otimizador da função de perda, a gente usa ela para comparar os modelos que criamos. No caso vamos comparar com o modelo glm ajustado acima (mas, por exemplo, em caso de eventos raros a 'accuracy' não vai ser muito informativa, daí poderíamos usar 'auc', 'gini', etc.).\nmodelo_keras_1 %\u0026gt;% compile( loss = \u0026#39;binary_crossentropy\u0026#39;, optimizer = optimizer_sgd(lr = 0.4), metrics = c(\u0026#39;accuracy\u0026#39;) ) modelo_keras_1_fit \u0026lt;- modelo_keras_1 %\u0026gt;% fit( x = df$x, y = df$y_1, epochs = 20, batch_size = 1000, verbose = 0 ) # coefficients modelo_keras_1 %\u0026gt;% get_layer(\u0026quot;camada_unica\u0026quot;) %\u0026gt;% get_weights # [[1]] # [,1] # [1,] 2.000054 # # [[2]] # [1] -1.015561 # accuracy loss_and_metrics_1 \u0026lt;- modelo_keras_1 %\u0026gt;% evaluate(df$x, df$y_1, batch = 100000, verbose = 0) loss_and_metrics_1[[2]] # [1] 0.85053 Resultados idênticos! Era para assim ser porque construímos a mesma hipótese e a memsa função de perda do glm.\nRegressão logística para \\(Y_2\\) (com glm) Para modelar \\(Y_2\\) vamos pisar em terrenos que os modelos lineares não pisam. Primeiro tento ajustar uma curva uasndo x e a transformação tanh(x). Esse preditor eu suponho que escolhi depois de uma minuciosa e demorada inspeção dos dados (tentei simular mais ou menos o que eu faria numa modelagem onde eu que teria que construir as features na mão).\n# modelo glm 2 ------------------------------------------------------ modelo_glm_2 \u0026lt;- glm(y_2 ~ x + tanh(x), data = df, family = binomial) # coefficients coef(modelo_glm_2) # (Intercept) x tanh(x) # -1.6698641 0.3043212 2.0936353 # accuracy conf_matrix_glm_2 \u0026lt;- table(modelo_glm_2$fitted.values \u0026gt; 0.5, df$y_2) sum(diag(conf_matrix_glm_2))/sum(conf_matrix_glm_2) # [1] 0.82204 Acurácia de 82%, nada mal. Mas a hipótese e parâmetros foram distintos do verdadeiro gerador dos dados. Vamos usar redes neurais para resolver o problema de não linearidade.\nDeep Learning para \\(Y_2\\) (com keras) Hipótese para \\(E[Y_2|x]\\).\n# modelo keras 2 ------------------------------------------------------- input_keras_2 \u0026lt;- layer_input(1, name = \u0026quot;modelo_keras_2\u0026quot;) output_keras_2 \u0026lt;- input_keras_2 %\u0026gt;% layer_dense(units = 1, name = \u0026quot;camada_um\u0026quot;) %\u0026gt;% layer_activation(\u0026quot;tanh\u0026quot;, input_shape = 1, name = \u0026quot;tanh_de_dentro\u0026quot;) %\u0026gt;% layer_dense(units = 1, input_shape = 1, name = \u0026quot;camada_dois\u0026quot;) %\u0026gt;% layer_activation(\u0026quot;sigmoid\u0026quot;, input_shape = 1, name = \u0026quot;link_logistic\u0026quot;) modelo_keras_2 \u0026lt;- keras_model(input_keras_2, output_keras_2) summary(modelo_keras_2) Model _____________________________________________________________ Layer (type) Output Shape Param # ============================================================= modelo_keras_2 (InputLayer) (None, 1) 0 _____________________________________________________________ camada_um (Dense) (None, 1) 2 _____________________________________________________________ tanh_de_dentro (Activation) (None, 1) 0 _____________________________________________________________ camada_dois (Dense) (None, 1) 2 _____________________________________________________________ link_logistic (Activation) (None, 1) 0 ============================================================= Total params: 4.0 Trainable params: 4.0 Non-trainable params: 0.0 _____________________________________________________________ Quatro parâmetros ‘treináveis’, é isso aí! Dois parâmetros de dentro do tanh e os dois parâmetros de fora. Precisamos que o keras nos devolva -1, 2, -1 e 2 do jeito que geramos os dados.\nFunção de custo\nmodelo_keras_2 %\u0026gt;% compile( loss = \u0026#39;binary_crossentropy\u0026#39;, optimizer = optimizer_sgd(lr = 0.1), metrics = c(\u0026#39;accuracy\u0026#39;) ) modelo_keras_2_fit \u0026lt;- modelo_keras_2 %\u0026gt;% fit( x = df$x, y = df$y_2, epochs = 20, batch_size = 100, verbose = 0 ) # coefficients modelo_keras_2 %\u0026gt;% get_layer(\u0026quot;camada_um\u0026quot;) %\u0026gt;% get_weights # [[1]] # [,1] # [1,] 2.012015 # # [[2]] # [1] -1.058052 modelo_keras_2 %\u0026gt;% get_layer(\u0026quot;camada_dois\u0026quot;) %\u0026gt;% get_weights # [[1]] # [,1] # [1,] 1.981977 # # [[2]] # [1] -1.006567 # accuracy loss_and_metrics_2 \u0026lt;- modelo_keras_2 %\u0026gt;% evaluate(df$x, df$y_2, batch_size = 100000) loss_and_metrics_2[[2]] # [1] 0.82221 Precisão de 82% também, mas agora os parâmetros estão bem próximos daqueles que geraram os dados! Acabamos de ver um conjunto de parâmetros sendo encontrados mesmo com relação não linear entre eles e a média.\nA precisão entre os dois modelos até que se equiparou, mas o gráfico das hipóteses encontradas (abaixo) mostra que a curva do glm está pior do que a curva do keras.\ndf %\u0026gt;% select(x, y_2) %\u0026gt;% mutate(x_cat = cut_number(x, n = 50)) %\u0026gt;% group_by(x_cat) %\u0026gt;% summarise(p = mean(y_2), x = mean(x), keras = logistic(-1.006567 + 1.981977 * tanh(-1.058052 + 2.012015 * x)), glm = logistic(-1.6698641 + 0.3043212*x + 2.0936353 * tanh(x)), n = n()) %\u0026gt;% mutate(logit_p = logit(p)) %\u0026gt;% gather(Modelo, estimativa, keras, glm) %\u0026gt;% ggplot() + geom_point(aes(x = x_cat, y = p)) + geom_line(aes(x = x_cat, y = estimativa, colour = Modelo, group = Modelo)) + theme_minimal() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + labs(x = \u0026quot;x\u0026quot;, colour = \u0026quot;resposta\u0026quot;) ## `summarise()` ungrouping output (override with `.groups` argument) (Bônus) PCA com autoencoer PCA e autoencodes servem na prática para reduzir a dimensionalidade dos dados. PCA é um caso particular de autoencoder com apenas uma camada e funções de ativação lineares. O post Construindo Autoencoders ensina a fazer e recomendo a leitura.\nResumo: autoencoder é uma técnica incrível que generaliza o PCA.\nDiscussão Na minha opinião aconteceu de que muita coisa antiga e consagrada teve seu nome mudado e apresentado como novo e isso acabou ofuscando as grandes contribuições realmente relevantes das pesquisas em torno das redes neurais e do deep learning.\nPercebe-se que o Deep Learning generalizou bastante coisa e por isso eu declaro o post bem sucedido se o escrito acima despertou curiosidade em aprender mais sobre deep learning para agregar ao trabalho que já havia sendo feito. Vale mais a pena trazer todos os praticantes de estatística e machine learning juntos nessa novidade do que nos dividirmos.\nAcredito que mais do que nunca a fundamentação teórica e interpretações terão seu valor potencializado com a disseminação do deep learning. Com o mito de que deep learning seja uma panaceia e com a facilidade que ela nos trouxe para fazer um modelo preditivo, há o risco de sermos soterrados por caixas pretas feitas por pessoas negligentes com aspectos importantes como interpretabilidade, causalidade e generalização. Talvez o bayesianismo se desponte (mais uma vez) como a solução para problemas qualitativos num mundo cada vez mais obscuro trazendo à luz os excessos dos modelos complexos e os benefícios dos modelos simples.\nPuxando o gancho do bayesianismo (e inferências em geral), os resultados já obtidos em cima de modelos lineares ainda se aplicam em deep learning. E também temos a vantagem de que todas as demais ferramentas que se usam em deep learning e que não afetam a linearidade dos parâmetros podem ser utilizadas, como convolucional, recorrente, max pooling, drop out, autoencoder e tantas outras.\nPara finalizar, na prática sugiro aplicar deep learning com o Keras, um pacote incrível que usa o tensorflow ou o theano por trás. Acredito que vocês verão muitos posts sobre o assunto por aqui! (podem encher o saco do Dan Falbel, um dos sócios da curso-r.com, que está envolvido no desenvolvimento desse pacote em R =]).\nCuriosidades N-ésimo menor deep learning Vimos acima o menor e o segundo menor Deep Learnings (que de profundo não têm nada =P). Mas podemos ir o tão profundo quanto quisermos! A representação de redes neurais sai fácil:\nJá a representação matemática fica esquisita:\n\\[E[Y|x] = \\frac{1}{1 + \\exp{\\left(\\beta_{p-1} + \\beta_p\\frac{1}{\\frac{\\vdots}{1 + \\exp{\\left(\\beta_{6} + \\beta_{7}\\frac{1}{1 + \\exp{\\left(\\beta_4 + \\beta_5\\frac{1}{1 + \\exp{\\left(\\beta_2 + \\beta_3\\frac{1}{1 + \\exp{\\left(\\beta_0 + \\beta_1x\\right)}}\\right)}}\\right)}}\\right)}}}\\right)}}\\]\nVocabulário Os jargões e termos do deep learning foram herdados de um outro contexto diferente do da modelagem preditiva estudada na estatística e por isso acabaram surgindo inúmeros sinônimos. Alguns deles são:\nfunção de ativação = função de ligação Softmax = verossimilhança da multinomial sigmoide = função com formato de S (no tensorflow o padrão é a logistic) pesos = parâmetros/betas/coeficientes binary crossentropy = deviance da distribuição binomial (regressão logística) É isso aí, temos que nos manter curiosos, questionar e dialogar. Abs!\n","permalink":"https://blog.curso-r.com/posts/2017-07-29-segundo-menor-dl/","tags":["deep learning","redes neurais","regressão logística","keras","glm"],"title":"Regressão Logística em: a menor deep learning do mundo"},{"author":["William"],"categories":["Tutoriais"],"contents":" No primeiro post sobre arrumação de base de dados, a gente viu como usar as funções do stringr para arrumar o nome das variáveis. Seguindo a dica do Julio, o quebrador de captchas, vamos falar do pacote janitor, que traz algumas funções para dar aquele trato nas BDs.\nAntes de mais nada, instale e carregue o pacote:\ninstall.packages(\u0026quot;janitor\u0026quot;) devtools::install_github(\u0026quot;sfirke/janitor\u0026quot;) # Versão de desenvolvimento library(tidyverse) library(janitor) Arrumando o nome das variáveis Assim como no post passado, utilizaremos a base com informações de pacientes com arritmia cardíaca, cujas variáveis selecionadas foram:\ndados %\u0026gt;% names ## [1] \u0026quot;ID\u0026quot; \u0026quot;Sexo\u0026quot; \u0026quot;Nascimento\u0026quot; ## [4] \u0026quot;Idade\u0026quot; \u0026quot;Inclusão\u0026quot; \u0026quot;Cor\u0026quot; ## [7] \u0026quot;Peso\u0026quot; \u0026quot;Altura\u0026quot; \u0026quot;cintura\u0026quot; ## [10] \u0026quot;IMC\u0026quot; \u0026quot;Superfície corporal\u0026quot; \u0026quot;Tabagismo\u0026quot; ## [13] \u0026quot;cg.tabag (cig/dia)\u0026quot; \u0026quot;Alcool (dose/semana)\u0026quot; \u0026quot;Drogas ilícitas\u0026quot; ## [16] \u0026quot;Cafeína/dia\u0026quot; \u0026quot;Refrig/dia\u0026quot; \u0026quot;Sedentario\u0026quot; ## [19] \u0026quot;ativ. Fisica\u0026quot; Os nomes têm letras maiúsculas, acentos, parênteses, pontos e barras, o que atrapalha na hora da programação. Para resolver esse problema, usamos a função clean_names().\ndados %\u0026gt;% clean_names() %\u0026gt;% names ## [1] \u0026quot;id\u0026quot; \u0026quot;sexo\u0026quot; \u0026quot;nascimento\u0026quot; ## [4] \u0026quot;idade\u0026quot; \u0026quot;inclusao\u0026quot; \u0026quot;cor\u0026quot; ## [7] \u0026quot;peso\u0026quot; \u0026quot;altura\u0026quot; \u0026quot;cintura\u0026quot; ## [10] \u0026quot;imc\u0026quot; \u0026quot;superficie_corporal\u0026quot; \u0026quot;tabagismo\u0026quot; ## [13] \u0026quot;cg_tabag_cig_dia\u0026quot; \u0026quot;alcool_dose_semana\u0026quot; \u0026quot;drogas_ilicitas\u0026quot; ## [16] \u0026quot;cafeina_dia\u0026quot; \u0026quot;refrig_dia\u0026quot; \u0026quot;sedentario\u0026quot; ## [19] \u0026quot;ativ_fisica\u0026quot; Veja que a função removeu os parênteses, pontos e barras e substituiu os espaços por _. No entanto, ela não remove os acentos. Assim, podemos adicionar mais uma linha ao pipeline para chegar onde queremos.\ndados %\u0026gt;% clean_names() %\u0026gt;% names %\u0026gt;% abjutils::rm_accent() ## [1] \u0026quot;id\u0026quot; \u0026quot;sexo\u0026quot; \u0026quot;nascimento\u0026quot; ## [4] \u0026quot;idade\u0026quot; \u0026quot;inclusao\u0026quot; \u0026quot;cor\u0026quot; ## [7] \u0026quot;peso\u0026quot; \u0026quot;altura\u0026quot; \u0026quot;cintura\u0026quot; ## [10] \u0026quot;imc\u0026quot; \u0026quot;superficie_corporal\u0026quot; \u0026quot;tabagismo\u0026quot; ## [13] \u0026quot;cg_tabag_cig_dia\u0026quot; \u0026quot;alcool_dose_semana\u0026quot; \u0026quot;drogas_ilicitas\u0026quot; ## [16] \u0026quot;cafeina_dia\u0026quot; \u0026quot;refrig_dia\u0026quot; \u0026quot;sedentario\u0026quot; ## [19] \u0026quot;ativ_fisica\u0026quot; E para substituir na base.\nnomes \u0026lt;- dados %\u0026gt;% clean_names() %\u0026gt;% names %\u0026gt;% abjutils::rm_accent() names(dados) \u0026lt;- nomes Removendo linhas e colunas vazias Esse banco de dados também tinha outro problema: linhas vazias. Na verdade, elas não eram completamente vazias, pois havia algumas informações de identificação do paciente, mas nenhuma outra variável tinha sido computada.\ndados[3,] ## # A tibble: 1 × 19 ## id sexo nascimento idade inclusao cor peso altura ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 3 \u0026lt;NA\u0026gt; NA NA NA \u0026lt;NA\u0026gt; NA NA ## # … with 11 more variables: cintura \u0026lt;chr\u0026gt;, imc \u0026lt;dbl\u0026gt;, ## # superficie_corporal \u0026lt;chr\u0026gt;, tabagismo \u0026lt;chr\u0026gt;, cg_tabag_cig_dia \u0026lt;dbl\u0026gt;, ## # alcool_dose_semana \u0026lt;dbl\u0026gt;, drogas_ilicitas \u0026lt;chr\u0026gt;, cafeina_dia \u0026lt;dbl\u0026gt;, ## # refrig_dia \u0026lt;dbl\u0026gt;, sedentario \u0026lt;chr\u0026gt;, ativ_fisica \u0026lt;chr\u0026gt; Essa foi a solução que eu pensei para resolver o problema utilizando a função remove_empty().\ndados %\u0026lt;\u0026gt;% as.data.frame %\u0026gt;% `row.names\u0026lt;-`(dados$id) %\u0026gt;% select(-id) %\u0026gt;% remove_empty(which = c(\u0026quot;rows\u0026quot;)) %\u0026gt;% mutate(id = row.names(.)) %\u0026gt;% select(id, everything()) dados %\u0026gt;% as_tibble() ## # A tibble: 4 × 19 ## id sexo nascimento idade inclusao cor peso altura ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 F 1964-01-31 00:00:00 41 2006-02-17 00:00:00 branca 75 1.63 ## 2 2 M 1959-01-28 00:00:00 45 2005-11-29 00:00:00 negra 71 1.7 ## 3 4 M 1957-09-13 00:00:00 50 2008-02-13 00:00:00 NT 80 1.64 ## 4 5 F 1938-02-06 00:00:00 71 2009-06-25 00:00:00 parda 56 1.51 ## # … with 11 more variables: cintura \u0026lt;chr\u0026gt;, imc \u0026lt;dbl\u0026gt;, ## # superficie_corporal \u0026lt;chr\u0026gt;, tabagismo \u0026lt;chr\u0026gt;, cg_tabag_cig_dia \u0026lt;dbl\u0026gt;, ## # alcool_dose_semana \u0026lt;dbl\u0026gt;, drogas_ilicitas \u0026lt;chr\u0026gt;, cafeina_dia \u0026lt;dbl\u0026gt;, ## # refrig_dia \u0026lt;dbl\u0026gt;, sedentario \u0026lt;chr\u0026gt;, ativ_fisica \u0026lt;chr\u0026gt; Eu precisei converter para data.frame primeiro porque não é possível definir os nomes das linhas de uma tibble. Se a linha estivesse completamente vazia, bastaria usar diretamente a função remove_empty_rows().\nEquivalentemente para colunas, existe a função remove_empty_cols().\nIdentificando linhas duplicadas O pacote janitor possui uma função para identificar entradas duplicadas numa base de dados: get_dupes(). Vamos criar uma base genérica para testá-la.\np_nome \u0026lt;- c(\u0026quot;Athos\u0026quot;, \u0026quot;Daniel\u0026quot;, \u0026quot;Fernando\u0026quot;, \u0026quot;Julio\u0026quot;, \u0026quot;William\u0026quot;) sobrenome \u0026lt;- c(\u0026quot;Damiani\u0026quot;, \u0026quot;Falbel\u0026quot;, \u0026quot;Corrêa\u0026quot;, \u0026quot;Trecenti\u0026quot;, \u0026quot;Amorim\u0026quot;) base_qualquer \u0026lt;- tibble(nome = sample(p_nome, 25, replace = T), sobrenome = sample(sobrenome, 25, replace = T), variavel_importante = rnorm(25)) get_dupes(base_qualquer, nome, sobrenome) ## # A tibble: 15 × 4 ## nome sobrenome dupe_count variavel_importante ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Athos Trecenti 2 -2.47 ## 2 Athos Trecenti 2 1.07 ## 3 Daniel Amorim 2 1.42 ## 4 Daniel Amorim 2 -0.134 ## 5 Daniel Corrêa 2 0.469 ## 6 Daniel Corrêa 2 0.0716 ## 7 Daniel Damiani 2 1.66 ## 8 Daniel Damiani 2 0.721 ## 9 Fernando Amorim 2 0.485 ## 10 Fernando Amorim 2 0.317 ## 11 Fernando Damiani 3 0.0695 ## 12 Fernando Damiani 3 0.112 ## 13 Fernando Damiani 3 -0.0940 ## 14 William Corrêa 2 0.575 ## 15 William Corrêa 2 -0.462 Todas as linhas na tibble resultante representam uma combinação de nome-sobrenome repetida.\nOutras funções Por fim, o janitor também tem funções equivalentes à table() para produzir tabelas de frequência:\ntabyl() - similar a table(), mas pipe-ável e com mais recursos. adorn_totals() - acrescenta o total das linhas ou colunas. adorn_crosstab() - deixa tabelas de contingência mais bonitas. mtcars %\u0026gt;% tabyl(cyl) ## cyl n percent ## 4 11 0.34375 ## 6 7 0.21875 ## 8 14 0.43750 mtcars %\u0026gt;% tabyl(cyl) %\u0026gt;% adorn_totals ## cyl n percent ## 4 11 0.34375 ## 6 7 0.21875 ## 8 14 0.43750 ## Total 32 1.00000 É isso! Espero que essas dicas e o pacote janitor ajudem a agilizar as suas análises. Dúvidas, críticas ou sugestões, deixe um comentário ou nos envie uma mensagem. :)\n","permalink":"https://blog.curso-r.com/posts/2017-07-24-janitor/","tags":["base de dados","pacotes","janitor"],"title":"Arrumando BDs: o pacote janitor"},{"author":["Julio"],"categories":["análises"],"contents":" Nesse post vamos discutir um pouco sobre modelar CAPTCHAs. Vou assumir que você já viu o post de introdução e o post sobre download, leitura e classificação manual de CAPTCHAs.\nDigamos que você tenha uma base de dados de treino composta por \\(N\\) imagens com os textos classificados. Nossa resposta nesse caso é uma palavra de \\(k\\) caracteres (vamos considerar \\(k\\) fixado), sendo que cada caractere \\(c\\) pode ter \\(p\\) valores.\nO problema de modelar o CAPTCHA diretamente é que a variável resposta tem um número exponencial de combinações de acordo com o número de caracteres:\n\\[ \\Omega = p^k. \\]\nPor exemplo, um CAPTCHA com \\(k=6\\) e \\(p=36\\) (26 letras e 10 números), que é muito comum, possui um total de 2.176.782.336 (\u0026gt; 2 bilhões) combinações! E não preciso dizer que é completamente inviável baixar e modelar tudo isso de CAPTCHAs.\nA alternativa imediata que aparece é tentar separar a imagem em um pedaço para cada caractere e fazer um modelo para prever caracteres. Assim nossa resposta é reduzida para \\(p\\) categorias, que é bem mais fácil de tratar.\nVamos usar como exemplo o CAPTCHA dos TRTs. Primeiro, o download:\nlibrary(decryptr) library(tidyverse) arq_captcha \u0026lt;- decryptr::download_trt(dest = \u0026#39;img\u0026#39;, n = 1) Visualizando a imagem:\n\u0026quot;../../static/data/captcha-segment/captcha705f7bad4a3d.jpeg\u0026quot; %\u0026gt;% read_captcha() %\u0026gt;% purrr::pluck(1) %\u0026gt;% plot() Infelizmente, segmentar a imagem nos lugares corretos é uma tarefa difícil. Pior até do que predizer as letras. Para simplificar, vamos fazer um corte fixado das letras:\narq_captcha %\u0026gt;% read_captcha() %\u0026gt;% purrr::pluck(1) %\u0026gt;% plot() abline(v = 30 + 15 * 0:5, col = \u0026#39;red\u0026#39;) Podemos também limitar os eixos x (tirar os espaços vazios à esquerda e à direita) e y (superiores e inferiores).\n\u0026quot;../../static/data/captcha-segment/captcha705f7bad4a3d.jpeg\u0026quot; %\u0026gt;% decryptr:::load_image() %\u0026gt;% magrittr::extract(-c(1:9, 31:dim(.)[1]), -c(1:15, 106:dim(.)[2]), TRUE) %\u0026gt;% grDevices::as.raster() %\u0026gt;% graphics::plot() abline(v = 15 * 1:5, col = \u0026#39;red\u0026#39;) abline(v = 15 * c(0, 6), col = \u0026#39;black\u0026#39;) abline(h = c(0, 21), col = \u0026#39;blue\u0026#39;) Agora temos uma imagem de tamanho dimensões 21x15 por caractere. Nosso próximo desafio é transformar isso em algo tratável por modelos de regressão. Para isso, colocamos cada pixel em uma coluna da nossa base de dados.\nNo caso do TRT, cada CAPTCHA gerará uma tabela de 6 linhas e 315 (21 * 15) colunas. Podemos usar esse código para montar:\narq_captcha %\u0026gt;% decryptr:::load_image() %\u0026gt;% magrittr::extract(-c(1:9, 31:dim(.)[1]), -c(1:15, 106:dim(.)[2]), 1) %\u0026gt;% as_tibble() %\u0026gt;% rownames_to_column(\u0026#39;y\u0026#39;) %\u0026gt;% gather(x, value, -y) %\u0026gt;% mutate_at(vars(x, y), funs(parse_number)) %\u0026gt;% mutate(letra = (x - 1) %/% 15 + 1, x = x - (letra - 1) * 15) %\u0026gt;% mutate_at(vars(x, y), funs(sprintf(\u0026#39;%02d\u0026#39;, .))) %\u0026gt;% unite(xy, x, y) %\u0026gt;% spread(xy, value, sep = \u0026#39;\u0026#39;) %\u0026gt;% mutate(y = c(\u0026#39;a\u0026#39;, \u0026#39;f\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;w\u0026#39;, \u0026#39;x\u0026#39;)) %\u0026gt;% select(y, everything(), -letra) Muito bem! Agora basta rodar o mesmo para toda a base de treino e rodar um modelo. Para esse post, vamos usar uma base de 2300 CAPTCHAs classificados. Essa base fica com 13800 linhas e 315 colunas. Vamos usar 11000 linhas para treino e as 2800 restantes para teste. O modelo utilizado será um randomForest padrão.\nlibrary(randomForest) dados \u0026lt;- readRDS(\u0026#39;../../static/data/captcha-segment/d_segment_captcha.rds\u0026#39;) %\u0026gt;% mutate(y = factor(y)) # monta bases de treino e teste set.seed(4747) # reprodutibilidade ids_treino \u0026lt;- sample(seq_len(nrow(dados)), 11000, replace = FALSE) d_train \u0026lt;- dados[ids_treino, ] d_test \u0026lt;- dados[-ids_treino, ] model \u0026lt;- randomForest(y ~ . - captcha_id, data = d_train) O resultado do modelo pode ser verificado na tabela de observados versus preditos na base de teste. O acerto foi de 99.4% em cada caractere! O maior erro ocorreu no confundimento da letra n com a letra h. Assumindo que o erro não depende da posição do caractere no CAPTCHA, teremos um acerto de aproximadamente 96.7% para a imagem.\nd_test %\u0026gt;% mutate(pred = predict(model, newdata = .)) %\u0026gt;% count(y, pred) %\u0026gt;% spread(pred, n, fill = \u0026#39;.\u0026#39;) %\u0026gt;% remove_rownames() %\u0026gt;% knitr::kable(caption = \u0026#39;Tabela de acertos e erros.\u0026#39;) y 2 3 4 5 6 7 8 9 a b d e f h j k m n r s t u v w x y 2 109 1 . . . . . . . . . . . . . . . . . . . . . . . . 3 . 105 . . . . 1 . . . . . . . . . . . . . . . . . . . 4 . . 116 . . . . . . . . . . . . . . . . . . . . . . . 5 . . . 97 . . . . . . . . . . . . . . . . . . . . . . 6 . . . . 108 . . . . . . . . . . . . . . . . . . . . . 7 . . . . . 111 . . . . . . . . . . . . . . . . . . . . 8 . . . . . . 120 . . . . . . . . . . . . . . . . . . . 9 . . . . . . . 51 . . . . . . . . . . . . . . . . . . a . . . . . . . . 63 . . . . . . . . . . . . . . . . . b . . . . . . . . . 110 . . . . . . . . . . . . . . . . d . . . . . . . . . . 108 . . . . . . . . 1 . . . . . . e . . . . . . . . . . . 110 . . . . . . . . . . . . . . f . . . . . . . . . . . . 116 . . . . . . . . . . . . . h . . . . . . . . . 1 . . . 116 . . . 1 . . . . . . . . j . . . . . . . . . . . . . . 115 . . . . . . . . . . . k . . . . . . . . . . . . . . . 126 . . . . . . . . . . m . . . . . . . . . . . . . . . . 122 2 . . . . . . . . n . . . . . . . . . 1 . . . 3 . . 1 132 . . . . . . . . r . . . . . . . . . . . . 1 . . . . . 91 . . . . . . . s . . . . . . . . . . . . . . . . . . . 113 . . . . . . t . . . . . . . . . . . . . . . . . . . . 106 . . . . . u . . . . . . . . . . 1 . . . . . . . . . . 99 . . . . v . . . . . . . . . . . . . . . . . . . . . . 99 . . 2 w . . . . . . . . . . . . . . . . . . . . . . . 107 . . x . . . . . . . . . . . . . . . . . . . . . . . . 132 . y . . . . . . . . . . . . . . . . . . . . . . . . . 102 Nem tudo são rosas O resultado para o CAPTCHA do TRT é bastante satisfatório, mas infelizmente não generaliza para outros CAPTCHAs. Tome por exemplo o CAPTCHA da Receita Federal abaixo. Nesse caso, a posição dos caracteres muda significativamente de imagem para imagem, e assim fica difícil cortar em pedaços.\nO mesmo modelo aplicado ao CAPTCHA da Receita possui acerto de 78.8% do caractere, o que equivale a apenas 23.8% de acerto para toda a imagem. Veja os resultados na tabela abaixo.\nmodel \u0026lt;- randomForest(y ~ . - captcha_id, data = d_train) y 1 2 3 4 5 6 7 8 9 a b c d e f g h i j k l m n o p q r s t u v w x y z 1 28 . . . . . . . . . . . . . 2 . . 2 4 . 3 . . . . . . . 10 . . . . . . 2 . 28 . . . . . . . . . 2 . 1 1 . . . 2 . . . . . . . . . . . . . 1 . 8 3 1 . 29 . 3 . . 1 1 . . . . . . . . . 6 1 . . . . . . . . . . . . . . . 4 . . . 35 . . . . 1 . . . . . . . . . 1 . . 1 . . . . 1 . . . . 1 . . . 5 . . . . 30 . . . . . 1 . . 1 . . . . 1 . . . . . . 2 . 1 . . . . . . . 6 . . . 1 . 38 . . . . 3 . . 2 . . 1 . . . . . . . . . . . 1 . 1 1 . 1 . 7 1 . . . . . 38 . . . . . 1 . . . . . 2 . . . . . . . . 1 . . 2 . . . . 8 . . . . . 2 . 31 . . 3 . . . . . 5 1 . 2 . . . . . . . 1 . . . . 1 . . 9 . . . 1 . . 1 . 41 . . . . . 1 . . . . . . . . . . . . . 1 . 1 . 1 . 2 a . 1 . . . . . . . 79 . . . . . . . . . . . . . . 1 . . 1 . . . . 1 . 1 b . . . . . . . 1 . 1 60 . . 1 . . 4 . . 1 . . . 3 2 2 2 2 1 . . . 1 . . c . 1 . . . . . . . . 2 62 1 5 1 2 . . 1 . . . . 6 1 1 . 1 1 . 1 . . . . d . . 1 . . . . . . 3 3 . 35 3 . 4 1 . 1 1 2 1 1 8 2 2 . . . 12 . 1 1 . . e . . . 1 . 1 . . . 4 1 1 . 83 2 . . . . . . . . . 2 . 1 1 . . . 3 . . . f . . . . . . . . . 1 . . . 3 79 . . . . 1 . 1 . . 2 . . . 4 . . . . 1 . g . . . . . . . . 2 1 . 1 . . . 64 . . 2 . . . . 3 . 17 . 2 1 . . . . . . h . . . . . . . 1 . . 3 . . 1 . . 87 . . 2 1 2 2 . . . 3 . 2 . . . . . . i 2 . . . . . . . . 2 . . . . 1 . . 36 2 . 1 . . . . . 1 . 4 . . 1 2 2 . j . . . . . . . . 2 . 1 . 1 . . . . 4 70 . . . . 1 . 1 . 1 1 1 2 . . . . k . . . . . . . 1 . . 1 . . . . . . . . 79 . . 1 . . . 1 . 1 . 1 1 1 . . l 3 . . . . . . . . . 1 . . 2 1 . 1 . . . 38 . . . . . . . 12 . . 1 . . . m . . . . . . . . . . . . . . . . 2 . . . . 101 3 . 1 . 2 . . . 1 . 1 . . n . . . . . . . . . . 1 . 1 1 1 . 8 . . . . 2 67 . . . 4 . . 2 . 2 1 . . o . . 1 . 1 . . . . 2 1 15 4 3 . 1 . . 2 . . . . 62 2 9 . 2 . 1 1 . . . . p 2 . . 1 . . . . . . 2 . . 2 2 3 1 . . . . . 1 1 75 . 4 . 3 . . 2 . . . q . . . . . . . . . 1 . 1 . 3 . 7 . . . . . 1 . 2 1 79 . . . . 1 . . . 1 r . . . . . . . . . . 2 . . . 4 . . . . 8 . . 1 . 3 . 65 . 4 . 1 . 1 . 1 s . . . . . . . 1 . 3 2 1 . 2 . . . . 2 . . . . . . . 1 87 . . 1 2 . . . t . . . 1 . . . . . . 2 1 . 1 2 2 . . 1 1 . . . . . 1 1 1 78 . 1 1 1 . 1 u . . . . . . . . 1 2 1 . 2 . 1 . 1 1 3 . . . 1 . . 1 . . . 80 2 2 . . . v . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 93 3 . 5 . w . . . . . . . . . . . . . . . . . . 1 . . . 1 . . . . . . . 1 100 . . . x . . . . . 2 . . . 1 . . . . . . . . . 1 . . . . . . . . . . . 3 91 1 . y . . 1 1 1 . . . . 2 . . . . 1 1 . . 2 . . 1 . 2 1 . . . 2 1 15 1 1 75 . z . . . . . . . . 1 1 . . . 2 . . 1 . . . . . . . . . . . . . . . . . 85 Claro que seria possível melhorar o poder preditivo com uma modelagem mais cuidadosa: nós usamos todos os parâmetros padrão da randomForest e não consideramos outros possíveis modelos. Mas acreditamos que o problema essencial está na segmentação, e não na modelagem após a segmentação.\nNos próximos posts, vamos mostrar como resolver o CAPTCHA da Receita com maior acurácia utilizando técnicas de Deep Learning que consideram a etapa de segmentação dentro da modelagem.\nWrap-up Não dá para considerar todas as combinações de valores de um CAPTCHA diretamente num modelo de regressão. Uma forma de resolver um CAPTCHA é segmentando a imagem em pedaços de mesma largura. Para montar a base de treino, criamos uma coluna para cada pixel. Um CAPTCHA corresponde a uma base com \\(k\\) linhas e número de colunas igual ao número de pixels. No CAPTCHA do TRT os resultados são satisfatórios. Já para CAPTCHA da Receita essa estratégia pode ser ineficaz. Vamos evoluir essa análise para técnicas que consideram a etapa de segmentação dentro da modelagem. É isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2017-06-29-captcha-segment/","tags":["captcha"],"title":"Quebrando CAPTCHAs - Parte III: Segmentação de imagens"},{"author":["Daniel"],"categories":["tutoriais"],"contents":" Salvar data.frames para ler depois é uma tarefa muito comum para quem trabalha com R, principalmente quando o seu processo possui algumas etapas mais demoradas e você não quer ter que rodar tudo de novo.\nVeja aqui 3 formas fáceis, e rápidas para salvar o seu banco de dados e não perder tempo lendo novamente.\nsaveRDS Talvez a função mais conhecida para salvar objetos do R. Ela salva em um formato binário que só pode ser lido pelo R. Por padrão comprime o arquivo após salvar, o que economiza espaço no disco, mas pode fazê-la levar mais tempo para rodar.\nConsidere um data.frame gerado pelo código abaixo:\nnrOfRows \u0026lt;- 1e7 x \u0026lt;- data.frame( Integers = 1:nrOfRows, # integer Logicals = sample(c(TRUE, FALSE, NA), nrOfRows, replace = TRUE), # logical Text = factor(sample(state.name, nrOfRows, replace = TRUE)), # text Numericals = runif(nrOfRows, 0.0, 100), # numericals stringsAsFactors = FALSE) Agora veja o tempo que demoramos para salvá-lo com o saveRDS.\nsystem.time({ saveRDS(x, \u0026quot;~/Desktop/saveRDS.rds\u0026quot;) }) # user system elapsed # 19.300 0.112 19.386 O espaço ocupado pelo arquivo é de 95MB. Indicando para a função que você não deseja comprimir:\nsystem.time({ saveRDS(x, \u0026quot;~/Desktop/saveRDS2.rds\u0026quot;, compress = FALSE) }) # user system elapsed # 0.260 0.116 0.377 O tempo vai para menos de 1s. Mas agora o arquivo ficou com 200MB. O pacote readr tem uma função chamada write_rds que é um wrapper de saveRDS que por padrão não comprime os arquivos, já que o Hadley diz que armazenamento é, em geral, muito mais barato do que tempo de processamento.\nÉ importante também verificar o tempo que demoramos para ler o arquivo novamente para o R. No caso ler o arquivo comprimido demora 2x mais do que o arquivo não comprimido.\nsystem.time({ a \u0026lt;- readRDS(\u0026quot;~/Desktop/saveRDS.rds\u0026quot;) }) # user system elapsed # 1.068 0.040 1.105 system.time({ a \u0026lt;- readRDS(\u0026quot;~/Desktop/saveRDS2.rds\u0026quot;) }) # user system elapsed # 0.380 0.068 0.447 Para salvar data.frames do R no disco, saveRDS é sempre a minha primeira opção: é relativamente rápido para ler e escrever e não exige instalação de nenhum pacote.\nAlém disso, o saveRDS serve para praticamente qualquer tipo de objeto do R, ou seja, você pode usá-lo para salvar os modelos que você ajustou ou qualquer outra coisa.\nAs principais desvantagens dessa função para as outras que mostrarei a seguir são:\nsó pode ser lido pelo R não permite que você leia apenas um subset das linhas/ colunas. feather feather é um formato de arquivo desenvolvido por duas pessoas muito fodas. Wes McKinney, autor do Pandas (principal biblioteca de manipulação de dados do python) e Hadley Wickham, principal desenvolvedor do tidyverse.\nO feather é bem rápido para salvar data.frames no disco, tempo comparável a salvar o arquivo sem comprimir usando o saveRDS. Mas só isso não é o suficiente para ser necessário usá-lo, já que neste caso o saveRDS rápido o suficiente.\nA principal vantagem do feather é que ele foi criado para ser um formato de compartilhamento de data.frames entre diversas linguagens de programação. Existem pacotes para ler arquivos .feather escritos em R, python, Julia: as três principais linguagens para análise de dados.\nO feather também permite que você leia apenas algumas linhas ou colunas do dataset, o que muitas vezes é útil para fazer consultas mais rápidas na base sem ter que ler tudo para a RAM de uma vez só.\nlibrary(feather) system.time({ write_feather(x, \u0026quot;~/Desktop/feather.feather\u0026quot;) }) # user system elapsed # 0.172 0.084 0.253 O arquivo produzido pesa 162MB. Para ler o arquivo salvo:\nsystem.time({ a \u0026lt;- read_feather(\u0026quot;~/Desktop/feather.feather\u0026quot;) }) # user system elapsed # 0.112 0.020 0.131 Usando o feather para ler apenas algumas linhas e colunas.\na \u0026lt;- feather(\u0026quot;~/Desktop/feather.feather\u0026quot;) b \u0026lt;- a[5000:6000, 1:3] fst fst é um pacote para ler e escrever data.frames de forma muito rápida.\nA imagem (retirada daqui) acima mostra a sua velocidade. O fst é mais ou menos 3 vezes mais rápido para ler os arquivos salvos e cerca de 2x mais rápido para escrevê-los. O arquivo salvo pelo fst é também um pouco menor: 130MB.\nLer e escrever é, assim como as outras opções, tão simples como usar uma função:\nlibrary(fst) # salvar write.fst(x, \u0026quot;/home/daniel/Desktop/dataset.fst\u0026quot;) # ler a \u0026lt;- read.fst(\u0026quot;~/Desktop/dataset.fst\u0026quot;) # ler apenas algumas linhas e colunas b \u0026lt;- read.fst(\u0026quot;~/Desktop/dataset.fst\u0026quot;, c(\u0026quot;Logicals\u0026quot;, \u0026quot;Text\u0026quot;), 2000, 4990) Note Como o Sillas mencionou nos comentários, a versão do CRAN do fst salva datas como numericos no arquivo. Os números podedm sser convertidos para data novamente usando a função as.Date do pacote zoo, mas tem que tomar cuidado!\nConclusão Use sempre saveRDS e readRDS, se precisar de velocidade, salve com o argumento compress = FALSE para não comprimir o arquivo.\nSe você for ler a base em python ou Julia e quiser um formato padronizado, use o feather.\nSe você for realmente ler e escrever os seus dados muitas vezes e você precisar de velocidade, use o fst.\n","permalink":"https://blog.curso-r.com/posts/2017-07-20-escrevendo-data-frames/","tags":["base de dados"],"title":"Salvando data.frames: uma comparação"},{"author":["Athos"],"categories":["Tutoriais"],"contents":" Versionamento de código é uma ferramenta que veio para ficar. Não dá para calcular o quanto de dor de cabeça, horas perdidas e frustrações ela economiza e já economizou na história da humanidade.\nUm dos programas que faz isso para a gente se chama Git. E não confundir com Github (que vamos usar para exemplificar mais para frente) que é um serviço que hospeda repositórios “controlados” pelo Git.\nAo final do post você estará apto(a) a trabalhar com Github no seu RStudio.\nMotivos para usar Github nos seus projetos de R 1) Deixa seu código atualizado e acessível de qualquer computador.\nCasa -\u0026gt; Trabalho, Trabalho -\u0026gt; Casa, por exemplo. Não há necessidade de carregar o laptop pra lá e pra cá caso você tenha vários Quartéis Generais.\n2) Colaboradores conseguirão contribuir sempre a partir da versão mais atualizada.\nSeu colega não vai mais precisar te pedir aquele versao_final_20170711.zip por email. Ele pode acessar o seu repositório quando quiser, mexer no código e depois pedir autorização para atualizar o projeto.\n3) Recupere uma versão funcional em caso de imprevistos.\nArrumar um bugzinho singelo, salvar o código, dormir, acordar no dia seguinte, perceber que o programa não funciona mais. Uma história de terror, porém corriqueira. O Github consegue recuperar o seu código a partir de uma atualização anterior.\n4) Seu projeto no ar.\nGithub é, além de tudo, uma rede social e é prática comum pessoas acessarem Githubs de projetos alheios para consumí-los. Além disso te fornece uma página em branco para você documentar e deixar seu trabalho para a posteridade. Existem pacotes de R que nem estão no CRAN, mas já estão no Github! A função devtools::install_github() existe para isso.\nPré-requisitos Você precisa ter instalado na sua máquina…\nR/RStudio (versões recentes para que tudo funcione direitinho) Git Uma conta no Github Preparação de um Projeto com RStudio e Github A vida com Github + RStudio segue a seguinte rotina (escreva esse fluxo num pedaço de papel e guarde na sua carteira):\nPasso 1 - Repositório: Criar repositório do projeto no Github Vá à sua conta do Github e crie um repositório com o nome do seu projeto.\nNo exemplo acima eu poderia acessar o conteúdo do meu projeto no link https://github.com/athospd/projecao_de_precos. Mas de modo geral, você acessaria o seu projeto no link https://github.com/\u0026lt;nome-do-usuario\u0026gt;/\u0026lt;nome-do-projeto\u0026gt;.\nBoas práticas:\nNomes sem espaço e sem caracteres estranhos. Deixe o nome bem simples!\nRUIM: projeção de preços BOM: projecao_de_precos Coloque uma descrição sucinta e direta sobre o seu projeto. Um link para um site com maiores detalhes também pode colocar aí.\nCrie um README.md. Logo mais volto a tocar nesse assunto, mas no momento da criação do seu repositório, já deixe um engatilhado!\nOBS: Incluir uma licença open-source é opcional, mas é fácil colocar, deixa claro que as outras pessoas podem usar e deixa seu projeto parecendo profissa rs.\nPasso 2 - .Rproj: Criação do Projeto no RStudio Agora precisamos criar um .Rproj que esteja atrelado ao seu repositório no Github. Por sorte, o RStudio já pensou em tudo e fez disso uma tarefa fácil.\nO caminho é:\nNew Project \u0026gt; Version Control \u0026gt; Git \u0026gt; Copia e cola URL do repositório \u0026gt; Create Project\nAo final do processo você terá todos os arquivos do prjeto no seu computador local.\nPasso 3 - Commit: Editando e “Commitando” as mudanças no código Dar ‘commit’ é o que se faz quando você resolve aceitar as mudanças que você fez no seu código/projeto.\nVamos montar uma história simulando duas mudanças no código, resultando em dois commits.\nMudança 1 Implementei a função soma e depois dei commit porque achei que já estava muito boa.\nMudança 2 Implementei a função subtrai e apesar de estar ainda com muitos bugs eu resolvi commitar porque precisava ir embora do Trabalho. Chegando em casa eu arrumo.\nAgora tem dois commits prontos para serem guardados no nosso repositório central do Github. Lembre-se que é no Github que todos os colaboradores vão continuar o projeto. Hora do PUSH!\nPasso 4 - Push: Subindo os commits para o Github Push é tão fácil quanto apertar um botão.\nOBS: Nesse momento pode ser que seja pedido login e senha para autorizar o PUSH. Mas provavelmente só será solicitado na primeira vez porque ele vai colocar o seu computador como origem conhecida (e confiável). Caso continue solicitando é aconselhável configurar o SSH (segue um tutorialzinho bacana).\nNo site do Github já vai estar as mudanças prontas para quem quiser ver.\nPasso 5 - Pull: Baixando o estado atual do projeto Suponha que entre o trajeto do meu trabalho para a minha casa meu colega tenha aprimorado a função soma.\nPara eu continuar da onde ele parou eu devo dar um PULL, ou seja, puxar pra minha máquina o código que está lá no Github.\nEntão, toda vez que abrir seu projeto, dê um PULL para voltar a ficar de onde parou.\nPasso extra - Issues: Documentando e delegando problemas Issues são questões em aberto sobre seu projeto. Geralmente se descreve um bug a ser consertado ou uma ideia a ser implementada no futuro. O Github permite delegar essas tarefas a alguém e também classificá-las se quiser.\nIndo além O fluxo mostrado acima representa o dia a dia de um programador solitário com eventuais parceiros colaboradores, mas o mundo da programação colaborativa com Git e RStudio é vasto! Para você saber, algumas coisas que não cobrimos aqui, mas que são bem comuns:\nFork Pull request Branches Gist Programas similares Em relação a programas de versionamento, o RStudio também trabalha bem com o SVN.\nJá serviços de hospedagem de repositórios, qualquer um funciona. O Bitbucket é muito bom!\nResumão Passo 1 - Repositório: Criar repositório do projeto no Github Passo 2 - .Rproj: Criação do Projeto no RStudio Passo 3 - Commit: Editando e “Commitando” as mudanças no código Passo 4 - Push: Subindo os commits para o Github Passo 5 - Pull: Baixando o estado atual do projeto Passo extra - Issues: Documentando e delegando problemas Essa é uma prática que facilita a vida do analista e que nós incentivamos nos cursos que ministramos. Se houver dúvida o Curso-R.com está sempre a disposição!\n","permalink":"https://blog.curso-r.com/posts/2017-07-17-rstudio-e-github/","tags":["github","rstudio","git"],"title":"RStudio e Github no dia a dia"},{"author":["Fernando"],"categories":["top 10"],"contents":" Os criadores do Markdown, segundo suas próprias palavras, criaram a linguagem com o objetivo de construir um “formato de texto fácil de ler, fácil de escrever e opcionalmente conversível para HTML”1. Com o passar do tempo, esse objetivo foi expandido pela comunidade, pois a simplicidade também é útil para outros formatos.\nAs extensões andam em dois sentidos. Um deles é construir tradutores universais entre todas as linguagens de marcação, como o pandoc, permitindo que seja fácil construir documentos a partir de textos em Markdown. A outra classe de extensões incorpora as vantagens obtidas a outras ferramentas, como o Rmarkdown, que adiciona o resultado de códigos em R aos textos escritos em Markdown.\nEssa segunda classe de extensões é particularmente importante para os estatísticos, já que a comunicação é uma parte muito importante do seu trabalho. Usando Rmarkdown, é fácil construir relatórios reprodutíveis, concisos e, por último mas não menos importante, bonitos.\nNeste post, vou listar pra vocês os meus 3 pacotes favoritos para formatar e organizar documentos em Rmarkdown, descrevendo brevemente o que eu mais gosto neles. Os três pacotes, assim como o próprio pacote rmarkdown, estão sendo desenvolvidos pela comunidade e pelo Rstudio, de tal forma que não vou mencionar explicitamente nenhuma das pessoas envolvidas nos projetos, mas antes de começar a lista quero deixar registrado que muito do que a gente tem hoje foi culpa do Yihui Xie. Muito obrigado, Yihui!\nPrimeira posição: rticles O pacote rticles contém vários templates para documentos em rmarkdown. Os modelos são todos baseados nos padrões de artigos acadêmicos, mas eles são bonitos e flexíveis o suficiente para garantir sua aplicabilidade em vários outros contextos.\nPra aplicações mais gerais, acho o template da Statistic in Medicine bastante adequado.\nSegunda posição: tufte Na nossa segunda posição, vem o pacote tufte. Ele contém templates para documentos em rmarkdown e ferramentas gráficas que se baseiam nas práticas de visualização do Edward Tufte 2, sujeito que está no banner desse post. A ideia aqui é construir relatórios fluidos e que incorporem formas de informação quantitativas (gráficos e tabelas) de maneira mais fluida.\nVendo alguns exemplos no site do Rstudio dá pra ver como a coisa é elegante e prática. A grande sacada é usar a margem com mais liberdade: algumas notas de rodapé, por exemplo, na verdade são pequenos comentários, então você pode inseri-los na margem da página pra sugerir essa relação mais íntima com o que você escreveu.\nTerceira posição: bookdown Na terceira posição vem um pacote um pouco mais jovem do que os anteriores. Dessa vez o objetivo é construir documentos mais longos, como uma apostila ou um relatório. A estrutura de arquivos é bastante simples e é possível criar capítulos, seções, sumários etc como se você estivesse rascunhando alguma coisa em rmarkdown.\nAlém de estruturar a escrita de um documento grande, esse pacote também se comunica bem com o tufte e já prevê alguma maneiras de publicar o seu trabalho. É possível compilar os books em um html, mais ou menos como os livros do Hadley, ou num pdf, sugerindo uma maneira bem legal de escrever uma dissertação ou uma tese. Se você integrar o bookdown com Travis, então, o céu é o limite.\nhttps://web.archive.org/web/20040402182332/http://daringfireball.net/projects/markdown/↩︎\nhttps://www.edwardtufte.com/tufte/↩︎\n","permalink":"https://blog.curso-r.com/posts/2017-07-15-top-3-rmarkdown/","tags":["r-markdown","rstudio"],"title":"Top 3 pacotes pra usar com rmarkdown"},{"author":["William"],"categories":["Tutoriais"],"contents":" Arrumar base de dados é uma tarefa chata, perigosa e pode consumir uma grande parcela do tempo total do projeto.\nA tarefa é chata porque é um grande retrabalho que muitas vezes cai na mão dos estatísticos. Mandar de volta para o responsável corrigir é uma opção, mas já passei por situações em que o tempo perdido no vai-e-vem da bd foi bem maior do que o tempo que eu gastaria para arrumar eu mesmo (e no fim, o banco ainda continha erros). Bds apropriadas para a análise poderiam (e deveriam) ser montados pelo pesquisador ou responsável desde o início, pois, em geral, são bem mais simples do que as que costumamos receber. Por isso, sempre vale espalhar as boas práticas de construção de bds na hora de pegar um trabalho.\nÉ uma tarefa perigosa porque mexer nos dados é sempre uma fonte de erro, principalmente quando fazemos alterações diretamente na planilha e/ou não temos conhecimento técnico sobre as variáveis.\nPor fim, se a base é muito grande e tem muitos erros, precisamos encontrar formas eficientes para corrigi-la, pois mexer em variável por variável, se não for inviável, pode demandar muito tempo.\nSendo assim, vamos começar aqui uma pequena série de posts para ajudar nessa tarefa, utilizando sempre as ferramentas do tidyverse, em especial do pacote stringr. Neste post, começarei mostrando uma forma rápida para padronizar os nomes das variáveis da bd, deixando-os mais adequados para o restante da análise.\nPara isso, vou utilizar como exemplo algumas variáveis de uma base com que trabalhei alguns anos atrás. A bd original (que continha informações de pacientes do Instituto do Coração) tinha cerca 170 variáveis, então selecionei apenas algumas.\nlibrary(tidyverse) dados %\u0026gt;% names ## [1] \u0026quot;Sexo\u0026quot; \u0026quot;Nascimento\u0026quot; \u0026quot;Idade\u0026quot; ## [4] \u0026quot;Inclusão\u0026quot; \u0026quot;Cor\u0026quot; \u0026quot;Peso\u0026quot; ## [7] \u0026quot;Altura\u0026quot; \u0026quot;cintura\u0026quot; \u0026quot;IMC\u0026quot; ## [10] \u0026quot;Superfície corporal\u0026quot; \u0026quot;Tabagismo\u0026quot; \u0026quot;cg.tabag (cig/dia)\u0026quot; ## [13] \u0026quot;Alcool (dose/semana)\u0026quot; \u0026quot;Drogas ilícitas\u0026quot; \u0026quot;Cafeína/dia\u0026quot; ## [16] \u0026quot;Refrig/dia\u0026quot; \u0026quot;Sedentario\u0026quot; \u0026quot;ativ. Fisica\u0026quot; Vejam que os nomes têm letras maiúsculas, acentos, parênteses, pontos e barras. Não é impossível fazer a análise com esses nomes, mas geralmente atrapalha bastante quando precisamos selecionar algumas dessas colunas. O ideal seria ter os nomes padronizados, até para ficar mais fácil de lembrarmos deles.\nPara deixar o exemplo reprodutível sem a necessidade de baixar a bd, gerei o código para criar um vetor com o nome das variáveis.\ndados %\u0026gt;% names %\u0026gt;% paste0(\u0026quot;\u0026#39;\u0026quot;, ., \u0026quot;\u0026#39;\u0026quot;, collapse = \u0026quot;, \u0026quot;) %\u0026gt;% paste0(\u0026quot;c(\u0026quot;, ., \u0026quot;)\u0026quot;) ## [1] \u0026quot;c(\u0026#39;Sexo\u0026#39;, \u0026#39;Nascimento\u0026#39;, \u0026#39;Idade\u0026#39;, \u0026#39;Inclusão\u0026#39;, \u0026#39;Cor\u0026#39;, \u0026#39;Peso\u0026#39;, \u0026#39;Altura\u0026#39;, \u0026#39;cintura\u0026#39;, \u0026#39;IMC\u0026#39;, \u0026#39;Superfície corporal\u0026#39;, \u0026#39;Tabagismo\u0026#39;, \u0026#39;cg.tabag (cig/dia)\u0026#39;, \u0026#39;Alcool (dose/semana)\u0026#39;, \u0026#39;Drogas ilícitas\u0026#39;, \u0026#39;Cafeína/dia\u0026#39;, \u0026#39;Refrig/dia\u0026#39;, \u0026#39;Sedentario\u0026#39;, \u0026#39;ativ. Fisica\u0026#39;)\u0026quot; Para padronizar os nomes (todos ao mesmo tempo), utilizei o código abaixo. Se você não está familiarizado com as expressões regulares (regex), temos um pequeno tuturial no material do nosso curso. Veja o que cada linha faz.\nstringr::str_trim(): remove espaços do começo e do final. Não tinha nenhum caso neste exemplo, mas é sempre bom garantir. stringr::str_to_lower(): transforma letras maiúsculas em minúsculas. abjutils::rm_accent(): remove os acentos das palavras. stringr::str_replace_all(\"[/' '.()]\", \"_\"): substitui barras, espaços e parênteses por subtraço _. stringr::str_replace_all(\"_+\", \"_\"): substitui um ou mais subtraços juntos por apenas um subtraço. stringr::str_replace(\"_$\", \"\"): remove os subtraços no final dos nomes. nomes \u0026lt;- dados %\u0026gt;% names %\u0026gt;% stringr::str_trim() %\u0026gt;% stringr::str_to_lower() %\u0026gt;% abjutils::rm_accent() %\u0026gt;% stringr::str_replace_all(\u0026quot;[/\u0026#39; \u0026#39;.()]\u0026quot;, \u0026quot;_\u0026quot;) %\u0026gt;% stringr::str_replace_all(\u0026quot;_+\u0026quot;, \u0026quot;_\u0026quot;) %\u0026gt;% stringr::str_replace(\u0026quot;_$\u0026quot;, \u0026quot;\u0026quot;) nomes ## [1] \u0026quot;sexo\u0026quot; \u0026quot;nascimento\u0026quot; \u0026quot;idade\u0026quot; ## [4] \u0026quot;inclusao\u0026quot; \u0026quot;cor\u0026quot; \u0026quot;peso\u0026quot; ## [7] \u0026quot;altura\u0026quot; \u0026quot;cintura\u0026quot; \u0026quot;imc\u0026quot; ## [10] \u0026quot;superficie_corporal\u0026quot; \u0026quot;tabagismo\u0026quot; \u0026quot;cg_tabag_cig_dia\u0026quot; ## [13] \u0026quot;alcool_dose_semana\u0026quot; \u0026quot;drogas_ilicitas\u0026quot; \u0026quot;cafeina_dia\u0026quot; ## [16] \u0026quot;refrig_dia\u0026quot; \u0026quot;sedentario\u0026quot; \u0026quot;ativ_fisica\u0026quot; Agora basta atribuir os nomes de volta aos dados.\nnames(dados) \u0026lt;- nomes Claro que o código utilizado funciona bem para esse exemplo. Se os nomes tivessem outros problemas, precisaríamos acrescentar mais linhas contendo outras mudanças. No entanto, essas alterações já resolvem a maioria dos casos mais comuns e é bem fácil modificar o código para lidar com outros problemas.\nDúvidas, críticas ou sugestões, deixe um comentário ou nos envie uma mensagem. :)\n","permalink":"https://blog.curso-r.com/posts/2017-07-13-bds_nomes_variaveis/","tags":["base de dados","tidyverse"],"title":"Arrumando BDs: nome das variáveis"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" No meu último post anunciei que começaríamos uma série sobre CAPTCHAs. Uma da nossas iniciativas principais nesse tema é a criação do pacote decryptr. Hoje veremos como usar algumas das funções principais desse pacote.\nSuposições do decryptr Ao criar o decryptr reduzimos um pouco o escopo de CAPTCHAs que gostaríamos de incluir. Fizemos isso para não ficarmos malucos, pois existem diversos tipos de testes disponíveis na web!\nAs suposições são:\nApenas imagens jpg ou png. Uma imagem possui apenas números e letras. A quantidade de caracteres de um CAPTCHA é fixa. Dois CAPTCHAs de mesma origem têm sempre as mesmas dimensões. Não conseguimos nem queremos quebrar o reCAPTCHA. Instalação O decryptr ainda não está no CRAN. Isso significa que para instalá-lo você precisará do devtools:\nif (!require(devtools)) install.packages(\u0026#39;devtools\u0026#39;) devtools::install_github(\u0026#39;decryptr/decryptr\u0026#39;) As funções principais do decryptr são\ndownload(): baixar imagens da web. read_captcha(): adiciona metadados úteis a uma string com o caminho do CAPTCHA. load_captcha(): carrega a imagem na memória. plot.captcha(): método S3 para desenhar o CAPTCHA na tela. classify.captcha(): método S3 para classificar CAPTCHAs manualmente. prepare.captcha(): método S3 para carregar CAPTCHAs em um formato adequado para modelagem usando o Keras. model.captcha(): método S3 para modelar os CAPTCHAs. predict.captcha(): método S3 para classificar um CAPTCHA a partir de um modelo ajustado e um caminho de imagem. Fluxo de utilização O modo de uso planejado do decryptr está descrito na Figura ??.\nComo ainda não temos a teoria completa para ajuste de modelos, nesse post vamos ficar com a utilização das funções de download, visualização e classificação.\nDownload A função download() tem cinco parâmetros:\nurl= o link do CAPTCHA que queremos baixar. dest= a pasta que queremos salvar a imagem. n= a quantidade de CAPTCHAs a serem baixados. secure= se TRUE, fará o download com a opção ssl_verifypeer = FALSE (veja esse post) type= extensão do arquivo (jpg/jpeg ou png). Essa não é uma das funções mais seguras do mundo, já que dependemos de uma boa conexão com o servidor de onde os CAPTCHAs serão baixados. A função também não trata de problemas com bloqueio de IP.\nPara facilitar a utilização do decryptr, adicionamos algumas funções do tipo download_*(), que já contêm os padrões para download de alguns sites específicos:\ndownload_rfb: Consulta de CNPJ da Receita federal. download_saj: Sistema SAJ (vários Tribunais Estaduais). download_tjmg: Tribunal de Justiça de Minas Gerais. download_tjrj: Tribunal de Justiça do Rio de Janeiro. download_tjrs: Tribunal de Justiça do Rio Grande do Sul. download_trt: Tribunais Regionais do Trabalho. Nesses casos, os únicos parâmetros são dest= e n=. Exemplo:\nlibrary(decryptr) download_tjmg(\u0026#39;img/tjmg\u0026#39;, n = 5) # salva arquivo em ./img/tjmg/captcha\u0026lt;id\u0026gt;.jpeg Visualização Para plotar um CAPTCHA basta ler o arquivo com read_captcha() e depois usar a função plot(). Exemplo:\nlibrary(decryptr) \u0026#39;../../static/data/captcha-dados/tjmg/captcha4d2f1097adba_73301.jpeg\u0026#39; %\u0026gt;% read_captcha() %\u0026gt;% purrr::pluck(1) %\u0026gt;% plot() Vale mencionar que esse não é um ggplot() então nem tente somar layers nesse gráfico 😄.\nClassificação A classificação manual de CAPTCHAs é importante para possibilitar o treino de modelos preditivos. Para classificar um CAPTCHA você pode utilizar a função classify(), assim:\n\u0026#39;img/tjmg/captcha4d2f795d4e4_92522.jpeg\u0026#39; %\u0026gt;% read_captcha() %\u0026gt;% classify() Essa função fará duas coisas:\nPlota o CAPTCHA na tela. Abre um console para o usuário digitar o valor do CAPTCHA manualmente. Ao escrever o valor o CAPTCHA, pressione \u0026lt;enter\u0026gt;. Após isso, a função classify() irá adicionar sua classificação após o nome da imagem, como no exemplo acima: _92522. A função classify() gera uma cópia para que seja impossível de perder a imagem original.\nAlgumas opções do classify():\ndest= colocar uma pasta para classificar os CAPTCHAs. Por padrão é a pasta onde os originais estão. answer= adicionar uma resposta ao invés de esperar abrir o console. Essa opção é útil quando as classficações são feitas automaticamente (e.g., por um quebrador de CAPTCHAs que usa o áudio no lugar da imagem.) Wrap-up Baixar com download() ou download_*(). Visualizar com read_captcha() pipe plot(). Classificar com read_captcha() pipe classify(). Caso encontre problemas, adicione issues no repositório do pacote.\nÉ isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2017-06-29-captcha-decryptr/","tags":["captcha","decryptr"],"title":"Quebrando CAPTCHAs - Parte II: O pacote decryptr"},{"author":["Daniel"],"categories":["Tutoriais"],"contents":" Estou bem longe de ter experiência em modelagem bayesiana usando linguagens de programação probabilística como Stan ou BUGS, mas vi esse pacote chamado greta que me chamou a atenção.\nO greta é um pacote feito totalmente em R, mas que usa o TensorFlow como backend para fazer os seus cálculos. Isso tudo por intermédio do reticulate. A vantagem de usar o TensorFlow como backend é a escalabilidade: o greta pode ser rápido até mesmo em bases de dados grandes e também pode ser acelerada usando clusters de CPU’s ou GPU’s.\nO greta já está disponível no CRAN e pode ser instalado com:\ninstall.packages(\u0026quot;greta\u0026quot;) Vou mostrar um exemplo simples copiado da página de início do site do próprio greta e depois vou implementar um modelo que o Fernando implementou aqui no blog em um outro post sobre estimar um modelo heterocedástico no R.\nExemplo simples Vamos implementar o modelo de regressão linear mais simples possível. Temos duas variáveis contínuas \\(x\\) e \\(y\\) e queremos estimar um modelo da forma:\n\\[y = a + b*x + \\epsilon\\]\nem que \\(\\epsilon\\) possui distribuição normal com média zero e desvio padrão \\(\\sigma\\). No greta isso pode ser feito da seguinte forma:\nlibrary(greta) # define as variáveis x e y x \u0026lt;- iris$Petal.Length y \u0026lt;- iris$Sepal.Length # define a distribuição priori dos parâmetros a = normal(0, 5) b = normal(0, 3) sd = lognormal(0, 3) # define o modelo mean \u0026lt;- a + b * x distribution(y) = normal(mean, sd) m \u0026lt;- model(a, b, sd) # retira amostras usando mcmc draws \u0026lt;- mcmc(m, n_samples = 1000) Você pode fazer um gráfico para visualizar as estimativas dos parâmetros usando o bayesplot:\nbayesplot::mcmc_trace(draws, facet_args = list(ncol = 1)) Podemos obter as estimativas pontuais dos parâmetros pegando, por exemplo, a mediana dessa amostra do MCMC.\napply(draws[[1]], 2, median) ## a b sd ## 4.2805710 0.4219092 0.3697692 Esse resultado é muito similar ao que pode ser obtido por uma regressão linear simples:\nlm(y ~ x) ## ## Call: ## lm(formula = y ~ x) ## ## Coefficients: ## (Intercept) x ## 4.3066 0.4089 Modelo linear heterocedástico Neste post o Fernando simulou um banco de dados que é heterocedástico e depois ajustou um modelo deste tipo de diversas maneiras, vou acrescentar mais uma aqui, desta vez usando o greta.\nVou simular o banco de dados da mesma forma que o Fernando:\nlibrary(ggplot2) N \u0026lt;- 1000 set.seed(11071995) X \u0026lt;- sample((N/100):(N*3), N) Y \u0026lt;- rnorm(N,X,4*sqrt(X)) qplot(X,Y) + theme_bw(15) + geom_point(color = \u0026#39;darkorange\u0026#39;) X2 \u0026lt;- sqrt(X) dataset \u0026lt;- data.frame(Y,X,X2) Agora o código em greta para ajustar esse modelo:\n# definir os vetores y \u0026lt;- dataset$Y x \u0026lt;- dataset$X x2 \u0026lt;- dataset$X2 # definir prioris dos parâmetros alpha \u0026lt;- gamma(1, 1) beta \u0026lt;- normal(0, 10) # definir ligações mean \u0026lt;- beta * x sd \u0026lt;- alpha * x2 # definir o modelo distribution(y) = normal(mean, sd) m \u0026lt;- model(alpha, beta) # ajustar draws \u0026lt;- mcmc(m, n_samples = 1000) Agora as estimativas pontuais:\napply(draws[[1]], 2, median) ## alpha beta ## 4.077689 1.002791 Como esperado, chegamos em estimativas bem próximas do que foi simulado.\nPor hoje é isso!! Abraços!\n","permalink":"https://blog.curso-r.com/posts/2017-07-09-greta/","tags":["tensorflow","bayes"],"title":"Greta"},{"author":["Athos"],"categories":["Tutoriais"],"contents":" WoE (weight of evidence) é uma ferramenta bastante usada em aplicações de regressão logística, principalmente na área de score de crédito. Simploriamente falando, ele transforma categorias em números que refletem a diferença entre elas pelo critério de separação do Y = 1 e Y = 0.\nSe você ainda não sabe o que é ou quer ler mais sobre o assunto, um texto que eu gostei de ler:\nData Exploration with Weight of Evidence and Information Value in R O autor desse texto é o Kim Larsen, criador do pacote Information que é completo e cheio de ferramentas sofisticadas em torno do WoE. Porém, no dia a dia do meu trabalho volta e meia eu tinha que construir rotinas próprias para fazer as versões em WoE das minhas variáveis, mesmo com vários pacotes completos disponíveis. A principal motivação era que eles não eram muito práticos e não se encaixavam na filosofia do tidyverse. Daí acabei juntando essas rotinas num pacote chamado tidywoe e deixando no ar. A ideia é que ela faça o analista ganhar em tempo, legibilidade e reprodutibilidade. Abaixo segue como usar.\nInstalação Para instalar, basta rodar abaixo.\n# install.packages(\u0026quot;devtools\u0026quot;) devtools::install_github(\u0026quot;athospd/tidywoe\u0026quot;) Dados e pacotes para os exemplos library(forcats) library(ggplot2) library(dplyr) library(tidywoe) # install.packages(\u0026quot;FactoMineR\u0026quot;) data(tea, package = \u0026quot;FactoMineR\u0026quot;) tea_mini \u0026lt;- tea %\u0026gt;% select(breakfast, how, where, price) Como usar Tem duas funções que importam: - add_woe() - adiciona os woe’s num data frame. - woe_dictionary() - cria dicionário que mapeia as categorias com os woe’s.\nadd_woe() A função add_woe() serve para adicionar as versões WoE’s das variáveis em sua amostra de dados.\ntea_mini %\u0026gt;% add_woe(breakfast) breakfast how where price how_woe where_woe price_woe breakfast tea bag chain store p_unknown -0.0377403 -0.0451204 -0.2564295 breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Você pode selecionar as variáveis que vc quiser selecionando-as como se fosse no dplyr::select().\ntea_mini %\u0026gt;% add_woe(breakfast, where:price) woe_dictionary() A função woe_dictionary() é uma das duas partes necessárias para fazer o add_woe() funcionar (a outra parte são os dados). Ele constrói o dicionário de categorias e seus respectivos woe’s.\ntea_mini %\u0026gt;% woe_dictionary(breakfast) variable explanatory n_tot n_breakfast n_Not.breakfast p_breakfast p_Not.breakfast woe how tea bag 170 80 90 0.5555556 0.5769231 -0.0377403 how tea bag+unpackaged 94 50 44 0.3472222 0.2820513 0.2078761 how unpackaged 36 14 22 0.0972222 0.1410256 -0.3719424 where chain store 192 90 102 0.6250000 0.6538462 -0.0451204 Usando um dicionário customizado Muitas vezes há o interesse em ajustar na mão alguns valores de woe para consertar a ordem dos efeitos de uma dada variável ordinal. Esse é o motivo de o add_woe() poder receber um dicionário passado pelo usuário. Isso se faz por meio do argumento .woe_dictionary.\nA maneira mais fácil de se fazer isso é montar um dicionário inicial com o woe_dictionary() e depois alterar os valores nele para alcançar os ajustes desejados. Exemplo:\n# Construa um dicionário inicial tea_mini_woe_dic \u0026lt;- tea_mini %\u0026gt;% woe_dictionary(breakfast) # Mexa um pouquinho nos woes tea_mini_woe_dic_arrumado \u0026lt;- tea_mini_woe_dic %\u0026gt;% mutate(woe = if_else(explanatory == \u0026quot;p_unknown\u0026quot;, 0, woe)) # Passe esse dicionário para o add_woe() tea_mini %\u0026gt;% add_woe(breakfast, .woe_dictionary = tea_mini_woe_dic_arrumado) breakfast how where price how_woe where_woe price_woe breakfast tea bag chain store p_unknown -0.0377403 -0.0451204 0.0000000 breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Not.breakfast tea bag chain store p_variable -0.0377403 -0.0451204 0.1872882 Exemplo de exploração O woe_dictionary() devolve uma tabela arrumada, bem conveniente para explorar mais. Por exemplo, a tabela está pronta para o ggplot.\ntea_mini_woe_dic_arrumado %\u0026gt;% mutate(explanatory = explanatory %\u0026gt;% as.factor %\u0026gt;% fct_reorder(woe)) %\u0026gt;% filter(variable %in% c(\u0026quot;price\u0026quot;, \u0026quot;how\u0026quot;)) %\u0026gt;% ggplot() + geom_bar(aes(x = explanatory, y = woe), stat = \u0026quot;identity\u0026quot;) + facet_wrap(~variable, scales = \u0026quot;free_x\u0026quot;) + theme(axis.text.x = element_text(angle = 30)) Aqui está o github do pacote para contribuições. Pretendo colocar bastante coisa nova no pacote ainda.\nAbs\n","permalink":"https://blog.curso-r.com/posts/2017-07-06-tidywoe/","tags":["woe"],"title":"WoE em R com tidywoe"},{"author":["William"],"categories":["Tutoriais"],"contents":" Se você ainda não é adepta ou adepto do tidyverse, provavelmente precisa setar stringsAsFactors = FALSE em algum momento ou sempre trabalha com fatores em vez de strings.\nStrings são sequências de caracteres que podem ser acessados pela sua posição. Assim, podemos usar expressões regulares para modificar partes da sequência que correspondam a um determinado padrão. Fatores são inteiros com categorias (ou labels) formadas por strings guardadas apenas uma vez no atributo levels. Por facilitarem a ordenação de valores de texto, eles são importantes para a criação de variáveis dummies e a definição de contrastes em funções de modelagem estatística, como a lm(), e a organização de atributos gráficos, como no pacote ggplot2.\nHá motivos históricos para funções como read.table() e data.frame() fazerem, por default, a coerção de caracteres para fatores. Você pode ler sobre isso nos posts stringsAsFactors: An unauthorized biography, do Roger Peng, e stringsAsFactors = , do Thomas Lumley. O que mudou é que, de um tempo para cá, nem toda variável não-numérica é, de fato, uma variável categórica. E, quando este é o caso, queremos trabalhá-las como strings.\nAlém de não transformar strings em fatores, o tidyverse também dispõe de um pacote só para manipular fatores: o forcats (for categorial variables). Para começar a usá-lo, instale e carregue o pacote.\ninstall.packages(\u0026quot;forcats\u0026quot;) library(forcats) library(tidyverse) O forcats é um pacote bastante simples. Basicamente, ele é composto por funções de apenas dois tipos:\nFunções que começam com fct_, que recebem uma lista de fatores e devolvem um fator. Funções que começam com lvl_, que modificam os níveis de um fator. Veja a seguir exemplos de como utilizar as principais funções.\nfct_recode Altera categorias específicas de um fator.\nfator \u0026lt;- factor(c(\u0026quot;Scorsese\u0026quot;, \u0026quot;DiCaprio\u0026quot;, \u0026quot;Patty Jenkins\u0026quot;, \u0026quot;Gal Gadot\u0026quot;)) # Alterando apenas uma fct_recode(fator, direcao = \u0026quot;Scorsese\u0026quot;) ## [1] direcao DiCaprio Patty Jenkins Gal Gadot ## Levels: DiCaprio Gal Gadot Patty Jenkins direcao # Alterando todas fct_recode(fator, direcao = \u0026quot;Scorsese\u0026quot;, direcao = \u0026quot;Patty Jenkins\u0026quot;, elenco = \u0026quot;DiCaprio\u0026quot;, elenco = \u0026quot;Gal Gadot\u0026quot;) ## [1] direcao elenco direcao elenco ## Levels: elenco direcao fct_collapse Junta categorias em grupos manualmente definidos.\n# Objeto simulando uma amostra do personagem # de série favorito de 100 pessoas nomes \u0026lt;- c(\u0026quot;Sheldon\u0026quot;, \u0026quot;Leonard\u0026quot;, \u0026quot;Penny\u0026quot;, \u0026quot;Howard\u0026quot;, \u0026quot;Rajesh\u0026quot;, \u0026quot;Ted\u0026quot;, \u0026quot;Marshall\u0026quot;, \u0026quot;Robin\u0026quot;, \u0026quot;Lily\u0026quot;, \u0026quot;Barney\u0026quot;, \u0026quot;Michael\u0026quot;, \u0026quot;Jim\u0026quot;, \u0026quot;Pam\u0026quot;, \u0026quot;Dwight\u0026quot;, \u0026quot;Andy\u0026quot;) per_fav\u0026lt;- sample(x = nomes, size = 100, replace = T) %\u0026gt;% as.factor head(per_fav) ## [1] Sheldon Robin Lily Sheldon Jim Pam ## 15 Levels: Andy Barney Dwight Howard Jim Leonard Lily Marshall Michael ... Ted # Conta o número de observações em cada categoria fct_count(per_fav) ## # A tibble: 15 × 2 ## f n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Andy 5 ## 2 Barney 11 ## 3 Dwight 5 ## 4 Howard 7 ## 5 Jim 6 ## 6 Leonard 10 ## 7 Lily 5 ## 8 Marshall 4 ## 9 Michael 6 ## 10 Pam 9 ## 11 Penny 9 ## 12 Rajesh 4 ## 13 Robin 5 ## 14 Sheldon 6 ## 15 Ted 8 # Junta as categorias per_fav2 \u0026lt;- fct_collapse(per_fav, TBBT = c(\u0026quot;Sheldon\u0026quot;, \u0026quot;Leonard\u0026quot;, \u0026quot;Penny\u0026quot;, \u0026quot;Howard\u0026quot;, \u0026quot;Rajesh\u0026quot;), HIMYM = c(\u0026quot;Ted\u0026quot;, \u0026quot;Marshall\u0026quot;, \u0026quot;Robin\u0026quot;, \u0026quot;Lily\u0026quot;, \u0026quot;Barney\u0026quot;), TheOffice = c(\u0026quot;Michael\u0026quot;, \u0026quot;Jim\u0026quot;, \u0026quot;Pam\u0026quot;, \u0026quot;Dwight\u0026quot;, \u0026quot;Andy\u0026quot;)) head(per_fav2) ## [1] TBBT HIMYM HIMYM TBBT TheOffice TheOffice ## Levels: TheOffice HIMYM TBBT fct_count(per_fav2) ## # A tibble: 3 × 2 ## f n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 TheOffice 31 ## 2 HIMYM 33 ## 3 TBBT 36 fct_reorder Ordena as categorias de um fator segundo uma função. No exemplo abaixo, ordenamos os fatores da variável carb (número de carburadores) segundo a mediana da variável mpg (milhas por galão de combustível).\nmtcars %\u0026gt;% ggplot(aes(x = as.factor(carb), y = mpg)) + geom_boxplot() mtcars %\u0026gt;% mutate(carb = fct_reorder(.f = as.factor(carb), .x = mpg, .fun = median)) %\u0026gt;% ggplot(aes(x = carb, y = mpg)) + geom_boxplot() fct_lump Agrupa as categorias menos (ou mais) comuns.\nletras \u0026lt;- factor(letters[rpois(100, 5)]) fct_count(letras) ## # A tibble: 10 × 2 ## f n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 b 10 ## 2 c 13 ## 3 d 12 ## 4 e 25 ## 5 f 16 ## 6 g 6 ## 7 h 9 ## 8 i 2 ## 9 j 4 ## 10 l 2 # Por default, a categoria \u0026quot;Other\u0026quot; sempre será menor # que as outras categorias. letras %\u0026gt;% fct_lump(other_level = \u0026quot;Outros\u0026quot;) %\u0026gt;% fct_count ## # A tibble: 10 × 2 ## f n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 b 10 ## 2 c 13 ## 3 d 12 ## 4 e 25 ## 5 f 16 ## 6 g 6 ## 7 h 9 ## 8 i 2 ## 9 j 4 ## 10 l 2 # Espeficicando o argumento \u0026quot;n = 4\u0026quot;, preservamos os # 4 valores mais comuns letras %\u0026gt;% fct_lump(n = 4, other_level = \u0026quot;Outros\u0026quot;) %\u0026gt;% fct_count ## # A tibble: 5 × 2 ## f n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 c 13 ## 2 d 12 ## 3 e 25 ## 4 f 16 ## 5 Outros 33 # Espeficicando o argumento \u0026quot;n = -4\u0026quot;, preservamos # apenas os 4 valores que menos aparecem letras %\u0026gt;% fct_lump(n = -4, other_level = \u0026quot;Principais\u0026quot;) %\u0026gt;% fct_count ## # A tibble: 5 × 2 ## f n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 g 6 ## 2 i 2 ## 3 j 4 ## 4 l 2 ## 5 Principais 85 lvls_reorder Troca a ordem das categorias de um fator.\nfator \u0026lt;- factor(c(\u0026quot;casado\u0026quot;, \u0026quot;viuvo\u0026quot;, \u0026quot;solteiro\u0026quot;, \u0026quot;divorciado\u0026quot;)) fator ## [1] casado viuvo solteiro divorciado ## Levels: casado divorciado solteiro viuvo lvls_reorder(fator, c(3, 1, 2, 4)) ## [1] casado viuvo solteiro divorciado ## Levels: solteiro casado divorciado viuvo O pacote forcats ainda tem outras funções úteis para tratar com fatores, como fct_expand(), fct_explicit_na(), fct_infreq(), fct_reorder2(), lvls_revalue(), entre outras. No RStudio, é sempre útil navegar entre as funções de um pacote digitando, por exemplo, forcats:: e pressionando TAB.\nDúvidas, críticas ou sugestões, deixe um comentário ou nos envie uma mensagem. :)\n","permalink":"https://blog.curso-r.com/posts/2017-07-01-forcats/","tags":["pacotes","fatores","forcats"],"title":"Forcats"},{"author":["Julio"],"categories":["discussões"],"contents":" CAPTCHAs? SIM, CAPTCHAs Sabe aquelas imagens chatas que aparecem quando você está preenchendo um formulário ou quer acessar uma página específica, pedindo para você decifrar o texto? Isso é o que chamamos de CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart). Captchas foram criados para impedir que robôs acessem determinadas páginas na web de forma irrestrita. Algumas empresas como a Google também usam essas coisinhas para utilizar o conhecimento de seres humanos para dominar o mundo.\nExistem captchas de todo tipo: difíceis, fáceis, que fazem sentido e que não fazem sentido. Um exemplo de CAPTCHA que faz sentido são os presentes em formulários para criação de emails. Imagine se alguém fizesse um programa que criasse bilhões de contas de e-mail do gmail! Morte horrível.\nUm exemplo de CAPTCHA que não faz sentido são os sites de serviços públicos, como a Receita Federal ou de alguns Tribunais de Justiça. Algumas justificativas para isso são: i) não onerar os sistemas (me poupe, basta fazer uma API) ou ii) a falsa ideia de que assim estão protegendo as pessoas (de acessar dados públicos?). Se uma informação é pública ela deve ser acessível, ponto. O que é errado não é acessar a informação, e sim fazer mau uso dela.\nPensando nisso, fiquei imaginando:\nSerá que é possível quebrar CAPTCHAs usando modelos estatísticos?\nTornando curta uma história longa, sim, é possível! O resultado dessa brincadeira está na organização decryptr. Claro que não são todos os CAPTCHAs que conseguimos quebrar, mas estamos fazendo pesquisa, brincando nas Rackathons (hackathons com R) e discutindo várias ideias para tornar isso viável. É um esforço da comunidade para tornar os serviços públicos mais acessíveis.\nSérie de posts Como esse tema é extenso e envolve várias técnicas estatísticas e computacionais avançadas, decidimos montar uma série de posts. O plano de posts segue abaixo, mas pode mudar conforme os trabalhos forem realizados.\nIntrodução - feito! 😄 O pacote decryptr: baixar, visualizar e classificar CAPTCHAs. Resolvendo CAPTCHAs com segmentação manual. Trabalhando com as imagens completas. Arquitetura do pacote decryptr. Estendendo o decryptr para quebrar seus próprios CAPTCHAs. Redes neurais aplicadas a CAPTCHAs. Utilizando o Keras para quebrar CAPTCHAs. Quebrando CAPTCHAs usando o áudio - case da Receita Federal. Tópicos e opiniões sobre o tema. É isso! Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2017-06-29-captcha-intro/","tags":["captcha"],"title":"Quebrando CAPTCHAs - Introdução"},{"author":["Daniel"],"categories":["conceitos"],"contents":" Autoencoders são redes neurais treinadas com o objetivo de copiar o seu input para o seu output. Esse interesse pode parecer meio estranho, mas na prática o objetivo é aprender representações (encodings) dos dados, que podem ser usadas para redução de dimensionalidade ou até mesmo compressão de arquivos.\nBasicamente, um autoencoder é dividido em duas partes:\num encoder que é uma função \\(f(x)\\) que transforma o input para uma representação \\(h\\) um decoder que é uma função \\(g(x)\\) que transforma a representação \\(h\\) em sua reconstrução \\(r\\) Imagem do blog do Keras\nConstruindo o seu primeiro autoencoder Nesse pequeno tutorial, vou usar o keras para definir e treinar os nossos autoencoders. Como base de dados vou usar algumas simulações e o banco de dados mnist (famoso para todos que já mexeram um pouco com deep learning). O mnist é um banco de dados de imagens de tamanho 28x28 de dígitos escritos à mão. Esse dataset promoveu grandes avanços na área de reconhecimento de imagens.\nlibrary(keras) encoding_dim \u0026lt;- 32 # definindo o input input \u0026lt;- layer_input(shape = 784) # definindo o encoder encoded \u0026lt;- layer_dense(input, encoding_dim, activation = \u0026quot;relu\u0026quot;) # definindo o decoder decoded \u0026lt;- layer_dense(encoded, 784, activation = \u0026quot;sigmoid\u0026quot;) autoencoder \u0026lt;- keras_model(input, decoded) encoder \u0026lt;- keras_model(input, encoded) # definindo o decoder encoded_input \u0026lt;- layer_input(shape = encoding_dim) decoder_layer \u0026lt;- autoencoder$get_layer(index = -1L) # última camada do autoencoder decoder \u0026lt;- keras_model(encoded_input, decoder_layer(encoded_input)) Com esse código definimos um modelo da seguinte forma:\n\\[ X = (X*W_1 + b_1)*W_2 + b_2 \\]\nEm que:\n\\(X\\) é o nosso input com dimensão (?, 784) \\(W_1\\) é uma matriz de pesos com dimensões (784, 32) \\(b_1\\) é uma matriz de forma (?, 32) \\(W_2\\) é uma matriz de pesos com dimensões (32, 784) \\(b_2\\) é uma matriz de forma (?, 784) Note que ? aqui é o número de observaçãoes da base de dados. Agora vamos estimar \\(W_1\\), \\(W_2\\), \\(b_1\\) e \\(b_2\\) de modo a minimizar alguma função de perda.\nInicialmente vamos usar a binary crossentropy por pixel que é definida por:\n\\[-\\sum_{i=1}y_i*log(\\hat{y}_i)\\]\nIsso é definido no keras usando:\nautoencoder %\u0026gt;% compile(optimizer=\u0026#39;adadelta\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;) Não vou entrar em detalhes do que é o adadelta, mas é uma variação do método de otimização conhecido como gradient descent.\nAgora vamos carregar a base de dados e em seguida treinar o nosso autoencoder`.\nmnist \u0026lt;- dataset_mnist() # o mnist é um banco de imagens 28x28, vamos transformar cada imagem em um vetor # de tamanho 784, cada elemento representado um pixel. x_train \u0026lt;- mnist$train$x %\u0026gt;% apply(1, as.numeric) %\u0026gt;% t() x_test \u0026lt;- mnist$test$x %\u0026gt;% apply(1, as.numeric) %\u0026gt;% t() # vamos transformar as imagens p/ o intervalo 0-1 para que # a função de perda funcione corretamente. x_train \u0026lt;- x_train/255 x_test \u0026lt;- x_test/255 Estimamos os parâmetros desse modelo no keras fazendo:\nautoencoder %\u0026gt;% fit( x_train, x_train, epochs = 50, batch_size = 256, shuffle = TRUE, validation_data = list(x_test, x_test) ) Depois de rodar todas as iterações, você poderá usar o seu encoder e o seu decoder para entender o que eles fazem com as imagens.\nVeja o exemplo a seguir em que vamos obter os encodings para as 10 primeiras imagens da base de teste e depois reconstruir a imagem usando o decoder.\nencoded_imgs \u0026lt;- predict(encoder, x_test[1:10,]) dim(encoded_imgs) encoded_imgs[1,] # representação vetorial de uma imagem. ## [1] 10 32 ## [1] 0.0000000 10.1513205 3.5742311 2.6635208 6.3097358 3.4840517 ## [7] 9.1041250 6.6329145 1.6385922 9.8017225 9.5529270 1.6670935 ## [13] 5.7208562 4.8035479 3.9149191 0.6408147 1.2716029 3.1215091 ## [19] 13.7575903 0.0000000 1.8692881 3.2142215 0.7444992 5.0728440 ## [25] 8.2932110 9.9866810 2.7651572 11.1291723 5.2460670 5.6875997 ## [31] 10.6097431 3.6338394 O encoder transforma a matriz de (10, 784) para uma matriz com dimensao (10, 2). Podemos reconstruir a imagem, a pardir da imagem que foi comprimida usando o nosso decoder.\npredict(decoder, encoded_imgs) %\u0026gt;% split(1:10) %\u0026gt;% lapply(matrix, ncol = 28) %\u0026gt;% Reduce(cbind, .) %\u0026gt;% as.raster() %\u0026gt;% plot() Compare as reconstruções com as imagens originais abaixo:\nx_test[1:10,] %\u0026gt;% split(1:10) %\u0026gt;% lapply(matrix, ncol = 28) %\u0026gt;% Reduce(cbind, .) %\u0026gt;% as.raster() %\u0026gt;% plot() Um ponto interessante é que esse modelo faz uma aproximação da solução por componentes principais! Na verdade, a definição do quanto são parecidos é quase-equivalente. Isso quer dizer que os pesos \\(W\\) encontrados pelo PCA e pelo autoencoder serão diferentes, mas o sub-espaço criado pelos mesmos será equivalente.\nSe são equivalentes, qual a vantagem de usar autoencoders ao invés de PCA? O PCA para por aqui, você define que serão apenas relações lineares, e você reduz dimensão apenas reduzindo o tamanho da matriz. Em autoencoders você tem diversas outras saídas para aprimorar o método.\nA primeira delas é simplesmente adicionar uma condição de esparsidade nos pesos. Isso vai reduzir o tamanho do vetor latente (como é chamada a camada do meio do autoencoder) também, pois ele terá mais zeros.\nIsso pode ser feito rapidamente com o keras. Basta adicionar um activity_regularizer em nossa camada de encoding. Isso vai adicionar na função de perda um termo que toma conta do valor dos outputs da camada intermediária.\n# definindo o input input \u0026lt;- layer_input(shape = 784) # definindo o encoder encoded \u0026lt;- layer_dense(input, encoding_dim, activation = \u0026quot;relu\u0026quot;, activity_regularizer = regularizer_l1(l = 10e-5)) # definindo o decoder decoded \u0026lt;- layer_dense(encoded, 784, activation = \u0026quot;sigmoid\u0026quot;) autoencoder \u0026lt;- keras_model(input, decoded) autoencoder %\u0026gt;% compile(optimizer=\u0026#39;adadelta\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;) autoencoder %\u0026gt;% fit( x_train, x_train, epochs = 50, batch_size = 256, shuffle = TRUE, validation_data = list(x_test, x_test) ) Outra forma de melhorar o seu autoencoder é permitir que o encoder e o decoder sejam redes neurais profundas. Com isso, ao invés de tentar encontrar transformações lineares, você permitirá que o autoencoder encontre transformações não lineares.\nMais uma vez fazemos isso com o keras:\ninput \u0026lt;- layer_input(shape = 784) encoded \u0026lt;- layer_dense(input, 128, activation = \u0026quot;relu\u0026quot;) %\u0026gt;% layer_dense(64, activation = \u0026quot;relu\u0026quot;) %\u0026gt;% layer_dense(32, activation = \u0026quot;relu\u0026quot;) decoded \u0026lt;- layer_dense(encoded, 64, activation = \u0026quot;relu\u0026quot;) %\u0026gt;% layer_dense(128, activation = \u0026quot;relu\u0026quot;) %\u0026gt;% layer_dense(784, activation = \u0026quot;sigmoid\u0026quot;) autoencoder \u0026lt;- keras_model(input, decoded) autoencoder %\u0026gt;% compile(optimizer=\u0026#39;adadelta\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;) autoencoder %\u0026gt;% fit( x_train, x_train, epochs = 50, batch_size = 256, shuffle = TRUE, validation_data = list(x_test, x_test) ) Existem formas ainda mais inteligentes de construir esses autoencoders, mas o post iria ficar muito longo e não ia sobrar asssunto para o próximo. Se você quiser saber mais, recomendo fortemente a leitura deste artigo do blog do Keras e desse capítulo.\nUma família bem moderna de autoencoders são os VAE (Variational Autoencoders). Esses autoencoders aprendem modelos de variáveis latentes. Isso é interessante porque permite que você gere novos dados, parecidos com os que você usou para treinar o seu autoencoder. Você pode encontrar uma implementação desse modelo aqui.\nÉ isso! Abraços\n","permalink":"https://blog.curso-r.com/posts/2017-06-26-construindo-autoencoders/","tags":["autoencoders","keras","deep-learning"],"title":"Construindo Autoencoders"},{"author":["Athos"],"categories":["conceitos"],"contents":" Componentes principais são bastante utilizados em modelagem estatística, mas a sua definição matemática rigorosa faz com que a ACP pareça um conceito mais abstrato do que somos capazes de compreender, em particular quando falam “maximizar a variância total” e “diminuir a dimensionalidade”. A primeira reação é um grande “HEIN?”.\nAbaixo tem alguns gráficos em 3 dimensões que dão uma boa ilustração sobre o que essas duas afirmativas querem dizer e, mesmo sendo um exemplo simples, irão ajudar a extrapolar a ideia para problemas mais complexos.\nO primeiro gráfico é a representação com 100% da informação em três dimensões. O segundo gráfico é como fica a representação do mesmo conjunto de dados, mas com uma dimensão a menos (redução de dimensionalidade). Ao diminuir uma dimensão, perdemos informação (não é mais 100% da variância total) e essa perda é mensurada pela variância explicada pelas dimensões que deixamos para trás. As duas primeiras dimensões projetam os dados de forma que apresentam a máxima a variância total possível em apenas duas dimensões. pacotes para os exemplos\nlibrary(magrittr) library(tidyr) library(dplyr) Observação: não esqueça de girar os gráficos! =)\nExemplo 1 - Tetraedro a \u0026lt;- 1 tetraedro \u0026lt;- data.frame( x = c(a * sqrt(3)/3, - a * sqrt(3)/6, - a * sqrt(3)/6, 0), y = c(0, - a/2, a/2, 0), z = c(0, 0, 0, a * sqrt(6)/3), cor = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;), id = 1:4) tetraedro_linhas \u0026lt;- combn(x = tetraedro$id, m = 2) %\u0026gt;% t %\u0026gt;% as.data.frame.matrix %\u0026gt;% set_names(c(\u0026quot;id1\u0026quot;, \u0026quot;id2\u0026quot;)) %\u0026gt;% mutate(id_par = 1:n()) %\u0026gt;% gather(id_ordem, id, id1, id2) %\u0026gt;% left_join(tetraedro, by = \u0026quot;id\u0026quot;) %\u0026gt;% arrange(id_ordem) Contribuição dos componentes na variância total\ntetraedro_pc \u0026lt;- prcomp(tetraedro %\u0026gt;% dplyr::select(x, y, z)) # PCA acontece aqui knitr::kable(summary(tetraedro_pc)$importance) PC1 PC2 PC3 Standard deviation 0.4082483 0.4082483 0.4082483 Proportion of Variance 0.3333300 0.3333300 0.3333300 Cumulative Proportion 0.3333300 0.6666700 1.0000000 3 dimensões (100% da variância)\nlibrary(plotly) plot_ly(tetraedro_linhas, x = ~x, y = ~y, z = ~z) %\u0026gt;% add_lines() %\u0026gt;% add_markers() 2 dimensões (67% da variância)\ntetraedro_pc_pred \u0026lt;- tetraedro_pc %\u0026gt;% predict %\u0026gt;% as.data.frame plot_ly(tetraedro_pc_pred, x = ~PC2, y = ~PC3, z = ~PC3) %\u0026gt;% add_lines() %\u0026gt;% add_markers() Exemplo 2 - Uma forma legal x \u0026lt;- c() y \u0026lt;- c() z \u0026lt;- c() c \u0026lt;- c() for (i in 1:62) { r \u0026lt;- 20 * cos(i / 20) x \u0026lt;- c(x, r * cos(i)) y \u0026lt;- c(y, r * sin(i)) z \u0026lt;- c(z, i) c \u0026lt;- c(c, i) } forma_legal \u0026lt;- data.frame(x, y, z, c) Contribuição dos componentes na variância total\nforma_legal_pc \u0026lt;- prcomp(forma_legal %\u0026gt;% dplyr::select(x, y, z)) # PCA acontece aqui knitr::kable(summary(forma_legal_pc)$importance) PC1 PC2 PC3 Standard deviation 18.04936 10.13750 9.854732 Proportion of Variance 0.61975 0.19550 0.184750 Cumulative Proportion 0.61975 0.81525 1.000000 3 dimensões (100% da variância)\nplot_ly(forma_legal, x = ~x, y = ~y, z = ~z, type = \u0026quot;scatter3d\u0026quot;, mode = \u0026quot;markers+lines\u0026quot;, line = list(width = 6, color = ~c, colorscale = \u0026#39;Viridis\u0026#39;), marker = list(size = 3.5, color = ~c, colorscale = \u0026#39;Greens\u0026#39;, cmin = -20, cmax = 50)) 2 dimensões (82% da variância)\nforma_legal_pc_pred \u0026lt;- forma_legal_pc %\u0026gt;% predict %\u0026gt;% as.data.frame plot_ly(forma_legal_pc_pred, x = ~PC1, y = ~PC2, z = 1, type = \u0026#39;scatter3d\u0026#39;, mode = \u0026#39;lines+markers\u0026#39;, line = list(width = 6, color = ~c, colorscale = \u0026#39;Viridis\u0026#39;), marker = list(size = 3.5, color = ~c, colorscale = \u0026#39;Greens\u0026#39;, cmin = -20, cmax = 50)) Para saber mais Meu livro predileto sobre esse assunto (e para muitos outros) é o An Introduction to Statistical Learning.\nabs\n","permalink":"https://blog.curso-r.com/posts/2017-06-24-componentes-principais-intuicao/","tags":["pca","componentes principais","plotly"],"title":"Componentes Principais: Intuição"},{"author":["Daniel"],"categories":["divulgação"],"contents":" Está no ar o bR Bloggers! bR Bloggers é um agregador de blogs sobre R escritos em língua portuguesa.\nAtualmente uma das melhores formas de aprender R e de ficar a par da comunidade R é ler o R-Bloggers. O R-Bloggers é um agregador que conta com mais de 700 blogs, que escrevem aproximadamente 300 posts por mês. Lendo tudo isso, não tem como você não aprender muito R!\nNo entanto, para muitas pessoas, uma barreira para ler os posts do R-Bloggers pode ser o fato de que todos os posts estão em inglês. Por isso, a criação do bR Bloggers.\nA ideia de um agregador de blogs para o português não é original, hoje mesmo descobri esse link: https://www.r-bloggers.com/lang/-/portuguese: Uma versão do R-Bloggers em português mantida pelo próprio criador do R-Bloggers. No entanto, a última postagem data de 2012: 5 anos atrás. Ainda não existia nem o dplyr(que é de Jan/2014). Não se falava em tidyverse e nem nada disso também. Pelo menos o ggplot2 já tinha seus 5 anos. Ou seja, tudo no R era diferente, menos a melhor forma de fazer os seus gráficos.\nRecentemente o Sillas Gonzaga autor do Paixão por Dados criou o R-Bloggers BR um bot do twitter que “twita” toda vez que algum blog da lista tem um novo post. Também temos uma lista de blogs de R em português mantida pelo Marcos Vital do Cantinho do R.\nEsses foram bons avanços para a comunidade R brasileira, mas ainda assim, dado o sucesso do R-Bloggers, sentíamos falta de um agregador no mesmo estilo para blogs em português. Por isso surgiu o bR Bloggers.\nEsperamos que o bR Bloggers incentive a comunidade a escrever bastante conteúdo sobre R em português! Que ele seja uma vitrine para que todos possam mostrar os seus trabalhos e que ajude muitas pessoas a aprender R.\nComo funciona? O br Bloggers foi feito em R. E todo o código fonte está disponível em uma organização do Github: https://github.com/brbloggers. São dois repositórios:\nbrbloggers: armazena o código front-end do site. Basicamente é um tema do Hugo. brloggers-backend: armazena o código responsável por atualizar os posts. Dentro do brbloggers-backend o principal arquivo é o main.R, que contém toda a lógica para obter os novos posts e salvá-los no seu diretório content.\nEsse script main.R é chamado pelo script run.sh que controla o processo de atualização do site:\nfaz pull do repositório brbloggers-backend roda o main.R faz commits e push desse repositório atualiza o repo brbloggers O run.sh é por sua vez chamado de 30 em 30 minutos por meio de um cronjob que foi definido em um servidor na Google Cloud Platform. Esse servidor é uma instância micro do GCP com R instalado para rodar esses códigos. Ou seja, o backend do brbloggers custa U$5,00 por mês para ficar no ar.\nJá o repositório brbloggers está linkado com um serviço chamado Netlify. Basicamente o Netlify recebe o código do front-end a cada novo commit no repositório do site e builda o site estático usando o Hugo. O Netlify também hospeda o site estático.\nO logo do bR Bloggers O logo do bR Bloggers foio criado pelo Julio usando o R. O código está disponível abaixo:\nlibrary(tidyverse) library(ggforce) # https://pt.wikipedia.org/wiki/Bandeira_do_Brasil#Cores cores_br \u0026lt;- list(verde = \u0026#39;#009C3B\u0026#39;, amarelo = \u0026#39;#FFDF00\u0026#39;, azul = \u0026#39;#002776\u0026#39;, branco = \u0026quot;#FFFFFF\u0026quot;) p \u0026lt;- tibble(x = 1, y = 1, lab1 = \u0026#39;b\u0026#39;, lab2 = \u0026#39;R\u0026#39;) %\u0026gt;% ggplot(aes(x0 = x, y0 = y, r = y)) + geom_circle(size = 5, n = 5.5, fill = \u0026#39;transparent\u0026#39;, color = cores_br$verde, linetype = 1) + geom_circle(size = 4, n = 5.5, fill = \u0026#39;transparent\u0026#39;, color = cores_br$amarelo, linetype = 2) + geom_text(aes(x, y, label = lab1), family = \u0026quot;Ubuntu\u0026quot;, size = 45, nudge_x = -.3, nudge_y = -.1, colour = cores_br$azul) + geom_text(aes(x, y, label = lab2), family = \u0026quot;Century\u0026quot;, size = 60, nudge_x = .3, colour = cores_br$azul, fontface = \u0026#39;bold\u0026#39;) + coord_equal() + theme_minimal(0) p ","permalink":"https://blog.curso-r.com/posts/2017-06-19-brbloggers/","tags":["curso-R"],"title":"bR Bloggers está no ar"},{"author":["William"],"categories":["conceitos"],"contents":" Se você utiliza o R regularmente, com certeza já se deparou com o termo environment. Ele aparece como um painel do RStudio, quando acessamos o código de uma função e (implicitamente) quando carregamos pacotes. Neste post, vamos tentar responder as três perguntas básicas sobre qualquer coisa no R: 1. o que é? 2. para que serve? e 3. como NÃO usar?\nO que é? Definindo de uma maneira bem simples, environments são locais onde objetos são armazenados, isto é, conjuntos de ligações entre símbolos e valores. Por exemplo, quando fazemos a atribuição abaixo,\na \u0026lt;- 4 estamos criando uma associação do símbolo a ao valor 4, que, por padrão, é guardada dentro do global environment.\nls(globalenv()) ## [1] \u0026quot;a\u0026quot; Assim, quando rodarmos o símbolo a, o R, por padrão, vai procurar dentro desse environment um valor para devolver. No caso, o valor 4.\na ## [1] 4 Mais formalmente, environments podem ser definidos como a junção de duas coisas: um conjunto de pares (símbolo, valor); e um ponteiro para um outro environment. Quando o R não encontra um valor para um símbolo no environment em que está procurando, ele passa a procurar no próximo, o environment para qual o primeiro está apontando, chamado de environment pai. Assim, os environments se estruturam como uma árvore, cuja raiz é um environment vazio.\nemptyenv() ## \u0026lt;environment: R_EmptyEnv\u0026gt; O que faz? É possível criar novos environments com a função new.env()\nmagrathea \u0026lt;- new.env() e criar objetos dentro desse environments com a função assign()\nassign(\u0026quot;a\u0026quot;, 8, envir = magrathea) ls(magrathea) ## [1] \u0026quot;a\u0026quot; Agora temos um objeto chamado a no global environment e no magrathea, que nós criamos. Note que o R inicia a busca no global environment.\na ## [1] 4 Vamos agora criar outro objeto dentro de magrathea.\nassign(\u0026quot;b\u0026quot;, 15, envir = magrathea) Observe que se procurarmos simplesmente por b, o R não vai encontrar um valor para associar.\nb Acontece que magrathea é um environment “abaixo” do global na hierarquia, e o R só estende a sua busca para environments acima (sim, estou pensando numa árvore de ponta-cabeça).\nparent.env(magrathea) ## \u0026lt;environment: R_GlobalEnv\u0026gt; Se criarmos agora um objeto no global\nc \u0026lt;- 16 e usarmos a função get() para procurá-lo no environment que criamos, o R irá encontrá-lo porque o global é o environment pai de magrathea.\nget(\u0026quot;c\u0026quot;, envir = magrathea) ## [1] 16 Essa estrutura é muito útil na hora de utilizar funções. Sempre que uma função é chamada, um novo environment é criado, o environment de avaliação, que contém os objetos usados como argumento da função, os objetos criados dentro da função e aponta para o environment onde a função foi criada (geralmente o global).\nf \u0026lt;- function(a, b) { c \u0026lt;- a + b return(c) } environment(f) ## \u0026lt;environment: R_GlobalEnv\u0026gt; Esse comportamento nos permite fazer duas coisas. Primeiro, os cálculos realizados dentro das funções não modificam os objetos do global.\nf(23, 42) ## [1] 65 c ## [1] 16 Segundo, podemos utilizar objetos dentro da função sem defini-los lá dentro.\nf \u0026lt;- function(b) { return(a + b) } f(108) ## [1] 112 Neste caso, como o R não encontrou o símbolo a dentro do environment de avaliação, ele foi buscar no pai, o global.\nComo não usar? Agora que temos uma visão ao menos superficial da estrutura de environments, podemos entender melhor porque usar a função attach() é uma prática não recomendada ao programar em R.\nSe utilizarmos a função search(), ela nos devolverá o “caminho” de environments, começando do global (magrathea não será exibido).\nsearch() ## [1] \u0026quot;.GlobalEnv\u0026quot; \u0026quot;package:stats\u0026quot; \u0026quot;package:graphics\u0026quot; ## [4] \u0026quot;package:grDevices\u0026quot; \u0026quot;package:utils\u0026quot; \u0026quot;package:datasets\u0026quot; ## [7] \u0026quot;package:methods\u0026quot; \u0026quot;Autoloads\u0026quot; \u0026quot;package:base\u0026quot; Repare que os pacotes carregados geram um novo environment na árvore.\nlibrary(ggplot2) search() ## [1] \u0026quot;.GlobalEnv\u0026quot; \u0026quot;package:ggplot2\u0026quot; \u0026quot;package:stats\u0026quot; ## [4] \u0026quot;package:graphics\u0026quot; \u0026quot;package:grDevices\u0026quot; \u0026quot;package:utils\u0026quot; ## [7] \u0026quot;package:datasets\u0026quot; \u0026quot;package:methods\u0026quot; \u0026quot;Autoloads\u0026quot; ## [10] \u0026quot;package:base\u0026quot; É por isso que, ao carregar um pacote, podemos utilizar as suas funções sem a necessidade de escrever coisas do tipo ggplot2::geom_point(). Agora, veja o que acontece quando usamos a função attach()\nmighty \u0026lt;- list(\u0026quot;Jason\u0026quot; = \u0026quot;vermelho\u0026quot;, \u0026quot;Zach\u0026quot; = \u0026quot;Preto\u0026quot;, \u0026quot;Billy\u0026quot; = \u0026quot;Azul\u0026quot;, \u0026quot;Trini\u0026quot; = \u0026quot;Amarela\u0026quot;, \u0026quot;Kimberly\u0026quot; = \u0026quot;Rosa\u0026quot;, \u0026quot;Thomas\u0026quot; = \u0026quot;Verde\u0026quot;) attach(mighty) search() ## [1] \u0026quot;.GlobalEnv\u0026quot; \u0026quot;mighty\u0026quot; \u0026quot;package:ggplot2\u0026quot; ## [4] \u0026quot;package:stats\u0026quot; \u0026quot;package:graphics\u0026quot; \u0026quot;package:grDevices\u0026quot; ## [7] \u0026quot;package:utils\u0026quot; \u0026quot;package:datasets\u0026quot; \u0026quot;package:methods\u0026quot; ## [10] \u0026quot;Autoloads\u0026quot; \u0026quot;package:base\u0026quot; Um novo environment mighty é criado acima do global! Isso quer dizer que se você não tiver total conhecimento dos objetos que estão sendo anexados, você estará criando uma lista de objetos “invisíveis” que podem ser avaliados mesmo dentro de funções. E veja o que acontece quando carregamos mais pacotes\nlibrary(dplyr) ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union search() ## [1] \u0026quot;.GlobalEnv\u0026quot; \u0026quot;package:dplyr\u0026quot; \u0026quot;mighty\u0026quot; ## [4] \u0026quot;package:ggplot2\u0026quot; \u0026quot;package:stats\u0026quot; \u0026quot;package:graphics\u0026quot; ## [7] \u0026quot;package:grDevices\u0026quot; \u0026quot;package:utils\u0026quot; \u0026quot;package:datasets\u0026quot; ## [10] \u0026quot;package:methods\u0026quot; \u0026quot;Autoloads\u0026quot; \u0026quot;package:base\u0026quot; O environment do pacote dplyr aparece antes do mighty. Isso quer dizer que os objetos do mighty podem ser mascarados por todos os pacotes que você carregar a seguir. Veja um simples exemplo de como as coisas podem dar errado.\ndados \u0026lt;- tibble::tibble(paciente = 1:30, cancer = rbinom(30, size = 1, prob = 0.5)) attach(dados) cancer ## [1] 1 0 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 Com o código acima, criamos um banco de dados representando 30 pacientes com (1) ou sem (0) um certo tipo de câncer. As variáveis paciente e cancer foram anexadas ao rodarmos attach(dados).\nAgora, imagine se esse banco de dados tiver informações de tempo até a remissão do câncer e quisermos rodar modelos de sobrevivência. Um passo natural seria carregar a biblioteca survival.\nlibrary(survival) ## ## Attaching package: \u0026#39;survival\u0026#39; ## The following object is masked from \u0026#39;dados\u0026#39;: ## ## cancer search() ## [1] \u0026quot;.GlobalEnv\u0026quot; \u0026quot;package:survival\u0026quot; \u0026quot;dados\u0026quot; ## [4] \u0026quot;package:dplyr\u0026quot; \u0026quot;mighty\u0026quot; \u0026quot;package:ggplot2\u0026quot; ## [7] \u0026quot;package:stats\u0026quot; \u0026quot;package:graphics\u0026quot; \u0026quot;package:grDevices\u0026quot; ## [10] \u0026quot;package:utils\u0026quot; \u0026quot;package:datasets\u0026quot; \u0026quot;package:methods\u0026quot; ## [13] \u0026quot;Autoloads\u0026quot; \u0026quot;package:base\u0026quot; O pacote survival também tem um objeto chamado cancer. Assim, ao carregá-lo, o environment survival ficará na frente do environment dados na árvore e, se não prestarmos atenção com o warning, esse será o nosso novo objeto cancer.\nhead(cancer) ## inst time status age sex ph.ecog ph.karno pat.karno meal.cal wt.loss ## 1 3 306 2 74 1 1 90 100 1175 NA ## 2 3 455 2 68 1 0 90 90 1225 15 ## 3 3 1010 1 56 1 0 90 90 NA 15 ## 4 5 210 2 57 1 1 90 60 1150 11 ## 5 1 883 2 60 1 0 100 90 NA 0 ## 6 12 1022 1 74 1 1 50 80 513 0 Assim, se for utilizar a função attach() é preciso ter muito cuidado com o que se está fazendo. E a melhor dica é não use.\nEsse post foi apenas uma introdução sobre como os environments funcionam. Ainda existe muito mais coisa por trás, como o conceito de namespaces. Se você quiser saber mais, recomendo como primeira parada esse post, do qual tirei boa parte das informações passadas aqui. Também vale a pena dar uma olhada nas definições nesse link.\nSugestões, dúvidas e críticas, deixe um comentário!\n","permalink":"https://blog.curso-r.com/posts/2017-06-19-environments/","tags":["environments"],"title":"Environments"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Entre os dias 25 e 27 de maio aconteceu a ROpenSci Unconf 2017. O encontro reuniu vários pop stars da comunidade R como Hadley Wickham, Joe Cheng (criador do shiny), Jeroen Ooms (criador do OpenCPU e autor de vários pacotes bacanas), Jenny Bryan (autora de vários pacotes bacanas como googlesheets), várias pessoas do #R-Ladies e muito mais.\nUma coisa muito legal dessa conferência é que ela funcionou como uma hackathon. Foi criada uma nova organização no github chamada ROpenSci Labs, e os presentes simplesmente começaram a subir pacotes fantásticos lá dentro. Recomendo muito dar uma olhada.\nDentre os pacotes que olhei, o que mais me chamou atenção foi o skimr e por isso estou fazendo esse post! O propósito do skimr é simples: fazer algumas estatísticas básicas univariadas de uma base de dados.\nO skimr ainda não está no CRAN, então para instalar recomendamos utilizar o devtools para instalar direto do GitHub, conforme código abaixo. Note que também será necessário instalar o pacote colformat do Hadley.\ndevtools::install_github(\u0026quot;hadley/colformat\u0026quot;) devtools::install_github(\u0026quot;ropenscilabs/skimr\u0026quot;) A função skim() calcula estatísticas básicas das variáveis e imprime no seu console. Note que a função separa estatísticas para variáveis numéricas ou fatores.\nlibrary(tidyverse) library(skimr) skim(iris) E tem mais! O mais legal do skimr é que ele usa a função colformat::spark_bar() para desenhar histogramas direto no seu console!\nskim(iris) %\u0026gt;% filter(stat == \u0026#39;hist\u0026#39;) %\u0026gt;% knitr::kable(caption = \u0026#39;HISTOGRAMA NA TABELA PORQUE SIM!\u0026#39;) O skimr também possui padrões de estatísticas básicas para cada tipo de variável. Você pode checar esses tipos com show_skimmers():\nshow_skimmers() %\u0026gt;% map_df(enframe, .id = \u0026#39;tipo\u0026#39;) %\u0026gt;% group_by(tipo) %\u0026gt;% summarise(stats = glue::collapse(value, sep = \u0026#39;, \u0026#39;)) %\u0026gt;% knitr::kable(caption = \u0026#39;Estatísticas básicas para cada tipo de variável.\u0026#39;) Criando suas próprias funções Você também pode usar funções próprias com o skimr. Por exemplo, digamos que você queira calcular o coeficiente de variação. Primeiro, adicione sua função dentro de uma lista:\nfuns \u0026lt;- list(cv = function(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)) e depois aplique a função skim_with():\n# append adiciona as suas funcoes nas existentes skim_with(numeric = funs, append = TRUE) E pronto! Agora você pode rodar skim() novamente:\nskim(iris) %\u0026gt;% filter(stat %in% c(\u0026#39;hist\u0026#39;, \u0026#39;cv\u0026#39;)) %\u0026gt;% knitr::kable(caption = \u0026#39;Histograma e coeficiente de variação.\u0026#39;) Para retornar ao skim() padrão, rode skim_with_defaults().\nWrap up Instale usando devtools::install_github() Rode a função skim(). Use dplyr::filter() para filtrar as estatísticas de interesse. Você pode adicionar suas próprias estatísticas com skim_with(). Acompanhe a evolução do skimr nesta página. O pacote ainda vai evoluir muito e não duvido nada que seja um bom candidado a entrar no tidyverse. O que vocês acham? Escrevam nos comentários!\nÉ isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2017-06-17-skimr/","tags":["pacotes","skimr"],"title":"Skimr: estatísticas básicas com ❤️"},{"author":["Athos"],"categories":["Tutoriais"],"contents":" Compilei um passo a passo mais simplificado dos posts que usei pra conseguir usar o Keras num servidor Ubuntu.\nEsse tutorial ensinará como instalar o Anaconda com Python 3.5 em um servidor Ubuntu 16.04 + Keras para R. Então ao final desse post você terá o Keras direto do seu R pronto para abalar os profundos mares da aprendizagem de máquinas.\nPré-requisitos O tutorial supõe que você possui um servidor Ubuntu 16.04 com R 3.4.0 ou versão superior instalado.\nInstalando Anaconda com Python 3.5 O Keras só funciona com o Python 2.7 ou 3.5, por isso temos que instalar o Anaconda 4.2.0 que é a versão que vem com o Python 3.5.\nPasso 1) Primeiro, sugiro que vá ao diretório /tmp para baixar o arquivo bash do Anaconda.\ncd /tmp Passo 2) Agora faça o download do bash do Anaconda 4.2.0.\ncurl -O https://repo.continuum.io/archive/Anaconda3-4.2.0-Linux-x86_64.sh Passo 3) Execute o bash.\nbash Anaconda3-4.2.0-Linux-x86_64.sh Passo 4) Você verá a seguinte saída. Aperta ENTER para continuar.\nWelcome to Anaconda3 4.2.0 (by Continuum Analytics, Inc.) In order to continue the installation process, please review the license agreement. Please, press ENTER to continue Passo 5) Daí vai apertando mais ENTER para ir até o final dos termos de uso. Quando chegar lá embaixo, vai perguntar se vc aceita o termos.\nDo you approve the license terms? [yes|no] Passo 6) Escreva yes para aceitar os termos. Deve aparecer um prompt como o mostrado abaixo: hora de escolha o local de instalação. Solte um ENTER caso goste do local padrão oferecido.\nAnaconda3 will now be installed into this location: /home/athos/anaconda3 - Press ENTER to confirm the location - Press CTRL-C to abort the installation - Or specify a different location below [/home/athos/anaconda3] \u0026gt;\u0026gt;\u0026gt; Passo 7) Daí a pergunta seguinte é para saber se você gostaria de fazer o comando conda funcionar quando for chamado no terminal. Pode aceitar.\n... installation finished. Do you wish the installer to prepend the Anaconda3 install location to PATH in your /home/athos/.bashrc ? [yes|no] [no] \u0026gt;\u0026gt;\u0026gt; Neste momento você terá o Anaconda 4.2.0 pronto para rodar.\nAgora, ao R!\ninstalação do Keras no R Passo 8) Essa parte é fácil. Rode os códigos abaixo para para instalar o pacote Keras e o Tensorflow.\ndevtools::install_github(\u0026quot;rstudio/keras\u0026quot;) # instalar o pacote do Keras para R keras::install_tensorflow() # instalar o Tensorflow Pronto! Agora é só ser impressionante e destruir no DeepLearning. No github do Keras tem a documentação completa pra você aprender mais sobre como usá-lo.\nAquele axé.\n","permalink":"https://blog.curso-r.com/posts/2017-06-08-keras-no-ubuntu/","tags":["deep learning","tensorflow","keras"],"title":"Instalando Anaconda + TensorFlow + Keras para R no Ubuntu 16.04 Server"},{"author":["Julio"],"categories":["divulgação"],"contents":" Esse é o Daniel Falbel, um dos sócios da Curso-R e autor de diversos posts no nosso blog. Boa pinta, não?\nMas esse não é só um rostinho bonito. Recente, o Dani fez uma contribuição PROFUNDA para a comunidade R: ele é um dos responsáveis pelo pacote keras, que faz uma interface com a biblioteca de mesmo nome utilizada para ajustar modelos de deep learning usando TensorFlow.\nVeja o arquivo DESCRIPTION (metadados) desse pacote:\nPackage: keras Type: Package Title: R Interface to Keras Version: 0.3.6 Authors@R: c( person(\u0026quot;JJ\u0026quot;, \u0026quot;Allaire\u0026quot;, role = c(\u0026quot;aut\u0026quot;, \u0026quot;cre\u0026quot;), email = \u0026quot;jj@rstudio.com\u0026quot;), person(\u0026quot;François\u0026quot;, \u0026quot;Chollet\u0026quot;, role = c(\u0026quot;aut\u0026quot;, \u0026quot;cph\u0026quot;)), person(\u0026quot;RStudio\u0026quot;, role = c(\u0026quot;cph\u0026quot;, \u0026quot;fnd\u0026quot;)), person(family = \u0026quot;Google\u0026quot;, role = c(\u0026quot;ctb\u0026quot;, \u0026quot;cph\u0026quot;, \u0026quot;fnd\u0026quot;)), person(\u0026quot;Yuan\u0026quot;, \u0026quot;Tang\u0026quot;, role = c(\u0026quot;ctb\u0026quot;, \u0026quot;cph\u0026quot;)), person(\u0026quot;Daniel\u0026quot;, \u0026quot;Falbel\u0026quot;, role = c(\u0026quot;ctb\u0026quot;, \u0026quot;cph\u0026quot;)) ) Description: Interface to \u0026#39;Keras\u0026#39;, a high-level neural networks API which runs on top of TensorFlow. \u0026#39;Keras\u0026#39; was developed with a focus on enabling fast experimentation, supports both convolutional networks and recurrent networks (as well as combinations of the two), and seamlessly on both CPUs and GPUs. ... Além do Daniel, temos como autores desse pacote nada menos que JJ Allaire (CEO do RStudio), Yuan Tang (um dos principais desenvolvedores do TensorFlow) e François Chollet, o próprio RStudio e a Google (essa não precisa de link).\nMuito orgulho de ter esse cara como amigo e sócio na Curso-R. Parabéns!\nNo futuro, postaremos tutoriais sobre o keras e mostraremos como estamos utilizando o pacote para quebrar CAPTCHAs usando métodos bastante inovadores. Enquanto isso, você pode olhar um tutorial para o pacote keras nesse link.\nÉ isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2017-06-05-keras/","tags":["deep learning","keras"],"title":"Keras: colocando deep learning no pipeline"},{"author":["William"],"categories":["Tutoriais"],"contents":" Já vimos como o Rstudio se torna uma ferramenta poderosa quando combinado com certos pacotes, como o knitr e shiny, ou outros recursos, como o markdown e o git. Hoje, veremos como transformar o Rstudio num elegante e interativo editor de imagens utilizando o pacote magick.\nEste post é um breve resumo das funcionalidades do magick. Para uma apresentação completa, visite o vignette do pacote.\nJá usamos o magick em outros posts do blog (às vezes por trás das cortinas) para tratar imagens. Esse pacote é um wrapper da biblioteca ImageMagick, provavelmente a biblioteca open-source para processamento de imagens mais amigável disponível hoje em dia.\nInstalação No Windows ou OS-X, instale via CRAN:\ninstall.packages(\u0026quot;magick\u0026quot;) No Linux, consulte o vignette do Magick para mais informações.\nLendo e escrevendo imagens Para começar a usar as funções do magick, carregue o pacote.\nlibrary(magick) ## Linking to ImageMagick 6.9.12.3 ## Enabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp ## Disabled features: fftw, ghostscript, x11 library(magrittr) Vamos utilizar a seguinte imagem como exemplo neste post:\nA função image_read() lê imagens de arquivos, como .jpg ou png. Diversas outras extensões são suportadas. Utilize str(magick::magick_config()) para verificar quais formatos estão disponíveis na sua versão do ImageMagick.\n## format width height colorspace matte filesize density ## 1 JPEG 2745 1780 sRGB FALSE 662239 300x300 A função image_write() exporta imagens em qualquer um dos formatos suportados.\nimage_write(freddie, path = \u0026quot;freddie_png.png\u0026quot;, format = \u0026quot;png\u0026quot;) Dica: no Rstudio, pasta rodar o objeto freddie para visualizar a imagem no painel Viewer.\nConvertendo Ao ler uma imagem com o magick, ela é guardada em memória em seu formato original. Para converter essa imagem, utilizamos a função image_convert().\nfreddie_png \u0026lt;- image_convert(freddie, \u0026quot;png\u0026quot;) image_info(freddie_png) ## format width height colorspace matte filesize density ## 1 PNG 2745 1780 sRGB FALSE 0 300x300 Neste ponto você já deve ter reparado que as (principais) funções do pacote magick utilizam o prefixo image_.\nCortando e editando A maioria das transformações que você pode fazer com as imagens utilizará um parâmetro geometry. Esse parâmetro requer uma sintaxe especial, da forma AXB+C+D, sendo que cada elemento (A, B, C e D) é opcional. Veja alguns exemplos:\nimage_crop(image, \"100x150+50+20\"): recorta uma região de tamanho 100px x 150px, começando +50px da esquerda para a direita e +20px de cima para baixo; image_scale(image, \"200\"): redimensiona proporcionalmente ao comprimento: 200px; image_scale(image, \"x200\"): redimensiona proporcionalmente à altura: 200px; image_border(frink, \"red\", \"20x10\"): adiciona uma borda de 20px (esquerda/direita) e 10px (cima/baixo) Para mais detalhes sobre essa sintaxe, consulte este link.\nVamos testar essas funções na nossa imagem!\nComeçaremos a redimensionando para facilitar o uso e a visualização de algumas funções.\nfreddie_resized \u0026lt;- image_scale(freddie, \u0026quot;500\u0026quot;) image_info(freddie_resized) ## format width height colorspace matte filesize density ## 1 JPEG 500 324 sRGB FALSE 0 300x300 Agora, vamos recortar um pedaço.\nimage_crop(freddie_resized, \u0026quot;100x150+280+30\u0026quot;) Colocar uma borda.\nimage_border(freddie_resized, \u0026quot;purple\u0026quot;, \u0026quot;20x10\u0026quot;) Girá-la de ponta-cabeça.\nimage_flip(freddie_resized) E inverter a direção.\nimage_flop(freddie_resized) Com o pipe, fica fácil aplicar todas as funções ao mesmo tempo.\nfreddie_resized %\u0026gt;% image_crop(\u0026quot;100x150+280+30\u0026quot;) %\u0026gt;% image_border(\u0026quot;purple\u0026quot;, \u0026quot;20x10\u0026quot;) %\u0026gt;% image_flip %\u0026gt;% image_flop Filtros e efeitos Agora vamos aplicar alguns filtros e efeitos.\nimage_blur(freddie_resized, 10, 5) image_noise(freddie_resized) image_annotate(freddie_resized, \u0026quot;Farrokh Bulsara\u0026quot;, size = 30, color = \u0026quot;red\u0026quot;, boxcolor = \u0026quot;white\u0026quot;, degrees = 9, location = \u0026quot;+60+30\u0026quot;) O que mais dá para fazer? Realmente recomendamos uma leitura do Vignette do magick para ter uma boa ideia de tudo o que ele permite fazer. Além de várias outras ferramentas de edição para imagens estáticas, ainda é possível mexer com GIFs e animações! Ou fazer coisas como:\nlibrary(ggplot2) library(grid) qplot(speed, dist, data = cars, geom = c(\u0026quot;point\u0026quot;, \u0026quot;smooth\u0026quot;)) grid.raster(freddie_cropped, width = 0.15, height = 0.3, hjust = -2, vjust = 1) ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39; ","permalink":"https://blog.curso-r.com/posts/2017-06-01-a-kind-of-magick/","tags":["imagens","pacotes","magick"],"title":"A kind of magick"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Já precisou extrair dados de arquivos pdf? Bom, eu já. Eu trabalho com jurimetria e preciso extrair dados de diários oficiais, petições, sentenças, então já viu né…\nA primeira pergunta que você precisa fazer antes de ler um pdf é: o arquivo é digital ou digitalizado?\nSe for digital, significa que ele pode ser transcrito diretamente para vários formatos: texto, html, xml e até mesmo data.frames diretamente. Vamos usar esse exemplo de PDF digital\nSe estiver no desktop, é possível ver o documento abaixo:\nSe for digitalizado, você precisará passar um algoritmo de OCR (Optical Character Recognition) para extrair os dados. Provavelmente seu output nesse caso será sempre texto. Vamos usar esse exemplo de PDF digitalizado\nSe estiver no desktop, é possível ver o documento abaixo:\nObs: é possível que seu arquivo seja digitalizado, mas já com uma OCR passada no próprio arquivo. Nesse caso, você pode tratar o documento como digital.\nOs créditos dos pacotes abaixo vão todos para o Jeroen Ooms, um dos maiores autores de pacotes da comunidade R nos últimos dez anos. Sou fã desse cara!\nPacote pdftools para PDFs digitais Para instalar o pdftools no Windows e no Mac, basta rodar\ninstall.packages(\u0026quot;pdftools\u0026quot;) Para instalar no Linux, siga as instruções desse link.\nPDF para texto library(tidyverse) library(stringr) library(pdftools) pdf \u0026lt;- \u0026#39;../../static/data/ocr/pdf_digital.pdf\u0026#39; txt \u0026lt;- pdf_text(pdf) # imprimindo só os 500 primeiros caracteres da primeira página cat(str_trunc(txt[1], 500)) ## TJ/SP - Comarca de São Paulo ## Movimento Judiciário ## ## Referência: Janeiro de 2011 ## Foro: ADAMANTINA ## Unidade: 02 CUMULATIVA ## Planilha: CIVEL ## ## ## Dados da Unidade ## 1. Total de feitos em andamento 2756 ## 2. Precatórias 6 ## 3. Processos ## 3.1 Processos cíveis 2078 ## 3.1.1 De Conhecimento 1111 ## 3.1.2 De... PDF para HTML ou XML Muitas vezes queremos pegar estruturas no texto que dependem da posição dos elementos. Por exemplo, o texto em um PDF pode estar dividido em várias colunas. Para isso, o ideal seria transformar o arquivo em dados semi-estruturados como HTML ou XML, que separam os elementos do conteúdo do PDF em tags.\nInfelizmente, o pdftools ainda não transforma em HTML nem XML. Para soltar um HTML, vamos montar uma função que chama pdftohtml do poppler por command line.\npdf_html \u0026lt;- function(pdf) { infos \u0026lt;- pdf_info(pdf) # pega infos do pdf html \u0026lt;- tempfile(fileext = \u0026#39;.html\u0026#39;) # cria arquivo temporário # monta comando a ser executado. # não sei se funciona em Windows ;) command \u0026lt;- sprintf(\u0026#39;pdftohtml -f 1 -l %s -q -i -s -noframes %s %s\u0026#39;, infos$pages, normalizePath(pdf), html) system(command) # roda comando e salva txt \u0026lt;- readr::read_file(html) # lê arquivo salvo file.remove(html) # remove arquivo temporário txt } Você pode brincar com o HTML usando o pacote rvest:\nlibrary(rvest) html \u0026lt;- pdf_html(pdf) html %\u0026gt;% read_html() %\u0026gt;% html_nodes(\u0026#39;div\u0026#39;) %\u0026gt;% head() ## {xml_nodeset (6)} ## [1] \u0026lt;div id=\u0026quot;page1-div\u0026quot; style=\u0026quot;position:relative;width:1263px;height:892px;\u0026quot;\u0026gt; ... ## [2] \u0026lt;div id=\u0026quot;page2-div\u0026quot; style=\u0026quot;position:relative;width:1263px;height:892px;\u0026quot;\u0026gt; ... ## [3] \u0026lt;div id=\u0026quot;page3-div\u0026quot; style=\u0026quot;position:relative;width:1263px;height:892px;\u0026quot;\u0026gt; ... ## [4] \u0026lt;div id=\u0026quot;page4-div\u0026quot; style=\u0026quot;position:relative;width:1263px;height:892px;\u0026quot;\u0026gt; ... ## [5] \u0026lt;div id=\u0026quot;page5-div\u0026quot; style=\u0026quot;position:relative;width:1263px;height:892px;\u0026quot;\u0026gt; ... ## [6] \u0026lt;div id=\u0026quot;page6-div\u0026quot; style=\u0026quot;position:relative;width:1263px;height:892px;\u0026quot;\u0026gt; ... PDF para tabelas Use o tabulizer! Apesar de depender do odiado rJava (que é um pacote chato de instalar e configurar) o tabulizer é capaz de extrair os dados diretamente para tabelas, de forma simples e intuitiva.\nPara instalar o tabulizer, siga as instruções dessa página. Já adianto que pode não ser uma tarefa fácil, principalmente por conta do rJava.\nExemplo: Uma vez montei esse código para estruturar um pdf contendo gastos em obras públicas. Além de usar o tabulizer, usei os pacotes usuais do tidyverse e a função abjutils::rm_accent() para tirar os acentos do texto.\nlibrary(tabulizer) Vamos usar esse pdf de exemplo.\nSe estiver no desktop, é possível ver o documento abaixo:\n# No meu pc demorou 40 segundos. tab \u0026lt;- extract_tables(\u0026#39;../../static/data/ocr/pdf_compras.pdf\u0026#39;) Essa função serve para arrumar os nomes zoados que vêm no arquivo:\narrumar_nomes \u0026lt;- function(x) { x %\u0026gt;% tolower() %\u0026gt;% str_trim() %\u0026gt;% str_replace_all(\u0026#39;[[:space:]]+\u0026#39;, \u0026#39;_\u0026#39;) %\u0026gt;% str_replace_all(\u0026#39;%\u0026#39;, \u0026#39;p\u0026#39;) %\u0026gt;% str_replace_all(\u0026#39;r\\\\$\u0026#39;, \u0026#39;\u0026#39;) %\u0026gt;% abjutils::rm_accent() } Agora veja a magia do tidyverse posta em prática:\ntab_tidy \u0026lt;- tab %\u0026gt;% # transforma matrizes em tibbles map(as_tibble) %\u0026gt;% # empilha bind_rows() %\u0026gt;% # arruma nomes a partir da primeira linha set_names(arrumar_nomes(.[1,])) %\u0026gt;% # tira primeira linha slice(-1) %\u0026gt;% # tira espaços extras mutate_all(funs(str_replace_all(., \u0026#39;[[:space:]]+\u0026#39;, \u0026#39; \u0026#39;))) %\u0026gt;% # tira espaços nas bordas mutate_all(str_trim) A Tabela 1 mostra as primeiras cinco linhas do resultado.\nTabela 1: Base arrumada a partir de arquivo pdf, usando o pacote tabulizer. uf municipios_atendidos tipo subtipo nome_do_empreendimento p_de_execucao total_pac__milhoes orgao BA ILHÉUS/BA Aeroporto Terminal de Passageiros Aeroporto de Ilhéus - PROJETO DE INFRAESTRUTURA E IMPLANTAÇÃO DO MOP Menor que 50% 2,1 Empresa Brasileira de Infraestrutura Aeroportuária PR LONDRINA/PR Aeroporto Terminal de Passageiros Aeroporto de Londrina - IMPLANTAÇÃO DO MÓDULO OPERACIONAL - MOP NO PROCESSAMENTO DE EMBARQUE Maior que 50% 4,0 Empresa Brasileira de Infraestrutura Aeroportuária PA MARABÁ/PA Aeroporto Terminal de Passageiros Aeroporto de Marabá - REFORMA COM AMPLIAÇÃO DO TPS EXISTENTE Maior que 50% 7,1 Empresa Brasileira de Infraestrutura Aeroportuária CE ACOPIARA/CE Centro de Artes e Esportes Unificados Modelo 3000m² Praças - Acopiara - CE - Modelo 3000m² Menor que 50% 2,0 Ministério da Cultura SP AMERICANA/SP Centro de Artes e Esportes Unificados Modelo 3000m² Praças - Americana - SP - Modelo 3000m² Menor que 50% 2,4 Ministério da Cultura Pacote tesseract para PDFs digitalizados O tesseract é uma biblioteca escrita em C e é uma das mais famosas ferramentas abertas para extração de textos a partir de imagens. O pacote em R de mesmo nome serve para usar essa biblioteca pelo R sem causar dores de cabeça.\nPara instalar o tesseract no Windows, basta rodar\ninstall.packages(\u0026#39;tesseract\u0026#39;) Para Mac e Linux, siga as instruções dessa página.\nA principal função do pacote tesseract é a ocr(). Seu input é o caminho de uma imagem (pdf, jpeg, tiff, entre outras) e seu output é um texto. Logo, nosso primeiro passo é transformar o pdf em imagem.\npdf \u0026lt;- \u0026#39;../../static/data/ocr/pdf_digitalizado.pdf\u0026#39; img \u0026lt;- pdf_render_page( pdf = pdf, # caminho do arquivo page = 1, # índice da página dpi = 300 # resolução (pontos por polegada) ) # salvando imagem num arquivo png png::writePNG(img, \u0026#39;../../static/data/ocr/pdf_digitalizado_img.png\u0026#39;) Se o PDF tiver mais páginas, você pode fazer um loop para salvar várias imagens. Agora, usamos a função ocr() no arquivo salvo.\nlibrary(tesseract) txt \u0026lt;- ocr(\u0026#39;../../static/data/ocr/pdf_digitalizado_img.png\u0026#39;) # imprimindo só os 300 primeiros caracteres do resultado cat(str_trunc(txt, 300)) ## This is a sample document to test the PDF Image+Text OCR Engine. ## This is a sample document to test the PDF Image+Text OCR Engine. ## This is a sample document to test the PDF Image+Text OCR Engine. ## This is a sample document to test the PDF Image+Text OCR Engine. ## This is a sample document to test the... Wrap-up Se seu pdf for digital, use pdftools::pdf_text(). Se seu pdf for digitalizado, use pdftools::pdf_render_page(), depois png::writePNG() e por fim tesseract::ocr(). É isso. Happy coding ;)\n","permalink":"https://blog.curso-r.com/posts/2017-05-27-ocr/","tags":["pdf","pacotes","ocr"],"title":"PDF e OCR"},{"author":["Fernando"],"categories":["análises"],"contents":" Quando eu trabalhei no Núcleo de Estudos da Violência da USP, obter informações da Secretaria de Segurança Pública de São Paulo (SSP) era uma tarefa meio esotérica. Coletávamos os dados de todos os DP’s de São Paulo, que são aproximadamente 100, e, como fazer isso manualmente era demorado, aplicávamos uma solução automática em dois passos. Primeiro raspávamos o site da SSP em Python e depois rodávamos uma macro em VBA chamada “Mestre Dos Magos”, que era responsável por consolidar as séries históricas em excel. Eu achava o procedimento um pouco hermético porque nenhum deles tinha sido feito por mim ou pela minha equipe, era uma herança que a gente não sabia como consertar se desse algum problema. Para dar um exemplo, no final da minha breve estadia no NEV o script não funcionava mais, então era necessário baixar tudo manualmente.\nDepois dessa época eu nunca mais mexi com esses dados. Eu sempre tive vontade de implementar uma solução em R, mas sempre faltou motivação. Felizmente, na ultima semana minha namorada precisou dos dados da SSP para um trabalho que está fazendo, só que dessa vez o interesse era em todos os 645 municípios do estado de São Paulo nos anos entre 2013 a 2016. Como não há como baixar todas essas informações de uma vez e downloads individuais tomam muito tempo, eu me senti motivado o suficiente para atacar o problema.\nPra ser sincero, foi bem mais fácil do que eu achei que seria. A construção do programa foi tão simples que cabe até mesmo neste post de blog, mas não é só por isso que ela está aqui. Esse é um exemplo minimal de todas as fases de construção de um scraper.\nOs quatro passos para construir um scraper Em um esquema aproximado, eu acredito que raspar (ou scrappear) um site pode ser feito em 4 passos:\nDefina a página que você quer raspar; Identifique exatamente as requisições que produzem o que você quer; Construa um programa que imite as requisições que você faria manualmente; Repita o passo 3. quantas vezes quiser. Mesmo que existam scrapers mais complicados, é verdade que seguir esses passos pelo menos te ajuda a chegar mais perto do que você realmente precisará fazer depois.\nDefina o que você quer Tanto o NEV quanto a minha namorada tinham interesse nas tabelas que apareciam numa URL específica do site: http://www.ssp.sp.gov.br/Estatistica/Pesquisa.aspx. A lógica por trás da divulgação dessas informações é a seguinte: você escolhe um ano, uma localidade geográfica/administrativa e um tipo de informação e ele te devolve uma tabela. Os anos disponíveis são os anos de 2001 a 2017, as localidades são os cruzamentos entre Municípios, Regiões e Delegacias (notando que você sempre pode escolher “Todos”) e os tipos de informação são taxas de delito, ocorrências registradas por ano, ocorrências registradas por mês e produtividade policial. Nesta aplicação vamos procurar contagens de ocorrências registradas por mês.\nIdentifique o que o site faz por trás Identificar o que o site faz por trás provavelmente é a fase mais complicada da construção de um scraper. A dificuldade é que não existe um algoritmo que faça isso pra você. Normalmente eu sigo alguns passos, mas eles exigem uma dose de insight para funcionar, o que implica que talvez não seja possível seguir todos os passos em sequência.\nEntre na página imediatamente anterior à página que você quer acessar. Abra as ferramentas de desenvolvedor dos seu navegador (isso normalmente é equivalente à “aperte F12”). Selecione a aba “Network” (ou “Rede”) na caixa de ferramentas do desenvolvedor. Vá para a página que você quer. Na lista de requisições que o site fez ao servidor, identifique aquela(s) que é(são) relevante(s) a sua pesquisa. Parece muito louco né? No geral, é muito louco sim, mas quando você dá sorte é fácil. Vou mostrar o que acontece no nosso exemplo:\nA página imediatamente anterior Eu quero a url http://www.ssp.sp.gov.br/Estatistica/Pesquisa.aspx com alguma seleção, então pra realizar o primeiro passo basta entrar em http://www.ssp.sp.gov.br/Estatistica/Pesquisa.aspx.\nFerramentas do desenvolvedor e aba “Network” Em qualquer navegador, as ferramentas do desenvolvedor mostram o background do que aparece na tela. Se estiver nessa aba, quando você entrar em http://www.ssp.sp.gov.br/Estatistica/Pesquisa.aspx e apertar F5, vai encontrar uma tela mais ou menos parecida com a tela abaixo:\nCada linha representa uma requisição, que é essencialmente o envio de um arquivo .html ao servidor. O conteúdo que visualizamos na tela é o resultado de todas essas requisições e, se destrincharmos cada uma delas, podemos identificar aquelas que são relevantes para o nosso problema.\nA requisição O próximo passo é identificar o que é que o navegador pede ao servidor quando te devolve o que você quer. Se você escolher o ano de 2017 na caixa de seleção de anos, por exemplo, vai encontrar uma tela parecida com essa aqui.\nAntes de prosseguir, é necessário fazer uma inspeção meticulosa de todas as requisições que aparecem, mas vou encurtar a discussão afirmando que a primeira delas é a mais importante. Existem muitas maneiras de deduzir que ela é uma boa candidata, como por exemplo olhando que ela é a única requisição relevante que recebe html, mas vou pular essa parte.\nSegundo o nosso dedo duro, a requisição utiliza o método “POST” e, quando clicamos nela, temos informações sobre ela no painel ao lado. Como eu falei acima, uma requisição é um arquivo .html que “pede” alguma ao servidor com base no seu conteúdo. No geral, um bom lugar para procurar o que a requisição está pedindo é o seu conjunto de parâmetros, na aba “Params” do painel de detalhamento da requisição.\nQuando inspecionamos essa requisição “POST”, identificamos que as coisas que são relevantes para o conteúdo da página estão nesses parâmetros. De fato, pensando ingenuamente, clicar apenas em “2017” deve mexer nos parâmetros que tem a ver com isso mas deve deixar os demais parâmetros fixos. Por sorte, os parâmetros observados batem com essa expectativa: “__EVENTTARGET” é ctl00$conteudo$$ddlAnos é um placeholder que tem a ver com a caixa de seleção em que mexemos, os dois próximos parâmetros estão zerados, “__VIEWSTATE” e “__EVENTVALIDATION” são parâmetros da sessão, e, por fim, temos os parâmetros da consulta, que estão todos zerados com exceção de ctl00$conteudo$$ddlAnos.\nParece que os parâmetros tem tudo a ver com a saída. Será que mexer apenas neles basta para copiar a requisição do navegador?\nCrie um robô que imita o que um humano faria Agora que sabemos um pouco mais sobre como as requisições funcionam, vamos tentar fazer o POST mais simples de todos: ele só tem os parâmetros da última imagem. Em R, o jeito mais fácil de fazer requisições é usando o pacote httr. Ele é bem intuitivo e flexível, de tal forma que fazer um POST é feito simplesmente chamando a função httr::POST:\nurl \u0026lt;- \u0026#39;http://www.ssp.sp.gov.br/Estatistica/Pesquisa.aspx\u0026#39; #código para dar um POST vazio no site httr::POST(url) #o resultado é simplesmente o resultado original da página Para colocar parâmetros num POST, basta usar o parâmetro body. Antes de complicar, vamos tentar o conjunto de parâmetros mais simples de todos: vamos ignorar tudo que a gente não sabe exatamente o que é e preencher só o que sabemos:\nparams \u0026lt;- list(`__EVENTTARGET` = \u0026quot;ctl00$conteudo$$ddlAnos\u0026quot;, `__EVENTARGUMENT` = \u0026quot;\u0026quot;, `__LASTFOCUS` = \u0026quot;\u0026quot;, `__VIEWSTATE` = \u0026quot;\u0026quot;, `__EVENTVALIDATION` = \u0026quot;\u0026quot;, `ctl00$conteudo$ddlAnos` = \u0026quot;2015\u0026quot;, `ctl00$conteudo$ddlRegioes` = \u0026quot;0\u0026quot;, `ctl00$conteudo$ddlMunicipios` = \u0026quot;0\u0026quot;, `ctl00$conteudo$ddlDelegacias` = \u0026quot;0\u0026quot;) Agora vamos fazer a requisição. O passo seguinte é apenas pra traduzir o resultado pra um formato mais fácil de mexer.\nresposta \u0026lt;- httr::POST(url, body = params, encode = \u0026#39;form\u0026#39;) %\u0026gt;% xml2::read_html() # traduz o resultado Como vamos saber se deu certo? Existem vários jeitos, mas, por simplicidade, aqui vamos apenas checar se alguma tabela contida em resposta é igual à tabela que extraímos manualmente do site. Vamos fazer isso usando a função rvest::html_table().\nresposta %\u0026gt;% rvest::html_table() # extrai todas as tabelas de um código em html. Como se vê acima, nada que se parece com uma tabela na resposta da nossa requisição é a tabelinha que identificamos no site. Muita coisa pode ter dado errado, mas vamos começar pelo que é mais evidente: nós ignoramos os parâmetros __VIEWSTATE e __EVENTVALIDATION. Em última instância, nos vamos precisar entender o que esses parâmetros significam, mas primeiro vamos tentar simplesmente obter uma cópia desses valores diretamente do site. Dando um ctrl+f no painel de ferramentas de desenvolvedor identificamos que o valores dessa variáveis sai de umas tags input nomeadas de acordo com o parâmetro que representam.\nview_state \u0026lt;- httr::POST(url) %\u0026gt;% xml2::read_html() %\u0026gt;% rvest::html_nodes(\u0026quot;input[name=\u0026#39;__VIEWSTATE\u0026#39;]\u0026quot;) %\u0026gt;% rvest::html_attr(\u0026quot;value\u0026quot;) event_validation \u0026lt;- httr::POST(url) %\u0026gt;% xml2::read_html() %\u0026gt;% rvest::html_nodes(\u0026quot;input[name=\u0026#39;__EVENTVALIDATION\u0026#39;]\u0026quot;) %\u0026gt;% rvest::html_attr(\u0026quot;value\u0026quot;) Com os nossos embustes em mãos, basta pular para a próxima etapa: tentar simular um clique no site. Nosso novo conjunto de parâmetros fica:\nparams \u0026lt;- list(`__EVENTTARGET` = \u0026quot;ctl00$conteudo$$ddlAnos\u0026quot;, `__EVENTARGUMENT` = \u0026quot;\u0026quot;, `__LASTFOCUS` = \u0026quot;\u0026quot;, `__VIEWSTATE` = view_state, `__EVENTVALIDATION` = event_validation, `ctl00$conteudo$ddlAnos` = \u0026quot;2015\u0026quot;, `ctl00$conteudo$ddlRegioes` = \u0026quot;0\u0026quot;, `ctl00$conteudo$ddlMunicipios` = \u0026quot;0\u0026quot;, `ctl00$conteudo$ddlDelegacias` = \u0026quot;0\u0026quot;) E o código da requisição fica:\nresposta \u0026lt;- httr::POST(url, body = params, encode = \u0026#39;form\u0026#39;) %\u0026gt;% xml2::read_html() %\u0026gt;% rvest::html_table() Exatamente a tabelinha que queríamos! Entretanto, nem tudo são flores. Como eu mencionei ali em cima, tanto a minha namorada quanto o NEV tinham interesse no número de BO’s, que não é o que obtivemos fazendo a requisição via R. Fuçando um pouco, é fácil ver que o que obtivemos são os números de produtividade policial. Será que é fácil mexer nos parâmetros pra obter os números de BO’s?\nFelizmente, a resposta é sim, mas não é tão simples quanto parece. Se de outra tela qualquer você clicar em “Ocorrências Registradas por Mês” você vai perceber que o “__EVENTTARGET” mudou para ctl00$conteudo$$btnMes, mas, a despeito do que se poderia imaginar, os outros parâmetros permaneceram com os nomes intactos.\nComo essa requisição sugere, você consegue variar os tipos de informação simplesmente variando o “__EVENTTARGET”. Com isso, uma requisição para obter as “Ocorrências Registradas por Mês” ficaria:\nparams \u0026lt;- list(`__EVENTTARGET` = \u0026quot;ctl00$conteudo$$ddlAnos\u0026quot;, `__EVENTARGUMENT` = \u0026quot;\u0026quot;, `__LASTFOCUS` = \u0026quot;\u0026quot;, `__VIEWSTATE` = view_state, `__EVENTVALIDATION` = event_validation, `ctl00$conteudo$ddlAnos` = \u0026quot;2015\u0026quot;, `ctl00$conteudo$ddlRegioes` = \u0026quot;0\u0026quot;, `ctl00$conteudo$ddlMunicipios` = \u0026quot;0\u0026quot;, `ctl00$conteudo$ddlDelegacias` = \u0026quot;0\u0026quot;) resposta \u0026lt;- httr::POST(url, body = params, encode = \u0026#39;form\u0026#39;) %\u0026gt;% xml2::read_html() É importante notar que cada vez que vamos um POST desse, é como se estivessemos entrando na página novamente, de tal forma que as informações que a página tem quando damos vários cliques seguidos são diferentes das informações de quando acessamos a página pelo R. Por exemplo, quando estamos navegando pela página pelos links, o “__EVENTTARGET” volta para ctl00$conteudo$$ddlAnos se você trocar de página saindo de uma página de “Ocorrências Registradas por Mês”, mas o tipo de informação vêm corretamente. Isso acontece porque, se você acessar várias páginas em sequência, a página sabe de onde você veio.\nRepita quantas vezes quiser Agora vem a repetição, o passo final da raspagem. Antes de qualquer coisa precisamos de duas pequenas generalizações: queremos baixar vários anos e vários municípios. Isso é fácil de fazer pois os índices dos municípios são as suas posições em ordem alfabética. Dessa forma, é possível fazer uma função que baixa as “Ocorrências Registradas por Mês” de um município em um determinado ano:\nbaixa_bo_municipio_ano \u0026lt;- function(ano, municipio){ pivot \u0026lt;- httr::GET(url) #serve apenas para pegarmos um view_state e um event_validation valido view_state \u0026lt;- pivot %\u0026gt;% xml2::read_html() %\u0026gt;% rvest::html_nodes(\u0026quot;input[name=\u0026#39;__VIEWSTATE\u0026#39;]\u0026quot;) %\u0026gt;% rvest::html_attr(\u0026quot;value\u0026quot;) event_validation \u0026lt;- pivot %\u0026gt;% xml2::read_html() %\u0026gt;% rvest::html_nodes(\u0026quot;input[name=\u0026#39;__EVENTVALIDATION\u0026#39;]\u0026quot;) %\u0026gt;% rvest::html_attr(\u0026quot;value\u0026quot;) params \u0026lt;- list(`__EVENTTARGET` = \u0026quot;ctl00$conteudo$$btnMes\u0026quot;, `__EVENTARGUMENT` = \u0026quot;\u0026quot;, `__LASTFOCUS` = \u0026quot;\u0026quot;, `__VIEWSTATE` = view_state, `__EVENTVALIDATION` = event_validation, `ctl00$conteudo$ddlAnos` = \u0026quot;2015\u0026quot;, `ctl00$conteudo$ddlRegioes` = \u0026quot;0\u0026quot;, `ctl00$conteudo$ddlMunicipios` = municipio, `ctl00$conteudo$ddlDelegacias` = \u0026quot;0\u0026quot;) httr::POST(url, body = params, encode = \u0026#39;form\u0026#39;) %\u0026gt;% xml2::read_html() %\u0026gt;% rvest::html_table() %\u0026gt;% dplyr::first() %\u0026gt;% #\u0026#39; serve pra pegar apenas a primeira tabela da página, #\u0026#39; se houver mais do que uma. Estou assumindo que a #\u0026#39; tabela que eu quero é sempre a primeira. dplyr::mutate(municipio = municipio, ano = ano) } Pra ilustar um grande loop, vamos baixar os BO’s de todos os municípios, no ano de 2016. A função demora um pouco pra rodar, então quem quiser ver como fica o resultado final pode clicar neste link.\nGRID \u0026lt;- expand.grid(municipio = 1:645, ano = \u0026#39;2016\u0026#39;, stringsAsFactors = F) D \u0026lt;- purrr::by_row(GRID, baixa_bo_municipio_ano, .to = \u0026quot;ocorrencias\u0026quot;) %\u0026gt;% tidyr::unnest(ocorrencias) Resultados Com esses dados dá pra fazer muitas coisas legais. É possível, por exemplo, fazer mapas como esse, que representa o número de homicídios em 2016, por município. Ele foi feito em R, mas como fazê-lo é assunto pra outro post!\n","permalink":"https://blog.curso-r.com/posts/2017-05-19-scrapper-ssp/","tags":["web scraping","dados abertos"],"title":"Web scraping do site da Secretaria de Segurança Pública de São Paulo"},{"author":["Fernando"],"categories":["Tutoriais"],"contents":" De todas as visualizações, mapas são aquelas que impressionam mais. É muito mais fácil alguém se maravilhar com o mapa mais simples do que com o ggplot2 mais complicado. Felizmente, considerando essa comparação, o R disponibiliza muitos recursos para construir mapas.\nDentro do tidyverse é possível construir mapas usando a função geom_map, do pacote ggplot2, mas está fora do escopo deste post explicar como ela funciona. Aqui vamos apenas descobrir como usá-la para agilizar a construção de um gráfico simples.\nNa Associação Brasileira de Jurimetria, nós temos um tipo favorito de mapa. Tipicamente temos interesse em diferenciar as unidades da federação por alguma variável quantitativa, seja ela categorizada ou não, e a ferramenta certa para isso é um mapa temático dos estados. Essa necessidade é tão frequente que as ferramentas mais importantes para construção desses gráficos estão num pacote chamado abjData.\nAs coisas estão dispostas de tal forma que, a partir de uma tabela que relaciona uma variavel e os estados brasileiros, construir um gráfico similar ao mapa abaixo pode ser feito chamando apenas uma função1.\ndataset %\u0026gt;% constroi_mapa_tematico() Neste post, vamos construir a função constroi_mapa_tematico e aprender a implementar alguns tweaks.\nA função O geom_map é uma função do ggplot2 que renderiza coordenadas de mapas. Ela pega um data_frame especial que diz quais coordenadas usar (e como usá-las) e plota no gráfico. Esse mapa normalmente é obtido aplicando a função fortify em um conjunto de dados geospaciais.\nPara os gráficos que vamos construir aqui, não vai ser necessário aplicar a função fortify, pois o resultado desse passo já está disponível no pacote abjData e ele é o data_frame br_uf_map. Tudo que vamos precisar fazer é pedir que o geom_map use esse cara.\ndevtools::install_github(\u0026#39;abjur/abjData\u0026#39;) #instala o pacote constroi_mapa_tematico \u0026lt;- function(dataset){ dataset %\u0026gt;% inner_join(abjData::br_uf_map) %\u0026gt;% { ggplot(.) + geom_map(aes(x = long, y = lat, map_id = id, fill = variavel), color = \u0026#39;gray30\u0026#39;, map = ., data = .) + theme_void() + coord_equal() } } O input da função é uma tabela dataset com duas colunas\nid, que representa as unidades da federação abreviada; variavel, variável numérica (ou fator) que vai colorir o gráfico. No exemplo acima essa tabela era:\ndataset %\u0026gt;% head(10) Os tweaks A função constroi_mapa_tematico devolve um ggplot2, então ainda dá pra mexer em alguns parâmetros estéticos após a construção do mapa. As três coisas que vamos aprender a fazer são\nAdicionar labels e títulos dataset %\u0026gt;% constroi_mapa_tematico() + ggtitle(\u0026quot;Roubos de carros no Brasil em 2014\u0026quot;) + scale_fill_continuous(name = \u0026quot;Taxa/100 mil hab.\u0026quot;) Alterar as cores da escala dataset %\u0026gt;% constroi_mapa_tematico() + ggtitle(\u0026quot;Roubos de carros no Brasil em 2014\u0026quot;) + scale_fill_continuous(name = \u0026quot;Taxa/100 mil hab.\u0026quot;, low = \u0026#39;white\u0026#39;, high = \u0026#39;red\u0026#39;, na.value = \u0026#39;white\u0026#39;) Alterar valores dataset %\u0026gt;% constroi_mapa_tematico() + ggtitle(\u0026quot;Roubos de carros no Brasil em 2014\u0026quot;) + scale_fill_continuous(name = \u0026quot;Taxa/100 mil hab.\u0026quot;, low = \u0026#39;green\u0026#39;, high = \u0026#39;red\u0026#39;, na.value = \u0026#39;white\u0026#39;, breaks = seq(0,300,50)) Os dados foram levantados pelo Fórum Brasileiro de Segurança Pública e obtidos via download no seu site http://www.forumseguranca.org.br/estatisticas/introducao/.↩︎\n","permalink":"https://blog.curso-r.com/posts/2017-05-04-mapas-tematicos-3-minutos/","tags":["mapas","gráficos","ggplot2"],"title":"Gráficos miojo: Mapas temáticos do Brasil em 3 minutos ou menos"},{"author":["William"],"categories":["análises"],"contents":" A cor é uma diferença notável entre os filmes da Marvel e da DC. Enquanto a Disney/Marvel Studios costuma lançar filmes com tons mais claros e alegres, a Warner tem optado por cenários escuros, com um aspecto mais sombrios. Essas escolhas são um reflexo do clima das histórias de cada universo: aventuras engraçaralhas com um drama superficial vs seja lá o que passa na cabeça do Zack Snyder.\nPara estudar melhor a paleta de cores utilizadas nos filmes, vamos aplicar a análise introduzida pelo Dani neste post, com pequenas alterações. Como amostra, selecionei 10 imagens de Batman vs Superman e 10 do Capitão América: guerra civil. Tentando deixar a análise o menos subjetiva possível, escolhi imagens de cenas emblemáticas e dos principais personagens. Abaixo as imagens que peguei de cada filme.\nSeguindo a análise do Dani, vamos utilizar as seguintes bibliotecas para a análise.\nlibrary(jpeg) library(tidyverse) library(glue) Eu salvei as imagens em arquivos do tipo “bvs_n.jpg” e “cw_n.jpg”, com n variando de 1 a 10. Isso facilitou a leitura desses arquivos. O código abaixo mostra como criar um vetor com o caminho das 10 imagens de cada filme. Se você quiser saber mais sobre a função glue(), visite este post.\narquivos_bvs \u0026lt;- glue(\u0026quot;images/bvs_{n}.jpg\u0026quot;, n = 1:10) arquivos_cw \u0026lt;- glue(\u0026quot;images/cw_{n}.jpg\u0026quot;, n = 1:10) Como vamos trabalhar com mais de uma imagem, eu criei a função ler_imagem() para ler os arquivos.\nler_imagem \u0026lt;- function(caminho) { img \u0026lt;- readJPEG(caminho) %\u0026gt;% apply(3, as.numeric) } Podemos então usar a função map() para aplicá-la a todos os 10 arquivos. A função reduce(rbind) transforma as 10 matrizes de pixels em uma matriz só, como se as imagens estivessem coladas uma embaixo da outra.\nimg_bvs \u0026lt;- map(arquivos_bvs, ler_imagem) %\u0026gt;% reduce(rbind) img_cw \u0026lt;- map(arquivos_cw, ler_imagem) %\u0026gt;% reduce(rbind) Abaixo estão as funções cria_paleta() e exibir() do post do Dani. A única diferença aqui é que a função cria_paleta() já recebe a matriz representando a imagem.\ncriar_paleta \u0026lt;- function(img_matrix, num_cores){ km \u0026lt;- kmeans(img_matrix, centers = num_cores) img_df \u0026lt;- tibble( r = img_matrix[,1], g = img_matrix[,2], b = img_matrix[,3], cluster = km$cluster ) centroides \u0026lt;- img_df %\u0026gt;% group_by(cluster) %\u0026gt;% summarise_all(mean) centroides \u0026lt;- centroides %\u0026gt;% mutate(cor = rgb(r, g, b)) sort(centroides$cor) } exibir \u0026lt;- function(x) { n \u0026lt;- length(x) old \u0026lt;- par(mar = c(0.5, 0.5, 0.5, 0.5)) on.exit(par(old)) image(1:n, 1, as.matrix(1:n), col = x, ylab = \u0026quot;\u0026quot;, xaxt = \u0026quot;n\u0026quot;, yaxt = \u0026quot;n\u0026quot;, bty = \u0026quot;n\u0026quot;) } Assim, basta aplicar essas funções aos objetos img_bvs e img_cw para obter as paletas. Primeiro para o Batman vs Superman:\npaleta_bvs \u0026lt;- criar_paleta(img_bvs, 10) exibir(paleta_bvs) E agora para o Capitão América:\npaleta_cw \u0026lt;- criar_paleta(img_cw, 10) exibir(paleta_cw) Observe que o filme da DC tem cores mais escuras e fortes, com vários tons de azul, indicando as cenas noturnas e de chuva. Já a paleta da Marvel apresenta cores mais claras, com vários tons representando o céu pálido das cenas externas.\nPodemos fazer a análise agora para o pôster de cada filme (o que aparece no IMDB):\nimg_bvs \u0026lt;- ler_imagem(\u0026quot;images/bvs_poster.jpg\u0026quot;) paleta_bvs \u0026lt;- criar_paleta(img_bvs, 10) exibir(paleta_bvs) img_cw \u0026lt;- ler_imagem(\u0026quot;images/cw_poster.jpg\u0026quot;) paleta_cw \u0026lt;- criar_paleta(img_cw, 10) exibir(paleta_cw) Veja que os diferentes tons de azul se repete no pôster do Batman vs Superman. Já o pôster do Capitão América é bem cinzento, com metade da paleta representando tons de cinza.\nFica então o desafio de repetir a análise para outros filmes e compartilhar o resultado com a gente. Comentários? Sugestões? Críticas? Mande a sua opinião!\n","permalink":"https://blog.curso-r.com/posts/2017-05-01-as-paletas-de-cores-da-marvel-vs-dc/","tags":["imagens","cinema","kmeans"],"title":"As cores da Marvel vs DC"},{"author":["Fernando"],"categories":["tutoriais"],"contents":" Diagramas de Venn são como slides de PowerPoint. Se eles tem poucos elementos concisos em uma ordem inteligente, um leitor consegue conectar as ideias expostas e aprender alguma coisa. Em caso contrário, o excesso de informação se transforma em um obstáculo para a comunicação.\nBons diagramas de Venn são capazes de te fazer perceber cruzamentos que não estão no seu radar. No blog do Andrew Gelman tem um exemplo interessante. Ele considera uma oração, em inglês, que pede:\nGod give me the serenity to accept things which cannot be changed; Give me courage to change things which must be changed; And the wisdom to distinguish one from the other.\nPelo jeito em que ela foi escrita, parece que só existem dois tipos de coisas: aquelas que não podem ser modificadas e aquelas que nós devemos mudar. Entretanto, esse não é o caso:\nSe uma coisa pode ser modificada e eu não devo mudá-la, ela não é importante. Se uma coisa coisa não pode ser modificada e eu devo mudá-la, eu preciso aceitá-la. Considerando esse diagrama, O Gelman até sugere uma prece mais cuidadosa:\nLord, grant me the serenity to accept things that cannot be changed, courage to change things that must be changed; discernment to ignore things that don’t need changing; acceptance that some things I need to change, i can’t; And the wisdom to understand Venn Diagrams.\nA despeito da utilidade de Diagramas de Venn bem feitos, o R oferece poucas maneiras de construí-los. No CRAN, existem apenas três pacotes especializados em construir esses gráficos, mas duas delas são um pouco insatisfatórias. Neste post, vamos descobrir como fazer diagramas de Venn usando o pacote VennDiagram.\nO Dataset Para não nos distrairmos com bases de dados complicadas, vamos usar uma tabela artifical inspirada nesse link. Cada linha dessa base representa as preferências por animais de estimação de uma pessoa hipotética. No total temos 34 pessoas na base e as opções possíveis são “Cães”,“Gatos”,“Lagartos” e “Serpentes”.\nO data.frame está disponível no código fonte desse post.\nPacotes Codificar os gráficos no pacote VennDiagram é um pouco chato, por isso vamos precisar de outros pacotes pra ajudar. No geral, apenas as coisas do tidyverse já vão servir.\nO comando abaixo chama os pacotes que vamos usar neste post.\nlibrary(VennDiagram) library(tidyverse) Diagramas de Venn usando o pacote VennDiagram Como eu já disse antes, esse pacote é bem chato, pois em todos os gráficos você precisa escrever diretamente o número de elementos em cada pedaço dos conjuntos e os parâmetros gráficos detalhados demais. Pra funcionar bem, você tem que dizer exatamente o que quer fazer.\nVamos começar fazendo um diagrama simples. Ele vai representar o conjunto de pessoas que gostam de cachorros.\nnumero_de_pessoas_que_gostam_de_cachorros \u0026lt;- dataset %\u0026gt;% dplyr::filter(Dog == 1) %\u0026gt;% nrow() grid.newpage() invisible(draw.single.venn(numero_de_pessoas_que_gostam_de_cachorros, #Número de elementos do conjunto category = \u0026quot;Pessoas que gostam de cachorros\u0026quot;, #Nome do conjunto lty = \u0026quot;blank\u0026quot;, #Grossura da borda dos conjuntos.\u0026quot;blank\u0026quot; quer dizer que não vai ter borda fill = \u0026quot;light blue\u0026quot;, #Cor do conjunto alpha = 0.5 #Transparência do conjunto. Varia de 0 a 1 )) Complicando um pouco mais, dessa vez vamos fazer um diagrama que represente as pessoas que gostam de gatos ou de cachorros.\nnumero_de_pessoas_que_gostam_de_cachorros \u0026lt;- dataset %\u0026gt;% dplyr::filter(Dog == 1) %\u0026gt;% nrow() numero_de_pessoas_que_gostam_de_gatos \u0026lt;- dataset %\u0026gt;% dplyr::filter(Cat == 1) %\u0026gt;% nrow() numero_de_pessoas_que_gostam_de_gatos_e_de_cachorros \u0026lt;- dataset %\u0026gt;% dplyr::filter(Dog == 1 \u0026amp; Cat == 1) %\u0026gt;% nrow() grid.newpage() invisible(draw.pairwise.venn(area1 = numero_de_pessoas_que_gostam_de_cachorros, #Número de elementos da primeira categoria area2 = numero_de_pessoas_que_gostam_de_gatos, #Número de elementos da segunda categoria cross.area = numero_de_pessoas_que_gostam_de_gatos_e_de_cachorros, #Número de elementos na intersecção category = c(\u0026quot;Pessoas que gostam\\nde cachorros\u0026quot;, \u0026quot;Pessoas que gostam\\nde gatos\u0026quot;), #Nome das categorias lty = c(\u0026quot;blank\u0026quot;, \u0026quot;blank\u0026quot;), #Grossura das bordas fill = c(\u0026quot;light blue\u0026quot;, \u0026quot;pink\u0026quot;), #Cores das bordas alpha = c(0.5, 0.5), #Transparência das bordas cat.pos = c(0, 0), #Posição dos títulos com relação aos círculos. 0 quer dizer \u0026quot;em cima\u0026quot; scaled = F #Constrói as áreas sem escala )) A falta de escalas deixou esse gráfico simétrico e esteticamente agradável. Entretanto, em algumas situações é mais legal representar os conjuntos em escala.\nnumero_de_pessoas_que_gostam_de_cachorros \u0026lt;- dataset %\u0026gt;% dplyr::filter(Dog == 1) %\u0026gt;% nrow() numero_de_pessoas_que_gostam_de_serpentes \u0026lt;- dataset %\u0026gt;% dplyr::filter(Snake == 1) %\u0026gt;% nrow() numero_de_pessoas_que_gostam_de_cachorros_e_de_serpentes \u0026lt;- dataset %\u0026gt;% dplyr::filter(Dog == 1 \u0026amp; Snake == 1) %\u0026gt;% nrow() grid.newpage() invisible(draw.pairwise.venn(area1 = numero_de_pessoas_que_gostam_de_cachorros, #Número de elementos da primeira categoria area2 = numero_de_pessoas_que_gostam_de_serpentes, #Número de elementos da segunda categoria cross.area = numero_de_pessoas_que_gostam_de_cachorros_e_de_serpentes, #Número de elementos na intersecção category = c(\u0026quot;Pessoas que gostam\\nde cachorros\u0026quot;, \u0026quot;Pessoas que gostam\\nde serpentes\u0026quot;), #Nome das categorias lty = c(\u0026quot;blank\u0026quot;, \u0026quot;blank\u0026quot;), #Grossura das bordas fill = c(\u0026quot;light blue\u0026quot;, \u0026quot;light green\u0026quot;), #Cores das bordas alpha = c(0.5, 0.5), #Transparência das bordas cat.pos = c(0, 0), #Posição dos títulos com relação aos círculos. 0 quer dizer \u0026quot;em cima\u0026quot; scaled = T #Constrói as áreas sem escala )) Até aqui tudo vai mais ou menos bem, mas a coisa fica bem mais chata quando você precisa representar um diagrama com três conjuntos. Você vai precisar passar pra função o tamanho de todos os pedacinhos.\nnumero_de_pessoas_que_gostam_de_gatos \u0026lt;- dataset %\u0026gt;% dplyr::filter(Cat == 1) %\u0026gt;% nrow() numero_de_pessoas_que_gostam_de_serpentes \u0026lt;- dataset %\u0026gt;% dplyr::filter(Snake == 1) %\u0026gt;% nrow() numero_de_pessoas_que_gostam_de_lagartos \u0026lt;- dataset %\u0026gt;% dplyr::filter(Lizard == 1) %\u0026gt;% nrow() numero_de_pessoas_que_gostam_de_gatos_e_de_serpentes \u0026lt;- dataset %\u0026gt;% dplyr::filter(Cat == 1 \u0026amp; Snake == 1) %\u0026gt;% nrow() numero_de_pessoas_que_gostam_de_gatos_e_de_lagartos \u0026lt;- dataset %\u0026gt;% dplyr::filter(Cat == 1 \u0026amp; Lizard == 1) %\u0026gt;% nrow() numero_de_pessoas_que_gostam_de_serpentes_e_de_lagartos \u0026lt;- dataset %\u0026gt;% dplyr::filter(Lizard == 1 \u0026amp; Snake == 1) %\u0026gt;% nrow() numero_de_pessoas_que_gostam_de_serpentes_e_lagartos_e_gatos \u0026lt;- dataset %\u0026gt;% dplyr::filter(Lizard == 1 \u0026amp; Snake == 1 \u0026amp; Cat == 1) %\u0026gt;% nrow() grid.newpage() invisible(draw.triple.venn(area1 = numero_de_pessoas_que_gostam_de_gatos, area2 = numero_de_pessoas_que_gostam_de_serpentes, area3 = numero_de_pessoas_que_gostam_de_lagartos, n12 = numero_de_pessoas_que_gostam_de_gatos_e_de_serpentes, n23 = numero_de_pessoas_que_gostam_de_serpentes_e_de_lagartos, n13 = numero_de_pessoas_que_gostam_de_gatos_e_de_lagartos, n123 = numero_de_pessoas_que_gostam_de_serpentes_e_lagartos_e_gatos, category = c(\u0026quot;Gatos\u0026quot;, \u0026quot;Serpentes\u0026quot;, \u0026quot;Lagartos\u0026quot;), lty = \u0026quot;blank\u0026quot;, fill = c(\u0026quot;light blue\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;light green\u0026quot;), cat.pos = c(45,45,45), scaled = T)) ","permalink":"https://blog.curso-r.com/posts/2017-04-29-diagramas-de-venn/","tags":["gráficos","pacotes"],"title":"Diagramas de Venn em R"},{"author":["Fernando"],"categories":["análises"],"contents":" Às vezes a venda de uma empresa é um sinal de que algo não vai bem. Até pode ser verdade que nada ruim estivesse acontecendo e, por qualquer motivo que seja, alguém achou melhor parar enquanto estava ganhando, mas eu custo a acreditar que o Porta Dos Fundos estava nessa situação. Principalmente considerando o futuro sombrio que pode esperá-los.\nA última aquisição da Viacom no Brasil foi a finada MTV. Depois de 20 anos de presença relevante no cenário musical brasileiro, o peso dos anos culminou na venda da MTV para a gigante americana. Hoje, a MTV se limita a produzir versões brasileiras péssimas de séries americanas ruins, dar emprego a subcelebridades ligadas ao Supla e reprisar lixos enlatados estadunidenses. Talvez esse não seja o destino que aguarda os integrantes do Porta, até porque alguns deles já tiveram relações diretas com a Viacom e o resultado não foi desastroso, mas, se eu fosse um deles, o triste fim da MTV Brasil soaria o meu alarme de cilada.\nInconformados com a venda e buscando entender com mais afinco os motivos que levaram à venda, neste post revisitamos a análise sobre a decadência do Porta dos Fundos.\nSetup Vamos proceder de uma maneira muito parecida com a que fizemos na última vez. O dataset do Willy era composto por informações sobre todos os vídeos do Porta e as suas colunas eram:\nO título do vídeo. A data de publicação. A contagem de visualizações. A contagem de Likes. Acontagem de Dislikes. O número de comentários. Neste post vamos usar o mesmo dataset, mas atualizado-o até a data da publicação deste post. Isso pode ser feito rodando o código abaixo, que também carrega os pactoes necessários para a análise.\nlibrary(tuber) yt_oauth(\u0026quot;seus\u0026quot;, \u0026quot;dados\u0026quot;) library(dplyr) # Manipulação de dados library(tidyr) # Manipulação de dados library(tibble) # Criação de dataframes library(lubridate) # Manipulação de datas library(purrr) # Funcionais library(ggplot2) # Gráficos get_videos_porta \u0026lt;- function(dates) { yt_search(term = \u0026quot;\u0026quot;, type = \u0026quot;video\u0026quot;, channel_id = \u0026quot;UCEWHPFNilsT0IfQfutVzsag\u0026quot;, published_after = dates$start, published_before = dates$end) } dates \u0026lt;- tibble(start = seq(ymd(\u0026quot;2012-01-01\u0026quot;), ymd(\u0026quot;2017-01-01\u0026quot;), by = \u0026quot;years\u0026quot;), end = seq(ymd(\u0026quot;2012-12-31\u0026quot;), ymd(\u0026quot;2017-12-31\u0026quot;), by = \u0026quot;years\u0026quot;)) %\u0026gt;% mutate(start = paste(start, \u0026quot;T0:00:00Z\u0026quot;, sep = \u0026quot;\u0026quot;), end = paste(end, \u0026quot;T0:00:00Z\u0026quot;, sep = \u0026quot;\u0026quot;)) videos \u0026lt;- by_row(.d = dates, ..f = get_videos_porta, .to = \u0026quot;videos_info\u0026quot;) get_videos_stats \u0026lt;- function(df_row) { get_stats(video_id = df_row$video_id) } dados \u0026lt;- bind_rows(videos$videos_info) %\u0026gt;% select(title, publishedAt, video_id) %\u0026gt;% by_row(..f = get_videos_stats, .to = \u0026quot;videos_stats\u0026quot;) Nas subseções seguintes, vamos revisitar as análises anteriores colocando algumas novas ideias no caldeirão.\nVídeos velhos X Vídeos novos Menos do que a compra pela Viacom, a série temporal de visualizações foi o que realmente me motivou a escrever esse texto.\ndados %\u0026gt;% mutate(views = map(videos_stats, .f = \u0026#39;viewCount\u0026#39;)) %\u0026gt;% unnest(views) %\u0026gt;% mutate(views = as.numeric(views), publishedAt = as_date(publishedAt)) %\u0026gt;% ggplot(aes(x = publishedAt, y = views)) + geom_line() + labs(x = \u0026quot;Data de publicação\u0026quot;, y = \u0026quot;Visualizações\u0026quot;) + theme_bw() O número de visualizações está em uma queda contínua, isso quer dizer que o porta dos fundos está recebendo menos views? Depende de como você interpreta esse dado. A resposta será “sim” se você assumir que o número de visualizações de longo prazo é negligenciável e que o grosso do número de views de um vídeo vem dos seus primeiros dias de vida. Pensando assim, interpretamos que o que a gente observa é aproximadamente igual à quantidade de views no começo da vida de cada um dos vídeos do PDF, de tal forma que se esse número desce, quer dizer que a quantidade de visualizações de um vídeo logo que ele sai também deve estar caindo.\nPara analisar esses dados de outra forma, eu vou abandonar a suposição de que a quantidade de views é negligenciável no longo prazo. Dessa vez, eu vou supor que quanto mais velho for o vídeo, mais visualizações ele tem, afinal as pessoas provavelmente voltam nele de tempos em tempos. Pra simplificar as coisas, também vou admitir que a quantidade de pessoas que fica voltando nele é mais ou menos constante. Juntando tudo isso, o que eu quero dizer é que o número esperado de pessoas que assistem a um vídeo velho em um certo dia muito distante da sua publicação não é negligenciável, mas é pequeno e contante.\nEm termos um pouco mais precisos, podemos entender toda essa conversa através da equação\n\\[\\text{Número de Views de um vídeo} = \\] \\[\\text{Views no começo da vida do vídeo} + \\text{Idade do vídeo} \\times \\text{Taxa} + \\text{Erro aleatório},\\]\nonde \\(Taxa\\) é o número de esperado de views de um vídeo velho em um dia qualquer. Nesses termos, a diferença entre o ponto que quero defender e o ponto que o Willy defendeu no post anterior é que ele assume que a Taxa é pequena demais para importar, enquanto eu não acho que ela seja negligenciável.\nPodemos dar uma olhada no que esse modelo diz sobre os dados considerando que, se um vídeo for muito velho, podemos obter uma estimativa razoável da quantidade de pessoas que ainda assistem um vídeo se dividirmos o número de visualizações pelo número de dias desde a sua publicação.\ndados %\u0026gt;% mutate(views = map(videos_stats, .f = \u0026#39;viewCount\u0026#39;)) %\u0026gt;% unnest(views) %\u0026gt;% mutate(views = as.numeric(views), publishedAt = as_date(publishedAt), idade = as.numeric(Sys.Date() - publishedAt)) %\u0026gt;% filter(publishedAt \u0026lt; as.Date(\u0026quot;2017-01-01\u0026quot;)) %\u0026gt;% ggplot(aes(x = publishedAt, y = (views)/idade)) + geom_line() + labs(x = \u0026quot;Data de publicação\u0026quot;, y = \u0026quot;Visualizações/Idade\u0026quot;) + theme_bw() + geom_smooth(alpha = 0) O gráfico acima suporta parcialmente a nossa teoria: a estabilidade na razão entre o número de visualizações e a idade não seria identificada a menos que todos os vídeos antigos do Porta estivessem sujeitos à mesma audiência recorrente (mais ou menos), mesmo com uma variabilidade grande. Além disso, se os vídeos antigos fossem simplesmente abandonados (caso em que a Taxa é igual a \\(0\\)), então deveríamos observar razões de Visualização por Idade muito menores para vídeos mais velhos.\nEntretanto, nosso modelo tem uma deficiência séria: sempre vamos observar um aumento na razão de views por idade no final da amostra, pois a idade desses vídeos vai ficando cada vez menor, dando mais peso ao número de views na infância do vídeo, o que pode distorcer as nossas interpretações.\nDe toda a forma, a establidade do começo do gráfico me convenceu de que a taxa é constante. Disso decorre que, como a curva está subindo, não ficando estável, devo assumir alguma das duas hipóteses: ou o porta dos fundos está sendo mais assitido de 2016 pra cá ou o meu modelo está se comportando exatamente do jeito que deveria. Como em nenhuma dessas Porta Dos Fundos está perdendo views, sou obrigado a concluir que, no mínimo, tudo está estável.\nLikes e Dislikes O Willy nos contou que a proporção de Likes por Dislike é muitíssimo grande nos vídeos do PDF: eles devem ter uma média de 26 likes por cada dislike, o que significa que o vídeo médio do PDF tem 96% de likes.\ndados %\u0026gt;% mutate(likes = map(videos_stats, .f = ~as.numeric(.x[[\u0026#39;likeCount\u0026#39;]])), dislikes = map(videos_stats, .f = ~as.numeric(.x[[\u0026#39;dislikeCount\u0026#39;]]))) %\u0026gt;% unnest(likes, dislikes) %\u0026gt;% mutate(likes = as.numeric(likes), dislikes = as.numeric(dislikes), publishedAt = lubridate::as_date(publishedAt), prop = likes/dislikes) %\u0026gt;% ggplot(aes(x = publishedAt)) + geom_line(aes(y = prop)) + labs(x = \u0026quot;Data de publicação\u0026quot;, y = \u0026quot;Likes/Dislikes\u0026quot;) + theme_bw() A despeito disso, também é verdade que existe uma classe de vídeos do PDF que é fuzilada pelo público. Pra se ter uma ideia, um vídeo de 2016, o “Delação”, chegou a ter apenas 40% de likes! Isso é o mesmo que dizer que para cada duas pessoas que gostaram do vídeos existem outras que não gostaram.\ng \u0026lt;- dados %\u0026gt;% mutate(likes = map(videos_stats, .f = ~as.numeric(.x[[\u0026#39;likeCount\u0026#39;]])), dislikes = map(videos_stats, .f = ~as.numeric(.x[[\u0026#39;dislikeCount\u0026#39;]]))) %\u0026gt;% unnest(likes, dislikes) %\u0026gt;% mutate(likes = as.numeric(likes), dislikes = as.numeric(dislikes), publishedAt = lubridate::as_date(publishedAt), prop = likes/(likes+dislikes)) %\u0026gt;% ggplot(aes(x = publishedAt, label = title, y = prop)) + geom_line(color = \u0026#39;black\u0026#39;) + labs(x = \u0026quot;Data de publicação\u0026quot;, y = \u0026quot;Proporção de Likes\u0026quot;) + theme_bw() + scale_y_continuous(labels = scales::percent) plotly::ggplotly(g) O comportamento geral desse gráfico dá a entender que nada muito importante aconteceu com a proporção de likes dos vídeos do PDF: quase todo mundo que clicou em alguma das mãozinhas embaixo do vídeo terminou escolhendo um jóinha. Existe um exército de excessões, que estão quase sempre relacionadas à religião, mas a estabilidade do gráfico já é suficiente para os nossos propósitos.\nConclusão As minhas análises não foram 100% conclusivas, mas indicam que o Porta navegava por águas mais ou menos tranquilas antes da aquisição. É verdade que o número de views é bastante difícil de interpretar, mas identificamos um padrão esquisito no começo de 2016. Sob uma certa perspectiva, pode-se dizer que os vídeos começaram a ficar um pouco mais populares do que os seus antecessores. A proporção de likes, por outro lado, é um pouco mais fácil de interpretar: ela ficou estável, ainda que o PDF costume enraivecer o seu público de tempos em tempos.\nConsiderando essas coisas, talvez a aquisição não seja tão terrível quanto parece. O Porta não estava tão ruim e é verdade que eles venderam apenas metade da empresa. Seguindo o provérbio que diz que “Em time que está ganhando não se mexe”, tudo indica que o canal do Youtube vai continuar nos mesmos moldes que vive hoje e é possível que as coisas que eles façam na TV fiquem razoavelmente boas. Pra finalizar, só torço para que dessa vez eles façam algo melhor do que o Show do Kibe, os programas do Danili Gentili e qualquer coisa do Hermes e Renato na TV paga.\nE que não chamem o Supla pra nada.\n","permalink":"https://blog.curso-r.com/posts/2017-04-28-porta-dos-fundos-decadencia-revisited/","tags":["Porta dos Fundos","API","YouTube"],"title":"O Porta dos Fundos está em decadência? (REVISITED)"},{"author":["Julio"],"categories":["divulgação"],"contents":" A versão 3.4 do R (You Stupid Darkness) foi lançada nesse final de semana! A atualização tem foco principal na performance. Principais mudanças:\nAgora temos um compilador JIT (Just In Time) como padrão! Isso significa que você não precisará mais usar a função compiler::cmpfun() para acelerar suas funções na maioria dos casos. Mais sobre isso abaixo. O for ficou mais eficiente. Agora a alocação dinâmica de vetores está mais rápida, diminuindo ainda mais a diferença entre utilizar for e funcionais como sapply(), que trabalham com um vetor pré alocado. Mais sobre isso abaixo. Otimizações em operações com matrizes. Não vou entrar em detalhes, mas talvez vocês notem algumas melhorias. Agora o método padrão para ordenar vetores é radix, o que pode aumentar a velocidade de ordenações para vetores com mais de mil entradas. Para uma lista completa de mudanças, veja esse post.\nFigura 1: R mais rápido! Imagem emprestada do time do Rcpp. Mais sobre o JIT compiler Veja esse exemplo extraído do livro Efficient R. Observe a existência de um \u0026lt;bytecode ...\u0026gt; na parte de baixo de uma das funções. Isso significa que o pacote compiler converteu essa função em um código que pode ser interpretado mais rápido.\nmean_r = function(x) { m = 0 n = length(x) for(i in seq_len(n)) m = m + x[i] / n m } cmp_mean_r \u0026lt;- compiler::cmpfun(mean_r) mean_r ## function(x) { ## m = 0 ## n = length(x) ## for(i in seq_len(n)) ## m = m + x[i] / n ## m ## } cmp_mean_r ## function(x) { ## m = 0 ## n = length(x) ## for(i in seq_len(n)) ## m = m + x[i] / n ## m ## } ## \u0026lt;bytecode: 0x7fb86252dfd8\u0026gt; Para mostrar a mudança, vamos comparar o desempenho das funções usando microbenchmark(). Essa função calcula o tempo de execução de uma expressão cem vezes e calcula estatísticas básicas dos tempos obtidos.\nNo meu servidor, que ainda está com o R 3.3.2, o resultado foi esse. Observe que o tempo da função compilada é quase dez vezes o tempo da função sem compilar.\nset.seed(1) x \u0026lt;- rnorm(5000) microbenchmark::microbenchmark(mean_r(x), cmp_mean_r(x), mean(x)) # Unit: microseconds # expr min lq mean median uq max neval # mean_r(x) 1931.835 2010.295 2302.82298 2102.5995 2357.6715 6186.706 100 # cmp_mean_r(x) 308.847 311.045 333.26221 314.8935 334.7330 569.117 100 # mean(x) 14.593 15.443 19.94897 19.0410 21.0405 51.375 100 No meu computador com o R 3.4, o resultado foi esse. Agora, a diferença entre a função sem compilar e compilada é praticamente impoerceptível. Esse é o efeito do JIT compiler.\nset.seed(1) x \u0026lt;- rnorm(5000) microbenchmark::microbenchmark(mean_r(x), cmp_mean_r(x), mean(x)) # Unit: microseconds # expr min lq mean median uq max neval # mean_r(x) 332.322 332.7220 336.2287 333.0125 334.3785 395.954 100 # cmp_mean_r(x) 332.305 332.7345 337.0889 333.1460 337.0930 381.306 100 # mean(x) 13.807 14.0960 14.7349 14.3060 14.5540 30.313 100 Mais sobre o for Veja esse código que calcula a média de mil valores em 100 entradas de uma lista.\nset.seed(1) input \u0026lt;- lapply(1:100, function(x) runif(1000)) mean_for \u0026lt;- function(x) { vet \u0026lt;- c() for(i in seq_along(x)) { vet[i] \u0026lt;- mean(x[[i]]) } } mean_sapply \u0026lt;- function(x) { sapply(x, mean) } No meu servidor, que ainda está com o R 3.3.2, o resultado foi esse. Veja como o desempenho do for é assustadoramente inferior.\nmicrobenchmark::microbenchmark(mean_for(x), mean_sapply(x)) # Unit: milliseconds # expr min lq mean median uq max neval # mean_for(x) 41.28675 43.22318 47.39574 44.02713 45.59184 84.80818 100 # mean_sapply(x) 14.78590 15.46421 16.36619 16.23018 17.28495 19.70854 100 No meu computador com o R 3.4, o resultado foi esse. Agora o for está praticamente empatado!\nmicrobenchmark::microbenchmark(mean_for(x), mean_sapply(x)) # Unit: milliseconds # expr min lq mean median uq max neval # mean_for(x) 15.16583 15.45924 16.71064 16.14545 17.51002 25.61860 100 # mean_sapply(x) 14.43704 14.90485 16.20864 15.53319 16.56801 27.36536 100 Mas cuidado! o for continua sendo uma ideia ruim no R, não só por desempenho, mas por questões de design. Utilizar funcionais ajuda na performance do computador e torna a vida do cientista de dados mais fácil (veja esse e esse posts que discutem um pouco sobre isso.)\nInstalação Se você usa Windows, uma dica é usar o pacote installr. Basta rodar isso aqui e ser feliz:\ninstall.packages(\u0026quot;installr\u0026quot;) installr::updateR() É isso! Happy coding ;)\nOBS: Se você ficou curiosa sobre o nome da versão, encontrei essa tirinha de 1965 do Peanuts. Acho que foi isso que deu origem ao nome!\nFigura 2: You Stupid Darknes! http://www.gocomics.com/peanuts/1965/09/09. ","permalink":"https://blog.curso-r.com/posts/2017-04-24-r34/","tags":["update"],"title":"R 3.4 disponível!"},{"author":["Daniel"],"categories":["análises"],"contents":" Uma aplicação interessante de algoritmos de clusterização é a obtenção de paletas de cores a partir de imagens. Veja como isso pode ser feito usando o R.\nEm primeiro lugar, vamos ler a imagem como uma matriz para o R. Existem diversas bibliotecas para carregar as imagens, vamos usar aqui a jpeg. Para esse caso ela é melhor porque já lê a imagem no formato que precisamos.\nlibrary(jpeg) library(magrittr) img \u0026lt;- readJPEG(\u0026quot;img/david-bowie.jpg\u0026quot;) A imagem lida pelo pacote jpeg é representada por um array com dimensões: c(altura, largura, n_bandas) que no nosso caso é c(1100, 727, 3). O número de bandas é 3: R, G e B.\nPodemos exibir a imagem no R, convertendo o array, primeiro em um obheto do tipo raster e depois simplesmente usando a função plot.\nplot(as.raster(img)) O problema de obter a paleta de cores de uma imagem pode ser formulado como um problema de clusterização: “obter grupos de individuos que possuem a menor diferença dentro de cada um e a maior diferença possível entre os grupos de acordo com algumas características das unidades amostrais”.\nNesse caso, os indivíduos são os pixels da imagem e as características que estamos interessados são os valores de R, de G e de B (valores que representam a cor do pixel). Para o algortimos de clusterização, precisamos de uma matriz com as 3 colunas R, G e B e largura*altura (numero de pixels) linhas representado os indivíduos. É exatamente essa conversão que o trecho de código a seguir realiza.\nimg_matrix \u0026lt;- apply(img, 3, as.numeric) Agora temos uma matriz com 3 colunas e 799.700 linhas. Vamos aplicar agora o algoritmo k-means, para organizar cada um desses pixels em um grupo. O K-means pede o número de grupos como input, vamos começar com 6.\nkm \u0026lt;- kmeans(img_matrix, centers = 6) O objeto gerado pela função kmeans armazena um vetor chamado cluster (do tamanho do número de linhas da matriz) com um identificador do grupo de cada observação da matriz.\nA cor que representa cada um dos grupos é representada pelo vetor c(r, g, b) com a média de todas as observações de cada um dos grupos. Podemos obter isso com algumas manipulações usando o dplyr.\nlibrary(tibble) library(dplyr) img_df \u0026lt;- tibble( r = img_matrix[,1], g = img_matrix[,2], b = img_matrix[,3], cluster = km$cluster ) centroides \u0026lt;- img_df %\u0026gt;% group_by(cluster) %\u0026gt;% summarise_all(mean) centroides ## # A tibble: 6 × 4 ## cluster r g b ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 0.221 0.273 0.441 ## 2 2 0.501 0.183 0.159 ## 3 3 0.877 0.762 0.645 ## 4 4 0.702 0.467 0.324 ## 5 5 0.354 0.462 0.549 ## 6 6 0.124 0.0433 0.216 Também transformamos uma cor r, g e b em uma representação hexadecimal. Assim conseguimos um vetor de caracteres que representa a a paleta de cores.\ncentroides \u0026lt;- centroides %\u0026gt;% mutate(cor = rgb(r, g, b)) centroides$cor ## [1] \u0026quot;#384670\u0026quot; \u0026quot;#802F28\u0026quot; \u0026quot;#E0C2A5\u0026quot; \u0026quot;#B37753\u0026quot; \u0026quot;#5A768C\u0026quot; \u0026quot;#200B37\u0026quot; Para exibir a paleta vamos usar a seguinte função que foi copiada e levemente modificada daqui\nexibir \u0026lt;- function(x) { n \u0026lt;- length(x) old \u0026lt;- par(mar = c(0.5, 0.5, 0.5, 0.5)) on.exit(par(old)) image(1:n, 1, as.matrix(1:n), col = x, ylab = \u0026quot;\u0026quot;, xaxt = \u0026quot;n\u0026quot;, yaxt = \u0026quot;n\u0026quot;, bty = \u0026quot;n\u0026quot;) } exibir(sort(centroides$cor)) Assim obtivemos uma paleta de cores da imagem que mostramos anteriormente. Vamos colocar todo o código que fizemos passo a passo aqui em uma única função para podermos facilmente criar a paleta de cores para outras imagens.\ncriar_paleta \u0026lt;- function(img, num_cores){ # transforma a imagem em uma matriz img_matrix \u0026lt;- apply(img, 3, as.numeric) # treina o algoritmo de k médias km \u0026lt;- kmeans(img_matrix, centers = num_cores) img_df \u0026lt;- tibble( r = img_matrix[,1], g = img_matrix[,2], b = img_matrix[,3], cluster = km$cluster ) # calcula os centroides dos grupos centroides \u0026lt;- img_df %\u0026gt;% group_by(cluster) %\u0026gt;% summarise_all(mean) # transforma a cor em hexadecimal centroides \u0026lt;- centroides %\u0026gt;% mutate(cor = rgb(r, g, b)) # vetor de cores sort(centroides$cor) } Vejamos agora o que acontece com essa bela imagem do filme Moonrise Kingdom do Wes Anderson, que é famoso por fazer filmes com belas paletas de cores.\nmoonrise \u0026lt;- readJPEG(\u0026quot;img/moonrise-kingdom.jpg\u0026quot;) plot(as.raster(moonrise)) paleta \u0026lt;- criar_paleta(moonrise, 6) exibir(paleta) É isso. Se você gostou, tente fazer com outras imagens e compartilhe com a gente os resultados.\n","permalink":"https://blog.curso-r.com/posts/2017-04-21-paleta-de-cores/","tags":["imagens","cluster","kmeans"],"title":"K-means e paleta de cores"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Tratar erros no R é importante para identificar problemas nos códigos e evitar retrabalho. Quem nunca rodou um algoritmo pesadíssimo que deu errado na última iteração? Nesse artigo, veremos como trabalhar com erros no R e a versão tidy dessas soluções.\nUsando try() e tryCatch() A forma tradicional de tratar erros no R é com a função tryCatch(). Essa função tem como primeiro argumento uma expressão a ser avaliada e argumentos diversos para trabalhar com os erros. A versão mais compacta do tryCatch() é escrita assim:\ntryCatch(sqrt(1), error = function(e) e) ## [1] 1 tryCatch(sqrt(\u0026#39;a\u0026#39;), error = function(e) e) ## \u0026lt;simpleError in sqrt(\u0026quot;a\u0026quot;): non-numeric argument to mathematical function\u0026gt; O try() é uma simplificação de tryCatch() que assume que não estamos interessados no erro, mas sim no resultado da função quando ela dá certo. O código abaixo não trava:\ntry(sqrt(1)) ## [1] 1 try(sqrt(\u0026#39;a\u0026#39;)) ## Error in sqrt(\u0026quot;a\u0026quot;) : non-numeric argument to mathematical function ## Error in sqrt(\u0026quot;a\u0026quot;) : non-numeric argument to mathematical function Existe até mesmo uma versão quieta do try(), usando o parâmetro silent =. Quando a expressão dá um erro, o try() retorna a mensagem de erro de forma invisível, ou seja, sem mostrar explicitamente para o usuário.\nx \u0026lt;- try(sqrt(\u0026#39;a\u0026#39;), silent = TRUE) x ## [1] \u0026quot;Error in sqrt(\\\u0026quot;a\\\u0026quot;) : non-numeric argument to mathematical function\\n\u0026quot; ## attr(,\u0026quot;class\u0026quot;) ## [1] \u0026quot;try-error\u0026quot; ## attr(,\u0026quot;condition\u0026quot;) ## \u0026lt;simpleError in sqrt(\u0026quot;a\u0026quot;): non-numeric argument to mathematical function\u0026gt; Usando advérbios do purrr Hoje em dia, o jeito mais arrumado de tratar erros é usando as funções purrr::possibly() e suas amigas, quietly() e safely(). Note que todas essas palavras são advérbios: o objetivo delas é alterar o comportamento de outros verbos (outras funções). Essa forma de pensar nos nomes das funções (funções são verbos, modificadores de funções são advérbios) faz parte do princípio tidy.\nsafely() retorna uma lista com elementos result e error. Quando a função não dá erro, error fica igual a NULL. Quando a função dá erro, error guarda a mensagem de erro e result guarda o valor do parâmetro otherwise =, que por padrão é NULL. possibly() é uma versão mais otimista do safely(), que exige a definição de otherwise = e não guarda as mensagens de erro. quietly() não trata erros (ou seja, ela trava quando dá erro), mas guarda informações sobre warnings e messages. Figura 1: Admita, você não imaginava que teria de pensar em gramática para programar em R. Vamos ver as três funções colocadas em prática. Como exemplo usaremos a função log, que i) retorna um número quando a entrada é um número positivo, ii) dá um warning quando a entrada é um número menor ou igual a zero, e iii) dá um erro se a entrada não é um número.\nlog(10) ## [1] 2.302585 log(-1) ## Warning in log(-1): NaNs produced ## [1] NaN log(\u0026#39;a\u0026#39;) ## Error in log(\u0026quot;a\u0026quot;): non-numeric argument to mathematical function Vamos fazer as versões modificadas de log:\nlibrary(purrr) safe_log \u0026lt;- safely(log) # outra forma fancy de escrever isso: log %\u0026gt;% safely() possible_log \u0026lt;- possibly(log, otherwise = \u0026#39;putz\u0026#39;) quiet_log \u0026lt;- quietly(log) Vamos mapear os seguintes elementos nessas funções:\nentradas \u0026lt;- list(10, -1, \u0026#39;a\u0026#39;) Agora, os resultados:\n## Esse código vai travar map(entradas, log) ## Warning in .Primitive(\u0026quot;log\u0026quot;)(x, base): NaNs produced ## Error in .Primitive(\u0026quot;log\u0026quot;)(x, base): non-numeric argument to mathematical function safely(): ## Retorna uma lista com erros e resultados NULL map(entradas, safe_log) ## Warning in .Primitive(\u0026quot;log\u0026quot;)(x, base): NaNs produced ## [[1]] ## [[1]]$result ## [1] 2.302585 ## ## [[1]]$error ## NULL ## ## ## [[2]] ## [[2]]$result ## [1] NaN ## ## [[2]]$error ## NULL ## ## ## [[3]] ## [[3]]$result ## NULL ## ## [[3]]$error ## \u0026lt;simpleError in .Primitive(\u0026quot;log\u0026quot;)(x, base): non-numeric argument to mathematical function\u0026gt; possibly(): ## Retorna uma lista com os resultados que deram certo map(entradas, possible_log) ## Warning in .Primitive(\u0026quot;log\u0026quot;)(x, base): NaNs produced ## [[1]] ## [1] 2.302585 ## ## [[2]] ## [1] NaN ## ## [[3]] ## [1] \u0026quot;putz\u0026quot; quietly(): ## Também trava, mesmo problema de log map(entradas, quiet_log) ## Error in .Primitive(\u0026quot;log\u0026quot;)(x, base): non-numeric argument to mathematical function ## Quando funciona, retorna todos os warnings e messages map(entradas[1:2], quiet_log) ## [[1]] ## [[1]]$result ## [1] 2.302585 ## ## [[1]]$output ## [1] \u0026quot;\u0026quot; ## ## [[1]]$warnings ## character(0) ## ## [[1]]$messages ## character(0) ## ## ## [[2]] ## [[2]]$result ## [1] NaN ## ## [[2]]$output ## [1] \u0026quot;\u0026quot; ## ## [[2]]$warnings ## [1] \u0026quot;NaNs produced\u0026quot; ## ## [[2]]$messages ## character(0) Combo com purrr e tibble Um combo que eu gosto bastante de usar é adicionar erros do código retornado por safely() dentro de uma tibble. Uma forma de fazer isso é com o código abaixo:\nlibrary(tibble) # adiciona um resultado default caso dê erro. No caso, NA. safe_log2 \u0026lt;- log %\u0026gt;% safely(otherwise = NA_real_) d_result \u0026lt;- entradas %\u0026gt;% map(safe_log2) %\u0026gt;% # ao invés de uma lista de tamanho 3 com 2 elementos (result e error), # temos uma lista de tamanho 2 (result e error) com 3 elementos. transpose() %\u0026gt;% # simplifica o vetor de resultados simplify_all() %\u0026gt;% # converte para data frame as_tibble() d_result ## # A tibble: 3 × 2 ## result error ## \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt; ## 1 2.30 \u0026lt;NULL\u0026gt; ## 2 NaN \u0026lt;NULL\u0026gt; ## 3 NA \u0026lt;smplErrr\u0026gt; Assim, é possível guardar as informações dos erros de forma concisa, sem perder a informação dos erros.\nE é isso. Happy coding ;)\nPS: Até pouco tempo atrás eu usava a função dplyr::failwith() para fazer o mesmo que possibly(). Porém, descobri que essa função será retirada do dplyr no futuro. Então se você é um usuário de failwith(), está na hora de mudar!\n","permalink":"https://blog.curso-r.com/posts/2017-04-09-try/","tags":["coding","pacotes","purrr"],"title":"Tratando erros: the tidy way"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Uma tarefa muito comum no R é colar textos. As funções mais importantes para isso são paste() e sprintf(), que vêm com o pacote base. Nesse artigo, vamos falar dessas duas funções e de um novo pacote do tidyverse, o glue.\npaste() A função paste() recebe um conjunto indeterminado de objetos como argumento através do ... e vai colando os objetos passados elemento a elemento. Isso significa que se você passar dois vetores de tamanho n, a função paste() retornará um vetor de tamanho n sendo cada posição a colagem dos dois vetores nessa posição. Por padrão, a colagem é feita com um separador de espaço simples (\" \"). Exemplo:\npaste(c(1, 2, 3), c(4, 5, 6)) ## [1] \u0026quot;1 4\u0026quot; \u0026quot;2 5\u0026quot; \u0026quot;3 6\u0026quot; É possível alterar o separador pelo argumento sep =. Um atalho útil para o separador vazio (\"\") é a função paste0:\npaste0(c(1, 2, 3), c(4, 5, 6)) ## [1] \u0026quot;14\u0026quot; \u0026quot;25\u0026quot; \u0026quot;36\u0026quot; Algumas vezes nosso interesse não é juntar vetores elemento a elemento, mas sim passar um vetor e colar todos seus elementos. Isso é feito com o parâmetro collapse =:\npaste(c(1, 2, 3, 4, 5, 6), collapse = \u0026#39;@\u0026#39;) ## [1] \u0026quot;1@2@3@4@5@6\u0026quot; Se você passar mais de um vetor e mandar colapsar os elementos, o paste() vai primeiro colar e depois colapsar:\npaste(c(1, 2, 3), c(4, 5, 6), collapse = \u0026#39;@\u0026#39;) ## [1] \u0026quot;1 4@2 5@3 6\u0026quot; Cuidado Tenha muito cuidado ao passar vetores com comprimentos diferentes no paste()! Assim como muitas funções do R, o paste() faz reciclagem, ou seja, ele copia os elementos do menor vetor até ele ficar com o comprimento do maior vetor1. O problema é que o paste() faz isso silenciosamente e não avisa se você inserir um vetor com comprimento que não é múltiplo dos demais. Veja que resultado bizarro:\npaste(5:9, 1:3, 4:5) ## [1] \u0026quot;5 1 4\u0026quot; \u0026quot;6 2 5\u0026quot; \u0026quot;7 3 4\u0026quot; \u0026quot;8 1 5\u0026quot; \u0026quot;9 2 4\u0026quot; Por essas e outras que dizemos que às vezes o R funciona bem demais…\nsprintf() O sprintf() é similar ao printf do C. Primeiro escrevemos um texto com %s no lugar das coisas que queremos substituir. Depois colocamos esses objetos nos outros argumentos da função, na ordem em que eles aparecem no texto.\nsprintf(\u0026#39;Aba%ste\u0026#39;, \u0026#39;ca\u0026#39;) ## [1] \u0026quot;Abacate\u0026quot; Quando o argumento é um vetor, a função retorna um vetor com as substituições ponto a ponto.\nsprintf(\u0026#39;Aba%ste\u0026#39;, c(\u0026#39;ca\u0026#39;, \u0026#39;ixas\u0026#39;)) ## [1] \u0026quot;Abacate\u0026quot; \u0026quot;Abaixaste\u0026quot; Se o texto contém mais de um %s e os objetos correspondentes são vetores, o sprintf() tenta reciclar os vetores para ficarem do mesmo tamanho. Isso só funciona quando todos os objetos têm comprimentos que são múltiplos do comprimento do maior objeto.\nIsso funciona:\nsprintf(\u0026#39;Aba%s%s\u0026#39;, c(\u0026#39;ca\u0026#39;), c(\u0026#39;xi\u0026#39;, \u0026#39;te\u0026#39;)) # ca foi reciclado ## [1] \u0026quot;Abacaxi\u0026quot; \u0026quot;Abacate\u0026quot; Isso não funciona:\nsprintf(\u0026#39;Aba%s%s\u0026#39;, c(\u0026#39;ca\u0026#39;, \u0026#39;ixaste\u0026#39;), c(\u0026#39;xi\u0026#39;, \u0026#39;te\u0026#39;, \u0026#39;.\u0026#39;)) ## Error in sprintf(\u0026quot;Aba%s%s\u0026quot;, c(\u0026quot;ca\u0026quot;, \u0026quot;ixaste\u0026quot;), c(\u0026quot;xi\u0026quot;, \u0026quot;te\u0026quot;, \u0026quot;.\u0026quot;)): arguments cannot be recycled to the same length Nem sempre queremos substituir pedaços do nosso texto por outros textos. No lugar do %s, é possível colocar padrões para números, por exemplo. Eu uso bastante o %d, que recebe inteiros. Uma funcionalidade legal do %d é a possibilidade de adicionar zeros à esquerda quando um número não atinge certa quantidade de dígitos. Assim, quando ordenamos um vetor de textos que começa com números, a ordenação é a mesma da versão numérica do vetor.\nExemplo:\nnums \u0026lt;- 1:11 sort(as.character(nums)) # ordenado pela string: 10 vem antes de 2 ## [1] \u0026quot;1\u0026quot; \u0026quot;10\u0026quot; \u0026quot;11\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;5\u0026quot; \u0026quot;6\u0026quot; \u0026quot;7\u0026quot; \u0026quot;8\u0026quot; \u0026quot;9\u0026quot; sort(sprintf(\u0026#39;%02d\u0026#39;, nums)) # ordenado pela string: 02 vem antes de 10 ## [1] \u0026quot;01\u0026quot; \u0026quot;02\u0026quot; \u0026quot;03\u0026quot; \u0026quot;04\u0026quot; \u0026quot;05\u0026quot; \u0026quot;06\u0026quot; \u0026quot;07\u0026quot; \u0026quot;08\u0026quot; \u0026quot;09\u0026quot; \u0026quot;10\u0026quot; \u0026quot;11\u0026quot; glue O glue é um pacote recente. Sua primeira aparição no GitHub foi em 23/12/2016! Isso significa que é provável que algumas coisas mudem, mas isso não nos impede de aproveitar o que a ferramenta tem de bom.\nA função glue() é uma generalização do sprintf() que permite chamar objetos do R diretamente ao invés de utilizar o %s. Os objetos podem estar no global environment ou descritos por meio de objetos nomeados nos argumentos do glue(). Basta inserir os objetos entre chaves {}:\nlibrary(glue) planeta \u0026lt;- \u0026#39;mundo\u0026#39; glue(\u0026#39;Olá {planeta} pela {y}a vez\u0026#39;, y = 1) ## Olá mundo pela 1a vez Tembém é possível adicionar expressões dentro das chaves:\np \u0026lt;- 1.123123123 glue(\u0026#39;{p * 100}% das pessoas adoram R.\u0026#39;) ## 112.3123123% das pessoas adoram R. glue(\u0026#39;{scales::percent(p)} das pessoas adoram R.\u0026#39;) ## 112% das pessoas adoram R. A função glue_collapse() é parecida com o paste() quando collapse = '', mas só aceita um objeto como entrada:\nx \u0026lt;- glue_collapse(1:10) x ## 12345678910 all.equal(x, paste(1:10, collapse = \u0026#39;\u0026#39;)) ## [1] \u0026quot;Attributes: \u0026lt; Modes: list, NULL \u0026gt;\u0026quot; ## [2] \u0026quot;Attributes: \u0026lt; Lengths: 1, 0 \u0026gt;\u0026quot; ## [3] \u0026quot;Attributes: \u0026lt; names for target but not for current \u0026gt;\u0026quot; ## [4] \u0026quot;Attributes: \u0026lt; current is not list-like \u0026gt;\u0026quot; ## [5] \u0026quot;target is glue, current is character\u0026quot; Se quiser colar os objetos elemento a elemento e depois colapsar, faça isso explicitamente em duas operações:\nglue(\u0026#39;{letters}/{LETTERS}\u0026#39;) %\u0026gt;% glue_collapse(\u0026#39;, \u0026#39;) ## a/A, b/B, c/C, d/D, e/E, f/F, g/G, h/H, i/I, j/J, k/K, l/L, m/M, n/N, o/O, p/P, q/Q, r/R, s/S, t/T, u/U, v/V, w/W, x/X, y/Y, z/Z O glue também tem uma função extra para trabalhar melhor com o %\u0026gt;%, o glue_data(). O primeiro argumento dessa função é uma lista ou data.frame, e seus nomes são utilizados como variáveis para alimentar as chaves das strings. Use o . para fazer operações com toda a base de dados:\nmtcars %\u0026gt;% head() %\u0026gt;% glue_data(\u0026#39;O carro {row.names(.)} rende {mpg} milhas por galão.\u0026#39;) ## O carro Mazda RX4 rende 21 milhas por galão. ## O carro Mazda RX4 Wag rende 21 milhas por galão. ## O carro Datsun 710 rende 22.8 milhas por galão. ## O carro Hornet 4 Drive rende 21.4 milhas por galão. ## O carro Hornet Sportabout rende 18.7 milhas por galão. ## O carro Valiant rende 18.1 milhas por galão. Resumo Use paste() para colar ou colapsar elementos usando um separador fixado. Use sprintf() quando quiser colocar objetos dentro de um texto complexo. Em todos os casos existe uma solução usando glue. Atualmente sempre que tenho um problema desse tipo uso o glue. Até o momento, não encontrei nenhum problema ou dificuldade. A vida do cientista de dados é mais feliz no tidyverse!\nÉ isso. Happy coding ;)\nExtra: O Guilherme Jardim Duarte fez uma ótima sugestão logo após a publicação deste artigo. No pacote stringi existe um operador %s+% que cola textos iterativamente, com uma sintaxe similar à linguagem python, que permite a colagem de textos usando um simples +. Exemplo:\nlibrary(stringi) \u0026#39;a\u0026#39; %s+% \u0026#39;ba\u0026#39; %s+% \u0026#39;ca\u0026#39; %s+% \u0026#39;xi\u0026#39; ## [1] \u0026quot;abacaxi\u0026quot; Você pode adicionar esse operador como um atalho no RStudio, análogo ao Ctrl+Shift+M que usamos para escrever o %\u0026gt;%. Para isso, veja esse tutorial sobre RStudio Addins.\nMais sobre isso no livro R inferno↩︎\n","permalink":"https://blog.curso-r.com/posts/2017-04-08-glue/","tags":["strings","pacotes","glue"],"title":"Colando textos"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Na jornada da ciência de dados, muitas vezes precisamos rodar um mesmo algoritmo em vários objetos distintos. Quando o algoritmo é pesado ou a lista de objetos é longa, é importante saber em que passo estamos e quanto vai demorar para terminar.\nUma forma de resolver esse problema é usando o pacote progress. O objeto progress_bar desse pacote é do tipo R6 e tem um método new() para criar objetos do tipo “barra”. Uma barra criada também é do tipo R6 e possui o método tick() para imprimir uma barra de progresso no console.\nNo exemplo abaixo, nosso interesse é aplicar a função funcao_demorada nos números 1:5 (um de cada vez, sem usar vetorização) e guardá-los numa lista.\nfuncao_demorada \u0026lt;- function(x) { Sys.sleep(0.5) x ^ 2 } nums \u0026lt;- 1:5 Podemos fazer isso usando o pacote progress:\nbarra \u0026lt;- progress::progress_bar$new(total = length(nums)) # cria a barra resultados \u0026lt;- list() for (x in nums) { barra$tick() # dá um passo resultados[[x]] \u0026lt;- funcao_demorada(x) } TRUE [===========================\u0026gt;-------------------------------------------] 40% TRUE [==========================================\u0026gt;----------------------------] 60% TRUE [========================================================\u0026gt;--------------] 80% TRUE [=======================================================================] 100% Como resultados, temos:\nresultados ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 4 ## ## [[3]] ## [1] 9 ## ## [[4]] ## [1] 16 ## ## [[5]] ## [1] 25 No entanto, sabemos que os laços for e while do R são problemáticos. A melhor e mais estilosa forma de fazer esse tipo de operação no R é usando funcionais.\nFuncionais são funções de funções. Usamos esses caras sempre que queremos aplicar uma função a diversos objetos. Eles são alternativas mais concisas, elegantes e muitas vezes mais eficientes do que os conhecidos for e while.\nExemplos de funcionais são os objetos da família **ply (lapply, apply, sapply etc.) Os funcionais do R básico foram generalizados no pacote plyr, que apresenta uma sintaxe organizada e intuitiva.\nUma vantagem do plyr é a possibilidade de adicionar barras de progresso como um parâmetro dos funcionais.\nresultados \u0026lt;- plyr::llply(nums, funcao_demorada, .progress = \u0026#39;text\u0026#39;) ## | | | 0% | |============== | 20% | |============================ | 40% | |========================================== | 60% | |======================================================== | 80% | |======================================================================| 100% Os resultados são idênticos e foram omitidos. Bem mais simples, não?\nUsando purr::map no lugar de plyr::llply Recentemente, boa parte das funções do plyr foram substituídas por alternativas nos pacotes dplyr (operações envolvendo data.frames) e purrr (operações envolvendo vetores e listas). Esses pacotes apresentam uma sintaxe mais próxima da filosofia tidy e portanto faz sentido estudá-los!\nInfelizmente, as funções do purrr ainda1 não têm um parâmetro para barras de progresso. Enquanto isso, podemos utilizar o progress::progress_bar mesmo.\nbarra \u0026lt;- progress::progress_bar$new(total = length(nums)) resultados \u0026lt;- purrr::map(nums, ~{ barra$tick() funcao_demorada(.x) }) TRUE [===========================\u0026gt;-------------------------------------------] 40% TRUE [==========================================\u0026gt;----------------------------] 60% TRUE [========================================================\u0026gt;--------------] 80% TRUE [=======================================================================] 100% O código fica parecido com solução usando for(), mas pelo menos estamos usando os pacotes mais recentes ;)\nEficiência As soluções que mostrei acima apresentam diferenças importantes de eficiência. Abaixo, encapsulei os códigos em funções e mudei levemente a operação que queremos fazer:\nnums \u0026lt;- 1:100 funcao_rapida \u0026lt;- function(x) { x ^ 2 } for(), com e sem progresso:\nfor_com \u0026lt;- function(nums) { barra \u0026lt;- progress::progress_bar$new(total = length(nums)) resultados \u0026lt;- list() for(x in nums) { barra$tick() resultados[[x]] \u0026lt;- funcao_rapida(x) } resultados } for_sem \u0026lt;- function(nums) { resultados \u0026lt;- list() for(x in nums) resultados[[x]] \u0026lt;- funcao_rapida(x) resultados } plyr::llply(), com e sem progresso:\nplyr_com \u0026lt;- function(nums) { plyr::llply(nums, funcao_rapida, .progress = \u0026#39;text\u0026#39;) } plyr_sem \u0026lt;- function(nums) { plyr::llply(nums, funcao_rapida) } purrr::map(), com e sem progresso:\npurrr_com \u0026lt;- function(nums) { barra \u0026lt;- progress::progress_bar$new(total = length(nums)) purrr::map(nums, ~{ barra$tick() funcao_rapida(.x) }) } purrr_sem \u0026lt;- function(nums) { purrr::map(nums, funcao_rapida) } Para testar a eficiência dos algoritmos, utilizamos a função microbenchmark::microbenchmark(). Essa função calcula o tempo de execução do algoritmo cem vezes e obtém algumas estatísticas básicas dos tempos obtidos.\nbenchmark \u0026lt;- microbenchmark::microbenchmark( for_com(nums), for_sem(nums), plyr_com(nums), plyr_sem(nums), purrr_com(nums), purrr_sem(nums) ) Os resultados da Tabela 1 são surpreendentes. Primeiro, as funções que não usam barras de progresso são muito mais rápidas, chegando a quase 10 vezes em alguns casos. A função do plyr é mais lenta que o for() quando usamos barras de progresso, mas é mais rápida quando não usamos. O purrr é o mais rápido nos dois casos.\nTabela 1: Resultados do benchmark. Os tempos estão em milisegundos. expr min mean median max for_com(nums) 8.666 10.367 9.952 17.155 for_sem(nums) 0.161 0.219 0.196 2.211 plyr_com(nums) 9.193 10.754 10.511 18.386 plyr_sem(nums) 0.116 0.155 0.156 0.271 purrr_com(nums) 8.776 10.376 9.982 14.424 purrr_sem(nums) 0.092 0.131 0.124 0.538 Mas não leve esses resultados tão a sério. Na prática, a parte mais demorada fica na função aplicada e não no funcional utilizado, implicando que essas diferenças sejam ignoráveis. Só recomendo mesmo abandonar o for() para operações desse tipo, pois o tempo de execução não cresce linearmente com o tamanho dos objetos, como é possível ver na Figura 1.\nFigura 1: Tempo do for em função do número de inputs Resumo Use o objeto progress::progress_bar sempre que quiser fazer barras de progresso. Use o método $new(total = n) para criar uma barra. Use o método $tick() dentro do loop para mostrar que andou um passo do algoritmo. Tome cuidado com a eficiência do algoritmo quando usa barras de progresso. Outros links Veja ?dplyr::progress_estimated. Pacote progress. Programação funcional. É provável que o purrr ganhe essa funcionalidade num futuro próximo. Veja essa discussão.↩︎\n","permalink":"https://blog.curso-r.com/posts/2017-04-08-progress/","tags":["coding","pacotes","progress"],"title":"As barras do progresso"},{"author":["William"],"categories":["análises"],"contents":" Se você estiver no centro de São Paulo, quanto será que você precisa andar para achar uma hamburgueria? Será que a sua casa fica a menos de 1 km de hospitais, delagacias ou corpo de bombeiros? Neste post, veremos como utilizar uma das APIs do Google Maps para obter informações de geolocalização a partir de uma pesquisa simples. Em seguida, vamos gerar mapas com o pacote leaflet para visualizar os dados coletados e responder essas perguntas.\nPasso 1: configurar a API do Google Places A API que vamos utilizar para acessar os dados do Google Maps é a Google Places. Para configurá-la, você precisa fazer o seguinte:\ncriar um projeto no Google APIs; adicionar a Google Places API Web Service à sua biblioteca de APIs; obter uma chave de API; e enviar uma requisição. Para mais informações sobre os itens 1, 2 e 3, bastar acessar os links acima. Aqui, vamos focar em como fazer o item 4.\nPasso 2: baixar os dados da API A API do Google Places permite fazer alguns tipos de buscas, como estabelecimentos específicos próximos a um local ou dentro de uma região pré-delimitada. Nosso objetivo aqui é requisitar os dados de todos os estabelecimentos, como hospitais, delegacias, supermercados, escolas etc, dentro de um raio de busca em torno de um ponto específico. Neste contexto, a requisição deve ser feita a partir de um link da forma\nhttps://maps.googleapis.com/maps/api/place/radarsearch/output?parameters\nsubstituindo output pelo formato da saída, xml ou json, e parameters pelos parâmetros de busca. Utilizaremos aqui o formato json, Javascript Object Notation. Para mais informações sobre JSON, consulte este link.\nUtilizaremos as seguintes bibliotecas nesta análise:\nlibrary(tibble) library(magrittr) library(dplyr) library(stringr) library(purrr) library(RCurl) library(jsonlite) library(leaflet) O que precisamos fazer é criar a url de requisição, acessá-la, guardar os dados no formato json em um objeto e convertê-lo para um data frame. A função get_googlemaps_data() abaixo faz exatamente isso. Mais especificamente, ela recebe os parâmetros de busca, uma chave de API e retorna um data frame com os dados de geolocalização (latitude e longitude) dos resultados encontrados.\nO argumento keyword= recebe o termo a ser pesquisado, isto é, se estivermos pesquisando por escolas, esse argumento receberá a string 'escola'. O argumento type= recebe um termo para filtrar os estabelecimentos pesquisados. Por exemplo: keyword = 'restaurante' e type = 'vegetariano'. Os argumentos central_lat= e central_log= representam, respectivamente, a latitude e a longitude do ponto central da busca. Os valores default são os do centro da cidade de São Paulo. O argumento radius= indica o raio máximo de busca. O default é 15 Km. O argumento key= deve receber a sua chave de API. get_googlemaps_data \u0026lt;- function(keyword, type = \u0026quot;\u0026quot;, central_lat = -23.55052, central_log = -46.63331, radius = 15000, key) { basic_url = \u0026quot;https://maps.googleapis.com/maps/api/place/radarsearch/json?\u0026quot; if(type != \u0026quot;\u0026quot;) { type %\u0026lt;\u0026gt;% str_replace_all(\u0026quot; \u0026quot;, \u0026quot;+\u0026quot;) %\u0026gt;% # Os espaços precisam ser str_c(\u0026quot;\u0026amp;type=\u0026quot;, .) # substituídos por \u0026#39;+\u0026#39;. } complete_url \u0026lt;- str_c(basic_url, # Criando a url de requisição \u0026quot;location=\u0026quot;, # com os parâmetros de busca. central_lat, \u0026quot;,\u0026quot;, central_log, \u0026quot;\u0026amp;radius=\u0026quot;, radius, type, \u0026quot;\u0026amp;keyword=\u0026quot;, str_replace_all(keyword, \u0026quot; \u0026quot;, \u0026quot;+\u0026quot;), \u0026quot;\u0026amp;key=\u0026quot;, key) json \u0026lt;- RCurl::getURL(complete_url) # Acessando a URL. list_info \u0026lt;- jsonlite::fromJSON(json) # Transformando json em lista. # Guardando a latitude e longitude em um df, assim como o lugar pesquisado. df \u0026lt;- tibble::tibble(lat = list_info$results$geometry$location$lat, long = list_info$results$geometry$location$lng, place = keyword) return(df) } Com a função get_googlemaps_data() em mão, basta rodar o código get_googlemaps_data(\"mercado\", key = sua_API_key) para obter a geolocalização de até 200 mercados em um raio de até 15 Km do centro de São Paulo. Sim, o limite é de 200 resultados. Não encontrei maneiras de aumentar esse limite.\nTambém podemos utilizar a função purrr:map_df() para gerar um data frame com várias buscas. A função map mapeia uma determinada função em cada elemento de um vetor/lista, retornando um data frame. Ainda não estamos construindo os mapas.\nplaces \u0026lt;- c(\u0026quot;pronto socorro\u0026quot;, \u0026quot;delegacia\u0026quot;, \u0026quot;bombeiros\u0026quot;, \u0026quot;hamburguer\u0026quot;, \u0026quot;pizza\u0026quot;) df_places \u0026lt;- places %\u0026gt;% purrr::map_df(.f = get_googlemaps_data, key = key) Às vezes, a requisição pode retornar com algum erro. Não consegui descobrir o porquê isso acontece. Nestes casos, a função get_googlemaps_data() também retornará um erro, provavelmente na construção do data frame. Se isso acontecer, basta rodar a função novamente, gerando uma nova requisição.\nPasso 3: os mapas Para construir os mapas, vamos utilizar a função leaflet::leaflet(). A ideia é, para cada ponto da pesquisa, adicionar um círculo de raio igual a 1 Km. Dessa forma, se você estiver fora desses círculos, quer dizer que você estará a mais de um quilômetro de um dos estabelecimentos pesquisados. Veja os exemplos a seguir.\nHospitais Eu pesquisei por “pronto socorro” porque a pesquisa hospitais também resulta em hospitais veterinários. Um desafio para quem for reproduzir a análise: coletar apenas a geolocalização de hospitais públicos.\nlibrary(leaflet) df_places %\u0026gt;% dplyr::filter(place == \u0026quot;pronto socorro\u0026quot;) %\u0026gt;% leaflet %\u0026gt;% addTiles() %\u0026gt;% addCircles(lng = ~long, lat = ~lat, weight = 5, radius = 1000, color = \u0026quot;blue\u0026quot;, fillOpacity = 0.5) Delegacias df_places %\u0026gt;% dplyr::filter(place == \u0026quot;delegacia\u0026quot;) %\u0026gt;% leaflet %\u0026gt;% addTiles() %\u0026gt;% addCircles(lng = ~long, lat = ~lat, weight = 5, radius = 1000, color = \u0026quot;blue\u0026quot;, fillOpacity = 0.5) Bombeiros df_places %\u0026gt;% dplyr::filter(place == \u0026quot;delegacia\u0026quot;) %\u0026gt;% leaflet %\u0026gt;% addTiles() %\u0026gt;% addCircles(lng = ~long, lat = ~lat, weight = 5, radius = 1000, color = \u0026quot;blue\u0026quot;, fillOpacity = 0.5) Hamburguerias Repare que o único local na região mais central de SP em que você pode ficar a mais de um quilômetro de uma hamburgueria é no meio do parque Ibirapuera.\ndf_places %\u0026gt;% dplyr::filter(place == \u0026quot;hamburguer\u0026quot;) %\u0026gt;% leaflet %\u0026gt;% addTiles() %\u0026gt;% addCircles(lng = ~long, lat = ~lat, weight = 5, radius = 1000, color = \u0026quot;blue\u0026quot;, fillOpacity = 0.5) Pizzarias Se a busca devolvesse todos os resultados possíveis, esse mapa teria um círculo azul com ~15 km de raio formado por milhares de círculos menores. =D\ndf_places %\u0026gt;% dplyr::filter(place == \u0026quot;pizza\u0026quot;) %\u0026gt;% leaflet %\u0026gt;% addTiles() %\u0026gt;% addCircles(lng = ~long, lat = ~lat, weight = 5, radius = 1000, color = \u0026quot;blue\u0026quot;, fillOpacity = 0.5) Sem dúvidas, a utilização dos dados aqui foi bem superficial, apenas ilustrativa. O ideal seria juntar essas informações de geolocalização com dados de criminalidade, saúde pública, socioeconômicos, consumo etc. Dependendo da disponibilidade de dados, há espaço para muitas análises interessantes utilizando essas informações do Google Maps. Com certeza voltaremos neste assunto em posts futuros. =)\nComentários? Sugestões? Críticas? Você está a menos de um quilômetro dos comentários! Deixe a sua mensagem!\n","permalink":"https://blog.curso-r.com/posts/2017-04-13-o-que-tem-a-um-km/","tags":["GoogleMaps","API","leaflet"],"title":"Você está a menos de 1 km de um Hambúrguer?"},{"author":["Athos"],"categories":["top 10"],"contents":" Em fevereiro desse ano ministramos o Curso de R de Verão 2017, parte do programa dos Cursos de Verão do IME-USP. Abaixo segue um compilado das 10 melhores dicas dadas durante este curso.\n1. Conheça e domine a filosofia por trás do Tidyverse O conceito “tidy” deu o oriente do curso de verão e não foi à toa: o tidyverse, carinhosamente chamado de “universo arrumadinho”, está intimamente associado ao dia-a-dia de um analista de dados e sua implementação em R é tida como uma pequena revolução para os R-eiros.\nA postagem Manifesto Tidy resume bem os motivos pelos quais valem a pena aprender os princípios e os pacotes do tidyverse.\n2. R + Shiny é uma grande alternativa às ferramentas de BI e Dashboards Se você ainda tem alguma dúvida sobre o poder do R em montar dashboards interativos, se dê a chance de visitar alguns exemplos:\nPolling Data VisCARF College Map CRAN Dash Esses exemplos mostram que é possível fazer desde simples gráficos de barras até sofisticados mapas a lá Google Maps. Esse potencial é imenso e tudo isso está a disposição de um mero mortal (e não mais apenas de um desenvolvedor de software especialista em web).\nE quando digo que o potencial é imenso, pode acreditar. O shiny coloca a disposição ao mesmo tempo inúmeros pacotes JavaScript de visualização e o R inteiro, interagindo entre si e aceitando receber informação de usuários em tempo real.\nPS: todos esses sites de exemplo foram feitos com R e apenas R, nenhum outro conhecimento foi pré-requisito.\nSe restou alguma dúvida se o R + Shiny é uma grande alternativa às ferramentas de BI e Dashboards, por favor jogue nos comentários para discutirmos!\n3. ggplot2 e tidyr tem tudo a ver A função gather() do pacote tidyr é frequentemente utilizada para deixar data.frames prontos para serem “plotados”. Veja um exemplo:\nCorrelação entre a variável mpg versus todas as outras do data.frame mtcars:\nlibrary(tidyr) library(ggplot2) library(dplyr) mtcars_para_grafico \u0026lt;- mtcars %\u0026gt;% gather(variavel, valor, -mpg) ggplot(mtcars_para_grafico, aes(x = valor, y = mpg)) + geom_point(aes(colour = variavel), show.legend = FALSE) + geom_smooth(se = FALSE) + facet_wrap(~variavel, scales = \u0026quot;free_x\u0026quot;) Frequentemente o que se quer é construir um gráfico do mesmo tipo para diferentes colunas, por isso o gather() é útil nesses casos. E o facet_wrap() do ggplot2 faz o serviço de construir um gráfico para cada coluna.\n4. Web Scraping é uma habilidade visada no mercado A internet é uma fonta riquíssima de dados e são as técnicas de web scraping que permite ao analista explorar seu potencial por inteiro. Em resumo, Web scraping é o ato de “raspar” dados disponíveis em sites da internet.\nOs dados são o principal ingrediente para um bom modelo estatístico e faz parte da responsabilidade do cientista de dados utilizar o máximo de informação disponível.\nCada vez mais as empresas estão reconhecendo o valor que os dados disponíveis publicamente na internet têm. Há empresas especializadas nisso e não é raro encontrar freelances envolvendo extração e estruturação de dados espalhados na rede.\nOs profissinais capazes de encarar esses desafios ainda são escaços, então fica a dica! Esse webinar dado pelo nosso professor Julio Trecenti é um bom ponto de partida para aprender como fazer web scraping no R.\n5. Avalie muito bem o problema antes de decidir que se trata de um problema de BIG DATA Big Data é um termo que ainda procura por uma definição oficial, mas já existe uma classe de obstáculos que são devidos a grandes volumes de dados. Então a pergunta primordial aqui é: como saber se seu problema é de Big Data?\n“Big data is extremely overhyped and not terribly well defined. Many people think they have big data, when they actually don’t.” - Hadley Wickham\nComo Hadley Wickham aponta, muitas pessoas acham que possuem um problema de big data quando na verdade não possuem. Ele resume o problema em três classes distintas:\nProblema de Big Data que na verdade é um problema comum se for feita uma amostragem/sumarização apropriada. Ferramentas apropriadas: hive, impala, terada, dplyr. 90% dos problemas. Problema de Big Data que na verdade são vários problemas comuns. Exemplo: ajustar um modelo de regressão por indivíduo, para milhares ou milhões de indivíduos. Paralelização é o que será necessário. Ferramentas: foreach, doParallel, doMC, Spark, Hadoop. 9% dos problemas. Problemas de Big Data que realmente dependem de todos os dados juntos no mesmo lugar. Exemplo: sistemas de recomendação em que se faz necessário a procura de relações bem raras e específicas entre transações. Ferramentas: geralmente sistemas dedicados e próprios para o problema. 1% dos problemas. E quanto ao tamanho do dado propriamente dito, para nós usuários de R a regra de bolso é simples:\nSe seus dados cabem na memória, então é “small data”. 6. Saber pedir ajuda é o tópico mais importante Por mais completo que seja um curso de R, nunca alguém chegará no ponto em que não restará dúvidas sobre como fazer alguma coisa no R, até porque há novidades a cada minuto no mundo do R. Por isso aprender a pedir ajuda é essencial. No curso foram passados os melhores jeitos de se obter ajuda:\nHelp / documentação do R (comandos help(funcao) ou ?funcao) Google Stack Overflow Coleginha Usem e abusem do fato de a comunidade R ser gigante e ativa.\n7. RMarkdown e Github vão bem no dia a dia do R-eiro A palavra de ordem aqui é Organização. RMarkdown ajuda a organizar melhor as ideias e conteúdos e o Github ajuda a organizar melhor os códigos.\nReferência para você aprender as ferramentas:\nRMarkdown, de autoria do nosso queridíssimo RStudio Inc., transforma rascunho de códigos em documentos apresentáveis. Como já dito antes, todas as postagens deste blog são feitas em RMarkdown.\nGithub oferece uma inteligência sobre o versionamento dos códigos de um projeto. Isso permite que diversos colaboradores não se percam nas milhares de contribuições. Não existirão mais os arquivos_final_final_v5_com_novas_analises_definitivo3.docx e você ganhará no controle de quem, quando e o que foi alterado em cada uma das versões.\nHoje em dia ele é amplamente utilizado por empresas de tecnologia e está sendo cada vez mais comum em equipes de empresas mais tradicionais.\n8. Aprenda Regex Regular Expressions servem para descrever padrões de textos. Por exemplo, para pedir para o R encontrar “todas as palavras que comecem com a letra A” em regex escrevemos str_detect(palavras, \"^A\"). O pequeno pedaço de símbolos \"^A\" é a maneira de traduzir em regex o padrão “palavras que começam em A”.\nPara quem quiser se aprofundar no assunto, consulte a documentação do regex no R: ?regex.\nOs pacotes stringi e stringr tiram proveito do regex e valem a pena serem explorados! Data mining passará a ser mamão com açúcar.\n9. Dê preferência aos funcionais em vez de for’s Na postagem Top 10 pacotes para Data Science foi destacado o pacote purrr e seu impacto ao fim do “for” nos códigos de R.\n“Usar funcionais” significa usar funções que aceitam funções como argumentos. Ambos os exemplos abaixo geram uma tabela para cada coluna do data.frame iris:\nversão sem funcionais\ntabelas1 \u0026lt;- list() nomes_iris \u0026lt;- names(iris) for(i in 1:length(iris)){ tabelas1[[nomes_iris[i]]] \u0026lt;- table(iris[,i]) } versão com funcionais\ntabelas2 \u0026lt;- purrr::map(iris, table) Do exemplo acima notamos que:\nhá um grande ganho de legibilidade do código usando o funcional map() em vez de um for. não foi preciso nenhum objeto auxiliar como tabelas1 \u0026lt;- list() e nomes_iris no segundo exemplo. não há resquícios de índices como o i para percorrer vetores. Quanto mais aptidão em funcionais um usuário tiver, mais ágil e produtivo ele será. Então aprendam funcionais!\n10. Coloque seus códigos em funções Colocar pedaços de códigos, longos ou curtos, beneficia o desenvolvimento, a leitura e a manutenção do código. No R, fazer função é simples e sem burocracia, então não há muito argumento para não fazer! Algumas dicas para identificar quando você precisa criar uma função:\nExcesso de CTRL+C/CTRL+V da mesma coisa no código, “só mudando umas coisinhas”. Essas “coisinhas” que mudam seriam os argumentos da sua função. Projetos distintos reutilizando partes de código de um projeto mais antigo. Um bloco de código faz uma tarefa muito bem definida e que daria para colocar um nome para ela. Por exemplo, suponha que x \u0026lt;- c(1:10). Seria melhor de entender x %\u0026gt;% eh_par do que x %% 2 == 0. ","permalink":"https://blog.curso-r.com/posts/2017-04-08-top10-dicas-verao-2017/","tags":["Curso-R"],"title":"Top 10 dicas do Curso R de verão 2017"},{"author":["Daniel"],"categories":["análises"],"contents":" Há bastante tempo tenho vontade de fazer análises usando dados de multas de São Paulo. O problema é: estes dados estão disponíveis? Na teoria, sim. Os dados de multas, como quantidade de multas por tipo de infração, dia e horário, e outros são divulgados no portal da Mobilidade Segura da Prefeitura de São Paulo. Na prática, é um pouco diferente. Apesar do site fornecer uma opção de exportação, a tabela exportada não é completa e muitas informações ficam faltando. Dá bastante trabalho para exportar todas as informações.\nPara não ter o trabalho de exportar tabela por tabela, fiz uma requisição para a prefeitura, por meio da Lei de Acesso à Informação, pedindo acesso direto ao banco de dados que fornece as informações para o Portal. A resposta foi a seguinte:\nPrezado Senhor Daniel, Agradecemos o contato e informamos que o seu pedido foi indeferido com fundamento no art. 16, inciso III, do Decreto nº 53.623/2012, pois a base de dados contém informações pessoais que não podem ser disponibilizadas a terceiros. Os demais dados encontram-se inseridos no Painel Mobilidade Segura para consulta pelos interessados com a possibilidade de exportar arquivos. Informamos ainda que na época da sua solicitação, os painéis se encontravam atualizados, visto que a atualização mensal ocorre após os 70 dias da data da infração\nA resposta ao meu ver é totalmente contraditória: ao mesmo tempo que eu não posso receber os dados porque eles são confidenciais, eu posso obtê-los pelo site, que “oferece possibilidade de exportação de arquivos”. (???)\nAntes mesmo de pedir acesso ao banco de dados tinha pensado em desenvolver um webscrapper para fazer o download automático das planilhas, mas a tecnologia com a qual o portal foi desenvolvido (QlikView) dificulta muito o desenvolviento.\nSobrou fazer o download manual mesmo. Até agora fiz o download de todos as planilhas de 2014 a 2016 contendo infrações capturadas tanto eletrônica quanto manualmente de carros (ainda faltam ônibus, utilitários, etc.) por hora do dia. Isso quer dizer que já temos planilhas suficientes para criar uma tabela:\ntipo de veículo (sempre seria carro) data (2014 a 2016) hora do dia eletronica/manual motivo da multa localizacao da multa (quando for um radar) quantidade de multas Disponibilizei esses dados neste repositório do Github.\nAgora com as análises a seguir, espero criar curiosidade e interesse para que mais pessoas possam ajudar no download completo dos dados. Na página inicial do repositório, adicionei, um mini-tutorial de como você pode ajudar fazendo os downloads.\nVamos às análises.\nObter os dados download.file(\u0026quot;https://github.com/dfalbel/spmultas/raw/master/data/carros_eletronicas.rda\u0026quot;, \u0026quot;carros_eletronicas.rda\u0026quot;) load(\u0026quot;carros_eletronicas.rda\u0026quot;) A partir de agora, você possui carregado um data.frame chamado carros_eletronicas que possui as informações das multas para carros de forma eletrônica.\nEm primeiro lugar, vamos analisar a quantidade de multas por dia em São Paulo desde 2014.\nVisualizar a série carros_summary \u0026lt;- carros_eletronicas %\u0026gt;% group_by(data) %\u0026gt;% summarise(qtd = sum(qtd)) carros_summary %\u0026gt;% ggplot(aes(data, qtd)) + geom_line() Vemos nesse gráfico que o número de multas (por radar) era sempre por volta de 10.000 durante 2014, em 2015 foi aumentando bastante durante o ano e em 2016 se estabilizou. Vamos ver agora, por tipo de enquadramento, isto é, por motivo da multa.\nExistem 11 tipos de enquadramentos eletrônicos. Para a visualização ficar mais fácil, vamos primeiro agrupar em grandes temas:\nAvançar o sinal vermelho Executar conversão proibida Parar sobre faixa de pedestres Rodízio Velocidade Transitar em faixa de ônibus ou exclusiva p/ determinado veículo O agrupamento final ficou assim:\ndepara \u0026lt;- carros_eletronicas %\u0026gt;% group_by(enquadramento) %\u0026gt;% summarise(qtd = sum(qtd)) %\u0026gt;% arrange(qtd) %\u0026gt;% select(-qtd) depara$agrup_enquadramento \u0026lt;- c( \u0026quot;Conversão proibida\u0026quot;, \u0026quot;Velocidade\u0026quot;, \u0026quot;Faixa de Pedestres\u0026quot;, \u0026quot;Faixa de ônibus\u0026quot;, \u0026quot;Sinal vermelho\u0026quot;, \u0026quot;Faixa de ônibus\u0026quot;, \u0026quot;Conversão proibida\u0026quot;, \u0026quot;Faixa de ônibus\u0026quot;, \u0026quot;Velocidade\u0026quot;, \u0026quot;Rodízio\u0026quot;, \u0026quot;Velocidade\u0026quot;) depara %\u0026gt;% knitr::kable() carros_summary_enquad \u0026lt;- carros_eletronicas %\u0026gt;% left_join(depara, by = \u0026quot;enquadramento\u0026quot;) %\u0026gt;% group_by(data, agrup_enquadramento) %\u0026gt;% summarise(qtd = sum(qtd)) carros_summary_enquad %\u0026gt;% ungroup() %\u0026gt;% ggplot(aes(data, qtd, color = agrup_enquadramento)) + geom_line() No gráfico, vemos que em 2015, o tipo de multa que mais aumentou em quantidade foi velocidade e rodízio.\nCuriosidades Anteriormente vimos como se comportou o número de multas de maneira geral na cidade. Vamos agora matar algumas curiosidades.\nQuais são os horários com mais multas em SP? carros_summary_hora \u0026lt;- carros_eletronicas %\u0026gt;% group_by(hora) %\u0026gt;% summarise(qtd = sum(qtd)) carros_summary_hora %\u0026gt;% ggplot(aes(x = hora, y = qtd)) + geom_bar(stat = \u0026quot;identity\u0026quot;) Notamos que o maior número de multas ocorre justamente na hora do rush. Isto é, entre 7 e 10 da manhã e 17h e 19h. Isso até faz sentido, mas nessas horas o trânsito da cidade está todo parado. Será que a distribuição fica diferente por tipo de multa? Principalmente as de velocidade.\ncarros_depara_enquad \u0026lt;- carros_eletronicas %\u0026gt;% left_join(depara, by = \u0026quot;enquadramento\u0026quot;) %\u0026gt;% group_by(hora, agrup_enquadramento) %\u0026gt;% summarise(qtd = sum(qtd)) carros_depara_enquad %\u0026gt;% ggplot(aes(x = hora, y = qtd, fill = agrup_enquadramento)) + geom_bar(stat = \u0026quot;identity\u0026quot;) Veja que interessante! O grande responsável pelo pico da hora do rush é o rodízio. (Essa é justamente a hora em que ele está valendo.) As multas de velocidade diminuem um pouco durante o trânsito e acontecem mais durante o dia e não durante a noite como poderíamos imaginar. Vemos também que as multas de farol vermelho acontecem mais durante a madrugada.\nQual é o dia da semana com mais multas? carros_depara_enquad_wday \u0026lt;- carros_eletronicas %\u0026gt;% left_join(depara, by = \u0026quot;enquadramento\u0026quot;) %\u0026gt;% group_by(dia_da_semana = wday(data), agrup_enquadramento) %\u0026gt;% summarise(qtd = sum(qtd)) carros_depara_enquad_wday %\u0026gt;% ggplot(aes(x = dia_da_semana, y = qtd, fill = agrup_enquadramento)) + geom_bar(stat = \u0026quot;identity\u0026quot;) O dia da semana com mais multas é quinta feira. Nos finais de semana, aumenta muito o número de multas por excesso de velocidade (claro, as ruas estão mais vazias).\nQuais são os radares que mais multam em SP? E porque? top10_locais \u0026lt;- carros_eletronicas %\u0026gt;% group_by(local) %\u0026gt;% summarise(n = sum(qtd)) %\u0026gt;% arrange(desc(n)) %\u0026gt;% slice(1:10) knitr::kable(top10_locais) Agora vamos ver os motivos, em cada um desses lugares.\ntop10_motivos \u0026lt;- top10_locais %\u0026gt;% left_join(carros_eletronicas, by = \u0026quot;local\u0026quot;) %\u0026gt;% left_join(depara, by = \u0026quot;enquadramento\u0026quot;) %\u0026gt;% mutate(local = stringr::str_wrap(local, width = 20) %\u0026gt;% forcats::fct_reorder(-n)) %\u0026gt;% group_by(local, agrup_enquadramento) %\u0026gt;% summarise(qtd = sum(qtd)) top10_motivos %\u0026gt;% ggplot(aes(x = local, y = qtd, fill = agrup_enquadramento)) + geom_bar(stat = \u0026quot;identity\u0026quot;) Por incrível que pareça, nos dois radares com mais multas, o motivo da multa é conversão proibida. A foto de onde fica esse radar saiu em uma notícia sobre o mesmo tema na Folha de São Paulo.\n","permalink":"https://blog.curso-r.com/posts/2017-04-01-multas-em-sp/","tags":["sptrans"],"title":"Multas em São Paulo"},{"author":["Julio"],"categories":["conceitos"],"contents":" No mundo do web scraping, muitas vezes precisamos acessar sites HTTPS, a versão Segura do HTTP (Hyper Text Transfer Protocol). Esse protocolo é utilizado para encriptar as mensagens trocadas por usuário e servidor.\nO pacote httr utiliza um padrão SSL (Secure Sockets Layer) para lidar com HTTPS. O SSL nada mais é que uma forma de informar ao servidor que você é você, garantindo que suas mensagens só possam ser interpretadas por esse servidor, e vice-versa. O padrão do httr funciona bem para a maioria dos sites, permitindo o acesso sem sofrimento.\nNo entanto, alguns sites dão o seguinte erro:\nhttr::GET(\u0026quot;https://esaj.tjsp.jus.br\u0026quot;) ## Response [https://esaj.tjsp.jus.br/esaj/portal.do?servico=740000] ## Date: 2022-07-13 18:39 ## Status: 200 ## Content-Type: text/html;charset=ISO-8859-1 ## Size: 71.4 kB ## ## ## ## ## ## ## ## ## \u0026lt;!DOCTYPE html PUBLIC \u0026quot;-//W3C//DTD XHTML 1.0 Transitional//EN\u0026quot; \u0026quot;http://www.w3... ## ## ... A solução para esse problema é bem simples. Basta mandar o httr ignorar o protocolo SSL usando a função httr::config(). Ignorar o SSL costuma ser uma má ideia, pois faz com que as mensagens entre usuário e servidor voltem a ser em texto puro, como se fosse HTTP. Mas no web scraping isso não é exatamente um problema.\nPara solucionar o problema acima, rode:\nhttr::GET(\u0026quot;https://esaj.tjsp.jus.br\u0026quot;, httr::config(ssl_verifypeer = FALSE)) ## Response [https://esaj.tjsp.jus.br/esaj/portal.do?servico=740000] ## Date: 2022-07-13 18:39 ## Status: 200 ## Content-Type: text/html;charset=ISO-8859-1 ## Size: 71.4 kB ## ## ## ## ## ## ## ## ## \u0026lt;!DOCTYPE html PUBLIC \u0026quot;-//W3C//DTD XHTML 1.0 Transitional//EN\u0026quot; \u0026quot;http://www.w3... ## ## ... E… Feliz web scraping!\nOBS: Certa vez um amigo teve problema com SSL mesmo tentando a solução acima numa máquina com Ubuntu Resolvemos o problema reinstalando a biblioteca libcurl4-openssl-dev via apt-get e o pacote curl do R. Assim:\nNo terminal:\nsudo apt-get update sudo apt-get install libcurl4-openssl-dev No R:\ninstall.packages(\u0026#39;curl\u0026#39;) ","permalink":"https://blog.curso-r.com/posts/2017-03-31-ssl/","tags":["Web scraping","SSL"],"title":"Requisições seguras"},{"author":["Athos"],"categories":["análises"],"contents":" No dia em que fui ao Lollapalooza eu descobri o Rspotify, um wraper da API do Spotify e daí me veio a ideia de juntar infos dos dois assuntos.\nA brincadeira aqui vai envolver\nWeb Scraping - para baixar e estruturar as tabelas de programação do Lolapalooza SP 2017 API do Spotify - por meio do pacote Rspotify todos os pacotes do tidyverse Lollapalooza deste ano em São Paulo contou com 47 bandas distribuídas em quatro palcos. A graça é associar a programação do Lolla com as informações de popularidade das bandas fornecidas pelo Spotify. Abaixo eu vou descrever como peguei os dados, listar as três hipóteses que criei e gerar alguns gráficos pra discutí-las.\nBase de dados Pré-requisitos Pacotes\n# instala o Rspotify if(!require(\u0026quot;Rspotify\u0026quot;)) devtools::install_github(\u0026quot;tiagomendesdantas/Rspotify\u0026quot;) library(Rspotify) library(magrittr) library(forcats) library(stringi) library(lubridate) library(httr) library(rvest) library(tidyverse) O Rspotify é um pacote novo e que ainda não está no CRAN, mas já está funcional.\nConta no Spotify\nPara utilizar a API do Spotify é necessário ter um cadastro no site deles, como se pode imaginar.\nApp no Spotify\nPara você receber um código de acesso para usar a API deles é preciso criar um App dentro da sua conta do Spotify, esse é o pré-requisito mais burocrático de todos. Eu aprendi a fazer isso seguindo os passos do README do próprio pacote Rspotify no Github (veja aqui).\nNo fim, você terá um app_id, um client_id e um client_secret em mãos.\nExtraindo programação do Lollapalooza SP 2017 O objetivo aqui é termos uma versão em data.frame das tabelas contidas no site lollapaloozabr.com/lineup-horarios/. Lá tem a agenda completa dos dois dias do evento.\nVamos ao código! Dica: a melhor maneira de aprender o que cada passo do código faz é ir rodando linha a linha e observando o resultado.\n# programacao do site do lollapalooza 2017 ---------------------- lolla2017_programacao \u0026lt;- \u0026quot;https://www.lollapaloozabr.com/lineup-horarios/\u0026quot; %\u0026gt;% GET() %\u0026gt;% read_html() %\u0026gt;% html_table() %\u0026gt;% set_names(c(\u0026quot;sabado\u0026quot;, \u0026quot;domingo\u0026quot;)) %\u0026gt;% at_depth(2, ~ .x %\u0026gt;% stri_replace_all_regex(\u0026quot; {2,}\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;% stri_replace_all_regex(\u0026quot;[\\\\n]{1}\u0026quot;, \u0026quot;,\u0026quot;) %\u0026gt;% stri_replace_all_regex(\u0026quot;[,]{2,}\u0026quot;, \u0026quot;\\\\\\n\u0026quot;) %\u0026gt;% read.csv(text = ., header = FALSE, col.names = c(\u0026quot;artist\u0026quot;, \u0026quot;hora\u0026quot;))) %\u0026gt;% map(~ .x[-1] %\u0026gt;% data_frame(palco = names(.), programacao_palco = .) %\u0026gt;% unnest(programacao_palco)) %\u0026gt;% data_frame(dia = names(.), programacao = .) %\u0026gt;% unnest(programacao) %\u0026gt;% separate(hora, c(\u0026quot;hora_ini\u0026quot;, \u0026quot;hora_fim\u0026quot;), sep = \u0026quot;-\u0026quot;) %\u0026gt;% mutate(artist = artist %\u0026gt;% tolower, hora_ini = paste(if_else(dia %in% \u0026quot;sabado\u0026quot;, \u0026quot;2017-03-25\u0026quot;, \u0026quot;2017-03-26\u0026quot;), hora_ini) %\u0026gt;% ymd_hm(), hora_ini = if_else(hour(hora_ini) \u0026lt; 12, hora_ini + hours(12), hora_ini), hora_fim = paste(if_else(dia %in% \u0026quot;sabado\u0026quot;, \u0026quot;2017-03-25\u0026quot;, \u0026quot;2017-03-26\u0026quot;), hora_fim) %\u0026gt;% ymd_hm(), hora_fim = if_else(hour(hora_fim) \u0026lt; 12, hora_fim + hours(12), hora_fim), dia = fct_relevel(dia, c(\u0026quot;sabado\u0026quot;, \u0026quot;domingo\u0026quot;))) Resultado\ndia palco artist hora_ini hora_fim sabado Palco Skol cage the elephant 2017-03-25 16:25:00 2017-03-25 17:25:00 sabado Palco Skol metallica 2017-03-25 21:00:00 2017-03-25 23:00:00 sabado Palco Skol doctor pheabes 2017-03-25 12:05:00 2017-03-25 13:05:00 sabado Palco Skol rancid 2017-03-25 18:35:00 2017-03-25 19:35:00 sabado Palco Skol suricato 2017-03-25 14:15:00 2017-03-25 15:15:00 sabado Palco Onix the 1975 2017-03-25 17:30:00 2017-03-25 18:30:00 sabado Palco Onix the outs 2017-03-25 13:10:00 2017-03-25 14:10:00 sabado Palco Onix the xx 2017-03-25 19:40:00 2017-03-25 20:55:00 É interessante reparar que para gerar essa simples tabelinha utilizamos os pacotes httr, rvest, purrr, dplyr, tidyr, lubridate, stringi e forcats. Só faltou o ggplot2 para zerar o tidyverse.\nOBS: 89 fm não é uma banda, era só um espaço reservado para fins de publicidade da rádio.\nExtraindo a popularidade das bandas do Lollapalooza no Spotify Agora vamos usar o pacote Rspotify para extrair as popularidades das bandas que estão listadas no data.frame lolla2017_programacao. Para tanto, usei uma playlist oficial no Spotify feita pela própria equipe do Lollapalooza. Essa playlist é identificada pelo id 1mHoPn6JpbtWtoBuvSXrVm lá no banco de dados do Spotify.\nmeu_token \u0026lt;- spotifyOAuth(app_id, client_id, client_secret) # coloque aqui suas infos fornecidas pelo Spotify. lolla2017_playlist \u0026lt;- getPlaylistSongs(\u0026quot;lollabr\u0026quot;, \u0026quot;1mHoPn6JpbtWtoBuvSXrVm\u0026quot;, token = meu_token) %\u0026gt;% mutate(artistInfo = map(artistId, getArtistinfo), artist = artist %\u0026gt;% tolower) %\u0026gt;% rename(track_popularity = popularity, track_id = id) %\u0026gt;% unnest(artistInfo) %\u0026gt;% select(artist, id, name, popularity, followers) Algumas bandas ficaram de fora da playlist e por isso fiz uma pesquisa por nome do artista na própria API do Spotify para recuperar o respectivo id. A função que faz isso é a searchArtist().\n# recuperando infos dos artistas esquecidos pela playlist ---------------------- possibly_searchArtist \u0026lt;- possibly(searchArtist, NA_character_) artistas_fora_da_playlist \u0026lt;- lolla2017_programacao %\u0026gt;% filter(!artist %in% lolla2017_playlist$artist) %$% artist %\u0026gt;% data_frame(artist = .) %\u0026gt;% mutate(search_artist = map(artist, ~ .x %\u0026gt;% possibly_searchArtist), artist_info = map2(search_artist, artist, ~ { if(.x %\u0026gt;% is.na) {data.frame(search_artist = NA)} else { .x %\u0026gt;% mutate(name = name %\u0026gt;% tolower) %\u0026gt;% filter(name %in% .y) %\u0026gt;% head(1) }})) %\u0026gt;% select(-search_artist) %\u0026gt;% unnest(artist_info) %\u0026gt;% select(artist, id, name, popularity, followers) Resultado\nartist id name popularity followers ricci 1EUMh6DZo2CfpolG75YQBL ricci 49 6116 jimmy eat world 3Ayl7mCk0nScecqOzvNp6s jimmy eat world 66 361785 89 fm NA NA NA NA martin garrix 60d24wfXkVzDSfLS6hyCjZ martin garrix 85 2244761 illusionize 3RloA7E4XMItSP4FjMBv3L illusionize 46 30054 Juntando tudo Agora vamos juntar a programação do Lolla com as infos do Spotify. A chave é artist.\nlolla2017 \u0026lt;- left_join(lolla2017_programacao, lolla2017_playlist %\u0026gt;% bind_rows(artistas_fora_da_playlist), by = \u0026quot;artist\u0026quot;) %\u0026gt;% select(-id, -name) %\u0026gt;% dplyr::filter(followers %\u0026gt;% is.na %\u0026gt;% not) Base final\ndia palco artist hora_ini hora_fim popularity followers sabado Palco Skol cage the elephant 2017-03-25 16:25:00 2017-03-25 17:25:00 72 745453 sabado Palco Skol metallica 2017-03-25 21:00:00 2017-03-25 23:00:00 80 3047126 sabado Palco Skol doctor pheabes 2017-03-25 12:05:00 2017-03-25 13:05:00 20 313 sabado Palco Skol rancid 2017-03-25 18:35:00 2017-03-25 19:35:00 60 220182 sabado Palco Skol suricato 2017-03-25 14:15:00 2017-03-25 15:15:00 41 52326 sabado Palco Onix the 1975 2017-03-25 17:30:00 2017-03-25 18:30:00 77 1600845 sabado Palco Onix the outs 2017-03-25 13:10:00 2017-03-25 14:10:00 28 3788 sabado Palco Onix the xx 2017-03-25 19:40:00 2017-03-25 20:55:00 76 2383923 Resultados Hipótese I Hipótese I: a organização usou a estratégia de distribuir a popularidade das bandas uniformemente no dia.\nUm dos vários desafios logísticos que o evento tem é a alocação das bandas na grade horária nos quatro diferentes palcos.\nEu fui no evento no sábado e ouvi falar que a banda Cage The Elephant tinha sido uma das primeiras bandas a se apresentar. Sabia da popularidade da banda (segundo o Spotify, está mais popular do que The Strokes) e na hora estranhei a decisão do evento de colocá-los para tocar tão cedo.\nlolla2017_grafico \u0026lt;- lolla2017 %\u0026gt;% mutate(hora = map2(hora_ini, hora_fim, ~ seq(.x, .y, 30*60) %\u0026gt;% floor_date(\u0026quot;30 minutes\u0026quot;))) %\u0026gt;% unnest(hora) %\u0026gt;% group_by(dia, hora, palco) %\u0026gt;% summarise(artist = first(artist), n = n(), mean_popularity = mean(popularity)) ## `summarise()` has grouped output by \u0026#39;dia\u0026#39;, \u0026#39;hora\u0026#39;. You can override using the ## `.groups` argument. lolla2017_grafico %\u0026gt;% ggplot(aes(x = ymd_hm(format(hora, \u0026quot;2017-03-26 %H%M\u0026quot;)), y = mean_popularity, colour = palco)) + geom_line() + geom_point() + geom_point(data = lolla2017_grafico %\u0026gt;% filter(artist %in% \u0026quot;cage the elephant\u0026quot;), colour = \u0026quot;red\u0026quot;, size = 2) + geom_text(data = lolla2017_grafico %\u0026gt;% filter(artist %in% \u0026quot;cage the elephant\u0026quot;) %\u0026gt;% head(1), aes(label = artist), colour = \u0026quot;red\u0026quot;, hjust = 0, vjust = -1) + facet_wrap(~dia) + labs(x = \u0026quot;Hora do dia\u0026quot;, y = \u0026quot;Popularidade média\u0026quot;) + theme(text = element_text(size = 16)) O gráfico acima vai de acordo com o senso comum de que os populares ficam para o final, não ajudando a confirmar a hipótese de que o Cage The Elefant estava mal posicionado.\nHipótese II Hipótese II: em termos de popularidade das bandas, o dia de domingo estava melhor do que o dia de sábado.\nEm conversas com amigos e conhecidos reparei que a maioria ou iria no domingo ou preferiria ir no domingo caso tivesse oportunidade. Isso me fez levantar a dúvida se realmente havia maior concentração de bandas boas no domingo.\nggplot(lolla2017 %\u0026gt;% mutate(artist = artist %\u0026gt;% fct_reorder(popularity, .desc = TRUE))) + geom_bar(aes(x = artist, y = popularity, fill = dia), stat = \u0026quot;identity\u0026quot;, position = \u0026quot;dodge\u0026quot;) + theme(text = element_text(size = 16), axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.4)) ggplot(lolla2017) + geom_density(aes(fill = dia, x = popularity, colour = dia), fill = NA) + theme(text = element_text(size = 16), axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.4)) Conclusão: nada indica que houve desbalanceamento. Acho que meu círculo de amigos tem algum viés estranho.\nHipótese III Hipótese III: a popularidade das bandas nos diferentes palcos estava equilibrada.\nQuando me questionei da hipótese I também pensei na dificuldade de posicionar as bandas nos diferentes palcos. Já que teriam milhares de pessoas disputando espaço, seria do interesse da organização deixá-los o mais espalhado possível por vários motivos: melhor fluxo das filas, maior conforto, menos risco de acidentes, entre outros, e um bom jeito de fazer isso seria deixando os palcos igualmente atrativos para não haver uma grande aglomeração em um único ponto.\nggplot(lolla2017 %\u0026gt;% mutate(palco = palco %\u0026gt;% as.factor %\u0026gt;% fct_reorder(popularity, mean))) + geom_boxplot(aes(fill = palco, y = popularity, x = 1)) + theme(text = element_text(size = 16), axis.text.x = element_blank()) + labs(x = \u0026quot;\u0026quot;) O palco Skol teve menor variação de popularidade, costumou contar sempre com artistas de média a alta popularidade, mas os palcos AXE e Onix foram visitados por artista de peso. O palco Perry’s foi o mais visitado por artistas de menor expressão.\nConsiderações finais O tema tratado aqui não foi útil, concordo, mas passamos por quase todas as etapas existentes em um processo típico de análise de dados. Fizemos web scraping, usamos APIs, arrumamos os dados, estruturamos as informações, criamos variáveis e geramos gráficos. Só ficou de fora a parte de modelagem. E não à toa todos os pacotes do tidyverse foram úteis nesse trabalho.\nA lição pra casa é encontrar uns dados interessantes na internet e aplicar as etapas que aprendemos aqui!\n","permalink":"https://blog.curso-r.com/posts/2017-03-27-lollapalooza-sp-2017-segundo-spotify/","tags":["spotify","lolapalooza","API"],"title":"Lollapalooza segundo o Spotify"},{"author":["William"],"categories":["tutoriais"],"contents":" O pacote knitr é um mecanismo rápido, elegante e flexível para gerar relatórios dinâmicos no R. Ele trabalha lado a lado com o rmarkdown para transformar arquivos .Rmd em diversos formatos, como html, pdf e até mesmo word. Misturando chunks de código em R com texto puro, LaTeX e html, a tarefa de criar outputs para análises estatísticas no R ficou muito mais simples.\nQuando estamos trabalhando com arquivos Rmarkdown no Rstudio, o diretório de trabalho é a própria pasta onde o Rmd está salvo. Assim, se o nosso relatório tem algum input (banco de dados, arquivos com código em R ou imagens, por exemplo), esses arquivos precisam estar nessa pasta. Isso pode ser uma chateação se você está dentro de um projeto e organizou os arquivos de forma diferente: banco de dados em uma pasta, .R em outra, imagens em outra, outputs em outra…\nA primeira ideia que vem à cabeça é usar a função setwd() dentro de algum chunk para mudar o diretório de trabalho dentro do arquivo Rmd. No entanto, se fizermos isso, o diretório de trabalho será mudado para aquele chunk, mas voltará a ser a pasta do arquivo .Rmd após a sua execução. Veja o warning abaixo.\nsetwd(\u0026#39;../\u0026#39;) # Warning message: you changed the working directory # to C:/novo_diretorio (probably via setwd()). # It will be restored to C:/diretorio_do_Rdm. # See the Note section in ?knitr::knit. Lendo a seção “Note” do ?knitr:knit, verificamos que mudar o diretório de trabalho via setwd() pode levar a terríveis consequências. Basicamente, figuras e arquivos de cache podem ser salvos no lugar errado, e o seu relatório não será gerado corretamente. Ainda lendo a seção “Note”, encontramos a maneira correta de mudar o diretório de trabalho: setar a opção opts_knit$set(root.dir = ...).\nopts_knit$set(root.dir = \u0026#39;../\u0026#39;) Vale ainda ressaltar que a mudança do diretório só vai ser definida para os chunks seguintes, isto é, se você fizer a mudança opts_knit$set(root.dir = '../') e, no mesmo chunk, tentar ler um arquivo no diretório pai (source(input.R), por exemplo), o arquivo não vai ser encontrado.\n","permalink":"https://blog.curso-r.com/posts/2017-03-25-knitr-mudando-diretorio-de-trabalho/","tags":["R Markdown","knitr"],"title":"Knitr: mudando o diretório de trabalho"},{"author":["William"],"categories":["análises"],"contents":" Há alguns anos eu acompanho o canal Porta dos fundos no YouTube, assistindo os vídeos quase sempre no dia de lançamento. Mesmo dividido entre esquetes boas e ruins, me considero um fã da trupe de humoristas (e do Totoro também), principalmente pelo humor sarcástico e pela satirização de diversos tabus da nossa sociedade.\nNos últimos meses, no entanto, meu entusiasmo com o canal vem diminuindo. A necessidade de postar três vídeos por semana para se manter relevante no sistema de recomendações do YouTube, o que mantém o canal rentável, me faz perdoar um ou outro conteúdo sem graça ou rasteiro, mas sinto que o que era exceção começou a virar regra.\nNão sei se eu que fiquei chato ou se outras pessoas compartilham a minha opinião. Então resolvi scrapear informações do canal e montar algumas visualizações para tirar essa dúvida.\nSegue um passo a passo de como fiz isso utilizando o R.\nPasso 1: instalar e configurar o pacote tuber O pacote tuber contém funções que permitem acessar a API do YouTube utilizando o R. Assim, podemos ter acesso a diversas estatísticas como número de likes, número de views, comentários de vídeos, entre outras.\nPara instalar o pacote, rode o código install.packages(\"tuber\") ou devtools::install_github(\"soodoku/tuber\", build_vignettes = TRUE) para baixar a versão de desenvolvimento mais recente.\nPara utilizar o tuber é preciso um id e um secret do Console de Desenvolvimento da Google. Após criar uma conta, basta habilitar todas as APIs do YouTube e a Freebase API.\nFeito isso, rode o código abaixo com o id e secret obtidos pela plataforma para configurar o acesso do tuber à API.\nlibrary(tuber) yt_oauth(app_id = \u0026quot;seu_app_id\u0026quot;, app_secret = \u0026quot;seu_app_secret\u0026quot;) Se tudo foi configurado corretamente, ele abrirá uma aba no seu navegador confirmando a autenticação, e você poderá voltar ao R para começar a scrapear.\nPasso 2: buscar o id dos vídeos do canal Para organizar as informações dos vídeos em um banco de dados e gerar as visualizações, vamos utilizar as seguintes bibliotecas.\nlibrary(dplyr) # Manipulação de dados library(tidyr) # Manipulação de dados library(tibble) # Criação de dataframes library(lubridate) # Manipulação de datas library(purrr) # Funcionais library(ggplot2) # Gráficos Precisamos do id de cada vídeo do Porta dos Fundos para baixar as suas estatísticas. A função tuber::yt_search() pesquisa por vídeos e suas informações. Rodando yt_search(term = \"Porta dos fundos\"), obtemos informações de alguns vídeos do canal, inclusive que o seu channel id é “UCEWHPFNilsT0IfQfutVzsag”. O channel id é essencial para obtermos todos os vídeos do Porta.\nPor default, a função yt_search() retorna no máximo 50 resultados. Contudo, se setarmos os parâmetros type = \"video\" e channal_id = \"id_de_algum_canal\", o número máximo passa a ser 500 resultados.\nPara facilitar o trabalho, eu criei a função get_videos_porta(). Ela recebe uma data de início e de término (em um dataframe com apenas uma linha) e devolve todos os vídeos do canal Porta dos Fundos nesse período.\nget_videos_porta \u0026lt;- function(dates) { yt_search(term = \u0026quot;\u0026quot;, type = \u0026quot;video\u0026quot;, channel_id = \u0026quot;UCEWHPFNilsT0IfQfutVzsag\u0026quot;, published_after = dates$start, published_before = dates$end) } Cada linha do dataframe de datas a seguir representa períodos de um ano, de 2012 a 2017. Isso implica que, em cada busca, vou receber os vídeos do Porta dos Fundos para cada um desses anos. O mutate formata as datas no padrão exigido pela função yt_search(). Veja help(yt_search) para mais informações.\ndates \u0026lt;- tibble(start = seq(ymd(\u0026quot;2012-01-01\u0026quot;), ymd(\u0026quot;2017-01-01\u0026quot;), by = \u0026quot;years\u0026quot;), end = seq(ymd(\u0026quot;2012-12-31\u0026quot;), ymd(\u0026quot;2017-12-31\u0026quot;), by = \u0026quot;years\u0026quot;)) %\u0026gt;% mutate(start = paste(start, \u0026quot;T0:00:00Z\u0026quot;, sep = \u0026quot;\u0026quot;), end = paste(end, \u0026quot;T0:00:00Z\u0026quot;, sep = \u0026quot;\u0026quot;)) Por fim, atribuímos ao objeto videos as informações de todos os videos do canal de 2012 a 2017.\nvideos \u0026lt;- by_row(.d = dates, ..f = get_videos_porta, .to = \u0026quot;videos_info\u0026quot;) Passo 3: pegar as estatísticas de cada vídeo Para facilitar essa etapa, eu criei a função get_videos_stats(), que recebe um dataframe de uma linha contendo uma coluna $video_id e, usando a função tuber::get_stats(), faz o scrape das estatísticas do vídeo.\nget_videos_stats \u0026lt;- function(df_row) { get_stats(video_id = df_row$video_id) } Cada elemento da coluna $video_info contém um dataframe com as informações dos vídeos de um determinado ano. Com a função dplyr::bind_rows(), juntamos esses dataframes em um só. Então selecionamos as colunas de interesse: title, publishedAt e video_id. Por fim, utilizamos os id’s para baixar as estatísticas de cada vídeo usando a função get_videos_stats(). As estatísticas são salvas na coluna $videos_stats do objeto dados.\ndados \u0026lt;- bind_rows(videos$videos_info) %\u0026gt;% select(title, publishedAt, video_id) %\u0026gt;% by_row(..f = get_videos_stats, .to = \u0026quot;videos_stats\u0026quot;) Passo 4: as visualizações A primeira visualização que resolvi fazer foi um gráfico do número de visualizações pela data de publicação. Uma análise descuidada desse gráfico pode indicar uma clara redução dos números de views ao longo do tempo. No entanto, é preciso levar em conta que vídeos mais antigos tendem a ter mais views por simplesmente estarem disponíveis há mais tempo. Apesar disso, dois fatores me fazem acreditar que a magnitude do número de views de um vídeo é alcançada nos primeiros dias após o lançamento. O primeiro se deve ao sistema de recomendações do YouTube. Na página inicial, nem sempre os vídeos recomendados são dos canais que você se inscreveu. Na página de canais inscritos, se você tiver muitas inscrições, é fácil perder um vídeo ou outro de um dos canais que acompanha. O segundo se deve à enorme quantidade de conteúdo disponível hoje em dia, muito² maior do que há quatro, cinco anos. Eu, por exemplo, sou inscrito em mais de vinte canais e não consigo acompanhar nem cinco deles. Para quem não pode ficar o dia todo vendo vídeos, realmente há muita competição por espaço no YouTube.\nE apresento ainda um terceiro fator, contrariando a expectativa de existirem apenas dois. Vivemos na era do hype. O que é velho, o que é notícia da semana passada, já não interessa mais.\ndados %\u0026gt;% mutate(views = map(videos_stats, .f = \u0026#39;viewCount\u0026#39;)) %\u0026gt;% unnest(views) %\u0026gt;% mutate(views = as.numeric(views), publishedAt = as_date(publishedAt)) %\u0026gt;% ggplot(aes(x = publishedAt, y = views)) + geom_line(aes(y = 1000000, colour = \u0026quot;1 Milhão\u0026quot;)) + geom_line(aes(y = 10000000, colour = \u0026#39;10 Milhões\u0026#39;)) + geom_line(aes(y = 20000000, colour = \u0026#39;20 Milhões\u0026#39;)) + geom_line() + labs(x = \u0026quot;Data de publicação\u0026quot;, y = \u0026quot;Visualizações\u0026quot;) + theme_bw() Também fiz um gráfico da proporção likes/dislikes pela data de publicação do vídeo. Parece haver uma leve redução dessa proporção no último ano, mas é arriscado tirar uma conclusão. Refazendo essa análise no fim de 2017, talvez fique mais claro se o público do canal concorda comigo sobre a qualidade do conteúdo nos últimos tempos.\ndados %\u0026gt;% mutate(likes = map(videos_stats, .f = \u0026#39;likeCount\u0026#39;), dislikes = map(videos_stats, .f = \u0026#39;dislikeCount\u0026#39;)) %\u0026gt;% unnest(likes, dislikes) %\u0026gt;% mutate(likes = as.numeric(likes), dislikes = as.numeric(dislikes), publishedAt = as_date(publishedAt), prop = likes/dislikes) %\u0026gt;% ggplot(aes(x = publishedAt)) + geom_line(aes(y = prop)) + labs(x = \u0026quot;Data de publicação\u0026quot;, y = \u0026quot;Likes/Dislikes\u0026quot;) + theme_bw() O Porta dos Fundos é sem dúvida um gigante no YouTube, mas os indícios dessa sucinta análise colaboram com a minha opinião de que o canal já viveu dias (bem) melhores. Apesar de essa decadência poder ser só uma fase ruim, nunca é cedo para se reinventar, ter novas ideias, definir as regras do jogo, assim como eles fizeram no início.\nE se faltar ideias, vídeos com o Totoro são sempre uma boa alternativa.\n","permalink":"https://blog.curso-r.com/posts/2017-03-20-porta-dos-fundos-decadencia/","tags":["Porta dos Fundos","API","YouTube"],"title":"O Porta dos Fundos está em decadência?"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Programadores eficientes não precisam escrever algoritmos que rodam rápido. Recomendo fortemente a leitura do livro Efficient R, que discute eficiência com o R de forma exaustiva. Também gosto muito da primeira parte dessa palestra do Hadley, onde ele defende que o cientista de dados deve usar seu tempo pensando no problema e não na forma que vai escrever seu código.\nCom isso em mente, vamos investigar o tema paralelização. Quando rodamos coisas em paralelo, mandamos os núcleos de processamento da máquina calcularem coisas diferentes ao mesmo tempo. A vantagem disso é que o tempo de execução dos algoritmos é dividido pelo número de núcleos disponíveis, sem exigir grandes mudanças no código utilizado.\nVamos mostrar como paralelizar um código usando a função llply() do pacote plyr. Essa função funciona de forma idêntica ao lapply(), ou seja, recebe uma lista ou vetor como input, aplica uma função em cada elemento, e retorna os resultados numa lista com o mesmo comprimento.\nA função dormir() manda o R esperar seg segundos antes de concluir, retornando seg.\ndormir \u0026lt;- function(seg = 1) { Sys.sleep(seg) return(seg) } É intuitivo afirmar que o tempo de execução de dormir() é compatível com seg.\nsystem.time({ dormir() }) ## user system elapsed ## 0.000 0.000 1.005 Nosso interesse é aplicar dormir() em cada elemento do vetor c(1, 2). Esse algoritmo demora 1 + 2 = 3 segundos.\nsegundos \u0026lt;- c(1, 2) system.time({ plyr::llply(segundos, dormir) }) ## user system elapsed ## 0.054 0.005 3.071 Agora vamos executar o mesmo código usando paralelização. Antes, precisamos\ncriar e registrar as cópias de R que rodam em paralelo; e adicionar o parâmetro .parallel = TRUE no llply() O primeiro passo é resolvido com os pacotes parallel e doParallel. Veja como fica o código:\ncl \u0026lt;- parallel::makePSOCKcluster(2) # cria as cópias do R que rodam em paralelo doParallel::registerDoParallel(cl) # registra as cópias do R para serem usadas no plyr system.time({ plyr::llply(segundos, dormir, .parallel = TRUE) }) ## user system elapsed ## 0.012 0.000 2.067 O tempo total de execução foi de ~2.2 segundos, um pouco mais do que dormir(2). Os dois décimos de segundo adicionais são necessários para preparar o terreno da paralelização. Inclusive, se você rodar o código em paralelo novamente, o tempo adicional cai para quase nada:\nsystem.time({ plyr::llply(segundos, dormir, .parallel = TRUE) }) ## user system elapsed ## 0.005 0.001 2.011 Se quiser parar de rodar coisas em paralelo, basta rodar stopCluster():\nparallel::stopCluster(cl) # para de rodar coisas em paralelo E é isso, caros errantes. Rappy coding :)\n","permalink":"https://blog.curso-r.com/posts/2017-03-14-parallel/","tags":["paralelização"],"title":"Paralelização no R"},{"author":["Daniel"],"categories":["divulgação"],"contents":" No início de Janeiro desse ano ocorreu a primeira rstudio::conf, uma conferência sobre R promovida pela Rstudio. A conferência parece ter sido bem animada! Teve até um passeio para o parque do Harry Potter na Florida, e surgiram memes como a imagem abaixo.\nEm termos de conteúdo, parece que foi sensacional. Para quem não teve oportunidade de ir, os videos da conferência foram divulgados aqui como bem lembrado pelo blog Análise Real.\nEu ainda não tive tempo de passar por todas as palestras e comecei pelo fim. Vi o vídeo da última sessão da conferência que era nada mais nada menos do que uma sessão de perguntas com J.J. Allaire, Hadley Wickham e Joe Cheng. Para quem não conhece, o primeiro é fundador do RStudio, outro é o criador do tidyverse, ggplot2, etc e o último é o criador do Shiny.\nA discussão foi muito interessante, tratando de importantes temas sobre o desenvolvimento do R. Vou resumir as minhas impressões nesse post, mas ressalvo que vale muito a pena ver o vídeo.\nA sessão começa com 2 perguntas do moderador Joseph Rickert. A primeira falando sobre a conferência em que J.J., Joe e Hadley, é claro, estão muito satisfeitos. Gosto bastante da resposta do Hadley para essa pergunta:\nNossa métrica de sucesso é o seu sucesso e é muito bom ver vocês usando as ferramentas que desenvolvemos para resolver problemas importantes. Isso é fantástico.\nA segunda pergunta é filosófica, mas o objetivo é saber dos participantes o que os motiva nos trabalhos open-source. J.J. Allaire argumenta que desenvolver software open-source é muito gratificante porque estes são duráveis, não estão ligados ao sucesso ou fracasso de nenhuma companhia. Ele acha que se você quer encontrar significado no seu trabalho e ver que seu software teve impacto na comunidade não tem nada melhor do que desenvolver open-source.\nA partir daí as perguntas são abertas ao público (11 min do vídeo). A primeira pergunta é bem interessante: Quais são os limites do R? A resposta do J.J. Allaire é muito boa e está razoavelmente bem explicada nessa entrevista que ele deu para o blog do RStudio em Outubro do ano passado. Basicamente, ele diz que se o R for encarado como uma linguagem de programação ele possui limites bem claros, porque o R não foi desenvolvido para ser uma linguagem de programação e sim para ser uma interface de usuário. O R, quando ainda era S, foi desenvolvido para ser uma interface para códigos em Fortran. Como interface de usuário, segundo Allaire, o R pode ser interligado sempre à melhor ferramenta de computação que existir. Portanto, se pensarmos no R como interface de usuário, não há limites. Ainda nesta pergunta, Joe Cheng fez uma comparação interessante entre R, Python e Go.\nUma outra pergunta interessante é sobre como convencer quem usa Excel a começar a usar o R. As respostas são muito legais! O Hadley pensa que a primeira parte é mostrar o como com o R você pode ter facilidade para fazer diversas coisas, apresentar as comodidades que ele trás. O JJ Allaire, ressalta que a curva de aprendizado do R é muito mais lenta, por isso, educar e ajduar a tornar o processo de aprendizagem mais simples é crucial para que a adoção do R aumente quando comparado ao Excel.\nEnfim, a palestra passa por muitos outros assuntos interessantes como Deep Learning no R, o futuro do R quanto a tipagem (relacionando com TypeScript, etc.), ferramentas de teste para Shiny, inovação e empoderamento no R, R Users Group, paralelização, CRAN e Github e planos do RStudio para os próximos anos.\nEnfim, espero que este post tenha dado vontade de assistir o vídeo. Convido-os a comentar as partes que vocês acharam mais interessantes!\n","permalink":"https://blog.curso-r.com/posts/2017-02-18-all-things-r-and-rstudio/","tags":["rstudio"],"title":"rstudio::conf: Sessão Final"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" Hoje estava fazendo uma análise exploratória e precisava estudar os quartis de uma variável contínua. A solução usando o tidyverse é tão elegante que valeu um post no blog.\nUsaremos os pacotes tibble, dplyr, tidyr e purrr:\nlibrary(tibble) library(dplyr) library(tidyr) library(purrr) Nesse exemplo, usamos a famosa base mtcars\ntab \u0026lt;- mtcars %\u0026gt;% group_by(am, vs) %\u0026gt;% nest() %\u0026gt;% mutate(quartis = map(data, ~enframe(quantile(.x$mpg, 1:3 / 4)))) %\u0026gt;% unnest_legacy(quartis) %\u0026gt;% spread(name, value) tab ## # A tibble: 4 × 5 ## # Groups: am, vs [4] ## vs am `25%` `50%` `75%` ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 0 14.0 15.2 16.6 ## 2 0 1 16.8 20.4 21 ## 3 1 0 18.6 21.4 22.2 ## 4 1 1 25.0 30.4 31.4 A função dplyr::group_by() faz um grupo para cada combinação de am e vs. A função tidyr::nest() guarda os dados de cada grupo numa list-column chamada data. A função purrr::map() aplica elegantemente a função quantile() para cada grupo de datas, sendo ajudada pela função tibble::enframe(), que coloca o resultado de quantile() em uma tibble. A função tidyr::unnest_legacy() coloca os resultados de volta em colunas-vetores. Terminamos com tidyr::spread() para espalhar os quartis nas colunas. O resultado pode ser jogado diretamente numa tabela:\nknitr::kable(tab) %\u0026gt;% kableExtra::kable_styling() vs am 25% 50% 75% 0 0 14.050 15.20 16.625 0 1 16.775 20.35 21.000 1 0 18.650 21.40 22.150 1 1 25.050 30.40 31.400 É possível mudar esse código para ter outras medidas-resumo, por exemplo. Para isso, podemos usar a função summary(), por exemplo, ou criar nossa própria função.\nmtcars %\u0026gt;% group_by(am, vs) %\u0026gt;% nest() %\u0026gt;% mutate( s = map(data, ~enframe(summary(.x$mpg))), s = map(s, ~ .x %\u0026gt;% mutate(value = as.numeric(value))) ) %\u0026gt;% select(vs, am, s) %\u0026gt;% unnest_legacy(s) %\u0026gt;% spread(name, value) %\u0026gt;% knitr::kable() %\u0026gt;% kableExtra::kable_styling() vs am 1st Qu. 3rd Qu. Max. Mean Median Min. 0 0 14.050 16.625 19.2 15.05000 15.20 10.4 0 1 16.775 21.000 26.0 19.75000 20.35 15.0 1 0 18.650 22.150 24.4 20.74286 21.40 17.8 1 1 25.050 31.400 33.9 28.37143 30.40 21.4 Como você resolveria essa task? Escreva nos comentários!\n","permalink":"https://blog.curso-r.com/posts/2017-02-20-quartis/","tags":["tidyverse","dplyr","purrr"],"title":"Medidas-resumo no tidyverse"},{"author":["Fernando"],"categories":["Tutoriais"],"contents":" Verificar as suposições dos modelos é muito importante quando fazemos inferência estatística. Em particular, a suposição de homocedasticidade1 dos modelos de regressão linear é especialmente importante, pois modifica o cálculo de erros padrão, intervalos de confiança e valores-p.\nNeste post, vou mostrar três pacotes do R que ajustam modelos da forma\n\\[ Y_i = \\beta_0 + \\sum_{k=1}^p\\beta_kx_{ik} + \\epsilon_i, \\ i = 1,\\ldots,n\\]\n\\[ \\epsilon_{i} \\sim \\textrm{N}(0,\\sigma_i), \\ i = 1,\\ldots,n \\ \\textrm{independentes, com }\\sigma_i^2 = \\alpha x_i^2. \\]\nAlém de mostrar como se faz, também vou ilustrar o desempenho dos pacotes em um exemplo simulado. O modelo que gerará os dados do exemplo terá a seguinte forma funcional\n\\[ Y_i = \\beta x_i + \\epsilon_i, \\ i = 1,...n \\] \\[ \\epsilon_i \\sim N(0, \\sigma_i)\\text{ independentes, com }\\sigma_i = \\alpha\\sqrt{|x_i|},\\]\ne os parâmetros do modelo serão os valores \\(\\beta = 1\\) e \\(\\alpha = 4\\). A heterocedasticidade faz com que os pontos desenhem um cone ao redor da reta de regressão.\nlibrary(ggplot2) N \u0026lt;- 1000 set.seed(11071995) X \u0026lt;- sample((N/100):(N*3), N) Y \u0026lt;- rnorm(N,X,4*sqrt(X)) qplot(X,Y) + theme_bw(15) + geom_point(color = \u0026#39;darkorange\u0026#39;) X2 \u0026lt;- sqrt(X) dataset \u0026lt;- data.frame(Y,X,X2) Usando o pacote gamlss Quando se ajusta um GAMLSS, você pode modelar os parâmetros de locação, escala e curtose ao mesmo tempo em que escolhe a distribuição dos dados dentre uma grande gama de opções. Escolhendo a distribuição normal e modelando apenas os parâmetros de locação e escala, o GAMLSS ajusta modelos lineares normais com heterocedasticidade.\nNo código abaixo, o parâmetro formula = Y ~ X-1 indica que a função de regressão será constituída por um preditor linear em X sem intercepto. Já o parâmetro sigma.formula = ~X2-1 indica que o desvio padrão será modelado por um preditor linear em X2 (ou raiz de X), também sem intercepto.\nlibrary(gamlss) fit_gamlss \u0026lt;- gamlss::gamlss(formula = Y ~ X-1, sigma.formula = ~X2-1, data = dataset, family = NO()) GAMLSS-RS iteration 1: Global Deviance = 19436.47 GAMLSS-RS iteration 2: Global Deviance = 19412.18 GAMLSS-RS iteration 3: Global Deviance = 19412.18 Conforme descrito no sumário abaixo, a estimativa de alfa está muito abaixo do valor simulado.\nsummary(fit_gamlss) ****************************************************************** Family: c(\u0026quot;NO\u0026quot;, \u0026quot;Normal\u0026quot;) Call: gamlss::gamlss(formula = Y ~ X - 1, sigma.formula = ~X2 - 1, family = NO(), data = dataset) Fitting method: RS() ------------------------------------------------------------------ Mu link function: identity Mu Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) X 1.029331 0.006297 163.5 \u0026lt;2e-16 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ------------------------------------------------------------------ Sigma link function: log Sigma Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) X2 0.190873 0.001025 186.3 \u0026lt;2e-16 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ------------------------------------------------------------------ No. of observations in the fit: 1000 Degrees of Freedom for the fit: 2 Residual Deg. of Freedom: 998 at cycle: 3 Global Deviance: 19412.18 AIC: 19416.18 SBC: 19426 ****************************************************************** Usando o pacote dglm Quando se ajusta um Modelo Linear Generalizado Duplo (MLGD em português e DGLM em inglês), você tem uma flexibilidade parecida com a de um GAMLSS. Entretanto, você não pode definir um modelo para a curtose e a classe de distribuições disponível é bem menor.\nO código abaixo, similar ao utilizado para ajustar o GAMLSS, ajusta um DGLM aos dados simulados.\nlibrary(dglm) fit \u0026lt;- dglm(Y~X-1, dformula = ~X2-1,data = dataset, family = gaussian, method = \u0026#39;reml\u0026#39;) Warning: glm.fit: algorithm did not converge Novamente, verifica-se que o alfa estimado está muito distante do verdadeiro alfa.\nsummary(fit) Call: dglm(formula = Y ~ X - 1, dformula = ~X2 - 1, family = gaussian, data = dataset, method = \u0026quot;reml\u0026quot;) Mean Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) X 1.029142 0.0117601 87.51128 0 (Dispersion Parameters for gaussian family estimated as below ) Scaled Null Deviance: 17738.57 on 1000 degrees of freedom Scaled Residual Deviance: 3550.323 on 999 degrees of freedom Dispersion Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) X2 0.3808943 0.001147175 332.0279 0 (Dispersion parameter for Gamma family taken to be 2 ) Scaled Null Deviance: 1571.507 on 1000 degrees of freedom Scaled Residual Deviance: 7918.216 on 999 degrees of freedom Minus Twice the Log-Likelihood: 19412.35 Number of Alternating Iterations: 22 Usando o pacote rstan Stan é uma linguagem de programação voltada para descrever e manipular objetos probabilísticos, como por exemplo variáveis aleatórias, processos estocásticos, distribuições de probabilidades etc. Essa linguagem foi projetada para tornar intuitivo e simples o ajuste de modelos estatísticos. Em particular, a forma de descrever modelos bayesianos é bem cômoda.\nO stan possui várias interfaces para R. A mais básica é o rstan, que será utilizada aqui. A principal função desse pacote é a função rstan, que possui dois parâmetros básicos:\num parâmetro model_code =, que recebe um código que descreve o modelo na linguagem stan. um parâmetro data =, que recebe uma lista contendo os inputs do modelo, tais como dados coletados, parâmetros de distribuições a priori, etc. Embora esse seja o mínimo que a função precisa, também podemos passar outras componentes. O parâmetro verbose = FALSE faz com que a função não imprima nada enquanto roda e o parâmetro control = list(...) passa uma lista de opções de controle para o algoritmo de ajuste.\nO retorno da função stan() é um objeto do tipo stanfit, que pode ser sumarizado da mesma forma que outros modelos em R, utilizando a função summary() e a função plot().\nO código abaixo ilustra a aplicação da função stan() ao nosso exemplo.\nlibrary(rstan) scode \u0026lt;- \u0026quot;data { int\u0026lt;lower=0\u0026gt; N; vector[N] y; vector[N] x; } parameters { real beta; real\u0026lt;lower=0\u0026gt; alpha; } model { beta ~ normal(0,10); alpha ~ gamma(1,1); y ~ normal(beta * x, alpha * sqrt(x)); }\u0026quot; dados \u0026lt;- list(N = nrow(dataset), y = dataset$Y, x = dataset$X) fit_stan \u0026lt;- rstan::stan(model_code = scode, verbose = FALSE, data = dados, control = list(adapt_delta = 0.99)) A figura abaixo descreve os intervalos de credibilidade obtidos para cada parâmetro do modelo. O ponto central de cada intervalo representa as estimativas pontuais dos parâmetros. Como se nota, as estimativas do modelo utilizando stan estão bem próximas dos valores verdadeiros.\nplot(fit_stan) Uma regressão linear é homocedástica quando a variabilidade dos erros não depende das covariáveis do modelo.↩︎\n","permalink":"https://blog.curso-r.com/posts/2017-02-21-regressao-heterocedastica/","tags":["modelagem","regressão","bayes"],"title":"Modelando a variância da normal"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" O leaflet é uma biblioteca javascript para criação de mapas interativos. O pacote leaflet do R é um htmlwidget que permite gerar esses mapas de forma direta no R, para usar em documentos RMarkdown e Shiny.\nUma das ferramentas que mais gosto do leaflet é a função markerClusterOptions(), que permite agrupar pontos no mapa em conjuntos de vários pontos.\nComo exemplo, utilizaremos uma base de dados que contém a localização e informações das varas da Justiça Estadual no Brasil. A tabela abaixo mostra as cinco primeiras linhas dessa base. A coluna lab já foi trabalhada para ser adicionada nos marcadores do mapa como popup.\nmuni_id muni_nm lon lat 1100015 Alta Floresta D’oeste -62.27467 -12.47017 1100023 Ariquemes -62.95718 -9.95190 1100031 Cabixi -60.63986 -13.47489 1100049 Cacoal -61.32475 -11.30123 1100056 Cerejeiras -61.26095 -13.20351 Para utilizar o pacote leaflet, basta instalar o pacote via install.packages(). Acesse o mapa dinâmico neste link. Experimente passear pelo mapa. Procure também algum lugar que tenha várias varas juntas, para ver o que o markerCluster faz nesse caso.\nlibrary(leaflet) abjData::muni %\u0026gt;% leaflet() %\u0026gt;% addTiles() %\u0026gt;% addMarkers( lng = ~lon, lat = ~lat, popup = ~muni_nm, clusterOptions = markerClusterOptions() ) A função leaflet() carrega o motor do leaflet, ainda em branco. A função addTiles() adiciona as camadas de mapas de acordo com o zoom. É possível escolher temas para essas camadas. A função addMarkers() mapeia as varas da base de dados de acordo com as respectivas latitude e longitude. Note que é necessário adicionar um ~ antes das variáveis para mapeá-las da base de dados. A opção popup permite adicionar um balão com informações ao clicar num marcador. A opção clusterOptions faz a mágica que agrupa os pontos. A região azul observada ao colocar o mouse sobre um cluster é a casca convexa dos marcadores agrupados. ","permalink":"https://blog.curso-r.com/posts/2017-02-21-markercluster/","tags":["mapas","leaflet"],"title":"leaflet com markerCluster"},{"author":["Athos"],"categories":["top 10"],"contents":" O R mudou muito nos últimos 5 anos graças a criações de novos pacotes focados nas questões mais práticas do dia a dia de um cientista de dados. Abaixo coloquei meu top 10 de pacotes que revolucionaram o jeito de programar em R e fizeram meu trabalho mais ágil e prazeroso:\n10. purrr (manipulação de vetores e listas) Nosso décimo lugar colocou o for em perigo de extinção. Com ele, aplicar funções em vetores, listas ou combinações dos dois é uma tarefa de poucas linhas e sem a necessidade de índices i, j, k’s confusos. E ainda, com o advento das tibbles, seus data.frames ganharam potencial de guardar muito mais do que meros números e strings e o purrr é seu mais forte aliado na hora de criar as chamadas list-columns.\n9. caret (modelagem estatística) Se você procura modelagem estatística (ferramenta básica do cientista de dados), dê chance ao caret. Esse pacote compilou os mais consagrados algoritmos de modelos preditivos (vulgo machine learning) já feitos no R e ainda implementou ferramentas típicas de um processo de construção de modelos, por exemplo, cross-validation, ajuste de hiperparâmetros, bases treino/teste, pré-processamentos e até paralelização. Além das diferentes técnicas de ajuste de modelos, os seus respectivos diagnósticos e visualizações também foram convenientemente compiladas, tudo num mesmo lugar, fazendo a procura de peças no gigantesco balde do R ser menos custosa.\n8. knitr/rmarkdown (relatórios) A dupla knitr e rmarkdown fizeram do R de patinho feio para o rei dos holofotes. Depois deles, relatórios no R são fáceis, bonitos e flexíveis. Escrever em RMarkdown é como escrever um rascunho, que depois é transformado em um arquivo decente em qualquer formato: pdf, word e html são os mais comuns. A ideologia por trás do RMarkdown é trazer o foco para a análise e deixar as perfumarias para segundo plano o máximo possível.\nOBS: todos os posts desse blog são feitos em RMarkdown!\n7. stringr (manipulação de strings) Stringr te dá tudo para extrair, criar e transformar strings. As funções aceitam regex extremamente versáril e eficiente. Mineração de texto vira brincadeira.\n6. lubridate (manipulação de datas) Se tem uma coisa que dá para dar dor de cabeça infinita são datas. Por isso inventaram o lubridate. Tudo o que você gostaria de fazer com datas e horas esse pacote fornece, desde soma e subtração até comparações lógicas e arredondamentos. É a ferramenta ideal para lidar com tempo.\n5. shiny (criação de aplicativos web) Shiny dá o poder de fazer aplicações na web a um analista de dados sem nenhum conhecimento prévio de html, css e javascript. Acredite se quiser, agora, fazer sites interativos e orientados por dados não é mais exclusividade dos desenvolvedores web. Com uma curva de aprendizagem ligeiramente alongada, pode-se criar de dashboards estáticos a mapas personalizados a lá google maps! É uma ótima maneira de apresentar resultados e serve muito bem como produto final de ferramentas de gestão.\n4. tidyr (transformação de data.frames) As principais funções são gather() e spread(). Elas pivotam/despivotam data.frames, ou derretem/condensam data.frames. Ela merece nosso quarto lugar porque possui uma grande sinergia com os terceiro e segundo lugares. Não raramente precisamos rearranjar conjunto de colunas em um par de colunas chave/valor ou vice-versa. Pivotar/despivotar é particularmente muito chato de fazer em SQL e o tidyr faz isso parecer trivial. Ele também possui outras funções úteis que recomendo dar uma olhada (ex: unite(), separete()).\n3. dplyr (manipulação de data.frames) Nossa medalha de bronze vai para dplyr porque ele trouxe os verbos de manipulação de base de dados para o nível mais simples e intuitivo. São eles:\nfilter() filtra linhas arrange() ordena linhas select() seleciona colunas distinct() retira linhas duplicadas mutate() constrói novas colunas group_by() + summarise() sumariza valores por um ou mais fatores E o pacote vai muito além desses verbos. Vale a pena explorar suas funções se você precisa deixar sua base pronta para analisar.\nOBS: foi feito para funcionar com o “pipe” (%\u0026gt;%).\n2. ggplot2 (visualização) A medalha de prata eu acho que é o pacote mais famoso do R.\nO R é conhecido pela sua rica capacidade gráfica, mas foi o ggplot2 que trouxe a alegria de viver para os usuários minimamente preocupados com a boa aparência de suas visualizações. O ggplot2 permite usar a “gramática de gráficos” (grammar of graphics) para construir gráficos em camadas e customizáveis. Há uma pequena curva de aprendizado, mas o tempo investido se paga no primeiro gráfico gerado. Gráficos que seriam veradeiras obras da engenharia se feitas no R-base não passariam de 5 linhas de ggplot2 e ainda ficariam mais bonitos!\nIndispensável na caixa de ferramentas de qualquer cientista de dados.\n1. magrittr (pipe!) A medalha de ouro nos fornece o tal do pipe (%\u0026gt;%). Em vez de h(g(f(x))), escreva x %\u0026gt;% f %\u0026gt;% g %\u0026gt;% h. Pronto! É tudo o que o pipe faz. Você pode se perguntar por que raios isso merece o nosso primeiro lugar, mas acredite: o pipe é revolucionário. Ele mudou o jeito de se programar em R. Com ele o ganho em legibilidade dos códigos e agilidade na programação é inimaginável. E ainda abriu portas para desenvolvimento de pacotes que sem ele não seriam viáveis, incluindo o dplyr e tidyr.\nE por isso que ele merece estar no primeiro lugar na nossa lista e no logo da nossa página.\nMenções honrosas Outros pacotes também merecem destaque! As medálhas de honra ao mérito vão para:\nforcats (utilidades para fatores) Rcpp (R para C++) FactoMiner (análise multivariada) RODBC (conexão com banco de dados) httr/xml2/rvest (ferramentas para web) flexdashboard (rmarkdown para formato de dashboard) janitor (limpeza de dados para modelagem) plyr (manipulação de vetores, listas e data.frames) roxygen2 (criação de pacotes de R) devtools (ferramentas diversas) htmlwidgets (integração entre R e bibliotecas JavaScript) leaflet (mapas interativos) E você, concorda? Coloque nos comentários os pacotes que moram em seu coração! =)\n","permalink":"https://blog.curso-r.com/posts/2017-02-21-top10-pacotes-para-data-science/","tags":["pacotes"],"title":"Top 10 pacotes para data science"},{"author":["William"],"categories":["conceitos"],"contents":" Os gráficos são técnicas de visualização de dados amplamente utilizadas em todas as áreas da pesquisa. A sua popularidade se deve à maneira como elucidam informações que estavam escondidas nas colunas do banco de dados, sendo que muitos deles podem ser compreendidos até mesmo por leigos no assunto que está sendo discutido.\nMas será que podemos definir formalmente o que é um gráfico estatístico?\nGraças ao estatístico norte-americano Leland Wilkinson, a resposta é sim.\nEm 2005, Leland publicou o livro The Grammar of Graphics, uma fonte de princípios fundamentais para a construção de gráficos estatísticos. No livro, ele defende que um gráfico é o mapeamento dos dados a partir de atributos estéticos (posição, cor, forma, tamanho) de objetos geométricos (pontos, linhas, barras, caixas). Simples assim.\nAlém de responder a pergunta levantada nesse post, os conceitos de Leland tiveram outra grande importância para a visualização de dados. Alguns anos mais tarde, o seu trabalho inspirou Hadley Wickham a criar o pacote ggplot2, que enterrou com muitas pás de terra as funções gráficas do R base.\nEm A Layered Grammar of Graphics, Hadley sugeriu que os principais aspectos de um gráfico (dados, sistema de coordenadas, rótulos e anotações) podiam ser divididos em camadas, construídas uma a uma na elaboração do gráfico. Essa é a essência do ggplot2.\nNo gráfico abaixo, temos informação de 32 carros com respeito a 4 variáveis: milhas por galão, tonelagem, transmissão e número de cilindros. O objeto geométrico escolhido para representar os dados foi o ponto. As posições dos pontos no eixo xy mapeia a associação entre a tonelagem e a quantidade de milhas por galão. A cor dos pontos mapeia o número de cilindros de cada carro, enquanto a forma dos pontos mapeia o tipo de transmissão. Observando o código, fica claro como cada linha/camada representa um aspecto diferente do gráfico.\nOs conceitos criados por Leland e Hadley defendem que essa estrutura pode ser utilizada para construir e entender qualquer tipo de gráfico, dando a eles, dessa maneira, a sua definição formal.\nggplot(mtcars) + geom_point(aes(x = disp, y = mpg, shape = as.factor(am), color = cyl)) + labs(x = \u0026quot;Tonelagem\u0026quot;, y = \u0026quot;Milhas por galão\u0026quot;, shape = \u0026quot;Transmissão\u0026quot;, color = \u0026quot;Cilindros\u0026quot;) + scale_shape_discrete(labels = c(\u0026quot;Automática\u0026quot;,\u0026quot;Manual\u0026quot;)) + theme_bw() + theme(legend.position = \u0026quot;bottom\u0026quot;) Por fim, é preciso frisar que, apesar de a gramática prover uma forte fundação para a construção de gráficos, ela não indica qual gráfico deve ser usado ou como ele deve parecer. Essas escolhas, fundamentadas na pergunta a ser respondida, nem sempre são triviais, e negligenciá-las pode gerar gráficos mal construídos e conclusões equivocadas. Cabe a nós, estatísticos, desenvolver, aprimorar e divulgar as técnicas de visualização adequadas para cada tipo de variável, assim como apontar ou denunciar os usos incorretos e mal-intencionados. Mas, em um mundo cuja veracidade das notícias é cada vez menos importante, é papel de todos ter senso crítico para entender e julgar as informações trazidas por um gráfico.\n","permalink":"https://blog.curso-r.com/posts/2017-02-20-o-que-e-um-grafico-estatistico/","tags":["gráficos","ggplot2"],"title":"O que é um gráfico estatístico?"},{"author":["Julio"],"categories":["Tutoriais"],"contents":" O pacote ggalt é uma extensão ao ggplot2 que permite fazer algumas coisas muito úteis. Uma delas é a possibilidade de fazer faixas de confiança para gráficos do tipo escada. Isso permite adicionar intervalos de confiança para modelos Kaplan-Meier, muito utilizados em Análise de Sobrevivência.\nÉ possível instalar o pacote ggalt usando a função install.packages():\ninstall.packages(\u0026#39;ggalt\u0026#39;) Para exemplificar a utilização do ggalt, vamos utilizar os pacotes abaixo.\nlibrary(dplyr) library(ggplot2) library(ggalt) library(broom) library(survival) Nesse exemplo, utilizaremos a base de dados lung que vem com o pacote survival. Primeiramente, ajustamos um modelo Kaplan-Meier simples, usando a função survfit(). Esse modelo tenta explicar a sobrevivência de pacientes com câncer de pulmão para cada sexo.\n# modelo kaplan-meier simples km \u0026lt;- survfit(Surv(time, status) ~ sex, data = lung) A função tidy() do pacote broom transforma o resultado do modelo numa tabela, já preparada para gerar gráficos.\nd_km \u0026lt;- tidy(km) knitr::kable(head(d_km)) time n.risk n.event n.censor estimate std.error conf.high conf.low strata 11 138 3 0 0.9782609 0.0126898 1.0000000 0.9542301 sex=1 12 135 1 0 0.9710145 0.0147075 0.9994124 0.9434235 sex=1 13 134 2 0 0.9565217 0.0181489 0.9911586 0.9230952 sex=1 15 132 1 0 0.9492754 0.0196777 0.9866017 0.9133612 sex=1 26 131 1 0 0.9420290 0.0211171 0.9818365 0.9038355 sex=1 30 130 1 0 0.9347826 0.0224847 0.9768989 0.8944820 sex=1 O gráfico da Figura 1 usa a função geom_ribbon() para gerar os intervalos de confiança. Essa função liga os pontos da faixa diretamente, o que é inadequado do ponto de vista estatístico.\nggplot(d_km, aes(x = time, y = estimate, colour = strata)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = strata), alpha = .1, size = .1) + labs(x = \u0026quot;Tempo\u0026quot;, y = \u0026quot;Estimativa\u0026quot;) + geom_step() + theme_bw(14) Figura 1: Intervalos de confiança usando apenas ggplot2 e geom_ribbon(). Na Figura 2, adicionamos o parâmetro stat = \"stepribbon\", que faz com que o ggplot utilize a função stat_stepribbon() do ggalt para o cálculo da faixa. Isso faz com que os intervalos tenham comportamento de escada, como queríamos.\nggplot(d_km, aes(x = time, y = estimate, colour = strata)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = strata), stat = \u0026#39;stepribbon\u0026#39;, alpha = .1, size = .1) + geom_step() + theme_bw(16) Figura 2: Intervalos de confiança usando stat = 'stepribbon'. É isso! Dúvidas, sugestões e críticas, mande aqui nos comentários.\n","permalink":"https://blog.curso-r.com/posts/2017-02-18-ggalt/","tags":["gráficos","pacotes","survival"],"title":"Pacote ggalt"},{"author":["Athos","Daniel","Fernando","Julio","William"],"categories":["divulgação"],"contents":" Esse é o blog do Curso-R. Nós somos o Athos, Daniel, Fernando, Julio e William e seremos os autores dos posts desse blog. Aqui, vamos divulgar pequenas análises, novidades sobre o R e pequenas descobertas sobre estatística e Data Science. Sinta-se a vontade para comentar no espaço abaixo. Adoramos comentários! Também fique a vontade para compartilhar os nossos posts.\nO Curso-R surgiu em 2015 no programa de verão do Instituto de Matemática e Estatística da Universidade de São Paulo (IME-USP). o curso foi chamado de “Programação em R: do casual ao avançado”. Desde o começo, nós abraçamos a filosofia Open Source - todo material que desenvolvemos fica disponível em nossa conta do Github de forma aberta para quem quiser usar. Em 2016 e 2017, ministramos novamente o curso no programa de verão e, depois de mais de 100 alunos formados, decidimos aumentar a gama de cursos e a frequência de ocorrência para que possamos atingir muito mais pessoas com essas tecnologias, pelas quais nós somos apaixonados.\nEsperamos que aprendam bastante e, principalmente, divirtam-se bastante lendo este blog!\n","permalink":"https://blog.curso-r.com/posts/2017-02-15-blog-curso-r/","tags":["curso-r"],"title":"O blog da Curso-R"},{"author":["Daniel"],"categories":["conceitos"],"contents":" O manifesto das ferramentas tidy do Hadley Wickham é um dos documentos mais importantes sobre R dos últimos tempos. Esse documento formaliza uma série de princípios que norteiam o desenvolvimento do tidyverse.\nO tidyverse, também chamado por muitos de hadleyverse, é um conjunto de pacotes que, por compartilharem esses princípios do manifesto tidy, podem ser utilizados naturalmente em conjunto. Pode-se dizer que existe o R antes do tidyverse e o R depois do tidyverse. A linguagem mudou muito, a comunidade abraçou fortemente o uso desses princípios e tem muita gente criando pacotes para conversar uns com os outros dessa forma. No entanto, usar a filosofia tidy não é a única forma de fazer pacotes do R, existem muitos pacotes excelentes que não utilizam essa filosofia. Como o próprio texto diz “O contrário de tidyverse não é o messyverse, e sim muitos outros universos de pacotes interconectados.”.\nOs princípios fundamentais do tidyverse são:\nReutilizar estruturas de dados existentes. Organizar funções simples usando o pipe. Aderir à programação funcional. Projetado para ser usado por seres humanos. No texto do manifesto tidy cada um dos lemas é descrito de forma detalhada. Aqui, selecionei os aspectos que achei mais importante de cada um deles.\nReutilizar estruturas de dados existentes Quando possível, é melhor utilizar estruturas de dados comuns do que criar uma estrutura específica para o seu pacote. Geralmente, é melhor reutilizar uma estrutura existente mesmo que ela não se encaixe perfeitamente. Organizar funções simples usando o pipe Faça com que suas funções sejam o mais simples possíveis. Uma função deve poder ser descrita com apenas uma sentença. A sua função deve fazer uma transformação no estilo copy-on-modify ou ter um efeito colateral. Nunca os dois. O nome das funções devem ser verbos. Exceto quando as funções do pacote usam sempre o mesmo verbo. Ex: adicionar ou modificar. Aderir à programação funcional O R é uma linguagem de programação funcional, não lute contra isso. Projetado para ser usado por seres humanos Desenvolva o seu pacote para ser usado por humanos. Foque em ter uma API clara para que você escreva o código de maneira intuitiva e rápida. Eficiência dos algoritmos é uma preocupação secundária, pois gastamos mais tempo escrevendo o código do que executando. Esses princípios são bem gerais, mas ajudam bastante a tomar decisões quando estamos escrevendo o nosso código. Para finalizar, clique aqui e veja uma busca no Github por “tidy” em repositórios de R. São mais de 2000 resultados quase todos seguindo essa filosofia e estendendo o universo arrumado.\n","permalink":"https://blog.curso-r.com/posts/2017-02-16-manifesto-tidy/","tags":["tidyverse"],"title":"Manifesto tidy"}]